#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any


ROOT = Path(__file__).resolve().parents[1]


@dataclass(frozen=True)
class PluginAccess:
    plugin_id: str
    plugin_type: str
    uses_dataset_loader: bool
    uses_dataset_loader_unbounded: bool
    uses_dataset_iter_batches: bool
    uses_sql_direct: bool
    uses_sql_assist: bool


def _read_text(path: Path) -> str:
    return path.read_text(encoding="utf-8")


def _plugin_type(manifest_path: Path) -> str:
    # Avoid adding YAML dependency; parse minimal `type:` line.
    for line in _read_text(manifest_path).splitlines():
        if line.strip().startswith("type:"):
            return line.split(":", 1)[1].strip()
    return ""


def _scan_plugin_py(text: str) -> tuple[bool, bool, bool, bool, bool]:
    uses_loader = "ctx.dataset_loader" in text
    uses_batches = "ctx.dataset_iter_batches" in text
    uses_sql_assist = ("ctx.sql" in text) or ("ctx.sql_exec" in text)

    # Indirect dataset access via the stat_plugins registry (common pattern for analysis plugins).
    if (
        "statistic_harness.core.stat_plugins.registry" in text
        and "run_plugin" in text
        and "run_plugin(" in text
    ):
        uses_loader = True
        # Registry currently calls ctx.dataset_loader() unbounded unless settings pass row_limit.
        unbounded = True
    else:
        unbounded = False

    # Heuristic "unbounded": `ctx.dataset_loader()` with no args.
    if uses_loader:
        for needle in ("ctx.dataset_loader()", "ctx.dataset_loader( )"):
            if needle in text:
                unbounded = True
                break
        if not unbounded:
            # crude: find dataset_loader( and see if first non-space after '(' is ')'
            idx = 0
            while True:
                idx = text.find("ctx.dataset_loader(", idx)
                if idx == -1:
                    break
                j = idx + len("ctx.dataset_loader(")
                while j < len(text) and text[j] in {" ", "\t", "\n", "\r"}:
                    j += 1
                if j < len(text) and text[j] == ")":
                    unbounded = True
                    break
                idx = j

    # Direct SQL: look for storage.connection() and SELECT/INSERT patterns.
    uses_sql = "storage.connection" in text or "ctx.storage.connection" in text
    uses_sql = uses_sql and ("SELECT " in text or "INSERT " in text or "DELETE " in text or "UPDATE " in text)

    return uses_loader, unbounded, uses_batches, uses_sql, uses_sql_assist


def generate(plugins_root: Path) -> list[PluginAccess]:
    out: list[PluginAccess] = []
    for pdir in sorted([p for p in plugins_root.iterdir() if p.is_dir()], key=lambda p: p.name):
        manifest = pdir / "plugin.yaml"
        entry = pdir / "plugin.py"
        if not manifest.exists() or not entry.exists():
            continue
        ptype = _plugin_type(manifest)
        text = _read_text(entry)
        uses_loader, unbounded, uses_batches, uses_sql, uses_sql_assist = _scan_plugin_py(text)
        out.append(
            PluginAccess(
                plugin_id=pdir.name,
                plugin_type=ptype,
                uses_dataset_loader=uses_loader,
                uses_dataset_loader_unbounded=unbounded,
                uses_dataset_iter_batches=uses_batches,
                uses_sql_direct=uses_sql,
                uses_sql_assist=uses_sql_assist,
            )
        )
    return out


def _as_json(items: list[PluginAccess]) -> dict[str, Any]:
    return {
        "plugin_count": len(items),
        "plugins": [
            {
                "plugin_id": i.plugin_id,
                "plugin_type": i.plugin_type,
                "uses_dataset_loader": i.uses_dataset_loader,
                "uses_dataset_loader_unbounded": i.uses_dataset_loader_unbounded,
                "uses_dataset_iter_batches": i.uses_dataset_iter_batches,
                "uses_sql_direct": i.uses_sql_direct,
                "uses_sql_assist": i.uses_sql_assist,
            }
            for i in items
        ],
    }


def _as_md(items: list[PluginAccess]) -> str:
    lines: list[str] = []
    lines.append("# Plugin Data Access Matrix")
    lines.append("")
    lines.append("Generated by `scripts/plugin_data_access_matrix.py`.")
    lines.append("")
    lines.append("| Plugin | Type | dataset_loader | loader_unbounded | iter_batches | direct_sql | sql_assist |")
    lines.append("|---|---|---:|---:|---:|---:|---:|")
    for i in items:
        lines.append(
            "| "
            + " | ".join(
                [
                    f"`{i.plugin_id}`",
                    i.plugin_type or "",
                    str(int(i.uses_dataset_loader)),
                    str(int(i.uses_dataset_loader_unbounded)),
                    str(int(i.uses_dataset_iter_batches)),
                    str(int(i.uses_sql_direct)),
                    str(int(i.uses_sql_assist)),
                ]
            )
            + " |"
        )
    lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--plugins-root", default="plugins")
    ap.add_argument("--out-json", default="docs/plugin_data_access_matrix.json")
    ap.add_argument("--out-md", default="docs/plugin_data_access_matrix.md")
    ap.add_argument("--verify", action="store_true")
    args = ap.parse_args()

    items = generate((ROOT / args.plugins_root).resolve())
    payload = _as_json(items)
    out_json = (ROOT / args.out_json).resolve()
    out_md = (ROOT / args.out_md).resolve()
    json_text = json.dumps(payload, indent=2, sort_keys=True) + "\n"
    md_text = _as_md(items)

    if args.verify:
        if not items:
            return 2
        if not out_json.exists() or out_json.read_text(encoding="utf-8") != json_text:
            return 2
        if not out_md.exists() or out_md.read_text(encoding="utf-8") != md_text:
            return 2
        return 0

    out_json.parent.mkdir(parents=True, exist_ok=True)
    out_json.write_text(json_text, encoding="utf-8")
    out_md.write_text(md_text, encoding="utf-8")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

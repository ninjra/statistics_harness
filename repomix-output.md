This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where line numbers have been added, content has been formatted for parsing in markdown style.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Line numbers have been added to the beginning of each line
- Content has been formatted for parsing in markdown style
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
codex/
  PROJECT_SPEC.md
config/
  app.yaml
docs/
  1-32-2026.txt
  evaluation.md
  plugin_dev_guide.md
  plugin_manifest.schema.json
  references.md
  report.schema.json
plugins/
  analysis_attribution/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  analysis_bocpd_gaussian/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_capacity_scaling/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  analysis_chain_makespan/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  analysis_close_cycle_capacity_impact/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_close_cycle_capacity_model/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_close_cycle_contention/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_close_cycle_duration_shift/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_close_cycle_revenue_compression/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_close_cycle_uplift/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_concurrency_reconstruction/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  analysis_conformal_feature_prediction/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_dependency_resolution_join/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  analysis_determinism_discipline/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  analysis_dp_gmm/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_gaussian_copula_shift/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_gaussian_knockoffs/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_graph_topology_curves/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_knockoff_wrapper_rf/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_notears_linear/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_online_conformal_changepoint/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_percentile_analysis/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  analysis_process_sequence/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_queue_delay_decomposition/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_scan_statistics/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_sequence_classification/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  analysis_tail_isolation/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  ingest_tabular/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  llm_prompt_builder/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  planner_basic/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  profile_basic/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
    README.md
  profile_eventlog/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  report_bundle/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  transform_normalize_mixed/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  transform_template/
    __init__.py
    config.schema.json
    output.schema.json
    plugin.py
    plugin.yaml
  __init__.py
scripts/
  fixtures/
    sample.csv
  api_smoke.py
  bootstrap.ps1
  bootstrap.sh
  install_dev.ps1
  install_dev.sh
  push_repo.ps1
  run_analysis.ps1
  run_cli_example.ps1
  run_cli_example.sh
  run_gauntlet.ps1
  run_tests.ps1
  run_tests.sh
  run_ui.ps1
  run_ui.sh
  ui_smoke.py
src/
  statistic_harness/
    core/
      __init__.py
      auth.py
      column_inference.py
      dataset_io.py
      evaluation.py
      known_issue_compiler.py
      migrations.py
      pipeline.py
      planner.py
      plugin_manager.py
      plugin_runner.py
      report.py
      stat_controls.py
      storage.py
      template.py
      tenancy.py
      types.py
      utils.py
      vector_store.py
    ui/
      static/
        app.css
      templates/
        admin.html
        auto_evaluate.html
        bootstrap.html
        evaluate.html
        index.html
        known_issues.html
        login.html
        plugins.html
        project_known_issues.html
        project_roles.html
        project_settings.html
        project.html
        projects.html
        raw_format.html
        raw_formats.html
        report.html
        row_trace.html
        run.html
        template_results.html
        template.html
        templates.html
        trace.html
        vectors.html
        wizard.html
      __init__.py
      server.py
    __init__.py
    cli.py
tests/
  fixtures/
    db/
      generate_golden_dbs.py
      v1.sqlite
      v10.sqlite
      v11.sqlite
      v12.sqlite
      v13.sqlite
      v14.sqlite
      v15.sqlite
      v16.sqlite
      v17.sqlite
      v18.sqlite
      v19.sqlite
      v2.sqlite
      v3.sqlite
      v4.sqlite
      v5.sqlite
      v6.sqlite
      v7.sqlite
      v8.sqlite
      v9.sqlite
    enertia_eventlog.csv
    ground_truth_enertia.yaml
    ground_truth_quorum.yaml
    ground_truth_synth.yaml
    ground_truth_tolerance.yaml
    make_synth_data.py
    quorum_close_cycle.csv
    synth_clusters.csv
    synth_linear.csv
    synth_shift_corr.csv
    synth_timeseries.csv
  plugins/
    test_attribution.py
    test_bocpd_gaussian.py
    test_capacity_scaling.py
    test_chain_makespan.py
    test_close_cycle_capacity_impact.py
    test_close_cycle_capacity_model.py
    test_close_cycle_contention.py
    test_close_cycle_revenue_compression.py
    test_concurrency_reconstruction.py
    test_conformal_feature_prediction.py
    test_dependency_resolution_join.py
    test_determinism_discipline.py
    test_dp_gmm.py
    test_eventlog_fixture_smoke.py
    test_gaussian_copula_shift.py
    test_gaussian_knockoffs.py
    test_graph_topology_curves.py
    test_knockoff_wrapper_rf.py
    test_notears_linear.py
    test_online_conformal_changepoint.py
    test_percentile_analysis.py
    test_process_sequence.py
    test_profile_eventlog.py
    test_queue_delay_decomposition.py
    test_scan_statistics.py
    test_sequence_classification.py
    test_tail_isolation.py
    test_transform_normalize_mixed.py
  conftest.py
  test_append_only.py
  test_auth.py
  test_dedupe_runs.py
  test_determinism.py
  test_enertia_evaluation.py
  test_evaluation.py
  test_feature_flags.py
  test_ingest_tabular.py
  test_llm_prompt_builder.py
  test_migrations.py
  test_missing_plugins.py
  test_parameter_normalization.py
  test_performance_smoke.py
  test_pipeline_integration.py
  test_plugin_discovery.py
  test_plugin_manifest_schema.py
  test_profile_basic.py
  test_quorum_evaluation.py
  test_raw_format_mappings.py
  test_report_schema.py
  test_row_trace.py
  test_sandbox.py
  test_security_paths.py
  test_template_combined_run.py
  test_template_conversion.py
  test_template_filters.py
  test_tenant_isolation.py
  test_ui_template_results.py
  test_ui_vectors_api.py
  test_ui_vectors.py
  test_upload_limits.py
  test_vector_store.py
tools/
  sitecustomize.py
.gitignore
.gitkeep
1-32-2026-recommendations-plan.md
AGENTS.md
CHANGELOG.md
close-cycle-third-server-impact-plan.md
cohesive-plan-docs-phase2-plan.md
cohesive-plan-docs-phase2-todos-plan.md
kernel-plugin-stability-plan.md
LICENSE
Makefile
pyproject.toml
queue-delay-plugins-evaluation-plan.md
README.md
```

# Files

## File: docs/1-32-2026.txt
````
  1: {
  2:   "ship_gate": "ANY_REGRESS=>DO_NOT_SHIP",
  3:   "recommendations": [
  4:     {
  5:       "id": "R1",
  6:       "title": "Stop truncating by default; ingest full dataset with streaming",
  7:       "improved": "Accuracy + completeness of every analysis (entire dataset).",
  8:       "risked": "Silent partial-data insights, false trends, missed sequences (current default max_rows=10000).",
  9:       "enforcement_location": "plugins/ingest_tabular/plugin.py (remove head/max_rows default); core/pipeline.py (pass ingest settings); add chunked reader+inserter.",
 10:       "regression_detection": "Integration test: fixture with >10k rows must persist exact rowcount; assert analysis plugins see full rowcount; performance test on large CSV to ensure runtime stays bounded."
 11:     },
 12:     {
 13:       "id": "R2",
 14:       "title": "Add Project/Dataset layer separate from Run",
 15:       "improved": "Frictionless reruns + durable ‘project’ semantics; one dataset ingested once, many analysis runs later.",
 16:       "risked": "Data duplication, inability to backfill new techniques on historical datasets without re-ingest, unclear lineage across reruns.",
 17:       "enforcement_location": "core/storage.py schema: add projects, datasets, dataset_versions; core/pipeline.py: create/lookup project_id by file hash; run references dataset_id.",
 18:       "regression_detection": "DB migration test from empty and from existing state.sqlite; unit tests: same file re-upload creates new run but reuses dataset_id."
 19:     },
 20:     {
 21:       "id": "R3",
 22:       "title": "Deduplicate uploads via content fingerprint (sha256) + stable IDs",
 23:       "improved": "Low-friction: re-uploading the same file becomes instant; enables reliable trace from ‘file’ to everything derived.",
 24:       "risked": "Unbounded DB growth, ambiguous tracing (same filename != same content), duplicate findings.",
 25:       "enforcement_location": "Upload path: compute sha256; store in sqlite uploads/projects; pipeline.run uses fingerprint to resolve project_id/dataset_id.",
 26:       "regression_detection": "Test: same file bytes under different filenames must map to same project; changed bytes must map to new project."
 27:     },
 28:     {
 29:       "id": "R4",
 30:       "title": "Store dataset rows inside SQLite (choose a scalable representation)",
 31:       "improved": "Meets your explicit requirement: “load all data into SQLite” + enables SQL-level lineage queries.",
 32:       "risked": "If you keep only canonical.csv on disk, you lose the ability to query/associate historically when files move or are pruned; also breaks your requirement.",
 33:       "enforcement_location": "New schema + ingest plugin: insert into sqlite within a transaction; consider per-dataset wide table (fast reads) vs generic row_json table (schema-flexible).",
 34:       "regression_detection": "Property test: round-trip CSV -> sqlite -> export produces identical row counts and stable column mapping; performance test: ingestion uses batch inserts and commits in chunks."
 35:     },
 36:     {
 37:       "id": "R5",
 38:       "title": "Make raw data append-only (block DELETE/UPDATE on raw tables)",
 39:       "improved": "Guarantees “must never go away” at the database layer, not just by convention.",
 40:       "risked": "Accidental cleanup code, ‘INSERT OR REPLACE’ patterns, or future ‘vacuum/cleanup’ features can silently destroy history.",
 41:       "enforcement_location": "SQLite triggers (BEFORE DELETE/UPDATE -> RAISE(ABORT)); storage layer disallows destructive operations; provide soft-delete/tombstone metadata if needed.",
 42:       "regression_detection": "DB tests: attempts to delete/update raw rows must fail; migration tests ensure triggers exist after schema upgrades."
 43:     },
 44:     {
 45:       "id": "R6",
 46:       "title": "Version all derived results (never overwrite plugin outputs)",
 47:       "improved": "Historical reproducibility + auditability; supports “new techniques over old data” without losing prior states.",
 48:       "risked": "Current `INSERT OR REPLACE` overwrites results for a run/plugin; you lose historical comparisons and audit trail.",
 49:       "enforcement_location": "storage.py: replace plugin_results PK with (run_id, plugin_id, plugin_version, executed_at) or add result_id; pipeline records plugin manifest version + code hash.",
 50:       "regression_detection": "Test: re-run same plugin twice yields two stored executions; query returns latest by default but retains history."
 51:     },
 52:     {
 53:       "id": "R7",
 54:       "title": "Normalize the parameter field into key/value entities + canonical forms",
 55:       "improved": "Enables grouping by ‘one or more parameters’ and robust cross-project association/tracing.",
 56:       "risked": "Free-form parameter strings will cause missed joins, false joins, and brittle logic (whitespace/casing/ordering differences).",
 57:       "enforcement_location": "New tables: parameter_entities, parameter_kv, row_parameter_link; add parsing/normalization step in ingest/profile plugin.",
 58:       "regression_detection": "Fixtures with same logical params in different text formats must normalize to same entity_id; fuzz tests for whitespace/order/case."
 59:     },
 60:     {
 61:       "id": "R8",
 62:       "title": "Add a lineage/association graph table and use recursive CTE for trace",
 63:       "improved": "Direct implementation of ‘trace a single CSV through all associated processes and parameters’.",
 64:       "risked": "Without explicit edges, tracing becomes ad-hoc, slow, and inconsistent across new plugins/techniques.",
 65:       "enforcement_location": "SQLite tables: entities(entity_id, type, key), edges(src_entity_id, dst_entity_id, kind, evidence_json, score); helper APIs in Storage.",
 66:       "regression_detection": "Golden test: known small dataset yields expected reachable nodes; performance test: trace query completes under budget for N projects."
 67:     },
 68:     {
 69:       "id": "R9",
 70:       "title": "Introduce an AutoPlanner that chooses plugins based on dataset profile",
 71:       "improved": "Meets your ‘don’t tell harness what to look for’ requirement while keeping runtime bounded and relevant.",
 72:       "risked": "Running every plugin always can be slow/noisy; requiring user selection is friction and misses insights.",
 73:       "enforcement_location": "core/pipeline.py: add planning phase after ingest/profile; plugin manifests add capability tags (needs_numeric/needs_timestamp/needs_eventlog).",
 74:       "regression_detection": "Planner unit tests: different fixture types select expected plugin bundles; snapshot tests for plan stability."
 75:     },
 76:     {
 77:       "id": "R10",
 78:       "title": "Process/sequence mining plugin for ‘grouped sequences of processes’",
 79:       "improved": "Directly targets your core insight: discover frequent variants, transitions, bottlenecks, rare paths by parameter groups.",
 80:       "risked": "Existing numeric-focused plugins won’t surface process variants; you miss the “sequence of processes” requirement.",
 81:       "enforcement_location": "New analysis plugin: detect event-log columns (case/parameter, activity/process, timestamp/order); compute frequent subsequences + transition anomalies; store results as findings with evidence rows.",
 82:       "regression_detection": "Synthetic event-log fixture with known variants; assert discovered top variants and transition matrix entries."
 83:     },
 84:     {
 85:       "id": "R11",
 86:       "title": "Add ‘evidence first’ for every finding (row pointers + minimal slices)",
 87:       "improved": "Inspectability: every insight is traceable to specific rows and parameter entities; supports debugging false positives.",
 88:       "risked": "Users will distrust findings without reproducible evidence; hard to validate “hidden trends” claims.",
 89:       "enforcement_location": "Finding schema: include dataset_id, row_ids, column_ids, and query snippet used to produce the finding; store evidence_json separately for indexing.",
 90:       "regression_detection": "Tests: every finding must include evidence pointers; schema validation enforces non-empty evidence for non-skipped plugins."
 91:     },
 92:     {
 93:       "id": "R12",
 94:       "title": "Add statistical controls to reduce false discoveries (ranking + correction)",
 95:       "improved": "Accuracy: fewer spurious ‘patterns’ when scanning many columns/parameters automatically.",
 96:       "risked": "Autonomous search across many hypotheses will generate false positives that look convincing.",
 97:       "enforcement_location": "Common library in core/: effect sizes, multiple-testing corrections, confidence scoring; plugins output comparable scores.",
 98:       "regression_detection": "Benchmark tests on null datasets should produce low finding counts; compare distribution of p-values/scores on synthetic null."
 99:     },
100:     {
101:       "id": "R13",
102:       "title": "Resource budgets (time/memory/row limits) with explicit reporting",
103:       "improved": "Performance + reliability on wide/deep datasets without manual tuning.",
104:       "risked": "One heavy plugin can stall the whole harness; OOM on large Excel/CSV; unpredictable runtimes.",
105:       "enforcement_location": "Pipeline runner: optional per-plugin process isolation with timeouts; plugin context includes budget; findings include whether sampling/approx was used.",
106:       "regression_detection": "Load test: large fixture must complete within budget; verify plugins report ‘sampled=true’ when budgets force sampling."
107:     },
108:     {
109:       "id": "R14",
110:       "title": "Thread-safe SQLite usage in the API server",
111:       "improved": "Stability when multiple projects/runs are triggered concurrently from UI/CLI.",
112:       "risked": "Shared connection/state can deadlock, throw thread errors, or corrupt expectations under concurrency.",
113:       "enforcement_location": "core/storage.py: open connections per request/run (or connection pool); set WAL; wrap writes in transactions; avoid global Pipeline with a single connection.",
114:       "regression_detection": "Concurrency test: start multiple runs in parallel; assert all complete and DB remains consistent; run under uvicorn with multiple workers if applicable."
115:     },
116:     {
117:       "id": "R15",
118:       "title": "Schema migrations with `PRAGMA user_version` + migration tests",
119:       "improved": "Safe evolution as you add techniques/tables over time; protects existing historical DB.",
120:       "risked": "Manual schema edits break old installs; silent partial migrations; loss of data or inability to read old projects.",
121:       "enforcement_location": "storage.py: migration runner; add schema_migrations table; tests that migrate from prior versions.",
122:       "regression_detection": "Golden DB file for vN -> migrate -> validate integrity + data preservation; CI runs migrations."
123:     },
124:     {
125:       "id": "R16",
126:       "title": "UX: eliminate manual upload_id + add project index/search/trace views",
127:       "improved": "Frictionless: upload → auto-run (or one-click) → results; plus global search by parameter/process/file hash.",
128:       "risked": "Current flow causes user error and slows iteration; tracing remains “developer only”.",
129:       "enforcement_location": "ui/server.py + templates: on upload, redirect to create_run automatically; add /projects list; add /trace endpoint; store upload metadata in sqlite.",
130:       "regression_detection": "UI smoke tests (FastAPI TestClient): upload returns run_id immediately; trace endpoint returns deterministic structure for fixtures."
131:     },
132:     {
133:       "id": "R17",
134:       "title": "Index strategy for trace + association (FTS5 / normalized lookup tables)",
135:       "improved": "Fast cross-project search by parameter/process; supports ‘associable’ discovery at scale.",
136:       "risked": "Trace queries become slow as DB grows; users perceive it as broken.",
137:       "enforcement_location": "SQLite: indexes on (entity.type, entity.key), (edges.src, edges.dst), (parameter_kv.key,value); optional FTS table for raw parameter text.",
138:       "regression_detection": "Performance regression test: N projects, M rows; trace/search stays under latency budget."
139:     },
140:     {
141:       "id": "R18",
142:       "title": "Backfill mechanism when adding new plugins/techniques",
143:       "improved": "Your stated goal: new techniques can mine deeper/wider across historical projects automatically.",
144:       "risked": "New plugins only apply to new uploads; historical DB becomes inconsistent; you miss ‘hidden’ patterns present in old data.",
145:       "enforcement_location": "Queue table in sqlite (analysis_jobs) + worker loop; CLI command `backfill --plugin <id> --all-projects` that schedules jobs; store plugin versioned outputs.",
146:       "regression_detection": "Test: adding a plugin schedules jobs for existing datasets; rerun does not duplicate jobs; job completion persists results without overwrites."
147:     },
148:     {
149:       "id": "R19",
150:       "title": "Data governance guardrails (PII detection tags + optional encryption)",
151:       "improved": "Security/compliance posture while retaining data; enables safe long-term retention.",
152:       "risked": "‘Never delete’ can conflict with policy requirements if sensitive fields exist; also increases blast radius if DB is copied.",
153:       "enforcement_location": "Profile plugin: detect likely PII patterns; tag columns; optionally encrypt sensitive columns (application-layer) or isolate into separate DB with restricted access.",
154:       "regression_detection": "PII fixture tests: known patterns detected; ensure tags propagate into report/findings; verify encrypted columns are not emitted in artifacts by default."
155:     },
156:     {
157:       "id": "R20",
158:       "title": "Column name canonicalization + safe SQL identifier handling",
159:       "improved": "Robust ingestion from arbitrary Excel/CSV headers; prevents SQL errors and injection via column names.",
160:       "risked": "Headers with spaces/quotes/duplicates/reserved words break table creation or cause ambiguous queries.",
161:       "enforcement_location": "Ingest step: map original headers -> safe internal column_ids; store mapping table; always use parameterized SQL and quoted identifiers.",
162:       "regression_detection": "Fuzz tests on random/hostile headers; ingestion must succeed and preserve mapping deterministically."
163:     }
164:   ]
165: }
````

## File: docs/plugin_manifest.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "title": "Statistic Harness Plugin Manifest",
 4:   "type": "object",
 5:   "additionalProperties": false,
 6:   "required": [
 7:     "id",
 8:     "name",
 9:     "version",
10:     "type",
11:     "entrypoint",
12:     "capabilities",
13:     "config_schema",
14:     "output_schema",
15:     "sandbox"
16:   ],
17:   "properties": {
18:     "id": {
19:       "type": "string",
20:       "pattern": "^[a-z0-9_]+$"
21:     },
22:     "name": {
23:       "type": "string",
24:       "minLength": 1
25:     },
26:     "version": {
27:       "type": "string",
28:       "minLength": 1
29:     },
30:     "type": {
31:       "type": "string",
32:       "enum": [
33:         "ingest",
34:         "profile",
35:         "analysis",
36:         "report",
37:         "llm",
38:         "planner",
39:         "transform"
40:       ]
41:     },
42:     "entrypoint": {
43:       "type": "string",
44:       "pattern": "^[^:]+:[A-Za-z_][A-Za-z0-9_]*$"
45:     },
46:     "depends_on": {
47:       "type": "array",
48:       "items": { "type": "string" },
49:       "default": []
50:     },
51:     "capabilities": {
52:       "type": "array",
53:       "items": { "type": "string" },
54:       "default": []
55:     },
56:     "config_schema": {
57:       "type": "string",
58:       "minLength": 1
59:     },
60:     "output_schema": {
61:       "type": "string",
62:       "minLength": 1
63:     },
64:     "sandbox": {
65:       "type": "object",
66:       "additionalProperties": false,
67:       "required": ["no_network", "fs_allowlist"],
68:       "properties": {
69:         "no_network": { "type": "boolean" },
70:         "fs_allowlist": {
71:           "type": "array",
72:           "items": { "type": "string" },
73:           "minItems": 1
74:         }
75:       }
76:     },
77:     "settings": {
78:       "type": "object",
79:       "additionalProperties": false,
80:       "required": ["description", "defaults"],
81:       "properties": {
82:         "description": { "type": "string" },
83:         "defaults": { "type": "object" }
84:       }
85:     }
86:   }
87: }
````

## File: plugins/analysis_attribution/__init__.py
````python
1: 
````

## File: plugins/analysis_attribution/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "process_column": {"type": ["string", "null"]},
 7:     "module_column": {"type": ["string", "null"]},
 8:     "user_column": {"type": ["string", "null"]},
 9:     "queue_column": {"type": ["string", "null"]},
10:     "eligible_column": {"type": ["string", "null"]},
11:     "start_column": {"type": ["string", "null"]},
12:     "wait_threshold_seconds": {"type": "number", "minimum": 0},
13:     "max_groups": {"type": "integer", "minimum": 1},
14:     "max_examples": {"type": "integer", "minimum": 0}
15:   }
16: }
````

## File: plugins/analysis_attribution/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "status": {"type": "string"},
 7:     "summary": {"type": "string"},
 8:     "metrics": {"type": "object"},
 9:     "findings": {"type": "array"},
10:     "artifacts": {"type": "array"},
11:     "budget": {
12:       "type": "object",
13:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
14:       "properties": {
15:         "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
16:         "sampled": {"type": "boolean"},
17:         "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
18:         "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
19:         "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
20:       },
21:       "additionalProperties": true
22:     },
23:     "error": {"type": ["object", "null"]}
24:   },
25:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"]
26: }
````

## File: plugins/analysis_attribution/plugin.yaml
````yaml
 1: id: analysis_attribution
 2: name: Attribution Analysis
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Attribute eligible wait to process/module/user with deterministic ranking.
 9:   defaults:
10:     process_column: null
11:     module_column: null
12:     user_column: null
13:     queue_column: null
14:     eligible_column: null
15:     start_column: null
16:     wait_threshold_seconds: 60
17:     max_groups: 5
18:     max_examples: 25
19: capabilities:
20:   - needs_eventlog
21:   - needs_timestamp
22: config_schema: config.schema.json
23: output_schema: output.schema.json
24: sandbox:
25:   no_network: true
26:   fs_allowlist:
27:     - appdata
28:     - plugins
29:     - run_dir
````

## File: plugins/analysis_attribution/README.md
````markdown
1: # Attribution Analysis
2: 
3: Attributes eligible-wait time to process, module, and user dimensions with deterministic ranking.
````

## File: plugins/analysis_bocpd_gaussian/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "hazard": {
 6:       "type": [
 7:         "number",
 8:         "null"
 9:       ]
10:     },
11:     "peak_threshold": {
12:       "type": "number"
13:     },
14:     "time_column": {
15:       "type": [
16:         "string",
17:         "null"
18:       ]
19:     },
20:     "value_column": {
21:       "type": [
22:         "string",
23:         "null"
24:       ]
25:     }
26:   },
27:   "title": "BOCPD Gaussian Config",
28:   "type": "object"
29: }
````

## File: plugins/analysis_bocpd_gaussian/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "evidence": {
187:                     "additionalProperties": true,
188:                     "properties": {
189:                       "column_ids": {
190:                         "items": {
191:                           "type": "integer"
192:                         },
193:                         "type": "array"
194:                       },
195:                       "dataset_id": {
196:                         "type": "string"
197:                       },
198:                       "dataset_version_id": {
199:                         "type": "string"
200:                       },
201:                       "query": {
202:                         "type": [
203:                           "string",
204:                           "null"
205:                         ]
206:                       },
207:                       "row_ids": {
208:                         "items": {
209:                           "type": "integer"
210:                         },
211:                         "type": "array"
212:                       },
213:                       "row_ranges": {
214:                         "items": {
215:                           "additionalProperties": false,
216:                           "properties": {
217:                             "end": {
218:                               "type": "integer"
219:                             },
220:                             "start": {
221:                               "type": "integer"
222:                             }
223:                           },
224:                           "required": [
225:                             "start",
226:                             "end"
227:                           ],
228:                           "type": "object"
229:                         },
230:                         "type": "array"
231:                       }
232:                     },
233:                     "required": [
234:                       "dataset_id",
235:                       "dataset_version_id",
236:                       "row_ids",
237:                       "column_ids"
238:                     ],
239:                     "type": "object"
240:                   },
241:                   "index": {
242:                     "type": "integer"
243:                   },
244:                   "kind": {
245:                     "const": "changepoint"
246:                   },
247:                   "prob": {
248:                     "type": "number"
249:                   }
250:                 },
251:                 "required": [
252:                   "kind",
253:                   "index",
254:                   "prob"
255:                 ],
256:                 "type": "object"
257:               }
258:             ]
259:           }
260:         ]
261:       },
262:       "type": "array"
263:     },
264:     "metrics": {
265:       "additionalProperties": false,
266:       "properties": {
267:         "count": {
268:           "type": "integer"
269:         }
270:       },
271:       "type": "object"
272:     },
273:     "status": {
274:       "enum": [
275:         "ok",
276:         "skipped",
277:         "error"
278:       ],
279:       "type": "string"
280:     },
281:     "summary": {
282:       "type": "string"
283:     }
284:   },
285:   "required": [
286:     "status",
287:     "summary",
288:     "metrics",
289:     "findings",
290:     "artifacts",
291:     "budget"
292:   ],
293:   "title": "Plugin Result",
294:   "type": "object"
295: }
````

## File: plugins/analysis_capacity_scaling/__init__.py
````python
1: 
````

## File: plugins/analysis_capacity_scaling/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "process_column": {"type": ["string", "null"]},
 7:     "module_column": {"type": ["string", "null"]},
 8:     "host_column": {"type": ["string", "null"]},
 9:     "queue_column": {"type": ["string", "null"]},
10:     "eligible_column": {"type": ["string", "null"]},
11:     "start_column": {"type": ["string", "null"]},
12:     "scale_factor": {"type": ["number", "null"]},
13:     "max_groups": {"type": "integer", "minimum": 1},
14:     "max_examples": {"type": "integer", "minimum": 0}
15:   }
16: }
````

## File: plugins/analysis_capacity_scaling/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "status": {"type": "string"},
 7:     "summary": {"type": "string"},
 8:     "metrics": {"type": "object"},
 9:     "findings": {"type": "array"},
10:     "artifacts": {"type": "array"},
11:     "budget": {
12:       "type": "object",
13:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
14:       "properties": {
15:         "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
16:         "sampled": {"type": "boolean"},
17:         "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
18:         "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
19:         "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
20:       },
21:       "additionalProperties": true
22:     },
23:     "error": {"type": ["object", "null"]}
24:   },
25:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"]
26: }
````

## File: plugins/analysis_capacity_scaling/plugin.yaml
````yaml
 1: id: analysis_capacity_scaling
 2: name: Capacity Scaling Scenario
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Model capacity scaling impact on eligible wait.
 9:   defaults:
10:     process_column: null
11:     module_column: null
12:     host_column: null
13:     queue_column: null
14:     eligible_column: null
15:     start_column: null
16:     scale_factor: null
17:     max_groups: 5
18:     max_examples: 25
19: capabilities:
20:   - needs_eventlog
21:   - needs_timestamp
22: config_schema: config.schema.json
23: output_schema: output.schema.json
24: sandbox:
25:   no_network: true
26:   fs_allowlist:
27:     - appdata
28:     - plugins
29:     - run_dir
````

## File: plugins/analysis_capacity_scaling/README.md
````markdown
1: # Capacity Scaling Scenario
2: 
3: Models how scaling capacity changes eligible-wait time using a configurable scale factor.
````

## File: plugins/analysis_chain_makespan/__init__.py
````python
1: 
````

## File: plugins/analysis_chain_makespan/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "sequence_column": {"type": ["string", "null"]},
 7:     "start_column": {"type": ["string", "null"]},
 8:     "end_column": {"type": ["string", "null"]},
 9:     "duration_column": {"type": ["string", "null"]},
10:     "max_sequences": {"type": "integer", "minimum": 1},
11:     "max_examples": {"type": "integer", "minimum": 0}
12:   }
13: }
````

## File: plugins/analysis_chain_makespan/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "status": {"type": "string"},
 7:     "summary": {"type": "string"},
 8:     "metrics": {"type": "object"},
 9:     "findings": {"type": "array"},
10:     "artifacts": {"type": "array"},
11:     "budget": {
12:       "type": "object",
13:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
14:       "properties": {
15:         "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
16:         "sampled": {"type": "boolean"},
17:         "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
18:         "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
19:         "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
20:       },
21:       "additionalProperties": true
22:     },
23:     "error": {"type": ["object", "null"]}
24:   },
25:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"]
26: }
````

## File: plugins/analysis_chain_makespan/plugin.yaml
````yaml
 1: id: analysis_chain_makespan
 2: name: Chain Makespan Analysis
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Compute per-sequence makespan, runtime, and idle gap statistics.
 9:   defaults:
10:     sequence_column: null
11:     start_column: null
12:     end_column: null
13:     duration_column: null
14:     max_sequences: 10
15:     max_examples: 25
16: capabilities:
17:   - needs_eventlog
18:   - needs_timestamp
19: config_schema: config.schema.json
20: output_schema: output.schema.json
21: sandbox:
22:   no_network: true
23:   fs_allowlist:
24:     - appdata
25:     - plugins
26:     - run_dir
````

## File: plugins/analysis_chain_makespan/README.md
````markdown
1: # Chain Makespan Analysis
2: 
3: Computes per-sequence makespan, runtime, and idle-gap statistics from event-log data.
````

## File: plugins/analysis_close_cycle_capacity_impact/__init__.py
````python
1: 
````

## File: plugins/analysis_close_cycle_capacity_impact/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "additionalProperties": true,
 4:   "properties": {
 5:     "artifacts": { "type": "array" },
 6:     "budget": {
 7:       "type": "object",
 8:       "additionalProperties": true,
 9:       "properties": {
10:         "cpu_limit_ms": { "anyOf": [{"type":"integer"},{"type":"null"}] },
11:         "notes": { "anyOf": [{"type":"string"},{"type":"null"}] },
12:         "row_limit": { "anyOf": [{"type":"integer"},{"type":"null"}] },
13:         "sampled": { "type": "boolean" },
14:         "time_limit_ms": { "anyOf": [{"type":"integer"},{"type":"null"}] }
15:       },
16:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"]
17:     },
18:     "error": { "type": ["object", "null"] },
19:     "findings": { "type": "array" },
20:     "metrics": { "type": "object" },
21:     "status": { "type": "string" },
22:     "summary": { "type": "string" }
23:   },
24:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"],
25:   "type": "object"
26: }
````

## File: plugins/analysis_close_cycle_capacity_model/__init__.py
````python
1: """Close-cycle capacity model plugin."""
````

## File: plugins/analysis_close_cycle_capacity_model/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "additionalProperties": true,
 4:   "properties": {
 5:     "artifacts": { "type": "array" },
 6:     "budget": {
 7:       "type": "object",
 8:       "additionalProperties": true,
 9:       "properties": {
10:         "cpu_limit_ms": { "anyOf": [{"type":"integer"},{"type":"null"}] },
11:         "notes": { "anyOf": [{"type":"string"},{"type":"null"}] },
12:         "row_limit": { "anyOf": [{"type":"integer"},{"type":"null"}] },
13:         "sampled": { "type": "boolean" },
14:         "time_limit_ms": { "anyOf": [{"type":"integer"},{"type":"null"}] }
15:       },
16:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"]
17:     },
18:     "error": { "type": ["object", "null"] },
19:     "findings": { "type": "array" },
20:     "metrics": { "type": "object" },
21:     "status": { "type": "string" },
22:     "summary": { "type": "string" }
23:   },
24:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"],
25:   "type": "object"
26: }
````

## File: plugins/analysis_close_cycle_contention/__init__.py
````python
1: __all__ = ["plugin"]
````

## File: plugins/analysis_close_cycle_contention/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "title": "Close Cycle Contention Config",
 4:   "type": "object",
 5:   "additionalProperties": false,
 6:   "properties": {
 7:     "process_column": { "type": ["string", "null"] },
 8:     "timestamp_column": { "type": ["string", "null"] },
 9:     "start_column": { "type": ["string", "null"] },
10:     "end_column": { "type": ["string", "null"] },
11:     "duration_column": { "type": ["string", "null"] },
12:     "server_column": { "type": ["string", "null"] },
13:     "user_column": { "type": ["string", "null"] },
14:     "param_column": { "type": ["string", "null"] },
15:     "close_cycle_start_day": { "type": "integer", "minimum": 1, "maximum": 31 },
16:     "close_cycle_end_day": { "type": "integer", "minimum": 1, "maximum": 31 },
17:     "min_close_count": { "type": "integer", "minimum": 1 },
18:     "min_open_count": { "type": "integer", "minimum": 1 },
19:     "slowdown_ratio_threshold": { "type": "number", "minimum": 1 },
20:     "correlation_threshold": { "type": "number", "minimum": -1, "maximum": 1 },
21:     "max_unique_ratio": { "type": "number", "minimum": 0, "maximum": 1 },
22:     "min_days": { "type": "integer", "minimum": 1 },
23:     "max_examples": { "type": "integer", "minimum": 1 },
24:     "min_recommendation_pct": { "type": "number", "minimum": 0, "maximum": 1 }
25:   }
26: }
````

## File: plugins/analysis_close_cycle_contention/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "additionalProperties": true,
113:         "properties": {
114:           "close_count": {
115:             "type": "integer"
116:           },
117:           "close_cycle_days": {
118:             "type": "integer"
119:           },
120:           "columns": {
121:             "items": {
122:               "type": "string"
123:             },
124:             "type": "array"
125:           },
126:           "correlation": {
127:             "type": "number"
128:           },
129:           "estimated_improvement_pct": {
130:             "type": "number"
131:           },
132:           "evidence": {
133:             "additionalProperties": true,
134:             "properties": {
135:               "column_ids": {
136:                 "items": {
137:                   "type": "integer"
138:                 },
139:                 "type": "array"
140:               },
141:               "dataset_id": {
142:                 "type": "string"
143:               },
144:               "dataset_version_id": {
145:                 "type": "string"
146:               },
147:               "query": {
148:                 "anyOf": [
149:                   {
150:                     "type": "string"
151:                   },
152:                   {
153:                     "type": "null"
154:                   }
155:                 ]
156:               },
157:               "row_ids": {
158:                 "items": {
159:                   "type": "integer"
160:                 },
161:                 "type": "array"
162:               },
163:               "row_ranges": {
164:                 "items": {
165:                   "additionalProperties": false,
166:                   "properties": {
167:                     "end": {
168:                       "type": "integer"
169:                     },
170:                     "start": {
171:                       "type": "integer"
172:                     }
173:                   },
174:                   "required": [
175:                     "start",
176:                     "end"
177:                   ],
178:                   "type": "object"
179:                 },
180:                 "type": "array"
181:               }
182:             },
183:             "required": [
184:               "dataset_id",
185:               "dataset_version_id",
186:               "row_ids",
187:               "column_ids"
188:             ],
189:             "type": "object"
190:           },
191:           "kind": {
192:             "type": "string"
193:           },
194:           "median_duration_close": {
195:             "type": "number"
196:           },
197:           "median_duration_open": {
198:             "type": "number"
199:           },
200:           "open_count": {
201:             "type": "integer"
202:           },
203:           "param_unique_ratio": {
204:             "anyOf": [
205:               {
206:                 "type": "number"
207:               },
208:               {
209:                 "type": "null"
210:               }
211:             ]
212:           },
213:           "process": {
214:             "type": "string"
215:           },
216:           "process_norm": {
217:             "type": "string"
218:           },
219:           "query": {
220:             "anyOf": [
221:               {
222:                 "type": "string"
223:               },
224:               {
225:                 "type": "null"
226:               }
227:             ]
228:           },
229:           "row_ids": {
230:             "items": {
231:               "type": "integer"
232:             },
233:             "type": "array"
234:           },
235:           "server_count": {
236:             "type": "integer"
237:           },
238:           "servers": {
239:             "items": {
240:               "type": "string"
241:             },
242:             "type": "array"
243:           },
244:           "slowdown_ratio": {
245:             "type": "number"
246:           }
247:         },
248:         "required": [
249:           "kind",
250:           "evidence"
251:         ],
252:         "type": "object"
253:       },
254:       "type": "array"
255:     },
256:     "metrics": {
257:       "additionalProperties": true,
258:       "properties": {
259:         "candidates": {
260:           "type": "integer"
261:         },
262:         "duration_column": {
263:           "anyOf": [
264:             {
265:               "type": "string"
266:             },
267:             {
268:               "type": "null"
269:             }
270:           ]
271:         },
272:         "param_column": {
273:           "anyOf": [
274:             {
275:               "type": "string"
276:             },
277:             {
278:               "type": "null"
279:             }
280:           ]
281:         },
282:         "process_column": {
283:           "anyOf": [
284:             {
285:               "type": "string"
286:             },
287:             {
288:               "type": "null"
289:             }
290:           ]
291:         },
292:         "server_column": {
293:           "anyOf": [
294:             {
295:               "type": "string"
296:             },
297:             {
298:               "type": "null"
299:             }
300:           ]
301:         },
302:         "timestamp_column": {
303:           "anyOf": [
304:             {
305:               "type": "string"
306:             },
307:             {
308:               "type": "null"
309:             }
310:           ]
311:         }
312:       },
313:       "type": "object"
314:     },
315:     "status": {
316:       "enum": [
317:         "ok",
318:         "skipped",
319:         "error"
320:       ],
321:       "type": "string"
322:     },
323:     "summary": {
324:       "type": "string"
325:     }
326:   },
327:   "required": [
328:     "status",
329:     "summary",
330:     "metrics",
331:     "findings",
332:     "artifacts",
333:     "budget"
334:   ],
335:   "title": "Close Cycle Contention Result",
336:   "type": "object"
337: }
````

## File: plugins/analysis_close_cycle_contention/plugin.yaml
````yaml
 1: id: analysis_close_cycle_contention
 2: name: Close Cycle Contention Analysis
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Detect close-cycle slowdowns and contention (e.g., qemail/qpec)
 9:   defaults:
10:     process_column: null
11:     timestamp_column: null
12:     start_column: null
13:     end_column: null
14:     duration_column: null
15:     server_column: null
16:     user_column: null
17:     param_column: null
18:     close_cycle_start_day: 20
19:     close_cycle_end_day: 5
20:     min_close_count: 20
21:     min_open_count: 10
22:     slowdown_ratio_threshold: 2.0
23:     correlation_threshold: 0.7
24:     max_unique_ratio: 0.05
25:     min_days: 5
26:     max_examples: 25
27:     min_recommendation_pct: 0.1
28: capabilities:
29:   - needs_eventlog
30:   - needs_timestamp
31: config_schema: config.schema.json
32: output_schema: output.schema.json
33: sandbox:
34:   no_network: true
35:   fs_allowlist:
36:   - appdata
37:   - plugins
38:   - run_dir
````

## File: plugins/analysis_close_cycle_duration_shift/__init__.py
````python
1: """Close cycle duration shift analysis plugin."""
````

## File: plugins/analysis_close_cycle_duration_shift/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "title": "Close Cycle Duration Shift Config",
 4:   "type": "object",
 5:   "additionalProperties": false,
 6:   "properties": {
 7:     "process_column": { "type": ["string", "null"] },
 8:     "timestamp_column": { "type": ["string", "null"] },
 9:     "start_column": { "type": ["string", "null"] },
10:     "end_column": { "type": ["string", "null"] },
11:     "duration_column": { "type": ["string", "null"] },
12:     "server_column": { "type": ["string", "null"] },
13:     "close_cycle_start_day": { "type": "integer", "minimum": 1, "maximum": 31 },
14:     "close_cycle_end_day": { "type": "integer", "minimum": 1, "maximum": 31 },
15:     "min_close_count": { "type": "integer", "minimum": 1 },
16:     "min_open_count": { "type": "integer", "minimum": 1 },
17:     "min_days": { "type": "integer", "minimum": 1 },
18:     "min_slowdown_ratio": { "type": "number", "minimum": 1 },
19:     "min_effect_seconds": { "type": "number", "minimum": 0 },
20:     "max_samples": { "type": "integer", "minimum": 100 },
21:     "permutations": { "type": "integer", "minimum": 0 },
22:     "p_value_threshold": { "type": "number", "minimum": 0, "maximum": 1 },
23:     "max_examples": { "type": "integer", "minimum": 1 }
24:   }
25: }
````

## File: plugins/analysis_close_cycle_duration_shift/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "additionalProperties": true,
 4:   "properties": {
 5:     "artifacts": {
 6:       "type": "array"
 7:     },
 8:     "budget": {
 9:       "additionalProperties": true,
10:       "properties": {
11:         "cpu_limit_ms": {
12:           "anyOf": [
13:             { "type": "integer" },
14:             { "type": "null" }
15:           ]
16:         },
17:         "notes": {
18:           "anyOf": [
19:             { "type": "string" },
20:             { "type": "null" }
21:           ]
22:         },
23:         "row_limit": {
24:           "anyOf": [
25:             { "type": "integer" },
26:             { "type": "null" }
27:           ]
28:         },
29:         "sampled": {
30:           "type": "boolean"
31:         },
32:         "time_limit_ms": {
33:           "anyOf": [
34:             { "type": "integer" },
35:             { "type": "null" }
36:           ]
37:         }
38:       },
39:       "required": [
40:         "row_limit",
41:         "sampled",
42:         "time_limit_ms",
43:         "cpu_limit_ms"
44:       ],
45:       "type": "object"
46:     },
47:     "error": {
48:       "type": ["object", "null"]
49:     },
50:     "findings": {
51:       "type": "array"
52:     },
53:     "metrics": {
54:       "type": "object"
55:     },
56:     "status": {
57:       "type": "string"
58:     },
59:     "summary": {
60:       "type": "string"
61:     }
62:   },
63:   "required": [
64:     "status",
65:     "summary",
66:     "metrics",
67:     "findings",
68:     "artifacts",
69:     "budget",
70:     "error"
71:   ],
72:   "type": "object"
73: }
````

## File: plugins/analysis_close_cycle_duration_shift/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: import csv
  4: import math
  5: from collections import Counter
  6: from typing import Iterable
  7: 
  8: import numpy as np
  9: import pandas as pd
 10: 
 11: from statistic_harness.core.types import PluginArtifact, PluginResult
 12: from statistic_harness.core.utils import write_json
 13: 
 14: 
 15: def _pick_column(
 16:     preferred: str | None,
 17:     columns: list[str],
 18:     role_by_name: dict[str, str],
 19:     roles: set[str],
 20:     patterns: list[str],
 21:     lower_names: dict[str, str],
 22:     exclude: set[str],
 23: ) -> str | None:
 24:     if preferred and preferred in columns:
 25:         return preferred
 26:     for col in columns:
 27:         if col in exclude:
 28:             continue
 29:         if role_by_name.get(col) in roles:
 30:             return col
 31:     for col in columns:
 32:         if col in exclude:
 33:             continue
 34:         name = lower_names[col]
 35:         if any(pattern in name for pattern in patterns):
 36:             return col
 37:     return None
 38: 
 39: 
 40: def _candidate_columns(
 41:     columns: list[str],
 42:     role_by_name: dict[str, str],
 43:     roles: set[str],
 44:     patterns: list[str],
 45:     lower_names: dict[str, str],
 46:     exclude: set[str],
 47: ) -> list[str]:
 48:     candidates: list[str] = []
 49:     for col in columns:
 50:         if col in exclude:
 51:             continue
 52:         if role_by_name.get(col) in roles:
 53:             candidates.append(col)
 54:     for col in columns:
 55:         if col in exclude or col in candidates:
 56:             continue
 57:         name = lower_names[col]
 58:         if any(pattern in name for pattern in patterns):
 59:             candidates.append(col)
 60:     return candidates
 61: 
 62: 
 63: def _score_process_column(name: str, series: pd.Series) -> float:
 64:     score = 0.0
 65:     lower_name = name.lower()
 66:     if lower_name in {"process", "process_id"}:
 67:         score += 3.0
 68:     if lower_name.endswith("_id") or lower_name.endswith("id"):
 69:         score += 1.5
 70:     for token in (
 71:         "queue",
 72:         "status",
 73:         "step",
 74:         "parent",
 75:         "child",
 76:         "hold",
 77:         "lock",
 78:         "schedule",
 79:         "master",
 80:         "dep",
 81:         "ext",
 82:         "attempt",
 83:         "priority",
 84:     ):
 85:         if token in lower_name:
 86:             score -= 2.0
 87: 
 88:     sample = series.dropna()
 89:     if sample.empty:
 90:         return score - 5.0
 91:     if sample.shape[0] > 5000:
 92:         sample = sample.sample(5000, random_state=0)
 93: 
 94:     if pd.api.types.is_numeric_dtype(sample):
 95:         score -= 1.5
 96:     else:
 97:         score += 1.5
 98: 
 99:     sample_str = sample.astype(str).str.strip()
100:     if not pd.api.types.is_numeric_dtype(sample):
101:         numeric_like = sample_str.str.match(r"^\\d+(\\.\\d+)?$").mean()
102:         if numeric_like > 0.8:
103:             score -= 2.0
104: 
105:     unique_ratio = sample.nunique(dropna=True) / max(1, sample.shape[0])
106:     score += (1.0 - unique_ratio) * 4.0
107:     if unique_ratio > 0.9:
108:         score -= 2.0
109: 
110:     lengths = sample_str.str.len()
111:     median_len = float(lengths.median()) if not lengths.empty else 0.0
112:     if 3 <= median_len <= 20:
113:         score += 0.5
114:     elif median_len > 40:
115:         score -= 0.5
116: 
117:     return score
118: 
119: 
120: def _choose_best_process_column(
121:     candidates: Iterable[str], df: pd.DataFrame
122: ) -> str | None:
123:     candidates = list(candidates)
124:     if not candidates:
125:         return None
126:     if len(candidates) == 1:
127:         return candidates[0]
128:     scored = []
129:     for col in candidates:
130:         scored.append((_score_process_column(str(col), df[col]), col))
131:     scored.sort(reverse=True, key=lambda item: item[0])
132:     return scored[0][1]
133: 
134: 
135: def _markdown_table(headers: list[str], rows: list[list[str]]) -> str:
136:     if not headers:
137:         return ""
138:     lines = []
139:     lines.append("| " + " | ".join(headers) + " |")
140:     lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
141:     for row in rows:
142:         lines.append("| " + " | ".join(row) + " |")
143:     return "\n".join(lines)
144: 
145: 
146: def _fmt_float(value: float | None, digits: int = 4) -> str:
147:     if value is None:
148:         return ""
149:     return f"{value:.{digits}f}"
150: 
151: 
152: class Plugin:
153:     def run(self, ctx) -> PluginResult:
154:         df = ctx.dataset_loader()
155:         if df.empty:
156:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
157: 
158:         columns_meta = []
159:         role_by_name: dict[str, str] = {}
160:         if ctx.dataset_version_id:
161:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
162:             if dataset_template and dataset_template.get("status") == "ready":
163:                 fields = ctx.storage.fetch_template_fields(
164:                     int(dataset_template["template_id"])
165:                 )
166:                 columns_meta = fields
167:                 role_by_name = {
168:                     field["name"]: (field.get("role") or "") for field in fields
169:                 }
170:             else:
171:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
172:                 role_by_name = {
173:                     col["original_name"]: (col.get("role") or "")
174:                     for col in columns_meta
175:                 }
176: 
177:         columns = list(df.columns)
178:         lower_names = {col: str(col).lower() for col in columns}
179:         used: set[str] = set()
180: 
181:         preferred_process = ctx.settings.get("process_column")
182:         process_candidates = _candidate_columns(
183:             columns,
184:             role_by_name,
185:             {"process", "activity", "event", "step", "task", "action"},
186:             ["process", "activity", "event", "step", "task", "action", "job"],
187:             lower_names,
188:             used,
189:         )
190:         process_col = None
191:         if preferred_process and preferred_process in columns:
192:             process_col = preferred_process
193:         else:
194:             process_col = _choose_best_process_column(process_candidates, df)
195:         if process_col:
196:             used.add(process_col)
197: 
198:         timestamp_col = _pick_column(
199:             ctx.settings.get("timestamp_column"),
200:             columns,
201:             role_by_name,
202:             {"timestamp"},
203:             ["timestamp", "time", "date"],
204:             lower_names,
205:             used,
206:         )
207:         if timestamp_col:
208:             used.add(timestamp_col)
209: 
210:         start_col = _pick_column(
211:             ctx.settings.get("start_column"),
212:             columns,
213:             role_by_name,
214:             {"start"},
215:             ["start", "begin", "queued", "enqueue"],
216:             lower_names,
217:             used,
218:         )
219:         if start_col:
220:             used.add(start_col)
221: 
222:         end_col = _pick_column(
223:             ctx.settings.get("end_column"),
224:             columns,
225:             role_by_name,
226:             {"end", "finish", "complete"},
227:             ["end", "finish", "complete", "stop", "dequeue"],
228:             lower_names,
229:             used,
230:         )
231:         if end_col:
232:             used.add(end_col)
233: 
234:         duration_col = _pick_column(
235:             ctx.settings.get("duration_column"),
236:             columns,
237:             role_by_name,
238:             {"duration", "latency", "elapsed", "runtime"},
239:             ["duration", "elapsed", "latency", "runtime", "seconds", "secs", "ms"],
240:             lower_names,
241:             used,
242:         )
243:         if duration_col:
244:             used.add(duration_col)
245: 
246:         server_col = _pick_column(
247:             ctx.settings.get("server_column"),
248:             columns,
249:             role_by_name,
250:             {"server", "host", "node", "instance"},
251:             ["server", "host", "node", "instance", "machine"],
252:             lower_names,
253:             used,
254:         )
255:         if server_col:
256:             used.add(server_col)
257: 
258:         if not process_col:
259:             return PluginResult(
260:                 "skipped", "No process/activity column detected", {}, [], [], None
261:             )
262: 
263:         base_timestamp_col = timestamp_col or start_col or end_col
264:         if not base_timestamp_col:
265:             return PluginResult(
266:                 "skipped", "No timestamp column detected", {}, [], [], None
267:             )
268: 
269:         work = df.copy()
270:         selected_cols: list[str] = []
271:         for col in [
272:             process_col,
273:             base_timestamp_col,
274:             duration_col,
275:             start_col,
276:             end_col,
277:             server_col,
278:         ]:
279:             if col and col in work.columns and col not in selected_cols:
280:                 selected_cols.append(col)
281:         work = work.loc[:, selected_cols]
282: 
283:         work["__timestamp"] = pd.to_datetime(
284:             work[base_timestamp_col], errors="coerce", utc=False
285:         )
286:         work = work.loc[work["__timestamp"].notna()].copy()
287:         if work.empty:
288:             return PluginResult(
289:                 "skipped", "No valid timestamps found", {}, [], [], None
290:             )
291: 
292:         duration_label = duration_col
293:         if duration_col and duration_col in work.columns:
294:             duration = pd.to_numeric(work[duration_col], errors="coerce")
295:             if duration.isna().all():
296:                 duration = pd.to_timedelta(
297:                     work[duration_col], errors="coerce"
298:                 ).dt.total_seconds()
299:         elif start_col and end_col and start_col in work.columns and end_col in work.columns:
300:             start_ts = pd.to_datetime(work[start_col], errors="coerce", utc=False)
301:             end_ts = pd.to_datetime(work[end_col], errors="coerce", utc=False)
302:             duration = (end_ts - start_ts).dt.total_seconds()
303:             duration_label = f"{start_col}->{end_col}"
304:         else:
305:             return PluginResult(
306:                 "skipped",
307:                 "No duration data available",
308:                 {},
309:                 [],
310:                 [],
311:                 None,
312:             )
313: 
314:         work["__duration"] = duration
315:         work = work.loc[work["__duration"].notna() & (work["__duration"] > 0)].copy()
316:         if work.empty:
317:             return PluginResult("skipped", "No valid durations found", {}, [], [], None)
318: 
319:         work["__process"] = work[process_col].astype(str).str.strip()
320:         work["__process_norm"] = work["__process"].str.lower()
321:         invalid = {"", "nan", "none", "null"}
322:         work = work.loc[~work["__process_norm"].isin(invalid)].copy()
323:         if work.empty:
324:             return PluginResult("skipped", "No valid process values", {}, [], [], None)
325: 
326:         close_start = int(ctx.settings.get("close_cycle_start_day", 20))
327:         close_end = int(ctx.settings.get("close_cycle_end_day", 5))
328:         min_close_count = int(ctx.settings.get("min_close_count", 200))
329:         min_open_count = int(ctx.settings.get("min_open_count", 200))
330:         min_days = int(ctx.settings.get("min_days", 20))
331:         min_slowdown_ratio = float(ctx.settings.get("min_slowdown_ratio", 1.2))
332:         min_effect_seconds = float(ctx.settings.get("min_effect_seconds", 1.0))
333:         max_samples = int(ctx.settings.get("max_samples", 5000))
334:         permutations = int(ctx.settings.get("permutations", 500))
335:         p_value_threshold = float(ctx.settings.get("p_value_threshold", 0.1))
336:         max_examples = int(ctx.settings.get("max_examples", 25))
337: 
338:         work["__day"] = work["__timestamp"].dt.day
339:         work["__date"] = work["__timestamp"].dt.date
340:         if close_start <= close_end:
341:             work["__close"] = (work["__day"] >= close_start) & (
342:                 work["__day"] <= close_end
343:             )
344:         else:
345:             work["__close"] = (work["__day"] >= close_start) | (
346:                 work["__day"] <= close_end
347:             )
348: 
349:         close = work.loc[work["__close"]].copy()
350:         open_rows = work.loc[~work["__close"]].copy()
351: 
352:         summary = {
353:             "process_column": process_col,
354:             "timestamp_column": base_timestamp_col,
355:             "duration_column": duration_label,
356:             "server_column": server_col,
357:             "close_cycle_start_day": close_start,
358:             "close_cycle_end_day": close_end,
359:             "min_close_count": min_close_count,
360:             "min_open_count": min_open_count,
361:             "min_days": min_days,
362:             "min_slowdown_ratio": min_slowdown_ratio,
363:             "min_effect_seconds": min_effect_seconds,
364:             "max_samples": max_samples,
365:             "permutations": permutations,
366:             "p_value_threshold": p_value_threshold,
367:         }
368: 
369:         evaluated: list[dict[str, object]] = []
370:         candidates: list[dict[str, object]] = []
371:         findings: list[dict[str, object]] = []
372: 
373:         if close.empty or open_rows.empty:
374:             return self._emit_results(ctx, summary, evaluated, candidates, findings, "No close-cycle rows found")
375: 
376:         process_labels = (
377:             work.groupby("__process_norm")["__process"]
378:             .agg(lambda series: series.value_counts().index[0])
379:             .to_dict()
380:         )
381:         close_days_by_process = (
382:             close.groupby("__process_norm")["__date"]
383:             .nunique()
384:             .to_dict()
385:         )
386: 
387:         all_processes = sorted(set(work["__process_norm"].unique()))
388: 
389:         for proc in all_processes:
390:             proc_close = close.loc[close["__process_norm"] == proc, "__duration"]
391:             proc_open = open_rows.loc[open_rows["__process_norm"] == proc, "__duration"]
392: 
393:             close_count = int(proc_close.shape[0])
394:             open_count = int(proc_open.shape[0])
395:             close_days_count = int(close_days_by_process.get(proc, 0))
396: 
397:             median_close = float(proc_close.median()) if close_count else None
398:             median_open = float(proc_open.median()) if open_count else None
399: 
400:             slowdown_ratio = None
401:             if median_open and median_open > 0 and median_close is not None:
402:                 slowdown_ratio = float(median_close / median_open)
403: 
404:             effect_seconds = None
405:             if median_close is not None and median_open is not None:
406:                 effect_seconds = float(median_close - median_open)
407: 
408:             reasons: list[str] = []
409:             if close_count < min_close_count:
410:                 reasons.append("close_count")
411:             if open_count < min_open_count:
412:                 reasons.append("open_count")
413:             if close_days_count < min_days:
414:                 reasons.append("close_days")
415:             if slowdown_ratio is None or slowdown_ratio < min_slowdown_ratio:
416:                 reasons.append("slowdown_ratio")
417:             if effect_seconds is None or effect_seconds < min_effect_seconds:
418:                 reasons.append("effect_seconds")
419: 
420:             # Permutation test (one-sided, close > open)
421:             p_value = 1.0
422:             if not reasons and permutations > 0:
423:                 close_sample = proc_close
424:                 open_sample = proc_open
425:                 if close_sample.shape[0] > max_samples:
426:                     close_sample = close_sample.sample(max_samples, random_state=ctx.run_seed)
427:                 if open_sample.shape[0] > max_samples:
428:                     open_sample = open_sample.sample(max_samples, random_state=ctx.run_seed)
429: 
430:                 close_arr = close_sample.to_numpy(dtype=float)
431:                 open_arr = open_sample.to_numpy(dtype=float)
432:                 combined = np.concatenate([close_arr, open_arr])
433:                 obs = float(np.median(close_arr) - np.median(open_arr))
434:                 if obs <= 0:
435:                     p_value = 1.0
436:                 else:
437:                     rng = np.random.default_rng(ctx.run_seed ^ (hash(proc) & 0xFFFFFFFF))
438:                     hits = 0
439:                     close_size = close_arr.shape[0]
440:                     for _ in range(permutations):
441:                         rng.shuffle(combined)
442:                         perm_close = combined[:close_size]
443:                         perm_open = combined[close_size:]
444:                         perm_diff = float(np.median(perm_close) - np.median(perm_open))
445:                         if perm_diff >= obs:
446:                             hits += 1
447:                     p_value = float((hits + 1) / (permutations + 1))
448:             if p_value > p_value_threshold:
449:                 reasons.append("p_value")
450: 
451:             passed = len(reasons) == 0
452:             reason_text = "ok" if passed else ",".join(reasons)
453: 
454:             evaluated.append(
455:                 {
456:                     "process": process_labels.get(proc, proc),
457:                     "process_norm": proc,
458:                     "close_count": close_count,
459:                     "open_count": open_count,
460:                     "close_days": close_days_count,
461:                     "median_close": median_close,
462:                     "median_open": median_open,
463:                     "slowdown_ratio": slowdown_ratio,
464:                     "effect_seconds": effect_seconds,
465:                     "p_value": p_value,
466:                     "passed": passed,
467:                     "reason": reason_text,
468:                 }
469:             )
470: 
471:             if not passed:
472:                 continue
473: 
474:             server_list: list[str] = []
475:             server_count = 0
476:             if server_col and server_col in work.columns:
477:                 servers = (
478:                     work.loc[work["__process_norm"] == proc, server_col]
479:                     .dropna()
480:                     .astype(str)
481:                     .str.strip()
482:                 )
483:                 if not servers.empty:
484:                     counter = Counter(servers)
485:                     server_list = [name for name, _ in counter.most_common(5)]
486:                     server_count = len(counter)
487: 
488:             row_ids = []
489:             for idx in close.loc[close["__process_norm"] == proc].index.tolist():
490:                 try:
491:                     row_ids.append(int(idx))
492:                 except (TypeError, ValueError):
493:                     continue
494:             row_ids = row_ids[:max_examples]
495: 
496:             process_label = process_labels.get(proc, proc)
497:             columns = [process_col, base_timestamp_col]
498:             if duration_col and duration_col in work.columns:
499:                 columns.append(duration_col)
500:             elif start_col and end_col:
501:                 columns.extend([start_col, end_col])
502:             if server_col:
503:                 columns.append(server_col)
504: 
505:             finding = {
506:                 "kind": "close_cycle_duration_shift",
507:                 "process": process_label,
508:                 "process_norm": proc,
509:                 "close_count": close_count,
510:                 "open_count": open_count,
511:                 "close_cycle_days": close_days_count,
512:                 "median_close": float(median_close) if median_close is not None else None,
513:                 "median_open": float(median_open) if median_open is not None else None,
514:                 "slowdown_ratio": float(slowdown_ratio) if slowdown_ratio is not None else None,
515:                 "effect_seconds": float(effect_seconds) if effect_seconds is not None else None,
516:                 "p_value": float(p_value),
517:                 "server_count": int(server_count),
518:                 "servers": server_list,
519:                 "columns": columns,
520:                 "row_ids": row_ids,
521:                 "query": f"process={process_label}",
522:             }
523:             findings.append(finding)
524:             candidates.append(
525:                 {
526:                     "process": process_label,
527:                     "process_norm": proc,
528:                     "close_count": close_count,
529:                     "open_count": open_count,
530:                     "close_cycle_days": close_days_count,
531:                     "median_close": float(median_close) if median_close is not None else None,
532:                     "median_open": float(median_open) if median_open is not None else None,
533:                     "slowdown_ratio": float(slowdown_ratio) if slowdown_ratio is not None else None,
534:                     "effect_seconds": float(effect_seconds) if effect_seconds is not None else None,
535:                     "p_value": float(p_value),
536:                 }
537:             )
538: 
539:         if not findings:
540:             return self._emit_results(ctx, summary, evaluated, candidates, findings, "No duration-shift candidates found")
541: 
542:         return self._emit_results(
543:             ctx,
544:             summary,
545:             evaluated,
546:             candidates,
547:             findings,
548:             "Detected duration-shift candidates",
549:         )
550: 
551:     def _emit_results(
552:         self,
553:         ctx,
554:         summary: dict[str, object],
555:         evaluated: list[dict[str, object]],
556:         candidates: list[dict[str, object]],
557:         findings: list[dict[str, object]],
558:         message: str,
559:     ) -> PluginResult:
560:         artifacts = []
561:         artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_duration_shift")
562:         out_path = artifacts_dir / "results.json"
563:         write_json(
564:             out_path,
565:             {
566:                 "summary": summary,
567:                 "candidates": candidates,
568:                 "evaluated": evaluated,
569:             },
570:         )
571:         artifacts.append(
572:             PluginArtifact(
573:                 path=str(out_path.relative_to(ctx.run_dir)),
574:                 type="json",
575:                 description="Close cycle duration shift candidates",
576:             )
577:         )
578: 
579:         csv_path = artifacts_dir / "results.csv"
580:         csv_headers = [
581:             "process",
582:             "close_count",
583:             "open_count",
584:             "close_days",
585:             "median_close",
586:             "median_open",
587:             "slowdown_ratio",
588:             "effect_seconds",
589:             "p_value",
590:             "reason",
591:         ]
592:         with csv_path.open("w", encoding="utf-8", newline="") as handle:
593:             writer = csv.writer(handle)
594:             writer.writerow(csv_headers)
595:             for entry in evaluated:
596:                 writer.writerow(
597:                     [
598:                         entry.get("process"),
599:                         entry.get("close_count"),
600:                         entry.get("open_count"),
601:                         entry.get("close_days"),
602:                         entry.get("median_close"),
603:                         entry.get("median_open"),
604:                         entry.get("slowdown_ratio"),
605:                         entry.get("effect_seconds"),
606:                         entry.get("p_value"),
607:                         entry.get("reason"),
608:                     ]
609:                 )
610:         artifacts.append(
611:             PluginArtifact(
612:                 path=str(csv_path.relative_to(ctx.run_dir)),
613:                 type="csv",
614:                 description="Duration-shift detail table",
615:             )
616:         )
617: 
618:         detected = [entry for entry in evaluated if entry.get("passed")]
619:         near = [entry for entry in evaluated if not entry.get("passed")]
620:         detected = sorted(detected, key=lambda r: float(r.get("slowdown_ratio") or 0), reverse=True)
621:         near = sorted(near, key=lambda r: float(r.get("slowdown_ratio") or 0), reverse=True)[:10]
622: 
623:         md_lines = [
624:             "# Close-cycle duration shift",
625:             "",
626:             "Summary:",
627:             f"- close_cycle_start_day: {summary.get('close_cycle_start_day')}",
628:             f"- close_cycle_end_day: {summary.get('close_cycle_end_day')}",
629:             f"- min_close_count: {summary.get('min_close_count')}",
630:             f"- min_open_count: {summary.get('min_open_count')}",
631:             f"- min_days: {summary.get('min_days')}",
632:             f"- min_slowdown_ratio: {summary.get('min_slowdown_ratio')}",
633:             f"- min_effect_seconds: {summary.get('min_effect_seconds')}",
634:             f"- max_samples: {summary.get('max_samples')}",
635:             f"- permutations: {summary.get('permutations')}",
636:             f"- p_value_threshold: {summary.get('p_value_threshold')}",
637:             "",
638:             "Detected:",
639:         ]
640: 
641:         headers = [
642:             "process",
643:             "median_close",
644:             "median_open",
645:             "ratio",
646:             "effect_s",
647:             "p",
648:             "close_count",
649:             "open_count",
650:         ]
651:         detected_rows = [
652:             [
653:                 str(row.get("process", "")),
654:                 _fmt_float(row.get("median_close")),
655:                 _fmt_float(row.get("median_open")),
656:                 _fmt_float(row.get("slowdown_ratio")),
657:                 _fmt_float(row.get("effect_seconds")),
658:                 _fmt_float(row.get("p_value"), 6),
659:                 str(row.get("close_count")),
660:                 str(row.get("open_count")),
661:             ]
662:             for row in detected
663:         ]
664:         md_lines.append(_markdown_table(headers, detected_rows) if detected_rows else "_None_")
665: 
666:         md_lines.extend(["", "Near misses (top ratios):"])
667:         near_rows = [
668:             [
669:                 str(row.get("process", "")),
670:                 _fmt_float(row.get("median_close")),
671:                 _fmt_float(row.get("median_open")),
672:                 _fmt_float(row.get("slowdown_ratio")),
673:                 _fmt_float(row.get("effect_seconds")),
674:                 _fmt_float(row.get("p_value"), 6),
675:                 str(row.get("close_count")),
676:                 str(row.get("open_count")),
677:                 str(row.get("reason", "")),
678:             ]
679:             for row in near
680:         ]
681:         near_headers = headers + ["reason"]
682:         md_lines.append(_markdown_table(near_headers, near_rows) if near_rows else "_None_")
683: 
684:         md_path = artifacts_dir / "results.md"
685:         md_path.write_text("\n".join(md_lines), encoding="utf-8")
686:         artifacts.append(
687:             PluginArtifact(
688:                 path=str(md_path.relative_to(ctx.run_dir)),
689:                 type="markdown",
690:                 description="Duration-shift summary",
691:             )
692:         )
693: 
694:         metrics = dict(summary)
695:         metrics["candidates"] = len(findings)
696:         return PluginResult("ok", message, metrics, findings, artifacts, None)
````

## File: plugins/analysis_close_cycle_duration_shift/plugin.yaml
````yaml
 1: id: analysis_close_cycle_duration_shift
 2: name: Close Cycle Duration Shift Analysis
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Detect per-process duration shifts between close and open cycles.
 9:   defaults:
10:     process_column: null
11:     timestamp_column: null
12:     start_column: null
13:     end_column: null
14:     duration_column: null
15:     server_column: null
16:     close_cycle_start_day: 20
17:     close_cycle_end_day: 5
18:     min_close_count: 200
19:     min_open_count: 200
20:     min_days: 20
21:     min_slowdown_ratio: 1.2
22:     min_effect_seconds: 1.0
23:     max_samples: 5000
24:     permutations: 500
25:     p_value_threshold: 0.1
26:     max_examples: 25
27: capabilities:
28:   - needs_eventlog
29:   - needs_timestamp
30: config_schema: config.schema.json
31: output_schema: output.schema.json
32: sandbox:
33:   no_network: true
34:   fs_allowlist:
35:   - appdata
36:   - plugins
37:   - run_dir
````

## File: plugins/analysis_close_cycle_revenue_compression/__init__.py
````python
1: """Revenue close-cycle compression model plugin."""
````

## File: plugins/analysis_close_cycle_revenue_compression/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "additionalProperties": true,
 4:   "properties": {
 5:     "artifacts": { "type": "array" },
 6:     "budget": {
 7:       "type": "object",
 8:       "additionalProperties": true,
 9:       "properties": {
10:         "cpu_limit_ms": { "anyOf": [{"type":"integer"},{"type":"null"}] },
11:         "notes": { "anyOf": [{"type":"string"},{"type":"null"}] },
12:         "row_limit": { "anyOf": [{"type":"integer"},{"type":"null"}] },
13:         "sampled": { "type": "boolean" },
14:         "time_limit_ms": { "anyOf": [{"type":"integer"},{"type":"null"}] }
15:       },
16:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"]
17:     },
18:     "error": { "type": ["object", "null"] },
19:     "findings": { "type": "array" },
20:     "metrics": { "type": "object" },
21:     "status": { "type": "string" },
22:     "summary": { "type": "string" }
23:   },
24:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"],
25:   "type": "object"
26: }
````

## File: plugins/analysis_close_cycle_revenue_compression/plugin.yaml
````yaml
 1: id: analysis_close_cycle_revenue_compression
 2: name: Close Cycle Revenue Compression
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Model capacity needed to compress revenue process close-window span to target days.
 9:   defaults:
10:     process_column: null
11:     start_column: null
12:     end_column: null
13:     duration_column: null
14:     queue_column: null
15:     eligible_column: null
16:     host_column: null
17:     revenue_selector_strategy: auto
18:     revenue_process_names: null
19:     revenue_process_patterns: null
20:     selector_min_coverage: 0.2
21:     selector_min_cardinality: 2
22:     selector_max_cardinality: 200
23:     selector_top_columns: 3
24:     selector_max_composite: 5000
25:     selector_min_close_rows: 100
26:     selector_min_close_share: 0.8
27:     selector_lift_threshold: 1.2
28:     selector_late_threshold: 0.7
29:     selector_min_late_fraction: 0.2
30:     selector_top_keys: 10
31:     close_window_mode: infer_or_default
32:     close_cycle_start_day: 20
33:     close_cycle_end_day: 5
34:     min_close_days: 5
35:     max_close_days: 20
36:     lookahead_days: 7
37:     min_close_confidence: 0.1
38:     min_close_data_ratio: 0.5
39:     target_days: 7
40:     max_scale: 10
41:     min_month_rows: 10
42:     max_months_output: 24
43:     max_wait_days: 30
44: capabilities:
45:   - needs_eventlog
46:   - needs_timestamp
47: config_schema: config.schema.json
48: output_schema: output.schema.json
49: sandbox:
50:   no_network: true
51:   fs_allowlist:
52:   - appdata
53:   - plugins
54:   - run_dir
````

## File: plugins/analysis_close_cycle_uplift/__init__.py
````python
1: """Close cycle uplift analysis plugin."""
````

## File: plugins/analysis_close_cycle_uplift/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "title": "Close Cycle Uplift Config",
 4:   "type": "object",
 5:   "additionalProperties": false,
 6:   "properties": {
 7:     "process_column": { "type": ["string", "null"] },
 8:     "timestamp_column": { "type": ["string", "null"] },
 9:     "start_column": { "type": ["string", "null"] },
10:     "end_column": { "type": ["string", "null"] },
11:     "duration_column": { "type": ["string", "null"] },
12:     "server_column": { "type": ["string", "null"] },
13:     "param_column": { "type": ["string", "null"] },
14:     "close_cycle_start_day": { "type": "integer", "minimum": 1, "maximum": 31 },
15:     "close_cycle_end_day": { "type": "integer", "minimum": 1, "maximum": 31 },
16:     "min_close_count": { "type": "integer", "minimum": 0 },
17:     "min_days": { "type": "integer", "minimum": 1 },
18:     "min_share_delta": { "type": "number", "minimum": 0, "maximum": 1 },
19:     "min_slowdown_ratio": { "type": "number", "minimum": 1 },
20:     "p_value_threshold": { "type": "number", "minimum": 0, "maximum": 1 },
21:     "max_examples": { "type": "integer", "minimum": 1 },
22:     "min_open_count": { "type": "integer", "minimum": 0 },
23:     "min_open_days": { "type": "integer", "minimum": 0 },
24:     "min_open_share": { "type": "number", "minimum": 0, "maximum": 1 },
25:     "min_open_ratio": { "type": "number", "minimum": 0, "maximum": 1 },
26:     "suppress_processes": {
27:       "type": ["array", "string", "null"],
28:       "items": { "type": "string" }
29:     },
30:     "suppress_process_regex": { "type": ["string", "null"] }
31:   }
32: }
````

## File: plugins/analysis_close_cycle_uplift/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "additionalProperties": true,
 4:   "properties": {
 5:     "artifacts": {
 6:       "type": "array"
 7:     },
 8:     "budget": {
 9:       "additionalProperties": true,
10:       "properties": {
11:         "cpu_limit_ms": {
12:           "anyOf": [
13:             { "type": "integer" },
14:             { "type": "null" }
15:           ]
16:         },
17:         "notes": {
18:           "anyOf": [
19:             { "type": "string" },
20:             { "type": "null" }
21:           ]
22:         },
23:         "row_limit": {
24:           "anyOf": [
25:             { "type": "integer" },
26:             { "type": "null" }
27:           ]
28:         },
29:         "sampled": {
30:           "type": "boolean"
31:         },
32:         "time_limit_ms": {
33:           "anyOf": [
34:             { "type": "integer" },
35:             { "type": "null" }
36:           ]
37:         }
38:       },
39:       "required": [
40:         "row_limit",
41:         "sampled",
42:         "time_limit_ms",
43:         "cpu_limit_ms"
44:       ],
45:       "type": "object"
46:     },
47:     "error": {
48:       "type": ["object", "null"]
49:     },
50:     "findings": {
51:       "type": "array"
52:     },
53:     "metrics": {
54:       "type": "object"
55:     },
56:     "status": {
57:       "type": "string"
58:     },
59:     "summary": {
60:       "type": "string"
61:     }
62:   },
63:   "required": [
64:     "status",
65:     "summary",
66:     "metrics",
67:     "findings",
68:     "artifacts",
69:     "budget",
70:     "error"
71:   ],
72:   "type": "object"
73: }
````

## File: plugins/analysis_close_cycle_uplift/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: import csv
  4: import math
  5: from collections import Counter
  6: from typing import Iterable
  7: 
  8: import pandas as pd
  9: 
 10: from statistic_harness.core.types import PluginArtifact, PluginResult
 11: from statistic_harness.core.utils import write_json
 12: 
 13: 
 14: def _markdown_table(headers: list[str], rows: list[list[str]]) -> str:
 15:     if not headers:
 16:         return ""
 17:     lines = []
 18:     lines.append("| " + " | ".join(headers) + " |")
 19:     lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
 20:     for row in rows:
 21:         lines.append("| " + " | ".join(row) + " |")
 22:     return "\n".join(lines)
 23: 
 24: 
 25: def _fmt_float(value: float | None, digits: int = 4) -> str:
 26:     if value is None:
 27:         return ""
 28:     return f"{value:.{digits}f}"
 29: 
 30: 
 31: def _pick_column(
 32:     preferred: str | None,
 33:     columns: list[str],
 34:     role_by_name: dict[str, str],
 35:     roles: set[str],
 36:     patterns: list[str],
 37:     lower_names: dict[str, str],
 38:     exclude: set[str],
 39: ) -> str | None:
 40:     if preferred and preferred in columns:
 41:         return preferred
 42:     for col in columns:
 43:         if col in exclude:
 44:             continue
 45:         if role_by_name.get(col) in roles:
 46:             return col
 47:     for col in columns:
 48:         if col in exclude:
 49:             continue
 50:         name = lower_names[col]
 51:         if any(pattern in name for pattern in patterns):
 52:             return col
 53:     return None
 54: 
 55: 
 56: def _candidate_columns(
 57:     columns: list[str],
 58:     role_by_name: dict[str, str],
 59:     roles: set[str],
 60:     patterns: list[str],
 61:     lower_names: dict[str, str],
 62:     exclude: set[str],
 63: ) -> list[str]:
 64:     candidates: list[str] = []
 65:     for col in columns:
 66:         if col in exclude:
 67:             continue
 68:         if role_by_name.get(col) in roles:
 69:             candidates.append(col)
 70:     for col in columns:
 71:         if col in exclude or col in candidates:
 72:             continue
 73:         name = lower_names[col]
 74:         if any(pattern in name for pattern in patterns):
 75:             candidates.append(col)
 76:     return candidates
 77: 
 78: 
 79: def _score_process_column(name: str, series: pd.Series) -> float:
 80:     score = 0.0
 81:     lower_name = name.lower()
 82:     if lower_name in {"process", "process_id"}:
 83:         score += 3.0
 84:     if lower_name.endswith("_id") or lower_name.endswith("id"):
 85:         score += 1.5
 86:     for token in (
 87:         "queue",
 88:         "status",
 89:         "step",
 90:         "parent",
 91:         "child",
 92:         "hold",
 93:         "lock",
 94:         "schedule",
 95:         "master",
 96:         "dep",
 97:         "ext",
 98:         "attempt",
 99:         "priority",
100:     ):
101:         if token in lower_name:
102:             score -= 2.0
103: 
104:     sample = series.dropna()
105:     if sample.empty:
106:         return score - 5.0
107:     if sample.shape[0] > 5000:
108:         sample = sample.sample(5000, random_state=0)
109: 
110:     if pd.api.types.is_numeric_dtype(sample):
111:         score -= 1.5
112:     else:
113:         score += 1.5
114: 
115:     sample_str = sample.astype(str).str.strip()
116:     if not pd.api.types.is_numeric_dtype(sample):
117:         numeric_like = sample_str.str.match(r"^\\d+(\\.\\d+)?$").mean()
118:         if numeric_like > 0.8:
119:             score -= 2.0
120: 
121:     unique_ratio = sample.nunique(dropna=True) / max(1, sample.shape[0])
122:     score += (1.0 - unique_ratio) * 4.0
123:     if unique_ratio > 0.9:
124:         score -= 2.0
125: 
126:     lengths = sample_str.str.len()
127:     median_len = float(lengths.median()) if not lengths.empty else 0.0
128:     if 3 <= median_len <= 20:
129:         score += 0.5
130:     elif median_len > 40:
131:         score -= 0.5
132: 
133:     return score
134: 
135: 
136: def _choose_best_process_column(
137:     candidates: Iterable[str], df: pd.DataFrame
138: ) -> str | None:
139:     candidates = list(candidates)
140:     if not candidates:
141:         return None
142:     if len(candidates) == 1:
143:         return candidates[0]
144:     scored = []
145:     for col in candidates:
146:         scored.append((_score_process_column(str(col), df[col]), col))
147:     scored.sort(reverse=True, key=lambda item: item[0])
148:     return scored[0][1]
149: 
150: 
151: class Plugin:
152:     def run(self, ctx) -> PluginResult:
153:         df = ctx.dataset_loader()
154:         if df.empty:
155:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
156: 
157:         columns_meta = []
158:         role_by_name: dict[str, str] = {}
159:         if ctx.dataset_version_id:
160:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
161:             if dataset_template and dataset_template.get("status") == "ready":
162:                 fields = ctx.storage.fetch_template_fields(
163:                     int(dataset_template["template_id"])
164:                 )
165:                 columns_meta = fields
166:                 role_by_name = {
167:                     field["name"]: (field.get("role") or "") for field in fields
168:                 }
169:             else:
170:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
171:                 role_by_name = {
172:                     col["original_name"]: (col.get("role") or "")
173:                     for col in columns_meta
174:                 }
175: 
176:         columns = list(df.columns)
177:         lower_names = {col: str(col).lower() for col in columns}
178:         used: set[str] = set()
179: 
180:         preferred_process = ctx.settings.get("process_column")
181:         process_candidates = _candidate_columns(
182:             columns,
183:             role_by_name,
184:             {"process", "activity", "event", "step", "task", "action"},
185:             ["process", "activity", "event", "step", "task", "action", "job"],
186:             lower_names,
187:             used,
188:         )
189:         process_col = None
190:         if preferred_process and preferred_process in columns:
191:             process_col = preferred_process
192:         else:
193:             process_col = _choose_best_process_column(process_candidates, df)
194:         if process_col:
195:             used.add(process_col)
196: 
197:         timestamp_col = _pick_column(
198:             ctx.settings.get("timestamp_column"),
199:             columns,
200:             role_by_name,
201:             {"timestamp"},
202:             ["timestamp", "time", "date"],
203:             lower_names,
204:             used,
205:         )
206:         if timestamp_col:
207:             used.add(timestamp_col)
208: 
209:         start_col = _pick_column(
210:             ctx.settings.get("start_column"),
211:             columns,
212:             role_by_name,
213:             {"start"},
214:             ["start", "begin", "queued", "enqueue"],
215:             lower_names,
216:             used,
217:         )
218:         if start_col:
219:             used.add(start_col)
220: 
221:         end_col = _pick_column(
222:             ctx.settings.get("end_column"),
223:             columns,
224:             role_by_name,
225:             {"end", "finish", "complete"},
226:             ["end", "finish", "complete", "stop", "dequeue"],
227:             lower_names,
228:             used,
229:         )
230:         if end_col:
231:             used.add(end_col)
232: 
233:         duration_col = _pick_column(
234:             ctx.settings.get("duration_column"),
235:             columns,
236:             role_by_name,
237:             {"duration", "latency", "elapsed", "runtime"},
238:             ["duration", "elapsed", "latency", "runtime", "seconds", "secs", "ms"],
239:             lower_names,
240:             used,
241:         )
242:         if duration_col:
243:             used.add(duration_col)
244: 
245:         server_col = _pick_column(
246:             ctx.settings.get("server_column"),
247:             columns,
248:             role_by_name,
249:             {"server", "host", "node", "instance"},
250:             ["server", "host", "node", "instance", "machine"],
251:             lower_names,
252:             used,
253:         )
254:         if server_col:
255:             used.add(server_col)
256: 
257:         param_col = _pick_column(
258:             ctx.settings.get("param_column"),
259:             columns,
260:             role_by_name,
261:             {"parameter", "params", "config", "meta"},
262:             ["param", "parameter", "params", "config", "meta"],
263:             lower_names,
264:             used,
265:         )
266: 
267:         if not process_col:
268:             return PluginResult(
269:                 "skipped", "No process/activity column detected", {}, [], [], None
270:             )
271: 
272:         base_timestamp_col = timestamp_col or start_col or end_col
273:         if not base_timestamp_col:
274:             return PluginResult(
275:                 "skipped", "No timestamp column detected", {}, [], [], None
276:             )
277: 
278:         work = df.copy()
279:         selected_cols: list[str] = []
280:         for col in [
281:             process_col,
282:             base_timestamp_col,
283:             duration_col,
284:             start_col,
285:             end_col,
286:             server_col,
287:             param_col,
288:         ]:
289:             if col and col in work.columns and col not in selected_cols:
290:                 selected_cols.append(col)
291:         work = work.loc[:, selected_cols]
292: 
293:         work["__timestamp"] = pd.to_datetime(
294:             work[base_timestamp_col], errors="coerce", utc=False
295:         )
296:         work = work.loc[work["__timestamp"].notna()].copy()
297:         if work.empty:
298:             return PluginResult(
299:                 "skipped", "No valid timestamps found", {}, [], [], None
300:             )
301: 
302:         duration_label = duration_col
303:         if duration_col and duration_col in work.columns:
304:             duration = pd.to_numeric(work[duration_col], errors="coerce")
305:             if duration.isna().all():
306:                 duration = pd.to_timedelta(
307:                     work[duration_col], errors="coerce"
308:                 ).dt.total_seconds()
309:         elif start_col and end_col and start_col in work.columns and end_col in work.columns:
310:             start_ts = pd.to_datetime(work[start_col], errors="coerce", utc=False)
311:             end_ts = pd.to_datetime(work[end_col], errors="coerce", utc=False)
312:             duration = (end_ts - start_ts).dt.total_seconds()
313:             duration_label = f"{start_col}->{end_col}"
314:         else:
315:             return PluginResult(
316:                 "skipped",
317:                 "No duration data available",
318:                 {},
319:                 [],
320:                 [],
321:                 None,
322:             )
323: 
324:         work["__duration"] = duration
325:         work = work.loc[work["__duration"].notna() & (work["__duration"] > 0)].copy()
326:         if work.empty:
327:             return PluginResult("skipped", "No valid durations found", {}, [], [], None)
328: 
329:         work["__process"] = work[process_col].astype(str).str.strip()
330:         work["__process_norm"] = work["__process"].str.lower()
331:         invalid = {"", "nan", "none", "null"}
332:         work = work.loc[~work["__process_norm"].isin(invalid)].copy()
333:         if work.empty:
334:             return PluginResult("skipped", "No valid process values", {}, [], [], None)
335: 
336:         close_start = int(ctx.settings.get("close_cycle_start_day", 20))
337:         close_end = int(ctx.settings.get("close_cycle_end_day", 5))
338:         min_close_count = int(ctx.settings.get("min_close_count", 200))
339:         min_days = int(ctx.settings.get("min_days", 20))
340:         min_share_delta = float(ctx.settings.get("min_share_delta", 0.002))
341:         min_slowdown_ratio = float(ctx.settings.get("min_slowdown_ratio", 1.05))
342:         p_value_threshold = float(ctx.settings.get("p_value_threshold", 0.1))
343:         max_examples = int(ctx.settings.get("max_examples", 25))
344:         min_open_count = int(ctx.settings.get("min_open_count", 200))
345:         min_open_days = int(ctx.settings.get("min_open_days", 10))
346:         min_open_share = float(ctx.settings.get("min_open_share", 0.002))
347:         min_open_ratio = float(ctx.settings.get("min_open_ratio", 0.02))
348:         suppress_list = ctx.settings.get("suppress_processes") or []
349:         if isinstance(suppress_list, str):
350:             suppress_list = [
351:                 item.strip()
352:                 for item in suppress_list.split(",")
353:                 if item.strip()
354:             ]
355:         suppress_set = {str(item).strip().lower() for item in suppress_list if str(item).strip()}
356:         suppress_regex = ctx.settings.get("suppress_process_regex")
357:         suppress_pattern = None
358:         if isinstance(suppress_regex, str) and suppress_regex.strip():
359:             import re
360: 
361:             suppress_pattern = re.compile(suppress_regex, re.IGNORECASE)
362: 
363:         work["__day"] = work["__timestamp"].dt.day
364:         work["__date"] = work["__timestamp"].dt.date
365:         if close_start <= close_end:
366:             work["__close"] = (work["__day"] >= close_start) & (
367:                 work["__day"] <= close_end
368:             )
369:         else:
370:             work["__close"] = (work["__day"] >= close_start) | (
371:                 work["__day"] <= close_end
372:             )
373: 
374:         close = work.loc[work["__close"]].copy()
375:         open_rows = work.loc[~work["__close"]].copy()
376: 
377:         def emit_results(
378:             summary: dict[str, object],
379:             evaluated: list[dict[str, object]],
380:             candidates: list[dict[str, object]],
381:             findings: list[dict[str, object]],
382:             message: str,
383:         ) -> PluginResult:
384:             artifacts = []
385:             artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_uplift")
386:             out_path = artifacts_dir / "results.json"
387:             write_json(
388:                 out_path,
389:                 {
390:                     "summary": summary,
391:                     "candidates": candidates,
392:                     "evaluated": evaluated,
393:                 },
394:             )
395:             artifacts.append(
396:                 PluginArtifact(
397:                     path=str(out_path.relative_to(ctx.run_dir)),
398:                     type="json",
399:                     description="Close cycle share-shift candidates",
400:                 )
401:             )
402: 
403:             csv_path = artifacts_dir / "results.csv"
404:             csv_headers = [
405:                 "process",
406:                 "close_count",
407:                 "open_count",
408:                 "open_days",
409:                 "open_ratio",
410:                 "close_days",
411:                 "close_share",
412:                 "open_share",
413:                 "share_delta",
414:                 "z_score",
415:                 "p_value",
416:                 "slowdown_ratio",
417:                 "suppressed",
418:                 "suppression_reason",
419:                 "reason",
420:             ]
421:             with csv_path.open("w", encoding="utf-8", newline="") as handle:
422:                 writer = csv.writer(handle)
423:                 writer.writerow(csv_headers)
424:                 for entry in evaluated:
425:                     writer.writerow(
426:                         [
427:                             entry.get("process"),
428:                             entry.get("close_count"),
429:                             entry.get("open_count"),
430:                             entry.get("open_days"),
431:                             entry.get("open_ratio"),
432:                             entry.get("close_days"),
433:                             entry.get("close_share"),
434:                             entry.get("open_share"),
435:                             entry.get("share_delta"),
436:                             entry.get("z_score"),
437:                             entry.get("p_value"),
438:                             entry.get("slowdown_ratio"),
439:                             entry.get("suppressed"),
440:                             entry.get("suppression_reason"),
441:                             entry.get("reason"),
442:                         ]
443:                     )
444:             artifacts.append(
445:                 PluginArtifact(
446:                     path=str(csv_path.relative_to(ctx.run_dir)),
447:                     type="csv",
448:                     description="Share-shift detail table",
449:                 )
450:             )
451: 
452:             detected = [
453:                 entry for entry in evaluated if entry.get("passed") and not entry.get("suppressed")
454:             ]
455:             suppressed = [entry for entry in evaluated if entry.get("suppressed")]
456:             near = [
457:                 entry for entry in evaluated if (not entry.get("passed")) and not entry.get("suppressed")
458:             ]
459:             detected = sorted(detected, key=lambda r: float(r.get("share_delta", 0)), reverse=True)
460:             near = sorted(near, key=lambda r: float(r.get("share_delta", 0)), reverse=True)[:10]
461:             suppressed = sorted(
462:                 suppressed, key=lambda r: float(r.get("share_delta", 0)), reverse=True
463:             )[:10]
464: 
465:             md_lines = [
466:                 "# Close-cycle share shift",
467:                 "",
468:                 "Summary:",
469:                 f"- close_cycle_start_day: {summary.get('close_cycle_start_day')}",
470:                 f"- close_cycle_end_day: {summary.get('close_cycle_end_day')}",
471:                 f"- min_close_count: {summary.get('min_close_count')}",
472:                 f"- min_days: {summary.get('min_days')}",
473:                 f"- min_share_delta: {summary.get('min_share_delta')}",
474:                 f"- min_slowdown_ratio: {summary.get('min_slowdown_ratio')}",
475:                 f"- p_value_threshold: {summary.get('p_value_threshold')}",
476:                 f"- min_open_count: {summary.get('min_open_count')}",
477:                 f"- min_open_days: {summary.get('min_open_days')}",
478:                 f"- min_open_share: {summary.get('min_open_share')}",
479:                 f"- min_open_ratio: {summary.get('min_open_ratio')}",
480:                 f"- suppress_processes: {summary.get('suppress_processes')}",
481:                 f"- suppress_process_regex: {summary.get('suppress_process_regex')}",
482:                 f"- median_close: {summary.get('median_close')}",
483:                 f"- median_open: {summary.get('median_open')}",
484:                 f"- slowdown_ratio: {summary.get('slowdown_ratio')}",
485:                 "",
486:                 "Detected:",
487:             ]
488: 
489:             headers = [
490:                 "process",
491:                 "close_share",
492:                 "open_share",
493:                 "delta",
494:                 "z",
495:                 "p",
496:                 "close_count",
497:                 "open_count",
498:             ]
499:             detected_rows = [
500:                 [
501:                     str(row.get("process", "")),
502:                     _fmt_float(row.get("close_share")),
503:                     _fmt_float(row.get("open_share")),
504:                     _fmt_float(row.get("share_delta")),
505:                     _fmt_float(row.get("z_score"), 2),
506:                     _fmt_float(row.get("p_value"), 6),
507:                     str(row.get("close_count")),
508:                     str(row.get("open_count")),
509:                 ]
510:                 for row in detected
511:             ]
512:             md_lines.append(_markdown_table(headers, detected_rows) if detected_rows else "_None_")
513: 
514:             md_lines.extend(["", "Detected reason summary:"])
515:             if detected:
516:                 slowdown_ratio = summary.get("slowdown_ratio")
517:                 min_slowdown_ratio = summary.get("min_slowdown_ratio")
518:                 min_open_ratio = summary.get("min_open_ratio")
519:                 min_open_count = summary.get("min_open_count")
520:                 min_open_days = summary.get("min_open_days")
521:                 min_close_count = summary.get("min_close_count")
522:                 min_close_days = summary.get("min_days")
523:                 for row in detected:
524:                     md_lines.append(
525:                         "- "
526:                         + f"{row.get('process')}: "
527:                         + f"close_share { _fmt_float(row.get('close_share')) } "
528:                         + f"vs open_share { _fmt_float(row.get('open_share')) } "
529:                         + f"(delta { _fmt_float(row.get('share_delta')) }); "
530:                         + f"z { _fmt_float(row.get('z_score'), 2) }, "
531:                         + f"p { _fmt_float(row.get('p_value'), 6) }; "
532:                         + f"open_ratio { _fmt_float(row.get('open_ratio'), 3) } "
533:                         + f">= { _fmt_float(min_open_ratio, 3) }; "
534:                         + f"open_count {row.get('open_count')} >= {min_open_count}, "
535:                         + f"open_days {row.get('open_days')} >= {min_open_days}; "
536:                         + f"close_count {row.get('close_count')} >= {min_close_count}, "
537:                         + f"close_days {row.get('close_days')} >= {min_close_days}; "
538:                         + f"global_slowdown_ratio { _fmt_float(slowdown_ratio, 3) } "
539:                         + f">= { _fmt_float(min_slowdown_ratio, 3) }"
540:                     )
541:             else:
542:                 md_lines.append("_None_")
543: 
544:             md_lines.extend(["", "Suppressed (expected close-only or manual):"])
545:             suppressed_rows = [
546:                 [
547:                     str(row.get("process", "")),
548:                     _fmt_float(row.get("close_share")),
549:                     _fmt_float(row.get("open_share")),
550:                     _fmt_float(row.get("share_delta")),
551:                     _fmt_float(row.get("z_score"), 2),
552:                     _fmt_float(row.get("p_value"), 6),
553:                     str(row.get("close_count")),
554:                     str(row.get("open_count")),
555:                     str(row.get("suppression_reason", "")),
556:                 ]
557:                 for row in suppressed
558:             ]
559:             suppressed_headers = headers + ["suppression_reason"]
560:             md_lines.append(
561:                 _markdown_table(suppressed_headers, suppressed_rows)
562:                 if suppressed_rows
563:                 else "_None_"
564:             )
565: 
566:             md_lines.extend(["", "Near misses (top share deltas):"])
567:             near_rows = [
568:                 [
569:                     str(row.get("process", "")),
570:                     _fmt_float(row.get("close_share")),
571:                     _fmt_float(row.get("open_share")),
572:                     _fmt_float(row.get("share_delta")),
573:                     _fmt_float(row.get("z_score"), 2),
574:                     _fmt_float(row.get("p_value"), 6),
575:                     str(row.get("close_count")),
576:                     str(row.get("open_count")),
577:                     str(row.get("reason", "")),
578:                 ]
579:                 for row in near
580:             ]
581:             near_headers = headers + ["reason"]
582:             md_lines.append(_markdown_table(near_headers, near_rows) if near_rows else "_None_")
583: 
584:             md_path = artifacts_dir / "results.md"
585:             md_path.write_text("\n".join(md_lines), encoding="utf-8")
586:             artifacts.append(
587:                 PluginArtifact(
588:                     path=str(md_path.relative_to(ctx.run_dir)),
589:                     type="markdown",
590:                     description="Share-shift summary",
591:                 )
592:             )
593: 
594:             metrics = dict(summary)
595:             metrics["candidates"] = len(findings)
596:             return PluginResult("ok", message, metrics, findings, artifacts, None)
597: 
598:         summary = {
599:             "process_column": process_col,
600:             "timestamp_column": base_timestamp_col,
601:             "duration_column": duration_label,
602:             "server_column": server_col,
603:             "param_column": param_col,
604:             "close_cycle_start_day": close_start,
605:             "close_cycle_end_day": close_end,
606:             "min_close_count": min_close_count,
607:             "min_days": min_days,
608:             "min_share_delta": min_share_delta,
609:             "min_slowdown_ratio": min_slowdown_ratio,
610:             "p_value_threshold": p_value_threshold,
611:             "min_open_count": min_open_count,
612:             "min_open_days": min_open_days,
613:             "min_open_share": min_open_share,
614:             "min_open_ratio": min_open_ratio,
615:             "suppress_processes": sorted(suppress_set),
616:             "suppress_process_regex": suppress_regex,
617:         }
618: 
619:         evaluated: list[dict[str, object]] = []
620:         candidates: list[dict[str, object]] = []
621:         findings: list[dict[str, object]] = []
622: 
623:         if close.empty or open_rows.empty:
624:             summary.update(
625:                 {
626:                     "median_close": None,
627:                     "median_open": None,
628:                     "slowdown_ratio": None,
629:                 }
630:             )
631:             return emit_results(
632:                 summary,
633:                 evaluated,
634:                 candidates,
635:                 findings,
636:                 "No close-cycle rows found",
637:             )
638: 
639:         median_close = float(close["__duration"].median())
640:         median_open = float(open_rows["__duration"].median())
641:         slowdown_ratio = None
642:         if median_open > 0:
643:             slowdown_ratio = float(median_close / median_open)
644:         summary.update(
645:             {
646:                 "median_close": median_close,
647:                 "median_open": median_open,
648:                 "slowdown_ratio": slowdown_ratio,
649:             }
650:         )
651: 
652:         process_labels = (
653:             close.groupby("__process_norm")["__process"]
654:             .agg(lambda series: series.value_counts().index[0])
655:             .to_dict()
656:         )
657: 
658:         close_counts = close["__process_norm"].value_counts()
659:         open_counts = open_rows["__process_norm"].value_counts()
660:         total_close = int(close_counts.sum())
661:         total_open = int(open_counts.sum())
662: 
663:         close_days_by_process = (
664:             close.groupby("__process_norm")["__date"]
665:             .nunique()
666:             .to_dict()
667:         )
668:         open_days_by_process = (
669:             open_rows.groupby("__process_norm")["__date"]
670:             .nunique()
671:             .to_dict()
672:         )
673: 
674:         all_processes = sorted(set(close_counts.index).union(set(open_counts.index)))
675:         global_ok = slowdown_ratio is not None and slowdown_ratio >= min_slowdown_ratio
676: 
677:         for proc in all_processes:
678:             close_count = int(close_counts.get(proc, 0))
679:             open_count = int(open_counts.get(proc, 0))
680:             close_days_count = int(close_days_by_process.get(proc, 0))
681:             open_days_count = int(open_days_by_process.get(proc, 0))
682: 
683:             p_close = close_count / total_close if total_close else 0.0
684:             p_open = open_count / total_open if total_open else 0.0
685:             share_delta = p_close - p_open
686:             open_ratio = (
687:                 open_count / (open_count + close_count)
688:                 if (open_count + close_count) > 0
689:                 else 0.0
690:             )
691: 
692:             pooled = (close_count + open_count) / (
693:                 total_close + total_open
694:             ) if (total_close + total_open) else 0.0
695:             se = math.sqrt(
696:                 pooled
697:                 * (1.0 - pooled)
698:                 * ((1.0 / total_close) + (1.0 / total_open))
699:             ) if pooled > 0 and total_close > 0 and total_open > 0 else 0.0
700:             z_score = (share_delta / se) if se > 0 else 0.0
701:             p_value = 0.5 * math.erfc(z_score / math.sqrt(2.0)) if se > 0 else 1.0
702: 
703:             reasons: list[str] = []
704:             if not global_ok:
705:                 reasons.append("global_slowdown")
706:             if close_count < min_close_count:
707:                 reasons.append("close_count")
708:             if close_days_count < min_days:
709:                 reasons.append("close_days")
710:             if share_delta < min_share_delta:
711:                 reasons.append("share_delta")
712:             if p_value > p_value_threshold:
713:                 reasons.append("p_value")
714: 
715:             suppression_reasons: list[str] = []
716:             if proc in suppress_set:
717:                 suppression_reasons.append("manual")
718:             process_label = process_labels.get(proc, proc)
719:             if suppress_pattern and suppress_pattern.search(process_label):
720:                 suppression_reasons.append("manual_regex")
721:             if open_count < min_open_count:
722:                 suppression_reasons.append("open_count")
723:             if open_days_count < min_open_days:
724:                 suppression_reasons.append("open_days")
725:             if p_open < min_open_share:
726:                 suppression_reasons.append("open_share")
727:             if open_ratio < min_open_ratio:
728:                 suppression_reasons.append("open_ratio")
729: 
730:             suppressed = len(suppression_reasons) > 0
731:             passed = len(reasons) == 0 and not suppressed
732:             reason_text = "ok" if len(reasons) == 0 else ",".join(reasons)
733:             suppression_text = (
734:                 ""
735:                 if not suppressed
736:                 else ",".join(sorted(set(suppression_reasons)))
737:             )
738: 
739:             evaluated.append(
740:                 {
741:                     "process": process_label,
742:                     "process_norm": proc,
743:                     "close_count": close_count,
744:                     "open_count": open_count,
745:                     "open_days": open_days_count,
746:                     "open_ratio": float(open_ratio),
747:                     "close_days": close_days_count,
748:                     "close_share": float(p_close),
749:                     "open_share": float(p_open),
750:                     "share_delta": float(share_delta),
751:                     "z_score": float(z_score),
752:                     "p_value": float(p_value),
753:                     "slowdown_ratio": slowdown_ratio,
754:                     "passed": passed,
755:                     "suppressed": suppressed,
756:                     "suppression_reason": suppression_text,
757:                     "reason": reason_text,
758:                 }
759:             )
760: 
761:             if not passed:
762:                 continue
763: 
764:             expected_close = p_open * total_close
765:             excess_close = close_count - expected_close
766: 
767:             server_list: list[str] = []
768:             server_count = 0
769:             if server_col and server_col in close.columns:
770:                 servers = (
771:                     close.loc[close["__process_norm"] == proc, server_col]
772:                     .dropna()
773:                     .astype(str)
774:                     .str.strip()
775:                 )
776:                 if not servers.empty:
777:                     counter = Counter(servers)
778:                     server_list = [name for name, _ in counter.most_common(5)]
779:                     server_count = len(counter)
780: 
781:             row_ids = []
782:             for idx in close.loc[close["__process_norm"] == proc].index.tolist():
783:                 try:
784:                     row_ids.append(int(idx))
785:                 except (TypeError, ValueError):
786:                     continue
787:             row_ids = row_ids[:max_examples]
788: 
789:             process_label = process_labels.get(proc, proc)
790:             columns = [process_col, base_timestamp_col]
791:             if duration_col and duration_col in work.columns:
792:                 columns.append(duration_col)
793:             elif start_col and end_col:
794:                 columns.extend([start_col, end_col])
795:             if server_col:
796:                 columns.append(server_col)
797:             if param_col:
798:                 columns.append(param_col)
799: 
800:             finding = {
801:                 "kind": "close_cycle_share_shift",
802:                 "process": process_label,
803:                 "process_norm": proc,
804:                 "close_count": close_count,
805:                 "open_count": open_count,
806:                 "close_cycle_days": close_days_count,
807:                 "close_share": float(p_close),
808:                 "open_share": float(p_open),
809:                 "share_delta": float(share_delta),
810:                 "expected_close": float(expected_close),
811:                 "excess_close": float(excess_close),
812:                 "z_score": float(z_score),
813:                 "p_value": float(p_value),
814:                 "median_close": float(median_close),
815:                 "median_open": float(median_open),
816:                 "slowdown_ratio": float(slowdown_ratio),
817:                 "server_count": int(server_count),
818:                 "servers": server_list,
819:                 "columns": columns,
820:                 "row_ids": row_ids,
821:                 "query": f"process={process_label}",
822:             }
823:             findings.append(finding)
824:             candidates.append(
825:                 {
826:                     "process": process_label,
827:                     "process_norm": proc,
828:                     "close_count": close_count,
829:                     "open_count": open_count,
830:                     "close_cycle_days": close_days_count,
831:                     "share_delta": float(share_delta),
832:                     "z_score": float(z_score),
833:                     "p_value": float(p_value),
834:                 }
835:             )
836: 
837:         if not findings:
838:             return emit_results(
839:                 summary,
840:                 evaluated,
841:                 candidates,
842:                 findings,
843:                 "No close-cycle share-shift candidates found",
844:             )
845: 
846:         return emit_results(
847:             summary,
848:             evaluated,
849:             candidates,
850:             findings,
851:             "Detected close-cycle share-shift candidates",
852:         )
````

## File: plugins/analysis_close_cycle_uplift/plugin.yaml
````yaml
 1: id: analysis_close_cycle_uplift
 2: name: Close Cycle Uplift Analysis
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Detect close-cycle slowdowns using share-shift statistics.
 9:   defaults:
10:     process_column: null
11:     timestamp_column: null
12:     start_column: null
13:     end_column: null
14:     duration_column: null
15:     server_column: null
16:     param_column: null
17:     close_cycle_start_day: 20
18:     close_cycle_end_day: 5
19:     min_close_count: 200
20:     min_days: 20
21:     min_share_delta: 0.002
22:     min_slowdown_ratio: 1.05
23:     p_value_threshold: 0.1
24:     max_examples: 25
25:     min_open_count: 200
26:     min_open_days: 10
27:     min_open_share: 0.002
28:     min_open_ratio: 0.02
29:     suppress_processes: []
30:     suppress_process_regex: null
31: capabilities:
32:   - needs_eventlog
33:   - needs_timestamp
34: config_schema: config.schema.json
35: output_schema: output.schema.json
36: sandbox:
37:   no_network: true
38:   fs_allowlist:
39:   - appdata
40:   - plugins
41:   - run_dir
````

## File: plugins/analysis_concurrency_reconstruction/__init__.py
````python
1: 
````

## File: plugins/analysis_concurrency_reconstruction/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "start_column": {"type": ["string", "null"]},
 7:     "end_column": {"type": ["string", "null"]},
 8:     "host_column": {"type": ["string", "null"]},
 9:     "capacity_column": {"type": ["string", "null"]},
10:     "capacity_limit": {"type": ["number", "null"]},
11:     "max_hosts": {"type": "integer", "minimum": 1},
12:     "max_examples": {"type": "integer", "minimum": 0}
13:   }
14: }
````

## File: plugins/analysis_concurrency_reconstruction/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "status": {"type": "string"},
 7:     "summary": {"type": "string"},
 8:     "metrics": {"type": "object"},
 9:     "findings": {"type": "array"},
10:     "artifacts": {"type": "array"},
11:     "budget": {
12:       "type": "object",
13:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
14:       "properties": {
15:         "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
16:         "sampled": {"type": "boolean"},
17:         "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
18:         "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
19:         "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
20:       },
21:       "additionalProperties": true
22:     },
23:     "error": {"type": ["object", "null"]}
24:   },
25:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"]
26: }
````

## File: plugins/analysis_concurrency_reconstruction/plugin.yaml
````yaml
 1: id: analysis_concurrency_reconstruction
 2: name: Concurrency Reconstruction
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Reconstruct concurrent execution levels from start/end timestamps.
 9:   defaults:
10:     start_column: null
11:     end_column: null
12:     host_column: null
13:     capacity_column: null
14:     capacity_limit: null
15:     max_hosts: 10
16:     max_examples: 25
17: capabilities:
18:   - needs_eventlog
19:   - needs_timestamp
20: config_schema: config.schema.json
21: output_schema: output.schema.json
22: sandbox:
23:   no_network: true
24:   fs_allowlist:
25:     - appdata
26:     - plugins
27:     - run_dir
````

## File: plugins/analysis_concurrency_reconstruction/README.md
````markdown
1: # Concurrency Reconstruction
2: 
3: Reconstructs concurrent execution levels from start/end timestamps and summarizes peak/average concurrency.
````

## File: plugins/analysis_conformal_feature_prediction/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "alpha": {
 6:       "type": "number"
 7:     },
 8:     "max_target_cols": {
 9:       "minimum": 1,
10:       "type": "integer"
11:     },
12:     "model": {
13:       "type": "string"
14:     }
15:   },
16:   "title": "Conformal Feature Prediction Config",
17:   "type": "object"
18: }
````

## File: plugins/analysis_conformal_feature_prediction/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "column": {
187:                     "type": "string"
188:                   },
189:                   "evidence": {
190:                     "additionalProperties": true,
191:                     "properties": {
192:                       "column_ids": {
193:                         "items": {
194:                           "type": "integer"
195:                         },
196:                         "type": "array"
197:                       },
198:                       "dataset_id": {
199:                         "type": "string"
200:                       },
201:                       "dataset_version_id": {
202:                         "type": "string"
203:                       },
204:                       "query": {
205:                         "type": [
206:                           "string",
207:                           "null"
208:                         ]
209:                       },
210:                       "row_ids": {
211:                         "items": {
212:                           "type": "integer"
213:                         },
214:                         "type": "array"
215:                       },
216:                       "row_ranges": {
217:                         "items": {
218:                           "additionalProperties": false,
219:                           "properties": {
220:                             "end": {
221:                               "type": "integer"
222:                             },
223:                             "start": {
224:                               "type": "integer"
225:                             }
226:                           },
227:                           "required": [
228:                             "start",
229:                             "end"
230:                           ],
231:                           "type": "object"
232:                         },
233:                         "type": "array"
234:                       }
235:                     },
236:                     "required": [
237:                       "dataset_id",
238:                       "dataset_version_id",
239:                       "row_ids",
240:                       "column_ids"
241:                     ],
242:                     "type": "object"
243:                   },
244:                   "kind": {
245:                     "const": "anomaly"
246:                   },
247:                   "lower": {
248:                     "type": "number"
249:                   },
250:                   "row_index": {
251:                     "type": "integer"
252:                   },
253:                   "score": {
254:                     "type": "number"
255:                   },
256:                   "upper": {
257:                     "type": "number"
258:                   }
259:                 },
260:                 "required": [
261:                   "kind",
262:                   "column",
263:                   "row_index",
264:                   "score",
265:                   "lower",
266:                   "upper"
267:                 ],
268:                 "type": "object"
269:               }
270:             ]
271:           }
272:         ]
273:       },
274:       "type": "array"
275:     },
276:     "metrics": {
277:       "additionalProperties": false,
278:       "properties": {
279:         "count": {
280:           "type": "integer"
281:         }
282:       },
283:       "type": "object"
284:     },
285:     "status": {
286:       "enum": [
287:         "ok",
288:         "skipped",
289:         "error"
290:       ],
291:       "type": "string"
292:     },
293:     "summary": {
294:       "type": "string"
295:     }
296:   },
297:   "required": [
298:     "status",
299:     "summary",
300:     "metrics",
301:     "findings",
302:     "artifacts",
303:     "budget"
304:   ],
305:   "title": "Plugin Result",
306:   "type": "object"
307: }
````

## File: plugins/analysis_dependency_resolution_join/__init__.py
````python
1: 
````

## File: plugins/analysis_dependency_resolution_join/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "start_column": {"type": ["string", "null"]},
 7:     "end_column": {"type": ["string", "null"]},
 8:     "id_column": {"type": ["string", "null"]},
 9:     "dependency_column": {"type": ["string", "null"]},
10:     "near_zero_threshold_seconds": {"type": "number", "minimum": 0},
11:     "max_examples": {"type": "integer", "minimum": 0}
12:   }
13: }
````

## File: plugins/analysis_dependency_resolution_join/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "status": {"type": "string"},
 7:     "summary": {"type": "string"},
 8:     "metrics": {"type": "object"},
 9:     "findings": {"type": "array"},
10:     "artifacts": {"type": "array"},
11:     "budget": {
12:       "type": "object",
13:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
14:       "properties": {
15:         "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
16:         "sampled": {"type": "boolean"},
17:         "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
18:         "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
19:         "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
20:       },
21:       "additionalProperties": true
22:     },
23:     "error": {"type": ["object", "null"]}
24:   },
25:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"]
26: }
````

## File: plugins/analysis_dependency_resolution_join/plugin.yaml
````yaml
 1: id: analysis_dependency_resolution_join
 2: name: Dependency Resolution Join
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Join dependency end times to quantify dependency lag.
 9:   defaults:
10:     start_column: null
11:     end_column: null
12:     id_column: null
13:     dependency_column: null
14:     near_zero_threshold_seconds: 2
15:     max_examples: 25
16: capabilities:
17:   - needs_eventlog
18:   - needs_timestamp
19: config_schema: config.schema.json
20: output_schema: output.schema.json
21: sandbox:
22:   no_network: true
23:   fs_allowlist:
24:     - appdata
25:     - plugins
26:     - run_dir
````

## File: plugins/analysis_dependency_resolution_join/README.md
````markdown
1: # Dependency Resolution Join
2: 
3: Joins dependency ids to their end timestamps and summarizes the lag between dependency completion and task start.
````

## File: plugins/analysis_determinism_discipline/__init__.py
````python
1: 
````

## File: plugins/analysis_determinism_discipline/config.schema.json
````json
1: {
2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
3:   "type": "object",
4:   "additionalProperties": true,
5:   "properties": {}
6: }
````

## File: plugins/analysis_determinism_discipline/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "status": {"type": "string"},
 7:     "summary": {"type": "string"},
 8:     "metrics": {"type": "object"},
 9:     "findings": {"type": "array"},
10:     "artifacts": {"type": "array"},
11:     "budget": {
12:       "type": "object",
13:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
14:       "properties": {
15:         "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
16:         "sampled": {"type": "boolean"},
17:         "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
18:         "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
19:         "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
20:       },
21:       "additionalProperties": true
22:     },
23:     "error": {"type": ["object", "null"]}
24:   },
25:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"]
26: }
````

## File: plugins/analysis_determinism_discipline/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import json
 4: 
 5: from statistic_harness.core.types import PluginArtifact, PluginResult
 6: from statistic_harness.core.utils import write_json
 7: 
 8: 
 9: class Plugin:
10:     def run(self, ctx) -> PluginResult:
11:         run_id = ctx.run_id
12:         rows = ctx.storage.fetch_plugin_results(run_id)
13: 
14:         missing_measurement = 0
15:         modeled_missing_assumption = 0
16:         checked = 0
17:         violations = []
18: 
19:         for row in rows:
20:             plugin_id = row.get("plugin_id") or "unknown"
21:             if plugin_id == "analysis_determinism_discipline":
22:                 continue
23:             findings = []
24:             try:
25:                 findings = json.loads(row.get("findings_json") or "[]")
26:             except json.JSONDecodeError:
27:                 findings = []
28:             if not isinstance(findings, list):
29:                 continue
30:             for item in findings:
31:                 if not isinstance(item, dict):
32:                     continue
33:                 checked += 1
34:                 measurement_type = item.get("measurement_type")
35:                 if not measurement_type:
36:                     missing_measurement += 1
37:                     violations.append(
38:                         {
39:                             "kind": "determinism_violation",
40:                             "plugin_id": plugin_id,
41:                             "issue": "missing_measurement_type",
42:                             "measurement_type": "error",
43:                         }
44:                     )
45:                     continue
46:                 assumptions = item.get("assumptions")
47:                 if measurement_type == "modeled" and (
48:                     not isinstance(assumptions, list)
49:                     or not assumptions
50:                     or not all(isinstance(a, str) and a.strip() for a in assumptions)
51:                 ):
52:                     modeled_missing_assumption += 1
53:                     violations.append(
54:                         {
55:                             "kind": "determinism_violation",
56:                             "plugin_id": plugin_id,
57:                             "issue": "modeled_missing_assumption",
58:                             "measurement_type": "error",
59:                         }
60:                     )
61: 
62:         summary = {
63:             "kind": "determinism_discipline_summary",
64:             "checked_findings": checked,
65:             "missing_measurement_type": missing_measurement,
66:             "modeled_missing_assumption": modeled_missing_assumption,
67:             "violations": len(violations),
68:             "measurement_type": "measured",
69:         }
70:         findings_out = [summary, *violations]
71: 
72:         artifacts_dir = ctx.artifacts_dir("analysis_determinism_discipline")
73:         out_path = artifacts_dir / "determinism_checks.json"
74:         write_json(out_path, {"summary": summary, "violations": violations})
75:         artifacts = [
76:             PluginArtifact(
77:                 path=str(out_path.relative_to(ctx.run_dir)),
78:                 type="json",
79:                 description="Determinism discipline checks",
80:             )
81:         ]
82: 
83:         return PluginResult(
84:             "ok",
85:             "Checked measurement_type discipline",
86:             {
87:                 "checked_findings": checked,
88:                 "missing_measurement_type": missing_measurement,
89:                 "modeled_missing_assumption": modeled_missing_assumption,
90:             },
91:             findings_out,
92:             artifacts,
93:             None,
94:         )
````

## File: plugins/analysis_determinism_discipline/plugin.yaml
````yaml
 1: id: analysis_determinism_discipline
 2: name: Determinism Discipline
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on:
 7:   - analysis_capacity_scaling
 8:   - analysis_chain_makespan
 9:   - analysis_concurrency_reconstruction
10:   - analysis_attribution
11:   - analysis_bocpd_gaussian
12:   - analysis_close_cycle_contention
13:   - analysis_conformal_feature_prediction
14:   - analysis_dependency_resolution_join
15:   - analysis_dp_gmm
16:   - analysis_gaussian_copula_shift
17:   - analysis_gaussian_knockoffs
18:   - analysis_graph_topology_curves
19:   - analysis_knockoff_wrapper_rf
20:   - analysis_notears_linear
21:   - analysis_online_conformal_changepoint
22:   - analysis_percentile_analysis
23:   - analysis_process_sequence
24:   - analysis_queue_delay_decomposition
25:   - analysis_scan_statistics
26:   - analysis_sequence_classification
27:   - analysis_tail_isolation
28: settings:
29:   description: Check analysis findings for measurement_type discipline and modeled assumptions.
30:   defaults: {}
31: capabilities: []
32: config_schema: config.schema.json
33: output_schema: output.schema.json
34: sandbox:
35:   no_network: true
36:   fs_allowlist:
37:     - appdata
38:     - plugins
39:     - run_dir
````

## File: plugins/analysis_determinism_discipline/README.md
````markdown
1: # Determinism Discipline
2: 
3: Checks analysis findings for measurement_type usage and modeled assumption coverage.
````

## File: plugins/analysis_dp_gmm/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "alpha": {
 6:       "type": [
 7:         "number",
 8:         "null"
 9:       ]
10:     },
11:     "burn_in": {
12:       "minimum": 0,
13:       "type": "integer"
14:     },
15:     "n_iter": {
16:       "minimum": 1,
17:       "type": "integer"
18:     },
19:     "sigma2": {
20:       "type": [
21:         "number",
22:         "null"
23:       ]
24:     }
25:   },
26:   "title": "DP GMM Config",
27:   "type": "object"
28: }
````

## File: plugins/analysis_dp_gmm/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "cluster_id": {
187:                     "type": "integer"
188:                   },
189:                   "evidence": {
190:                     "additionalProperties": true,
191:                     "properties": {
192:                       "column_ids": {
193:                         "items": {
194:                           "type": "integer"
195:                         },
196:                         "type": "array"
197:                       },
198:                       "dataset_id": {
199:                         "type": "string"
200:                       },
201:                       "dataset_version_id": {
202:                         "type": "string"
203:                       },
204:                       "query": {
205:                         "type": [
206:                           "string",
207:                           "null"
208:                         ]
209:                       },
210:                       "row_ids": {
211:                         "items": {
212:                           "type": "integer"
213:                         },
214:                         "type": "array"
215:                       },
216:                       "row_ranges": {
217:                         "items": {
218:                           "additionalProperties": false,
219:                           "properties": {
220:                             "end": {
221:                               "type": "integer"
222:                             },
223:                             "start": {
224:                               "type": "integer"
225:                             }
226:                           },
227:                           "required": [
228:                             "start",
229:                             "end"
230:                           ],
231:                           "type": "object"
232:                         },
233:                         "type": "array"
234:                       }
235:                     },
236:                     "required": [
237:                       "dataset_id",
238:                       "dataset_version_id",
239:                       "row_ids",
240:                       "column_ids"
241:                     ],
242:                     "type": "object"
243:                   },
244:                   "kind": {
245:                     "const": "cluster"
246:                   },
247:                   "size": {
248:                     "type": "integer"
249:                   }
250:                 },
251:                 "required": [
252:                   "kind",
253:                   "cluster_id",
254:                   "size"
255:                 ],
256:                 "type": "object"
257:               }
258:             ]
259:           }
260:         ]
261:       },
262:       "type": "array"
263:     },
264:     "metrics": {
265:       "additionalProperties": false,
266:       "properties": {
267:         "clusters": {
268:           "type": "integer"
269:         }
270:       },
271:       "type": "object"
272:     },
273:     "status": {
274:       "enum": [
275:         "ok",
276:         "skipped",
277:         "error"
278:       ],
279:       "type": "string"
280:     },
281:     "summary": {
282:       "type": "string"
283:     }
284:   },
285:   "required": [
286:     "status",
287:     "summary",
288:     "metrics",
289:     "findings",
290:     "artifacts",
291:     "budget"
292:   ],
293:   "title": "Plugin Result",
294:   "type": "object"
295: }
````

## File: plugins/analysis_gaussian_knockoffs/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "fdr_q": {
 6:       "type": "number"
 7:     },
 8:     "lasso_alpha": {
 9:       "type": "number"
10:     },
11:     "target_column": {
12:       "type": [
13:         "string",
14:         "null"
15:       ]
16:     }
17:   },
18:   "title": "Gaussian Knockoffs Config",
19:   "type": "object"
20: }
````

## File: plugins/analysis_gaussian_knockoffs/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "evidence": {
187:                     "additionalProperties": true,
188:                     "properties": {
189:                       "column_ids": {
190:                         "items": {
191:                           "type": "integer"
192:                         },
193:                         "type": "array"
194:                       },
195:                       "dataset_id": {
196:                         "type": "string"
197:                       },
198:                       "dataset_version_id": {
199:                         "type": "string"
200:                       },
201:                       "query": {
202:                         "type": [
203:                           "string",
204:                           "null"
205:                         ]
206:                       },
207:                       "row_ids": {
208:                         "items": {
209:                           "type": "integer"
210:                         },
211:                         "type": "array"
212:                       },
213:                       "row_ranges": {
214:                         "items": {
215:                           "additionalProperties": false,
216:                           "properties": {
217:                             "end": {
218:                               "type": "integer"
219:                             },
220:                             "start": {
221:                               "type": "integer"
222:                             }
223:                           },
224:                           "required": [
225:                             "start",
226:                             "end"
227:                           ],
228:                           "type": "object"
229:                         },
230:                         "type": "array"
231:                       }
232:                     },
233:                     "required": [
234:                       "dataset_id",
235:                       "dataset_version_id",
236:                       "row_ids",
237:                       "column_ids"
238:                     ],
239:                     "type": "object"
240:                   },
241:                   "feature": {
242:                     "type": "string"
243:                   },
244:                   "kind": {
245:                     "const": "feature_discovery"
246:                   },
247:                   "score": {
248:                     "type": "number"
249:                   },
250:                   "selected": {
251:                     "type": "boolean"
252:                   }
253:                 },
254:                 "required": [
255:                   "kind",
256:                   "feature",
257:                   "score",
258:                   "selected"
259:                 ],
260:                 "type": "object"
261:               }
262:             ]
263:           }
264:         ]
265:       },
266:       "type": "array"
267:     },
268:     "metrics": {
269:       "additionalProperties": false,
270:       "properties": {
271:         "selected": {
272:           "type": "integer"
273:         }
274:       },
275:       "type": "object"
276:     },
277:     "status": {
278:       "enum": [
279:         "ok",
280:         "skipped",
281:         "error"
282:       ],
283:       "type": "string"
284:     },
285:     "summary": {
286:       "type": "string"
287:     }
288:   },
289:   "required": [
290:     "status",
291:     "summary",
292:     "metrics",
293:     "findings",
294:     "artifacts",
295:     "budget"
296:   ],
297:   "title": "Plugin Result",
298:   "type": "object"
299: }
````

## File: plugins/analysis_graph_topology_curves/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "max_points": {
 6:       "minimum": 1,
 7:       "type": "integer"
 8:     },
 9:     "n_thresholds": {
10:       "minimum": 1,
11:       "type": "integer"
12:     }
13:   },
14:   "title": "Graph Topology Curves Config",
15:   "type": "object"
16: }
````

## File: plugins/analysis_graph_topology_curves/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "evidence": {
187:                     "additionalProperties": true,
188:                     "properties": {
189:                       "column_ids": {
190:                         "items": {
191:                           "type": "integer"
192:                         },
193:                         "type": "array"
194:                       },
195:                       "dataset_id": {
196:                         "type": "string"
197:                       },
198:                       "dataset_version_id": {
199:                         "type": "string"
200:                       },
201:                       "query": {
202:                         "type": [
203:                           "string",
204:                           "null"
205:                         ]
206:                       },
207:                       "row_ids": {
208:                         "items": {
209:                           "type": "integer"
210:                         },
211:                         "type": "array"
212:                       },
213:                       "row_ranges": {
214:                         "items": {
215:                           "additionalProperties": false,
216:                           "properties": {
217:                             "end": {
218:                               "type": "integer"
219:                             },
220:                             "start": {
221:                               "type": "integer"
222:                             }
223:                           },
224:                           "required": [
225:                             "start",
226:                             "end"
227:                           ],
228:                           "type": "object"
229:                         },
230:                         "type": "array"
231:                       }
232:                     },
233:                     "required": [
234:                       "dataset_id",
235:                       "dataset_version_id",
236:                       "row_ids",
237:                       "column_ids"
238:                     ],
239:                     "type": "object"
240:                   },
241:                   "kind": {
242:                     "const": "topology"
243:                   },
244:                   "metric": {
245:                     "type": "string"
246:                   },
247:                   "value": {
248:                     "type": "number"
249:                   }
250:                 },
251:                 "required": [
252:                   "kind",
253:                   "metric",
254:                   "value"
255:                 ],
256:                 "type": "object"
257:               }
258:             ]
259:           }
260:         ]
261:       },
262:       "type": "array"
263:     },
264:     "metrics": {
265:       "additionalProperties": false,
266:       "properties": {
267:         "beta1_peak": {
268:           "type": "number"
269:         }
270:       },
271:       "type": "object"
272:     },
273:     "status": {
274:       "enum": [
275:         "ok",
276:         "skipped",
277:         "error"
278:       ],
279:       "type": "string"
280:     },
281:     "summary": {
282:       "type": "string"
283:     }
284:   },
285:   "required": [
286:     "status",
287:     "summary",
288:     "metrics",
289:     "findings",
290:     "artifacts",
291:     "budget"
292:   ],
293:   "title": "Plugin Result",
294:   "type": "object"
295: }
````

## File: plugins/analysis_knockoff_wrapper_rf/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "fdr_q": {
 6:       "type": "number"
 7:     },
 8:     "n_estimators": {
 9:       "minimum": 1,
10:       "type": "integer"
11:     },
12:     "target_column": {
13:       "type": [
14:         "string",
15:         "null"
16:       ]
17:     }
18:   },
19:   "title": "Knockoff Wrapper RF Config",
20:   "type": "object"
21: }
````

## File: plugins/analysis_knockoff_wrapper_rf/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "evidence": {
187:                     "additionalProperties": true,
188:                     "properties": {
189:                       "column_ids": {
190:                         "items": {
191:                           "type": "integer"
192:                         },
193:                         "type": "array"
194:                       },
195:                       "dataset_id": {
196:                         "type": "string"
197:                       },
198:                       "dataset_version_id": {
199:                         "type": "string"
200:                       },
201:                       "query": {
202:                         "type": [
203:                           "string",
204:                           "null"
205:                         ]
206:                       },
207:                       "row_ids": {
208:                         "items": {
209:                           "type": "integer"
210:                         },
211:                         "type": "array"
212:                       },
213:                       "row_ranges": {
214:                         "items": {
215:                           "additionalProperties": false,
216:                           "properties": {
217:                             "end": {
218:                               "type": "integer"
219:                             },
220:                             "start": {
221:                               "type": "integer"
222:                             }
223:                           },
224:                           "required": [
225:                             "start",
226:                             "end"
227:                           ],
228:                           "type": "object"
229:                         },
230:                         "type": "array"
231:                       }
232:                     },
233:                     "required": [
234:                       "dataset_id",
235:                       "dataset_version_id",
236:                       "row_ids",
237:                       "column_ids"
238:                     ],
239:                     "type": "object"
240:                   },
241:                   "feature": {
242:                     "type": "string"
243:                   },
244:                   "kind": {
245:                     "const": "feature_discovery"
246:                   },
247:                   "score": {
248:                     "type": "number"
249:                   },
250:                   "selected": {
251:                     "type": "boolean"
252:                   }
253:                 },
254:                 "required": [
255:                   "kind",
256:                   "feature",
257:                   "score",
258:                   "selected"
259:                 ],
260:                 "type": "object"
261:               }
262:             ]
263:           }
264:         ]
265:       },
266:       "type": "array"
267:     },
268:     "metrics": {
269:       "additionalProperties": false,
270:       "properties": {
271:         "selected": {
272:           "type": "integer"
273:         }
274:       },
275:       "type": "object"
276:     },
277:     "status": {
278:       "enum": [
279:         "ok",
280:         "skipped",
281:         "error"
282:       ],
283:       "type": "string"
284:     },
285:     "summary": {
286:       "type": "string"
287:     }
288:   },
289:   "required": [
290:     "status",
291:     "summary",
292:     "metrics",
293:     "findings",
294:     "artifacts",
295:     "budget"
296:   ],
297:   "title": "Plugin Result",
298:   "type": "object"
299: }
````

## File: plugins/analysis_notears_linear/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "lambda_l1": {
 6:       "type": "number"
 7:     },
 8:     "lr": {
 9:       "type": "number"
10:     },
11:     "max_cols": {
12:       "minimum": 1,
13:       "type": "integer"
14:     },
15:     "max_iter": {
16:       "minimum": 1,
17:       "type": "integer"
18:     },
19:     "weight_threshold": {
20:       "type": "number"
21:     }
22:   },
23:   "title": "NOTEARS Linear Config",
24:   "type": "object"
25: }
````

## File: plugins/analysis_notears_linear/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "evidence": {
187:                     "additionalProperties": true,
188:                     "properties": {
189:                       "column_ids": {
190:                         "items": {
191:                           "type": "integer"
192:                         },
193:                         "type": "array"
194:                       },
195:                       "dataset_id": {
196:                         "type": "string"
197:                       },
198:                       "dataset_version_id": {
199:                         "type": "string"
200:                       },
201:                       "query": {
202:                         "type": [
203:                           "string",
204:                           "null"
205:                         ]
206:                       },
207:                       "row_ids": {
208:                         "items": {
209:                           "type": "integer"
210:                         },
211:                         "type": "array"
212:                       },
213:                       "row_ranges": {
214:                         "items": {
215:                           "additionalProperties": false,
216:                           "properties": {
217:                             "end": {
218:                               "type": "integer"
219:                             },
220:                             "start": {
221:                               "type": "integer"
222:                             }
223:                           },
224:                           "required": [
225:                             "start",
226:                             "end"
227:                           ],
228:                           "type": "object"
229:                         },
230:                         "type": "array"
231:                       }
232:                     },
233:                     "required": [
234:                       "dataset_id",
235:                       "dataset_version_id",
236:                       "row_ids",
237:                       "column_ids"
238:                     ],
239:                     "type": "object"
240:                   },
241:                   "kind": {
242:                     "const": "graph_edge"
243:                   },
244:                   "source": {
245:                     "type": "string"
246:                   },
247:                   "target": {
248:                     "type": "string"
249:                   },
250:                   "weight": {
251:                     "type": "number"
252:                   }
253:                 },
254:                 "required": [
255:                   "kind",
256:                   "source",
257:                   "target",
258:                   "weight"
259:                 ],
260:                 "type": "object"
261:               }
262:             ]
263:           }
264:         ]
265:       },
266:       "type": "array"
267:     },
268:     "metrics": {
269:       "additionalProperties": false,
270:       "properties": {
271:         "edges": {
272:           "type": "integer"
273:         }
274:       },
275:       "type": "object"
276:     },
277:     "status": {
278:       "enum": [
279:         "ok",
280:         "skipped",
281:         "error"
282:       ],
283:       "type": "string"
284:     },
285:     "summary": {
286:       "type": "string"
287:     }
288:   },
289:   "required": [
290:     "status",
291:     "summary",
292:     "metrics",
293:     "findings",
294:     "artifacts",
295:     "budget"
296:   ],
297:   "title": "Plugin Result",
298:   "type": "object"
299: }
````

## File: plugins/analysis_online_conformal_changepoint/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "alarm_rate_threshold": {
 6:       "type": "number"
 7:     },
 8:     "alarm_rate_window": {
 9:       "minimum": 1,
10:       "type": "integer"
11:     },
12:     "alpha": {
13:       "type": "number"
14:     },
15:     "calib_window": {
16:       "minimum": 1,
17:       "type": "integer"
18:     },
19:     "forecast_window": {
20:       "minimum": 1,
21:       "type": "integer"
22:     },
23:     "time_column": {
24:       "type": [
25:         "string",
26:         "null"
27:       ]
28:     },
29:     "value_column": {
30:       "type": [
31:         "string",
32:         "null"
33:       ]
34:     }
35:   },
36:   "title": "Online Conformal Changepoint Config",
37:   "type": "object"
38: }
````

## File: plugins/analysis_online_conformal_changepoint/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "evidence": {
187:                     "additionalProperties": true,
188:                     "properties": {
189:                       "column_ids": {
190:                         "items": {
191:                           "type": "integer"
192:                         },
193:                         "type": "array"
194:                       },
195:                       "dataset_id": {
196:                         "type": "string"
197:                       },
198:                       "dataset_version_id": {
199:                         "type": "string"
200:                       },
201:                       "query": {
202:                         "type": [
203:                           "string",
204:                           "null"
205:                         ]
206:                       },
207:                       "row_ids": {
208:                         "items": {
209:                           "type": "integer"
210:                         },
211:                         "type": "array"
212:                       },
213:                       "row_ranges": {
214:                         "items": {
215:                           "additionalProperties": false,
216:                           "properties": {
217:                             "end": {
218:                               "type": "integer"
219:                             },
220:                             "start": {
221:                               "type": "integer"
222:                             }
223:                           },
224:                           "required": [
225:                             "start",
226:                             "end"
227:                           ],
228:                           "type": "object"
229:                         },
230:                         "type": "array"
231:                       }
232:                     },
233:                     "required": [
234:                       "dataset_id",
235:                       "dataset_version_id",
236:                       "row_ids",
237:                       "column_ids"
238:                     ],
239:                     "type": "object"
240:                   },
241:                   "index": {
242:                     "type": "integer"
243:                   },
244:                   "kind": {
245:                     "const": "changepoint"
246:                   },
247:                   "score": {
248:                     "type": "number"
249:                   },
250:                   "time": {
251:                     "type": "integer"
252:                   }
253:                 },
254:                 "required": [
255:                   "kind",
256:                   "index",
257:                   "time",
258:                   "score"
259:                 ],
260:                 "type": "object"
261:               }
262:             ]
263:           }
264:         ]
265:       },
266:       "type": "array"
267:     },
268:     "metrics": {
269:       "additionalProperties": false,
270:       "properties": {
271:         "count": {
272:           "type": "integer"
273:         }
274:       },
275:       "type": "object"
276:     },
277:     "status": {
278:       "enum": [
279:         "ok",
280:         "skipped",
281:         "error"
282:       ],
283:       "type": "string"
284:     },
285:     "summary": {
286:       "type": "string"
287:     }
288:   },
289:   "required": [
290:     "status",
291:     "summary",
292:     "metrics",
293:     "findings",
294:     "artifacts",
295:     "budget"
296:   ],
297:   "title": "Plugin Result",
298:   "type": "object"
299: }
````

## File: plugins/analysis_percentile_analysis/__init__.py
````python
1: 
````

## File: plugins/analysis_percentile_analysis/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "process_column": {"type": ["string", "null"]},
 7:     "module_column": {"type": ["string", "null"]},
 8:     "queue_column": {"type": ["string", "null"]},
 9:     "eligible_column": {"type": ["string", "null"]},
10:     "start_column": {"type": ["string", "null"]},
11:     "end_column": {"type": ["string", "null"]},
12:     "max_groups": {"type": "integer", "minimum": 1}
13:   }
14: }
````

## File: plugins/analysis_percentile_analysis/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "status": {"type": "string"},
 7:     "summary": {"type": "string"},
 8:     "metrics": {"type": "object"},
 9:     "findings": {"type": "array"},
10:     "artifacts": {"type": "array"},
11:     "budget": {
12:       "type": "object",
13:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
14:       "properties": {
15:         "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
16:         "sampled": {"type": "boolean"},
17:         "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
18:         "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
19:         "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
20:       },
21:       "additionalProperties": true
22:     },
23:     "error": {"type": ["object", "null"]}
24:   },
25:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"]
26: }
````

## File: plugins/analysis_percentile_analysis/plugin.yaml
````yaml
 1: id: analysis_percentile_analysis
 2: name: Percentile Analysis
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Compute p50/p95/p99 for eligible wait and completion time per group.
 9:   defaults:
10:     process_column: null
11:     module_column: null
12:     queue_column: null
13:     eligible_column: null
14:     start_column: null
15:     end_column: null
16:     max_groups: 10
17: capabilities:
18:   - needs_eventlog
19:   - needs_timestamp
20: config_schema: config.schema.json
21: output_schema: output.schema.json
22: sandbox:
23:   no_network: true
24:   fs_allowlist:
25:     - appdata
26:     - plugins
27:     - run_dir
````

## File: plugins/analysis_percentile_analysis/README.md
````markdown
1: # Percentile Analysis
2: 
3: Computes p50/p95/p99 for eligible wait and completion time per group.
````

## File: plugins/analysis_process_sequence/__init__.py
````python
1: 
````

## File: plugins/analysis_process_sequence/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "max_examples": {
 6:       "minimum": 1,
 7:       "type": "integer"
 8:     },
 9:     "max_variants": {
10:       "minimum": 1,
11:       "type": "integer"
12:     },
13:     "min_variant_fraction": {
14:       "minimum": 0,
15:       "type": "number"
16:     }
17:   },
18:   "title": "Process Sequence Config",
19:   "type": "object"
20: }
````

## File: plugins/analysis_process_sequence/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "columns": {
187:                     "items": {
188:                       "type": "string"
189:                     },
190:                     "type": "array"
191:                   },
192:                   "count": {
193:                     "type": "integer"
194:                   },
195:                   "evidence": {
196:                     "additionalProperties": true,
197:                     "properties": {
198:                       "column_ids": {
199:                         "items": {
200:                           "type": "integer"
201:                         },
202:                         "type": "array"
203:                       },
204:                       "dataset_id": {
205:                         "type": "string"
206:                       },
207:                       "dataset_version_id": {
208:                         "type": "string"
209:                       },
210:                       "query": {
211:                         "type": [
212:                           "string",
213:                           "null"
214:                         ]
215:                       },
216:                       "row_ids": {
217:                         "items": {
218:                           "type": "integer"
219:                         },
220:                         "type": "array"
221:                       },
222:                       "row_ranges": {
223:                         "items": {
224:                           "additionalProperties": false,
225:                           "properties": {
226:                             "end": {
227:                               "type": "integer"
228:                             },
229:                             "start": {
230:                               "type": "integer"
231:                             }
232:                           },
233:                           "required": [
234:                             "start",
235:                             "end"
236:                           ],
237:                           "type": "object"
238:                         },
239:                         "type": "array"
240:                       }
241:                     },
242:                     "required": [
243:                       "dataset_id",
244:                       "dataset_version_id",
245:                       "row_ids",
246:                       "column_ids"
247:                     ],
248:                     "type": "object"
249:                   },
250:                   "fraction": {
251:                     "type": "number"
252:                   },
253:                   "kind": {
254:                     "const": "process_variant"
255:                   },
256:                   "variant": {
257:                     "items": {
258:                       "type": "string"
259:                     },
260:                     "type": "array"
261:                   }
262:                 },
263:                 "required": [
264:                   "kind",
265:                   "variant",
266:                   "count",
267:                   "fraction",
268:                   "columns"
269:                 ],
270:                 "type": "object"
271:               }
272:             ]
273:           },
274:           {
275:             "allOf": [
276:               {
277:                 "additionalProperties": true,
278:                 "properties": {
279:                   "evidence": {
280:                     "additionalProperties": true,
281:                     "properties": {
282:                       "column_ids": {
283:                         "items": {
284:                           "type": "integer"
285:                         },
286:                         "type": "array"
287:                       },
288:                       "dataset_id": {
289:                         "type": "string"
290:                       },
291:                       "dataset_version_id": {
292:                         "type": "string"
293:                       },
294:                       "query": {
295:                         "type": [
296:                           "string",
297:                           "null"
298:                         ]
299:                       },
300:                       "row_ids": {
301:                         "items": {
302:                           "type": "integer"
303:                         },
304:                         "type": "array"
305:                       },
306:                       "row_ranges": {
307:                         "items": {
308:                           "additionalProperties": false,
309:                           "properties": {
310:                             "end": {
311:                               "type": "integer"
312:                             },
313:                             "start": {
314:                               "type": "integer"
315:                             }
316:                           },
317:                           "required": [
318:                             "start",
319:                             "end"
320:                           ],
321:                           "type": "object"
322:                         },
323:                         "type": "array"
324:                       }
325:                     },
326:                     "required": [
327:                       "dataset_id",
328:                       "dataset_version_id",
329:                       "row_ids",
330:                       "column_ids"
331:                     ],
332:                     "type": "object"
333:                   },
334:                   "kind": {
335:                     "type": "string"
336:                   }
337:                 },
338:                 "required": [
339:                   "kind",
340:                   "evidence"
341:                 ],
342:                 "type": "object"
343:               },
344:               {
345:                 "additionalProperties": true,
346:                 "properties": {
347:                   "columns": {
348:                     "items": {
349:                       "type": "string"
350:                     },
351:                     "type": "array"
352:                   },
353:                   "count": {
354:                     "type": "integer"
355:                   },
356:                   "evidence": {
357:                     "additionalProperties": true,
358:                     "properties": {
359:                       "column_ids": {
360:                         "items": {
361:                           "type": "integer"
362:                         },
363:                         "type": "array"
364:                       },
365:                       "dataset_id": {
366:                         "type": "string"
367:                       },
368:                       "dataset_version_id": {
369:                         "type": "string"
370:                       },
371:                       "query": {
372:                         "type": [
373:                           "string",
374:                           "null"
375:                         ]
376:                       },
377:                       "row_ids": {
378:                         "items": {
379:                           "type": "integer"
380:                         },
381:                         "type": "array"
382:                       },
383:                       "row_ranges": {
384:                         "items": {
385:                           "additionalProperties": false,
386:                           "properties": {
387:                             "end": {
388:                               "type": "integer"
389:                             },
390:                             "start": {
391:                               "type": "integer"
392:                             }
393:                           },
394:                           "required": [
395:                             "start",
396:                             "end"
397:                           ],
398:                           "type": "object"
399:                         },
400:                         "type": "array"
401:                       }
402:                     },
403:                     "required": [
404:                       "dataset_id",
405:                       "dataset_version_id",
406:                       "row_ids",
407:                       "column_ids"
408:                     ],
409:                     "type": "object"
410:                   },
411:                   "fraction": {
412:                     "type": "number"
413:                   },
414:                   "kind": {
415:                     "const": "rare_variant"
416:                   },
417:                   "variant": {
418:                     "items": {
419:                       "type": "string"
420:                     },
421:                     "type": "array"
422:                   }
423:                 },
424:                 "required": [
425:                   "kind",
426:                   "variant",
427:                   "count",
428:                   "fraction",
429:                   "columns"
430:                 ],
431:                 "type": "object"
432:               }
433:             ]
434:           }
435:         ]
436:       },
437:       "type": "array"
438:     },
439:     "metrics": {
440:       "additionalProperties": false,
441:       "properties": {
442:         "transitions": {
443:           "type": "integer"
444:         },
445:         "variants": {
446:           "type": "integer"
447:         }
448:       },
449:       "type": "object"
450:     },
451:     "status": {
452:       "enum": [
453:         "ok",
454:         "skipped",
455:         "error"
456:       ],
457:       "type": "string"
458:     },
459:     "summary": {
460:       "type": "string"
461:     }
462:   },
463:   "required": [
464:     "status",
465:     "summary",
466:     "metrics",
467:     "findings",
468:     "artifacts",
469:     "budget"
470:   ],
471:   "title": "Plugin Result",
472:   "type": "object"
473: }
````

## File: plugins/analysis_process_sequence/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from collections import defaultdict
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.types import PluginArtifact, PluginResult
  8: from statistic_harness.core.utils import write_json
  9: 
 10: 
 11: class Plugin:
 12:     def run(self, ctx) -> PluginResult:
 13:         df = ctx.dataset_loader()
 14:         if df.empty:
 15:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 16: 
 17:         case_col = None
 18:         activity_col = None
 19:         timestamp_col = None
 20: 
 21:         columns_meta = []
 22:         if ctx.dataset_version_id:
 23:             columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 24: 
 25:         role_by_name = {
 26:             col["original_name"]: (col.get("role") or "") for col in columns_meta
 27:         }
 28:         lower_names = {col: col.lower() for col in df.columns}
 29: 
 30:         for col in df.columns:
 31:             if role_by_name.get(col) == "id":
 32:                 case_col = case_col or col
 33:             if role_by_name.get(col) == "timestamp":
 34:                 timestamp_col = timestamp_col or col
 35:         for col in df.columns:
 36:             lname = lower_names[col]
 37:             if case_col is None and (lname.endswith("id") or " case" in lname or "session" in lname):
 38:                 case_col = col
 39:             if activity_col is None and (
 40:                 "activity" in lname
 41:                 or "event" in lname
 42:                 or "step" in lname
 43:                 or "process" in lname
 44:                 or "action" in lname
 45:                 or "task" in lname
 46:             ):
 47:                 activity_col = col
 48:             if timestamp_col is None and ("time" in lname or "date" in lname):
 49:                 timestamp_col = col
 50: 
 51:         if case_col is None or activity_col is None:
 52:             return PluginResult(
 53:                 "skipped",
 54:                 "No event log columns detected",
 55:                 {},
 56:                 [],
 57:                 [],
 58:                 None,
 59:             )
 60: 
 61:         work = df.copy()
 62:         work = work.reset_index().rename(columns={"index": "row_index"})
 63:         sort_cols = [case_col]
 64:         if timestamp_col and timestamp_col in work.columns:
 65:             sort_cols.append(timestamp_col)
 66:         else:
 67:             sort_cols.append("row_index")
 68:         work = work.sort_values(sort_cols)
 69: 
 70:         sequences: dict[tuple[str, ...], int] = defaultdict(int)
 71:         examples: dict[tuple[str, ...], list[int]] = {}
 72:         transition_counts: dict[tuple[str, str], int] = defaultdict(int)
 73: 
 74:         for _, group in work.groupby(case_col, sort=False):
 75:             activities = [str(x) for x in group[activity_col].tolist()]
 76:             if not activities:
 77:                 continue
 78:             seq = tuple(activities)
 79:             sequences[seq] += 1
 80:             if seq not in examples:
 81:                 examples[seq] = [int(i) for i in group["row_index"].tolist()]
 82:             for a, b in zip(activities, activities[1:]):
 83:                 transition_counts[(a, b)] += 1
 84: 
 85:         if not sequences:
 86:             return PluginResult(
 87:                 "skipped",
 88:                 "No sequences detected",
 89:                 {},
 90:                 [],
 91:                 [],
 92:                 None,
 93:             )
 94: 
 95:         total_cases = sum(sequences.values())
 96:         max_variants = int(ctx.settings.get("max_variants", 20))
 97:         min_fraction = float(ctx.settings.get("min_variant_fraction", 0.0))
 98:         max_examples = int(ctx.settings.get("max_examples", 25))
 99: 
100:         findings = []
101:         sorted_variants = sorted(
102:             sequences.items(), key=lambda item: (-item[1], item[0])
103:         )
104:         for seq, count in sorted_variants[:max_variants]:
105:             fraction = count / total_cases if total_cases else 0.0
106:             if fraction < min_fraction:
107:                 continue
108:             row_ids = examples.get(seq, [])[:max_examples]
109:             findings.append(
110:                 {
111:                     "kind": "process_variant",
112:                     "variant": list(seq),
113:                     "count": int(count),
114:                     "fraction": float(fraction),
115:                     "columns": [case_col, activity_col]
116:                     + ([timestamp_col] if timestamp_col else []),
117:                     "evidence": {
118:                         "row_ids": row_ids,
119:                         "query": f"variant={seq}",
120:                     },
121:                 }
122:             )
123: 
124:         rare_variants = [
125:             (seq, count)
126:             for seq, count in sorted_variants
127:             if count == 1 and len(seq) > 1
128:         ]
129:         for seq, count in rare_variants[:max_variants]:
130:             row_ids = examples.get(seq, [])[:max_examples]
131:             findings.append(
132:                 {
133:                     "kind": "rare_variant",
134:                     "variant": list(seq),
135:                     "count": int(count),
136:                     "fraction": float(count / total_cases),
137:                     "columns": [case_col, activity_col]
138:                     + ([timestamp_col] if timestamp_col else []),
139:                     "evidence": {
140:                         "row_ids": row_ids,
141:                         "query": f"variant={seq}",
142:                     },
143:                 }
144:             )
145: 
146:         transitions = []
147:         for (a, b), count in sorted(
148:             transition_counts.items(), key=lambda item: (-item[1], item[0])
149:         ):
150:             transitions.append(
151:                 {
152:                     "kind": "transition",
153:                     "from": a,
154:                     "to": b,
155:                     "count": int(count),
156:                 }
157:             )
158: 
159:         artifacts_dir = ctx.artifacts_dir("analysis_process_sequence")
160:         out_path = artifacts_dir / "sequences.json"
161:         write_json(out_path, {"variants": findings, "transitions": transitions})
162:         artifacts = [
163:             PluginArtifact(
164:                 path=str(out_path.relative_to(ctx.run_dir)),
165:                 type="json",
166:                 description="Sequence mining results",
167:             )
168:         ]
169: 
170:         return PluginResult(
171:             "ok",
172:             "Computed process sequence variants",
173:             {"variants": len(findings), "transitions": len(transitions)},
174:             findings,
175:             artifacts,
176:             None,
177:         )
````

## File: plugins/analysis_process_sequence/plugin.yaml
````yaml
 1: id: analysis_process_sequence
 2: name: Process Sequence Mining
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on:
 7: - ingest_tabular
 8: capabilities:
 9: - needs_eventlog
10: settings:
11:   description: Sequence mining for event logs
12:   defaults:
13:     max_variants: 20
14:     min_variant_fraction: 0.0
15:     max_examples: 25
16: config_schema: config.schema.json
17: output_schema: output.schema.json
18: sandbox:
19:   no_network: true
20:   fs_allowlist:
21:   - appdata
22:   - plugins
````

## File: plugins/analysis_queue_delay_decomposition/__init__.py
````python
1: """Queue delay decomposition plugin."""
````

## File: plugins/analysis_queue_delay_decomposition/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "properties": {
 5:     "process_column": { "type": ["string", "null"] },
 6:     "queue_column": { "type": ["string", "null"] },
 7:     "eligible_column": { "type": ["string", "null"] },
 8:     "start_column": { "type": ["string", "null"] },
 9:     "end_column": { "type": ["string", "null"] },
10:     "dependency_columns": {
11:       "type": "array",
12:       "items": { "type": "string" },
13:       "default": []
14:     },
15:     "close_cycle_start_day": { "type": "integer", "minimum": 1, "maximum": 31 },
16:     "close_cycle_end_day": { "type": "integer", "minimum": 1, "maximum": 31 },
17:     "wait_threshold_seconds": { "type": "number", "minimum": 0 },
18:     "min_total_hours": { "type": "number", "minimum": 0 },
19:     "max_process_findings": { "type": "integer", "minimum": 1 },
20:     "target_process": { "type": ["string", "null"] },
21:     "exclude_processes": {
22:       "type": "array",
23:       "items": { "type": "string" },
24:       "default": []
25:     },
26:     "capacity_scale_factor": { "type": "number", "minimum": 0 },
27:     "max_examples": { "type": "integer", "minimum": 1 }
28:   },
29:   "additionalProperties": true
30: }
````

## File: plugins/analysis_queue_delay_decomposition/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "additionalProperties": true,
 4:   "properties": {
 5:     "artifacts": {
 6:       "type": "array"
 7:     },
 8:     "budget": {
 9:       "additionalProperties": true,
10:       "properties": {
11:         "cpu_limit_ms": {
12:           "anyOf": [
13:             {
14:               "type": "integer"
15:             },
16:             {
17:               "type": "null"
18:             }
19:           ]
20:         },
21:         "notes": {
22:           "anyOf": [
23:             {
24:               "type": "string"
25:             },
26:             {
27:               "type": "null"
28:             }
29:           ]
30:         },
31:         "row_limit": {
32:           "anyOf": [
33:             {
34:               "type": "integer"
35:             },
36:             {
37:               "type": "null"
38:             }
39:           ]
40:         },
41:         "sampled": {
42:           "type": "boolean"
43:         },
44:         "time_limit_ms": {
45:           "anyOf": [
46:             {
47:               "type": "integer"
48:             },
49:             {
50:               "type": "null"
51:             }
52:           ]
53:         }
54:       },
55:       "required": [
56:         "row_limit",
57:         "sampled",
58:         "time_limit_ms",
59:         "cpu_limit_ms"
60:       ],
61:       "type": "object"
62:     },
63:     "error": {
64:       "type": [
65:         "object",
66:         "null"
67:       ]
68:     },
69:     "findings": {
70:       "type": "array"
71:     },
72:     "metrics": {
73:       "type": "object"
74:     },
75:     "status": {
76:       "type": "string"
77:     },
78:     "summary": {
79:       "type": "string"
80:     }
81:   },
82:   "required": [
83:     "status",
84:     "summary",
85:     "metrics",
86:     "findings",
87:     "artifacts",
88:     "budget",
89:     "error"
90:   ],
91:   "type": "object"
92: }
````

## File: plugins/analysis_queue_delay_decomposition/plugin.yaml
````yaml
 1: id: analysis_queue_delay_decomposition
 2: name: Queue Delay Decomposition
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Decompose queue delay into eligible vs capacity waiting and summarize by process.
 9:   defaults:
10:     process_column: null
11:     queue_column: null
12:     eligible_column: null
13:     start_column: null
14:     end_column: null
15:     dependency_columns: []
16:     close_cycle_start_day: 20
17:     close_cycle_end_day: 5
18:     wait_threshold_seconds: 60
19:     min_total_hours: 1
20:     max_process_findings: 5
21:     target_process: qemail
22:     exclude_processes:
23:       - qlongjob
24:     capacity_scale_factor: 0.6667
25:     max_examples: 25
26: capabilities:
27:   - needs_eventlog
28:   - needs_timestamp
29: config_schema: config.schema.json
30: output_schema: output.schema.json
31: sandbox:
32:   no_network: true
33:   fs_allowlist:
34:     - appdata
35:     - plugins
36:     - run_dir
````

## File: plugins/analysis_sequence_classification/__init__.py
````python
1: """Standalone vs sequence classification plugin."""
````

## File: plugins/analysis_sequence_classification/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "properties": {
 5:     "process_column": { "type": ["string", "null"] },
 6:     "dependency_columns": {
 7:       "type": "array",
 8:       "items": { "type": "string" },
 9:       "default": []
10:     },
11:     "max_processes": { "type": "integer", "minimum": 1 },
12:     "min_sequence_ratio": { "type": "number", "minimum": 0, "maximum": 1 },
13:     "max_examples": { "type": "integer", "minimum": 1 }
14:   },
15:   "additionalProperties": true
16: }
````

## File: plugins/analysis_sequence_classification/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "additionalProperties": true,
 4:   "properties": {
 5:     "artifacts": {
 6:       "type": "array"
 7:     },
 8:     "budget": {
 9:       "additionalProperties": true,
10:       "properties": {
11:         "cpu_limit_ms": {
12:           "anyOf": [
13:             {
14:               "type": "integer"
15:             },
16:             {
17:               "type": "null"
18:             }
19:           ]
20:         },
21:         "notes": {
22:           "anyOf": [
23:             {
24:               "type": "string"
25:             },
26:             {
27:               "type": "null"
28:             }
29:           ]
30:         },
31:         "row_limit": {
32:           "anyOf": [
33:             {
34:               "type": "integer"
35:             },
36:             {
37:               "type": "null"
38:             }
39:           ]
40:         },
41:         "sampled": {
42:           "type": "boolean"
43:         },
44:         "time_limit_ms": {
45:           "anyOf": [
46:             {
47:               "type": "integer"
48:             },
49:             {
50:               "type": "null"
51:             }
52:           ]
53:         }
54:       },
55:       "required": [
56:         "row_limit",
57:         "sampled",
58:         "time_limit_ms",
59:         "cpu_limit_ms"
60:       ],
61:       "type": "object"
62:     },
63:     "error": {
64:       "type": [
65:         "object",
66:         "null"
67:       ]
68:     },
69:     "findings": {
70:       "type": "array"
71:     },
72:     "metrics": {
73:       "type": "object"
74:     },
75:     "status": {
76:       "type": "string"
77:     },
78:     "summary": {
79:       "type": "string"
80:     }
81:   },
82:   "required": [
83:     "status",
84:     "summary",
85:     "metrics",
86:     "findings",
87:     "artifacts",
88:     "budget",
89:     "error"
90:   ],
91:   "type": "object"
92: }
````

## File: plugins/analysis_sequence_classification/plugin.yaml
````yaml
 1: id: analysis_sequence_classification
 2: name: Standalone vs Sequence Classification
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Classify jobs as standalone or sequence-linked based on dependency pointers.
 9:   defaults:
10:     process_column: null
11:     dependency_columns: []
12:     max_processes: 10
13:     min_sequence_ratio: 0.0
14:     max_examples: 25
15: capabilities:
16:   - needs_eventlog
17: config_schema: config.schema.json
18: output_schema: output.schema.json
19: sandbox:
20:   no_network: true
21:   fs_allowlist:
22:     - appdata
23:     - plugins
24:     - run_dir
````

## File: plugins/analysis_tail_isolation/__init__.py
````python
1: 
````

## File: plugins/analysis_tail_isolation/config.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "process_column": {"type": ["string", "null"]},
 7:     "module_column": {"type": ["string", "null"]},
 8:     "user_column": {"type": ["string", "null"]},
 9:     "sequence_column": {"type": ["string", "null"]},
10:     "queue_column": {"type": ["string", "null"]},
11:     "eligible_column": {"type": ["string", "null"]},
12:     "start_column": {"type": ["string", "null"]},
13:     "wait_threshold_seconds": {"type": "number", "minimum": 0},
14:     "max_groups": {"type": "integer", "minimum": 1},
15:     "max_examples": {"type": "integer", "minimum": 0}
16:   }
17: }
````

## File: plugins/analysis_tail_isolation/output.schema.json
````json
 1: {
 2:   "$schema": "https://json-schema.org/draft/2020-12/schema",
 3:   "type": "object",
 4:   "additionalProperties": true,
 5:   "properties": {
 6:     "status": {"type": "string"},
 7:     "summary": {"type": "string"},
 8:     "metrics": {"type": "object"},
 9:     "findings": {"type": "array"},
10:     "artifacts": {"type": "array"},
11:     "budget": {
12:       "type": "object",
13:       "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
14:       "properties": {
15:         "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
16:         "sampled": {"type": "boolean"},
17:         "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
18:         "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
19:         "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
20:       },
21:       "additionalProperties": true
22:     },
23:     "error": {"type": ["object", "null"]}
24:   },
25:   "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"]
26: }
````

## File: plugins/analysis_tail_isolation/plugin.yaml
````yaml
 1: id: analysis_tail_isolation
 2: name: Tail Isolation
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Isolate tail eligible-wait segments above a threshold and attribute by dimension.
 9:   defaults:
10:     process_column: null
11:     module_column: null
12:     user_column: null
13:     sequence_column: null
14:     queue_column: null
15:     eligible_column: null
16:     start_column: null
17:     wait_threshold_seconds: 60
18:     max_groups: 5
19:     max_examples: 25
20: capabilities:
21:   - needs_eventlog
22:   - needs_timestamp
23: config_schema: config.schema.json
24: output_schema: output.schema.json
25: sandbox:
26:   no_network: true
27:   fs_allowlist:
28:     - appdata
29:     - plugins
30:     - run_dir
````

## File: plugins/analysis_tail_isolation/README.md
````markdown
1: # Tail Isolation
2: 
3: Isolates eligible-wait tail segments above a threshold and attributes them by process, module, user, or sequence.
````

## File: plugins/ingest_tabular/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "chunk_size": {
 6:       "minimum": 1,
 7:       "type": "integer"
 8:     },
 9:     "delimiter": {
10:       "type": [
11:         "string",
12:         "null"
13:       ]
14:     },
15:     "encoding": {
16:       "type": "string"
17:     },
18:     "input_file": {
19:       "type": "string"
20:     },
21:     "sheet_name": {
22:       "type": [
23:         "string",
24:         "null"
25:       ]
26:     }
27:   },
28:   "title": "Ingest Tabular Config",
29:   "type": "object"
30: }
````

## File: plugins/ingest_tabular/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "additionalProperties": true,
113:         "properties": {
114:           "evidence": {
115:             "additionalProperties": true,
116:             "properties": {
117:               "column_ids": {
118:                 "items": {
119:                   "type": "integer"
120:                 },
121:                 "type": "array"
122:               },
123:               "dataset_id": {
124:                 "type": "string"
125:               },
126:               "dataset_version_id": {
127:                 "type": "string"
128:               },
129:               "query": {
130:                 "type": [
131:                   "string",
132:                   "null"
133:                 ]
134:               },
135:               "row_ids": {
136:                 "items": {
137:                   "type": "integer"
138:                 },
139:                 "type": "array"
140:               },
141:               "row_ranges": {
142:                 "items": {
143:                   "additionalProperties": false,
144:                   "properties": {
145:                     "end": {
146:                       "type": "integer"
147:                     },
148:                     "start": {
149:                       "type": "integer"
150:                     }
151:                   },
152:                   "required": [
153:                     "start",
154:                     "end"
155:                   ],
156:                   "type": "object"
157:                 },
158:                 "type": "array"
159:               }
160:             },
161:             "required": [
162:               "dataset_id",
163:               "dataset_version_id",
164:               "row_ids",
165:               "column_ids"
166:             ],
167:             "type": "object"
168:           },
169:           "kind": {
170:             "type": "string"
171:           }
172:         },
173:         "required": [
174:           "kind",
175:           "evidence"
176:         ],
177:         "type": "object"
178:       },
179:       "type": "array"
180:     },
181:     "metrics": {
182:       "additionalProperties": false,
183:       "properties": {
184:         "cols": {
185:           "type": "integer"
186:         },
187:         "rows": {
188:           "type": "integer"
189:         }
190:       },
191:       "type": "object"
192:     },
193:     "status": {
194:       "enum": [
195:         "ok",
196:         "skipped",
197:         "error"
198:       ],
199:       "type": "string"
200:     },
201:     "summary": {
202:       "type": "string"
203:     }
204:   },
205:   "required": [
206:     "status",
207:     "summary",
208:     "metrics",
209:     "findings",
210:     "artifacts",
211:     "budget"
212:   ],
213:   "title": "Plugin Result",
214:   "type": "object"
215: }
````

## File: plugins/llm_prompt_builder/config.schema.json
````json
1: {
2:   "$schema": "http://json-schema.org/draft-07/schema#",
3:   "additionalProperties": false,
4:   "properties": {},
5:   "title": "LLM Prompt Builder Config",
6:   "type": "object"
7: }
````

## File: plugins/llm_prompt_builder/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "additionalProperties": true,
113:         "properties": {
114:           "evidence": {
115:             "additionalProperties": true,
116:             "properties": {
117:               "column_ids": {
118:                 "items": {
119:                   "type": "integer"
120:                 },
121:                 "type": "array"
122:               },
123:               "dataset_id": {
124:                 "type": "string"
125:               },
126:               "dataset_version_id": {
127:                 "type": "string"
128:               },
129:               "query": {
130:                 "type": [
131:                   "string",
132:                   "null"
133:                 ]
134:               },
135:               "row_ids": {
136:                 "items": {
137:                   "type": "integer"
138:                 },
139:                 "type": "array"
140:               },
141:               "row_ranges": {
142:                 "items": {
143:                   "additionalProperties": false,
144:                   "properties": {
145:                     "end": {
146:                       "type": "integer"
147:                     },
148:                     "start": {
149:                       "type": "integer"
150:                     }
151:                   },
152:                   "required": [
153:                     "start",
154:                     "end"
155:                   ],
156:                   "type": "object"
157:                 },
158:                 "type": "array"
159:               }
160:             },
161:             "required": [
162:               "dataset_id",
163:               "dataset_version_id",
164:               "row_ids",
165:               "column_ids"
166:             ],
167:             "type": "object"
168:           },
169:           "kind": {
170:             "type": "string"
171:           }
172:         },
173:         "required": [
174:           "kind",
175:           "evidence"
176:         ],
177:         "type": "object"
178:       },
179:       "type": "array"
180:     },
181:     "metrics": {
182:       "additionalProperties": false,
183:       "properties": {},
184:       "type": "object"
185:     },
186:     "status": {
187:       "enum": [
188:         "ok",
189:         "skipped",
190:         "error"
191:       ],
192:       "type": "string"
193:     },
194:     "summary": {
195:       "type": "string"
196:     }
197:   },
198:   "required": [
199:     "status",
200:     "summary",
201:     "metrics",
202:     "findings",
203:     "artifacts",
204:     "budget"
205:   ],
206:   "title": "Plugin Result",
207:   "type": "object"
208: }
````

## File: plugins/planner_basic/__init__.py
````python
1: 
````

## File: plugins/planner_basic/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "allow": {
 6:       "items": {
 7:         "type": "string"
 8:       },
 9:       "type": "array"
10:     },
11:     "deny": {
12:       "items": {
13:         "type": "string"
14:       },
15:       "type": "array"
16:     }
17:   },
18:   "title": "Planner Basic Config",
19:   "type": "object"
20: }
````

## File: plugins/planner_basic/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "evidence": {
187:                     "additionalProperties": true,
188:                     "properties": {
189:                       "column_ids": {
190:                         "items": {
191:                           "type": "integer"
192:                         },
193:                         "type": "array"
194:                       },
195:                       "dataset_id": {
196:                         "type": "string"
197:                       },
198:                       "dataset_version_id": {
199:                         "type": "string"
200:                       },
201:                       "query": {
202:                         "type": [
203:                           "string",
204:                           "null"
205:                         ]
206:                       },
207:                       "row_ids": {
208:                         "items": {
209:                           "type": "integer"
210:                         },
211:                         "type": "array"
212:                       },
213:                       "row_ranges": {
214:                         "items": {
215:                           "additionalProperties": false,
216:                           "properties": {
217:                             "end": {
218:                               "type": "integer"
219:                             },
220:                             "start": {
221:                               "type": "integer"
222:                             }
223:                           },
224:                           "required": [
225:                             "start",
226:                             "end"
227:                           ],
228:                           "type": "object"
229:                         },
230:                         "type": "array"
231:                       }
232:                     },
233:                     "required": [
234:                       "dataset_id",
235:                       "dataset_version_id",
236:                       "row_ids",
237:                       "column_ids"
238:                     ],
239:                     "type": "object"
240:                   },
241:                   "kind": {
242:                     "const": "plan"
243:                   },
244:                   "plugin_id": {
245:                     "type": "string"
246:                   }
247:                 },
248:                 "required": [
249:                   "kind",
250:                   "plugin_id"
251:                 ],
252:                 "type": "object"
253:               }
254:             ]
255:           }
256:         ]
257:       },
258:       "type": "array"
259:     },
260:     "metrics": {
261:       "additionalProperties": false,
262:       "properties": {
263:         "selected_plugins": {
264:           "items": {
265:             "type": "string"
266:           },
267:           "type": "array"
268:         }
269:       },
270:       "type": "object"
271:     },
272:     "status": {
273:       "enum": [
274:         "ok",
275:         "skipped",
276:         "error"
277:       ],
278:       "type": "string"
279:     },
280:     "summary": {
281:       "type": "string"
282:     }
283:   },
284:   "required": [
285:     "status",
286:     "summary",
287:     "metrics",
288:     "findings",
289:     "artifacts",
290:     "budget"
291:   ],
292:   "title": "Plugin Result",
293:   "type": "object"
294: }
````

## File: plugins/planner_basic/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: from pathlib import Path
 4: 
 5: from statistic_harness.core.planner import select_plugins
 6: from statistic_harness.core.plugin_manager import PluginManager
 7: from statistic_harness.core.types import PluginArtifact, PluginResult
 8: from statistic_harness.core.utils import write_json
 9: 
10: 
11: class Plugin:
12:     def run(self, ctx) -> PluginResult:
13:         if not ctx.dataset_version_id:
14:             return PluginResult(
15:                 "error",
16:                 "Missing dataset version",
17:                 {},
18:                 [],
19:                 [],
20:                 None,
21:             )
22:         manager = PluginManager(Path("plugins"))
23:         specs = manager.discover()
24:         selected = select_plugins(specs, ctx.storage, ctx.dataset_version_id)
25:         allow = ctx.settings.get("allow") or []
26:         deny = ctx.settings.get("deny") or []
27:         if allow:
28:             selected = [pid for pid in selected if pid in allow]
29:         if deny:
30:             selected = [pid for pid in selected if pid not in deny]
31:         artifacts_dir = ctx.artifacts_dir("planner_basic")
32:         plan_path = artifacts_dir / "plan.json"
33:         write_json(plan_path, {"selected_plugins": selected})
34:         artifacts = [
35:             PluginArtifact(
36:                 path=str(plan_path.relative_to(ctx.run_dir)),
37:                 type="json",
38:                 description="Planner output",
39:             )
40:         ]
41:         findings = [{"kind": "plan", "plugin_id": pid} for pid in selected]
42:         return PluginResult(
43:             "ok",
44:             "Planned plugin selection",
45:             {"selected_plugins": selected},
46:             findings,
47:             artifacts,
48:             None,
49:         )
````

## File: plugins/profile_basic/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "max_corr_cols": {
 6:       "minimum": 0,
 7:       "type": "integer"
 8:     }
 9:   },
10:   "title": "Profile Basic Config",
11:   "type": "object"
12: }
````

## File: plugins/profile_basic/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "additionalProperties": true,
113:         "properties": {
114:           "evidence": {
115:             "additionalProperties": true,
116:             "properties": {
117:               "column_ids": {
118:                 "items": {
119:                   "type": "integer"
120:                 },
121:                 "type": "array"
122:               },
123:               "dataset_id": {
124:                 "type": "string"
125:               },
126:               "dataset_version_id": {
127:                 "type": "string"
128:               },
129:               "query": {
130:                 "type": [
131:                   "string",
132:                   "null"
133:                 ]
134:               },
135:               "row_ids": {
136:                 "items": {
137:                   "type": "integer"
138:                 },
139:                 "type": "array"
140:               },
141:               "row_ranges": {
142:                 "items": {
143:                   "additionalProperties": false,
144:                   "properties": {
145:                     "end": {
146:                       "type": "integer"
147:                     },
148:                     "start": {
149:                       "type": "integer"
150:                     }
151:                   },
152:                   "required": [
153:                     "start",
154:                     "end"
155:                   ],
156:                   "type": "object"
157:                 },
158:                 "type": "array"
159:               }
160:             },
161:             "required": [
162:               "dataset_id",
163:               "dataset_version_id",
164:               "row_ids",
165:               "column_ids"
166:             ],
167:             "type": "object"
168:           },
169:           "kind": {
170:             "type": "string"
171:           }
172:         },
173:         "required": [
174:           "kind",
175:           "evidence"
176:         ],
177:         "type": "object"
178:       },
179:       "type": "array"
180:     },
181:     "metrics": {
182:       "additionalProperties": false,
183:       "properties": {
184:         "columns": {
185:           "type": "integer"
186:         }
187:       },
188:       "type": "object"
189:     },
190:     "status": {
191:       "enum": [
192:         "ok",
193:         "skipped",
194:         "error"
195:       ],
196:       "type": "string"
197:     },
198:     "summary": {
199:       "type": "string"
200:     }
201:   },
202:   "required": [
203:     "status",
204:     "summary",
205:     "metrics",
206:     "findings",
207:     "artifacts",
208:     "budget"
209:   ],
210:   "title": "Plugin Result",
211:   "type": "object"
212: }
````

## File: plugins/profile_eventlog/__init__.py
````python
1: 
````

## File: plugins/profile_eventlog/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "type": "object",
 4:   "additionalProperties": false,
 5:   "properties": {
 6:     "sample_rows": {
 7:       "type": "integer",
 8:       "minimum": 10
 9:     },
10:     "min_confidence": {
11:       "type": "number",
12:       "minimum": 0
13:     }
14:   }
15: }
````

## File: plugins/profile_eventlog/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "additionalProperties": true,
113:         "properties": {
114:           "evidence": {
115:             "additionalProperties": true,
116:             "properties": {
117:               "column_ids": {
118:                 "items": {
119:                   "type": "integer"
120:                 },
121:                 "type": "array"
122:               },
123:               "dataset_id": {
124:                 "type": "string"
125:               },
126:               "dataset_version_id": {
127:                 "type": "string"
128:               },
129:               "query": {
130:                 "type": [
131:                   "string",
132:                   "null"
133:                 ]
134:               },
135:               "row_ids": {
136:                 "items": {
137:                   "type": "integer"
138:                 },
139:                 "type": "array"
140:               },
141:               "row_ranges": {
142:                 "items": {
143:                   "additionalProperties": false,
144:                   "properties": {
145:                     "end": {
146:                       "type": "integer"
147:                     },
148:                     "start": {
149:                       "type": "integer"
150:                     }
151:                   },
152:                   "required": [
153:                     "start",
154:                     "end"
155:                   ],
156:                   "type": "object"
157:                 },
158:                 "type": "array"
159:               }
160:             },
161:             "required": [
162:               "dataset_id",
163:               "dataset_version_id",
164:               "row_ids",
165:               "column_ids"
166:             ],
167:             "type": "object"
168:           },
169:           "kind": {
170:             "type": "string"
171:           },
172:           "measurement_type": {
173:             "enum": [
174:               "measured",
175:               "modeled",
176:               "not_applicable",
177:               "error"
178:             ],
179:             "type": "string"
180:           }
181:         },
182:         "required": [
183:           "kind",
184:           "measurement_type",
185:           "evidence"
186:         ],
187:         "type": "object"
188:       },
189:       "type": "array"
190:     },
191:     "metrics": {
192:       "type": "object"
193:     },
194:     "status": {
195:       "enum": [
196:         "ok",
197:         "skipped",
198:         "error"
199:       ],
200:       "type": "string"
201:     },
202:     "summary": {
203:       "type": "string"
204:     }
205:   },
206:   "required": [
207:     "status",
208:     "summary",
209:     "metrics",
210:     "findings",
211:     "artifacts",
212:     "budget"
213:   ],
214:   "title": "Plugin Result",
215:   "type": "object"
216: }
````

## File: plugins/profile_eventlog/plugin.yaml
````yaml
 1: id: profile_eventlog
 2: name: Profile Eventlog
 3: version: 0.1.0
 4: type: profile
 5: entrypoint: plugin.py:Plugin
 6: depends_on:
 7: - ingest_tabular
 8: settings:
 9:   description: Event-log field inference
10:   defaults:
11:     sample_rows: 500
12:     min_confidence: 2.0
13: capabilities: []
14: config_schema: config.schema.json
15: output_schema: output.schema.json
16: sandbox:
17:   no_network: true
18:   fs_allowlist:
19:   - appdata
20:   - plugins
````

## File: plugins/report_bundle/config.schema.json
````json
1: {
2:   "$schema": "http://json-schema.org/draft-07/schema#",
3:   "additionalProperties": false,
4:   "properties": {},
5:   "title": "Report Bundle Config",
6:   "type": "object"
7: }
````

## File: plugins/report_bundle/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "additionalProperties": true,
113:         "properties": {
114:           "evidence": {
115:             "additionalProperties": true,
116:             "properties": {
117:               "column_ids": {
118:                 "items": {
119:                   "type": "integer"
120:                 },
121:                 "type": "array"
122:               },
123:               "dataset_id": {
124:                 "type": "string"
125:               },
126:               "dataset_version_id": {
127:                 "type": "string"
128:               },
129:               "query": {
130:                 "type": [
131:                   "string",
132:                   "null"
133:                 ]
134:               },
135:               "row_ids": {
136:                 "items": {
137:                   "type": "integer"
138:                 },
139:                 "type": "array"
140:               },
141:               "row_ranges": {
142:                 "items": {
143:                   "additionalProperties": false,
144:                   "properties": {
145:                     "end": {
146:                       "type": "integer"
147:                     },
148:                     "start": {
149:                       "type": "integer"
150:                     }
151:                   },
152:                   "required": [
153:                     "start",
154:                     "end"
155:                   ],
156:                   "type": "object"
157:                 },
158:                 "type": "array"
159:               }
160:             },
161:             "required": [
162:               "dataset_id",
163:               "dataset_version_id",
164:               "row_ids",
165:               "column_ids"
166:             ],
167:             "type": "object"
168:           },
169:           "kind": {
170:             "type": "string"
171:           }
172:         },
173:         "required": [
174:           "kind",
175:           "evidence"
176:         ],
177:         "type": "object"
178:       },
179:       "type": "array"
180:     },
181:     "metrics": {
182:       "additionalProperties": false,
183:       "properties": {},
184:       "type": "object"
185:     },
186:     "status": {
187:       "enum": [
188:         "ok",
189:         "skipped",
190:         "error"
191:       ],
192:       "type": "string"
193:     },
194:     "summary": {
195:       "type": "string"
196:     }
197:   },
198:   "required": [
199:     "status",
200:     "summary",
201:     "metrics",
202:     "findings",
203:     "artifacts",
204:     "budget"
205:   ],
206:   "title": "Plugin Result",
207:   "type": "object"
208: }
````

## File: plugins/transform_normalize_mixed/__init__.py
````python
1: __all__ = ["plugin"]
````

## File: plugins/transform_normalize_mixed/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "title": "Normalize Mixed Types Config",
 4:   "type": "object",
 5:   "additionalProperties": false,
 6:   "properties": {
 7:     "template_name": {
 8:       "type": "string"
 9:     },
10:     "lowercase": {
11:       "type": "boolean"
12:     },
13:     "strip": {
14:       "type": "boolean"
15:     },
16:     "collapse_whitespace": {
17:       "type": "boolean"
18:     },
19:     "numeric_coercion": {
20:       "type": "boolean"
21:     },
22:     "numeric_threshold": {
23:       "type": "number",
24:       "minimum": 0.0,
25:       "maximum": 1.0
26:     },
27:     "exclude_name_patterns": {
28:       "type": "array",
29:       "items": { "type": "string" }
30:     },
31:     "chunk_size": {
32:       "type": "integer",
33:       "minimum": 1
34:     },
35:     "sample_rows": {
36:       "type": "integer",
37:       "minimum": 0
38:     }
39:   }
40: }
````

## File: plugins/transform_normalize_mixed/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "additionalProperties": true,
113:         "properties": {
114:           "evidence": {
115:             "additionalProperties": true,
116:             "properties": {
117:               "column_ids": {
118:                 "items": {
119:                   "type": "integer"
120:                 },
121:                 "type": "array"
122:               },
123:               "dataset_id": {
124:                 "type": "string"
125:               },
126:               "dataset_version_id": {
127:                 "type": "string"
128:               },
129:               "query": {
130:                 "anyOf": [
131:                   {
132:                     "type": "string"
133:                   },
134:                   {
135:                     "type": "null"
136:                   }
137:                 ]
138:               },
139:               "row_ids": {
140:                 "items": {
141:                   "type": "integer"
142:                 },
143:                 "type": "array"
144:               },
145:               "row_ranges": {
146:                 "items": {
147:                   "additionalProperties": false,
148:                   "properties": {
149:                     "end": {
150:                       "type": "integer"
151:                     },
152:                     "start": {
153:                       "type": "integer"
154:                     }
155:                   },
156:                   "required": [
157:                     "start",
158:                     "end"
159:                   ],
160:                   "type": "object"
161:                 },
162:                 "type": "array"
163:               }
164:             },
165:             "required": [
166:               "dataset_id",
167:               "dataset_version_id",
168:               "row_ids",
169:               "column_ids"
170:             ],
171:             "type": "object"
172:           },
173:           "kind": {
174:             "type": "string"
175:           }
176:         },
177:         "required": [
178:           "kind",
179:           "evidence"
180:         ],
181:         "type": "object"
182:       },
183:       "type": "array"
184:     },
185:     "metrics": {
186:       "additionalProperties": false,
187:       "properties": {
188:         "coerced_columns": {
189:           "items": {
190:             "type": "string"
191:           },
192:           "type": "array"
193:         },
194:         "column_count": {
195:           "type": "integer"
196:         },
197:         "row_count": {
198:           "type": "integer"
199:         },
200:         "template_id": {
201:           "type": "integer"
202:         }
203:       },
204:       "type": "object"
205:     },
206:     "status": {
207:       "enum": [
208:         "ok",
209:         "skipped",
210:         "error"
211:       ],
212:       "type": "string"
213:     },
214:     "summary": {
215:       "type": "string"
216:     }
217:   },
218:   "required": [
219:     "status",
220:     "summary",
221:     "metrics",
222:     "findings",
223:     "artifacts",
224:     "budget"
225:   ],
226:   "title": "Plugin Result",
227:   "type": "object"
228: }
````

## File: plugins/transform_normalize_mixed/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: import json
  4: import re
  5: from typing import Any
  6: 
  7: from statistic_harness.core.template import mapping_hash
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import json_dumps, now_iso, quote_identifier, write_json
 10: 
 11: 
 12: _NUMERIC_RE = re.compile(r"^[+-]?(\d+(\.\d+)?|\.\d+)$")
 13: _WS_RE = re.compile(r"\s+")
 14: 
 15: 
 16: def _normalize_name(raw: str, fallback: str) -> str:
 17:     cleaned = str(raw).strip()
 18:     return cleaned if cleaned else fallback
 19: 
 20: 
 21: def _is_numeric_like(value: Any) -> bool:
 22:     if value is None:
 23:         return False
 24:     if isinstance(value, (int, float)) and not isinstance(value, bool):
 25:         return True
 26:     text = str(value).strip()
 27:     if not text:
 28:         return False
 29:     candidate = text.replace(",", "").replace("_", "")
 30:     return bool(_NUMERIC_RE.match(candidate))
 31: 
 32: 
 33: def _normalize_value(
 34:     value: Any,
 35:     *,
 36:     allow_numeric: bool,
 37:     lowercase: bool,
 38:     strip: bool,
 39:     collapse_whitespace: bool,
 40:     empty_as_null: bool = True,
 41: ) -> Any:
 42:     if value is None:
 43:         return None
 44:     if isinstance(value, bool):
 45:         return int(value)
 46:     if isinstance(value, (int, float)) and not isinstance(value, bool):
 47:         return value
 48:     text = str(value)
 49:     if strip:
 50:         text = text.strip()
 51:     if collapse_whitespace:
 52:         text = _WS_RE.sub(" ", text)
 53:     if lowercase:
 54:         text = text.lower()
 55:     if empty_as_null and text == "":
 56:         return None
 57:     if allow_numeric:
 58:         candidate = text.replace(",", "").replace("_", "")
 59:         if _NUMERIC_RE.match(candidate):
 60:             try:
 61:                 if "." in candidate:
 62:                     return float(candidate)
 63:                 return int(candidate)
 64:             except ValueError:
 65:                 return text
 66:     return text
 67: 
 68: 
 69: class Plugin:
 70:     def run(self, ctx) -> PluginResult:
 71:         dataset_version_id = ctx.dataset_version_id
 72:         if not dataset_version_id:
 73:             return PluginResult("error", "Missing dataset version", {}, [], [], None)
 74: 
 75:         template_name = str(ctx.settings.get("template_name") or "").strip()
 76:         lowercase = bool(ctx.settings.get("lowercase", True))
 77:         strip = bool(ctx.settings.get("strip", True))
 78:         collapse_whitespace = bool(ctx.settings.get("collapse_whitespace", True))
 79:         numeric_coercion = bool(ctx.settings.get("numeric_coercion", True))
 80:         numeric_threshold = float(ctx.settings.get("numeric_threshold", 0.98))
 81:         exclude_patterns = ctx.settings.get("exclude_name_patterns")
 82:         if not exclude_patterns:
 83:             exclude_patterns = ["id", "uuid", "guid", "key"]
 84:         exclude_patterns = [
 85:             str(pat).strip().lower()
 86:             for pat in exclude_patterns
 87:             if str(pat).strip()
 88:         ]
 89:         chunk_size = int(ctx.settings.get("chunk_size", 1000))
 90:         sample_rows = int(ctx.settings.get("sample_rows", 500))
 91: 
 92:         dataset = ctx.storage.get_dataset_version(dataset_version_id)
 93:         if not dataset:
 94:             return PluginResult("error", "Dataset not found", {}, [], [], None)
 95:         raw_format_id = dataset.get("raw_format_id")
 96: 
 97:         columns = ctx.storage.fetch_dataset_columns(dataset_version_id)
 98:         if not columns:
 99:             return PluginResult("skipped", "No columns found", {}, [], [], None)
100: 
101:         name_counts: dict[str, int] = {}
102:         mapping: dict[str, str] = {}
103:         column_safe_names: list[str] = []
104:         original_names: list[str] = []
105: 
106:         for idx, col in enumerate(columns, start=1):
107:             original = _normalize_name(col["original_name"], f"column_{idx}")
108:             base = original
109:             if base in name_counts:
110:                 name_counts[base] += 1
111:                 field_name = f"{base}_{name_counts[base]}"
112:             else:
113:                 name_counts[base] = 1
114:                 field_name = base
115:             mapping[field_name] = original
116:             original_names.append(original)
117:             column_safe_names.append(col["safe_name"])
118: 
119:         safe_by_original = {
120:             orig: safe for orig, safe in zip(original_names, column_safe_names)
121:         }
122: 
123:         coercion_allowed: dict[str, bool] = {}
124:         if numeric_coercion:
125:             sample_limit = max(sample_rows, 0)
126:             if sample_limit > 0:
127:                 with ctx.storage.connection() as conn:
128:                     raw_table = dataset["table_name"]
129:                     quoted_cols = ", ".join(
130:                         quote_identifier(col) for col in column_safe_names
131:                     )
132:                     sql = (
133:                         f"SELECT {quoted_cols} FROM {quote_identifier(raw_table)} "
134:                         "ORDER BY row_id LIMIT ?"
135:                     )
136:                     cur = conn.execute(sql, (sample_limit,))
137:                     sample = cur.fetchall()
138:                 counts = {col: {"num": 0, "total": 0} for col in column_safe_names}
139:                 for row in sample:
140:                     for col in column_safe_names:
141:                         value = row[col]
142:                         if value is None:
143:                             continue
144:                         counts[col]["total"] += 1
145:                         if _is_numeric_like(value):
146:                             counts[col]["num"] += 1
147:                 for col in column_safe_names:
148:                     total = counts[col]["total"]
149:                     ratio = counts[col]["num"] / total if total else 0.0
150:                     coercion_allowed[col] = ratio >= numeric_threshold
151:             else:
152:                 coercion_allowed = {col: True for col in column_safe_names}
153:         else:
154:             coercion_allowed = {col: False for col in column_safe_names}
155: 
156:         for col, name in zip(column_safe_names, original_names):
157:             lowered = name.lower()
158:             if any(token in lowered for token in exclude_patterns):
159:                 coercion_allowed[col] = False
160: 
161:         field_defs: list[dict[str, Any]] = []
162:         for field_name, original in zip(mapping.keys(), original_names):
163:             safe_name = safe_by_original.get(original, "")
164:             sqlite_type = "REAL" if coercion_allowed.get(safe_name) else "TEXT"
165:             field_defs.append(
166:                 {
167:                     "name": field_name,
168:                     "dtype": next(
169:                         (col.get("dtype") for col in columns if col["original_name"] == original),
170:                         None,
171:                     ),
172:                     "sqlite_type": sqlite_type,
173:                 }
174:             )
175: 
176:         if not template_name:
177:             if raw_format_id:
178:                 template_name = f"normalized_rawformat_{raw_format_id}"
179:             else:
180:                 template_name = f"normalized_{dataset_version_id}"
181: 
182:         templates = ctx.storage.list_templates()
183:         template = next(
184:             (item for item in templates if item.get("name") == template_name), None
185:         )
186:         if template:
187:             template_id = int(template["template_id"])
188:             template_fields = ctx.storage.fetch_template_fields(template_id)
189:             if len(template_fields) != len(field_defs):
190:                 return PluginResult(
191:                     "error",
192:                     "Template fields mismatch for existing template",
193:                     {},
194:                     [],
195:                     [],
196:                     None,
197:                 )
198:         else:
199:             template_id = ctx.storage.create_template(
200:                 template_name,
201:                 field_defs,
202:                 "Normalized mixed-type view",
203:                 "v1",
204:                 now_iso(),
205:             )
206:             template = ctx.storage.fetch_template(template_id)
207:             template_fields = ctx.storage.fetch_template_fields(template_id)
208: 
209:         template_fields = template_fields if template else []
210:         if not template_fields:
211:             return PluginResult("error", "Template fields missing", {}, [], [], None)
212: 
213:         table_name = template["table_name"] if template else None
214:         if not table_name:
215:             return PluginResult("error", "Template table missing", {}, [], [], None)
216: 
217:         normalized_settings = {
218:             "lowercase": lowercase,
219:             "strip": strip,
220:             "collapse_whitespace": collapse_whitespace,
221:             "numeric_coercion": numeric_coercion,
222:             "numeric_threshold": numeric_threshold,
223:             "exclude_name_patterns": exclude_patterns,
224:         }
225:         mapping_payload = {"mapping": mapping, "normalization": normalized_settings}
226:         mapping_json = json_dumps(mapping_payload)
227:         mapping_h = mapping_hash(mapping_payload)
228: 
229:         ctx.storage.upsert_dataset_template(
230:             dataset_version_id,
231:             template_id,
232:             mapping_json,
233:             mapping_h,
234:             "pending",
235:             now_iso(),
236:             now_iso(),
237:         )
238: 
239:         row_count = 0
240:         coerced_columns = [
241:             name for name, safe in safe_by_original.items() if coercion_allowed.get(safe)
242:         ]
243: 
244:         try:
245:             with ctx.storage.connection() as conn:
246:                 conn.execute(
247:                     f"DELETE FROM {quote_identifier(table_name)} WHERE dataset_version_id = ?",
248:                     (dataset_version_id,),
249:                 )
250:                 raw_table = dataset["table_name"]
251:                 quoted_cols = ", ".join(
252:                     quote_identifier(col) for col in column_safe_names
253:                 )
254:                 select_sql = (
255:                     f"SELECT row_id, row_index, {quoted_cols} "
256:                     f"FROM {quote_identifier(raw_table)} WHERE row_id > ? "
257:                     "ORDER BY row_id LIMIT ?"
258:                 )
259:                 last_row_id = 0
260:                 template_safe_cols = [field["safe_name"] for field in template_fields]
261:                 field_names = [field["name"] for field in template_fields]
262: 
263:                 while True:
264:                     cur = conn.execute(select_sql, (last_row_id, chunk_size))
265:                     batch_rows = cur.fetchall()
266:                     if not batch_rows:
267:                         break
268:                     batch = []
269:                     for row in batch_rows:
270:                         last_row_id = int(row["row_id"])
271:                         raw_by_name = {
272:                             orig: row[safe]
273:                             for orig, safe in zip(original_names, column_safe_names)
274:                         }
275:                         values = []
276:                         row_data = {}
277:                         for field_name in field_names:
278:                             raw_name = mapping.get(field_name)
279:                             value = raw_by_name.get(raw_name)
280:                             safe_name = safe_by_original.get(raw_name, "")
281:                             allow_numeric = coercion_allowed.get(safe_name, False)
282:                             normalized = _normalize_value(
283:                                 value,
284:                                 allow_numeric=allow_numeric,
285:                                 lowercase=lowercase,
286:                                 strip=strip,
287:                                 collapse_whitespace=collapse_whitespace,
288:                             )
289:                             values.append(normalized)
290:                             row_data[field_name] = normalized
291:                         row_json = json.dumps(row_data, ensure_ascii=False)
292:                         batch.append(
293:                             (
294:                                 dataset_version_id,
295:                                 row["row_index"],
296:                                 row_json,
297:                                 *values,
298:                             )
299:                         )
300:                         row_count += 1
301:                     ctx.storage.insert_template_rows(
302:                         table_name, template_safe_cols, batch, conn
303:                     )
304: 
305:             ctx.storage.upsert_dataset_template(
306:                 dataset_version_id,
307:                 template_id,
308:                 mapping_json,
309:                 mapping_h,
310:                 "ready",
311:                 now_iso(),
312:                 now_iso(),
313:             )
314:             ctx.storage.record_template_conversion(
315:                 dataset_version_id,
316:                 template_id,
317:                 "completed",
318:                 now_iso(),
319:                 now_iso(),
320:                 mapping_h,
321:                 row_count=row_count,
322:             )
323:         except Exception as exc:  # pragma: no cover - error flow
324:             ctx.storage.upsert_dataset_template(
325:                 dataset_version_id,
326:                 template_id,
327:                 mapping_json,
328:                 mapping_h,
329:                 "error",
330:                 now_iso(),
331:                 now_iso(),
332:             )
333:             ctx.storage.record_template_conversion(
334:                 dataset_version_id,
335:                 template_id,
336:                 "error",
337:                 now_iso(),
338:                 now_iso(),
339:                 mapping_h,
340:                 row_count=row_count,
341:                 error={"message": str(exc)},
342:             )
343:             raise
344: 
345:         artifacts_dir = ctx.artifacts_dir("transform_normalize_mixed")
346:         map_path = artifacts_dir / "mapping.json"
347:         write_json(map_path, mapping_payload)
348:         artifacts = [
349:             PluginArtifact(
350:                 path=str(map_path.relative_to(ctx.run_dir)),
351:                 type="json",
352:                 description="Normalization mapping",
353:             )
354:         ]
355: 
356:         return PluginResult(
357:             status="ok",
358:             summary="Normalized template generated",
359:             metrics={
360:                 "row_count": int(row_count),
361:                 "column_count": int(len(field_defs)),
362:                 "template_id": int(template_id),
363:                 "coerced_columns": coerced_columns,
364:             },
365:             findings=[],
366:             artifacts=artifacts,
367:             error=None,
368:         )
````

## File: plugins/transform_normalize_mixed/plugin.yaml
````yaml
 1: id: transform_normalize_mixed
 2: name: Normalize Mixed Types
 3: version: 0.1.0
 4: type: transform
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Normalize mixed-type datasets into a template with consistent typing
 9:   defaults:
10:     template_name: ""
11:     lowercase: true
12:     strip: true
13:     collapse_whitespace: true
14:     numeric_coercion: true
15:     numeric_threshold: 0.98
16:     exclude_name_patterns:
17:     - id
18:     - uuid
19:     - guid
20:     - key
21:     chunk_size: 1000
22:     sample_rows: 500
23: capabilities: []
24: config_schema: config.schema.json
25: output_schema: output.schema.json
26: sandbox:
27:   no_network: true
28:   fs_allowlist:
29:   - appdata
30:   - plugins
31:   - run_dir
````

## File: plugins/transform_template/__init__.py
````python
1: 
````

## File: plugins/transform_template/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "mapping": {
 6:       "additionalProperties": true,
 7:       "type": [
 8:         "object",
 9:         "null"
10:       ]
11:     },
12:     "mapping_json": {
13:       "type": [
14:         "string",
15:         "null"
16:       ]
17:     },
18:     "template_id": {
19:       "type": [
20:         "integer",
21:         "null"
22:       ]
23:     }
24:   },
25:   "title": "Transform Template Config",
26:   "type": "object"
27: }
````

## File: plugins/transform_template/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "additionalProperties": true,
113:         "properties": {
114:           "evidence": {
115:             "additionalProperties": true,
116:             "properties": {
117:               "column_ids": {
118:                 "items": {
119:                   "type": "integer"
120:                 },
121:                 "type": "array"
122:               },
123:               "dataset_id": {
124:                 "type": "string"
125:               },
126:               "dataset_version_id": {
127:                 "type": "string"
128:               },
129:               "query": {
130:                 "type": [
131:                   "string",
132:                   "null"
133:                 ]
134:               },
135:               "row_ids": {
136:                 "items": {
137:                   "type": "integer"
138:                 },
139:                 "type": "array"
140:               },
141:               "row_ranges": {
142:                 "items": {
143:                   "additionalProperties": false,
144:                   "properties": {
145:                     "end": {
146:                       "type": "integer"
147:                     },
148:                     "start": {
149:                       "type": "integer"
150:                     }
151:                   },
152:                   "required": [
153:                     "start",
154:                     "end"
155:                   ],
156:                   "type": "object"
157:                 },
158:                 "type": "array"
159:               }
160:             },
161:             "required": [
162:               "dataset_id",
163:               "dataset_version_id",
164:               "row_ids",
165:               "column_ids"
166:             ],
167:             "type": "object"
168:           },
169:           "kind": {
170:             "type": "string"
171:           }
172:         },
173:         "required": [
174:           "kind",
175:           "evidence"
176:         ],
177:         "type": "object"
178:       },
179:       "type": "array"
180:     },
181:     "metrics": {
182:       "additionalProperties": false,
183:       "properties": {
184:         "row_count": {
185:           "type": "integer"
186:         }
187:       },
188:       "type": "object"
189:     },
190:     "status": {
191:       "enum": [
192:         "ok",
193:         "skipped",
194:         "error"
195:       ],
196:       "type": "string"
197:     },
198:     "summary": {
199:       "type": "string"
200:     }
201:   },
202:   "required": [
203:     "status",
204:     "summary",
205:     "metrics",
206:     "findings",
207:     "artifacts",
208:     "budget"
209:   ],
210:   "title": "Plugin Result",
211:   "type": "object"
212: }
````

## File: plugins/transform_template/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import json
 4: 
 5: from statistic_harness.core.template import apply_template
 6: from statistic_harness.core.types import PluginArtifact, PluginResult
 7: from statistic_harness.core.utils import write_json
 8: 
 9: 
10: class Plugin:
11:     def run(self, ctx) -> PluginResult:
12:         if not ctx.dataset_version_id:
13:             return PluginResult("error", "Missing dataset version", {}, [], [], None)
14: 
15:         template_id = ctx.settings.get("template_id")
16:         mapping = ctx.settings.get("mapping")
17:         mapping_json = ctx.settings.get("mapping_json")
18:         if mapping is None and mapping_json:
19:             try:
20:                 mapping = json.loads(mapping_json)
21:             except json.JSONDecodeError as exc:
22:                 return PluginResult(
23:                     "error",
24:                     f"Invalid mapping JSON: {exc}",
25:                     {},
26:                     [],
27:                     [],
28:                     None,
29:                 )
30:         if not template_id or not isinstance(mapping, dict):
31:             return PluginResult(
32:                 "error", "template_id and mapping required", {}, [], [], None
33:             )
34: 
35:         row_count = apply_template(
36:             ctx.storage, ctx.dataset_version_id, int(template_id), mapping
37:         )
38: 
39:         artifacts_dir = ctx.artifacts_dir("transform_template")
40:         map_path = artifacts_dir / "mapping.json"
41:         write_json(map_path, mapping)
42:         artifacts = [
43:             PluginArtifact(
44:                 path=str(map_path.relative_to(ctx.run_dir)),
45:                 type="json",
46:                 description="Template mapping",
47:             )
48:         ]
49: 
50:         return PluginResult(
51:             "ok",
52:             "Template mapping applied",
53:             {"row_count": int(row_count)},
54:             [],
55:             artifacts,
56:             None,
57:         )
````

## File: plugins/transform_template/plugin.yaml
````yaml
 1: id: transform_template
 2: name: Template Transformer
 3: version: 0.1.0
 4: type: transform
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Apply template mapping to dataset
 9:   defaults:
10:     template_id: null
11:     mapping: {}
12: capabilities: []
13: config_schema: config.schema.json
14: output_schema: output.schema.json
15: sandbox:
16:   no_network: true
17:   fs_allowlist:
18:   - appdata
19:   - plugins
````

## File: scripts/fixtures/sample.csv
````
 1: process_id,process_name,start_ts,end_ts,duration_sec,user_id,params,status
 2: 1,qemail,2026-01-20T01:00:00,2026-01-20T01:02:00,120,u1,"queue=start;range=2026-01-20",ok
 3: 2,qpec,2026-01-20T01:02:00,2026-01-20T01:07:00,300,u1,"batch=close;src=erp",ok
 4: 3,qemail,2026-01-21T02:00:00,2026-01-21T02:01:00,60,u2,"queue=start;range=2026-01-21",ok
 5: 4,upload,2026-01-21T03:00:00,2026-01-21T03:04:00,240,u2,"file=client_a.xlsx;stage=ingest",ok
 6: 5,transform,2026-01-21T03:05:00,2026-01-21T03:20:00,900,u2,"template=v1;file=client_a.xlsx",ok
 7: 6,qpec,2026-01-22T01:05:00,2026-01-22T01:10:00,300,u1,"batch=close;src=erp",ok
 8: 7,qemail,2026-01-23T01:00:00,2026-01-23T01:05:00,300,u3,"queue=start;range=2026-01-23",ok
 9: 8,payout,2026-01-23T02:00:00,2026-01-23T02:20:00,1200,u3,"step=validate;batch=42",ok
10: 9,payout,2026-01-23T02:21:00,2026-01-23T02:50:00,1740,u3,"step=apply;batch=42",ok
11: 10,report,2026-01-23T03:00:00,2026-01-23T03:03:00,180,u3,"template=ops;batch=42",ok
````

## File: scripts/api_smoke.py
````python
  1: #!/usr/bin/env python3
  2: from __future__ import annotations
  3: 
  4: import argparse
  5: import json
  6: import os
  7: import subprocess
  8: import sys
  9: import time
 10: from pathlib import Path
 11: from typing import Any
 12: from urllib import error, parse, request
 13: 
 14: 
 15: DEFAULT_BASE_URL = "http://127.0.0.1:8000"
 16: 
 17: 
 18: def _http_request(
 19:     method: str,
 20:     url: str,
 21:     data: bytes | None = None,
 22:     headers: dict[str, str] | None = None,
 23:     timeout: int = 30,
 24: ) -> tuple[int, dict[str, str], bytes]:
 25:     req = request.Request(url, data=data, method=method)
 26:     if headers:
 27:         for key, value in headers.items():
 28:             req.add_header(key, value)
 29:     try:
 30:         with request.urlopen(req, timeout=timeout) as resp:
 31:             body = resp.read()
 32:             return resp.status, dict(resp.headers), body
 33:     except error.HTTPError as exc:
 34:         body = exc.read()
 35:         detail = body.decode("utf-8", errors="ignore")
 36:         raise RuntimeError(f"{method} {url} -> {exc.code} {detail}") from exc
 37: 
 38: 
 39: def _get_json(url: str, timeout: int = 30) -> dict[str, Any]:
 40:     status, _, body = _http_request("GET", url, timeout=timeout)
 41:     if status >= 400:
 42:         raise RuntimeError(f"GET {url} failed with {status}")
 43:     return json.loads(body.decode("utf-8"))
 44: 
 45: 
 46: def _post_json(url: str, payload: dict[str, Any], timeout: int = 30) -> dict[str, Any]:
 47:     data = json.dumps(payload).encode("utf-8")
 48:     status, _, body = _http_request(
 49:         "POST",
 50:         url,
 51:         data=data,
 52:         headers={"Content-Type": "application/json"},
 53:         timeout=timeout,
 54:     )
 55:     if status >= 400:
 56:         raise RuntimeError(f"POST {url} failed with {status}")
 57:     return json.loads(body.decode("utf-8"))
 58: 
 59: 
 60: def _post_form(url: str, payload: dict[str, Any], timeout: int = 30) -> dict[str, Any]:
 61:     encoded = parse.urlencode(payload).encode("utf-8")
 62:     status, _, body = _http_request(
 63:         "POST",
 64:         url,
 65:         data=encoded,
 66:         headers={"Content-Type": "application/x-www-form-urlencoded"},
 67:         timeout=timeout,
 68:     )
 69:     if status >= 400:
 70:         raise RuntimeError(f"POST {url} failed with {status}")
 71:     return json.loads(body.decode("utf-8"))
 72: 
 73: 
 74: def _post_raw_file(url: str, file_path: Path, timeout: int = 60) -> dict[str, Any]:
 75:     data = file_path.read_bytes()
 76:     status, _, body = _http_request(
 77:         "POST",
 78:         url,
 79:         data=data,
 80:         headers={"Content-Type": "application/octet-stream"},
 81:         timeout=timeout,
 82:     )
 83:     if status >= 400:
 84:         raise RuntimeError(f"POST {url} failed with {status}")
 85:     return json.loads(body.decode("utf-8"))
 86: 
 87: 
 88: def _wait_for_server(base_url: str, timeout: int) -> None:
 89:     deadline = time.time() + timeout
 90:     last_err = None
 91:     while time.time() < deadline:
 92:         try:
 93:             _get_json(f"{base_url}/api/uploads", timeout=5)
 94:             return
 95:         except Exception as exc:  # pragma: no cover - waiting loop
 96:             last_err = exc
 97:             time.sleep(0.5)
 98:     raise RuntimeError(f"Server not ready: {last_err}")
 99: 
100: 
101: def _start_server(base_url: str) -> subprocess.Popen:
102:     parsed = parse.urlparse(base_url)
103:     host = parsed.hostname or "127.0.0.1"
104:     port = parsed.port or 8000
105:     root = Path(__file__).resolve().parents[1]
106:     env = os.environ.copy()
107:     env["PYTHONPATH"] = str(root / "src") + os.pathsep + env.get("PYTHONPATH", "")
108:     cmd = [sys.executable, "-m", "statistic_harness.cli", "serve", "--host", host, "--port", str(port)]
109:     return subprocess.Popen(cmd, cwd=root, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
110: 
111: 
112: def _poll_run(base_url: str, run_id: str, timeout: int) -> dict[str, Any]:
113:     deadline = time.time() + timeout
114:     while time.time() < deadline:
115:         payload = _get_json(f"{base_url}/api/runs/{run_id}")
116:         status = (payload.get("run") or {}).get("status")
117:         if status in {"completed", "failed", "error"}:
118:             return payload
119:         time.sleep(1)
120:     raise RuntimeError(f"Run {run_id} did not finish before timeout")
121: 
122: 
123: def main() -> int:
124:     parser = argparse.ArgumentParser(description="API smoke test for Statistic Harness")
125:     parser.add_argument("--base-url", default=DEFAULT_BASE_URL)
126:     parser.add_argument("--fixture", default="scripts/fixtures/sample.csv")
127:     parser.add_argument("--timeout", type=int, default=120)
128:     parser.add_argument("--start-server", action="store_true")
129:     args = parser.parse_args()
130: 
131:     base_url = args.base_url.rstrip("/")
132:     fixture_path = Path(args.fixture)
133:     if not fixture_path.exists():
134:         raise SystemExit(f"Fixture not found: {fixture_path}")
135: 
136:     server_proc = None
137:     try:
138:         if args.start_server:
139:             server_proc = _start_server(base_url)
140:         _wait_for_server(base_url, args.timeout)
141: 
142:         print("Uploading fixture (raw upload)...")
143:         upload = _post_raw_file(
144:             f"{base_url}/api/upload/raw?filename={parse.quote(fixture_path.name)}",
145:             fixture_path,
146:         )
147:         print("Upload response:", upload)
148:         if not upload.get("upload_id"):
149:             raise RuntimeError("Upload did not return upload_id")
150: 
151:         print("Uploading same fixture to test dedupe...")
152:         upload2 = _post_raw_file(
153:             f"{base_url}/api/upload/raw?filename={parse.quote(fixture_path.name)}",
154:             fixture_path,
155:         )
156:         print("Dedupe response:", upload2)
157:         if not upload2.get("deduplicated"):
158:             raise RuntimeError("Expected deduplicated=true on second upload")
159: 
160:         print("Listing uploads...")
161:         uploads = _get_json(f"{base_url}/api/uploads")
162:         if not isinstance(uploads.get("uploads"), list):
163:             raise RuntimeError("Uploads payload missing list")
164: 
165:         upload_id = upload.get("upload_id")
166:         print("Saving known issues...")
167:         known_payload = {
168:             "strict": False,
169:             "notes": "smoke test",
170:             "expected_findings": [],
171:         }
172:         saved = _post_json(
173:             f"{base_url}/api/known-issues",
174:             {"upload_id": upload_id, "known_issues": known_payload},
175:         )
176:         if saved.get("status") != "ok":
177:             raise RuntimeError("Known issues save did not return status=ok")
178: 
179:         print("Starting auto-evaluate run...")
180:         run_resp = _post_form(
181:             f"{base_url}/api/runs/auto-evaluate",
182:             {"upload_id": upload_id},
183:         )
184:         run_id = run_resp.get("run_id")
185:         if not run_id:
186:             raise RuntimeError("auto-evaluate did not return run_id")
187: 
188:         print("Waiting for run completion...")
189:         _poll_run(base_url, run_id, args.timeout)
190: 
191:         print("Evaluating run (template mode)...")
192:         eval_payload = _post_json(
193:             f"{base_url}/api/runs/{run_id}/evaluate",
194:             {"mode": "template"},
195:         )
196:         if "result" not in eval_payload:
197:             raise RuntimeError("Evaluation payload missing result")
198: 
199:         print("Fetching evaluation JSON...")
200:         eval_saved = _get_json(f"{base_url}/api/runs/{run_id}/evaluation")
201:         if "result" not in eval_saved:
202:             raise RuntimeError("Saved evaluation missing result")
203: 
204:         print("Fetching report JSON...")
205:         _http_request("GET", f"{base_url}/api/runs/{run_id}/report.json")
206: 
207:         print("API smoke test completed successfully.")
208:         return 0
209:     finally:
210:         if server_proc:
211:             server_proc.terminate()
212:             try:
213:                 server_proc.wait(timeout=10)
214:             except subprocess.TimeoutExpired:
215:                 server_proc.kill()
216: 
217: 
218: if __name__ == "__main__":
219:     raise SystemExit(main())
````

## File: scripts/push_repo.ps1
````powershell
 1: # Push current branch to origin using WSL + explicit SSH key.
 2: $wsl = "C:\Windows\System32\wsl.exe"
 3: $git = "/usr/bin/git"
 4: $ssh = "/usr/bin/ssh"
 5: $repo = "/mnt/d/projects/statistics_harness/statistics_harness"
 6: $key = "/home/ninjra/.ssh/id_ed25519"
 7: 
 8: # Ensure SSH remote.
 9: & $wsl $git -C $repo remote set-url origin git@github.com:ninjra/statistics_harness.git
10: 
11: # Push with explicit key.
12: & $wsl env GIT_SSH_COMMAND="$ssh -i $key -o IdentitiesOnly=yes" $git -C $repo push origin HEAD
````

## File: scripts/run_analysis.ps1
````powershell
  1: param(
  2:   [string]$DatasetVersionId = "",
  3:   [int]$RunSeed = 123,
  4:   [string]$PythonExe = ""
  5: )
  6: 
  7: $ErrorActionPreference = "Stop"
  8: 
  9: function Test-BadPythonPath($exePath) {
 10:   return ($exePath -match "WindowsApps" -or $exePath -match "wsl.exe" -or $exePath -match "^/usr/" -or $exePath -match "\\wsl$")
 11: }
 12: 
 13: function Get-PythonCmd {
 14:   param([string]$Preferred)
 15: 
 16:   if ($Preferred) {
 17:     $Preferred = ($Preferred -replace "[\r\n\t]", "").Trim()
 18:     $Preferred = $Preferred -replace ":\s+\\", ":\\"  # fix accidental newline/space after drive colon
 19:     $Preferred = $Preferred -replace "\\\s+", "\\"    # fix accidental whitespace after backslash
 20: 
 21:     if ($Preferred -eq "py") {
 22:       $cmd = Get-Command py -ErrorAction SilentlyContinue
 23:       if ($cmd) { return @{ Exe = $cmd.Source; Args = @("-3") } }
 24:       throw "Python launcher 'py' not found on PATH."
 25:     }
 26:     $cmd = Get-Command $Preferred -ErrorAction SilentlyContinue
 27:     if ($cmd) {
 28:       $exe = $cmd.Source
 29:       if (Test-BadPythonPath $exe) {
 30:         Write-Host "Preferred Python resolves to '$exe' (not usable for native Windows). Falling back to WSL." -ForegroundColor Yellow
 31:       } else {
 32:         return @{ Exe = $exe; Args = @() }
 33:       }
 34:     }
 35:     if (Test-Path $Preferred) {
 36:       $exe = (Resolve-Path $Preferred).Path
 37:       if (Test-BadPythonPath $exe) {
 38:         Write-Host "Preferred Python resolves to '$exe' (not usable for native Windows). Falling back to WSL." -ForegroundColor Yellow
 39:       } else {
 40:         return @{ Exe = $exe; Args = @() }
 41:       }
 42:     }
 43:   }
 44: 
 45:   foreach ($name in @("py","python3","python")) {
 46:     $cmd = Get-Command $name -ErrorAction SilentlyContinue
 47:     if (!$cmd) { continue }
 48:     if ($name -eq "py") {
 49:       return @{ Exe = $cmd.Source; Args = @("-3") }
 50:     }
 51:     $exe = $cmd.Source
 52:     if (Test-BadPythonPath $exe) { continue }
 53:     return @{ Exe = $exe; Args = @() }
 54:   }
 55:   return $null
 56: }
 57: 
 58: $RepoRoot = (Resolve-Path (Join-Path $PSScriptRoot "..")).Path
 59: $VenvPath = Join-Path $RepoRoot ".venv"
 60: $PythonCmd = Get-PythonCmd -Preferred $PythonExe
 61: 
 62: $env:STAT_HARNESS_APPDATA = (Join-Path $RepoRoot "appdata")
 63: 
 64: $tmp = Join-Path $env:TEMP "stat_harness_run_analysis.py"
 65: $py = @"
 66: import os
 67: import sqlite3
 68: from pathlib import Path
 69: import yaml
 70: 
 71: from statistic_harness.core.pipeline import Pipeline
 72: from statistic_harness.core.tenancy import get_tenant_context
 73: from statistic_harness.core.report import build_report, write_report
 74: 
 75: def latest_dataset_version_id(db_path: Path) -> str | None:
 76:     conn = sqlite3.connect(db_path)
 77:     conn.row_factory = sqlite3.Row
 78:     row = conn.execute(
 79:         "SELECT dataset_version_id FROM dataset_versions ORDER BY created_at DESC LIMIT 1"
 80:     ).fetchone()
 81:     conn.close()
 82:     return row["dataset_version_id"] if row else None
 83: 
 84: ctx = get_tenant_context()
 85: 
 86: db_path = ctx.appdata_root / "state.sqlite"
 87: 
 88: dataset_version_id = os.environ.get("DATASET_VERSION_ID", "")
 89: if not dataset_version_id:
 90:     dataset_version_id = latest_dataset_version_id(db_path)
 91: if not dataset_version_id:
 92:     raise SystemExit("No dataset_version_id found. Upload data first.")
 93: 
 94: run_seed = int(os.environ.get("RUN_SEED", "123"))
 95: 
 96: analysis_ids = []
 97: for manifest in Path("plugins").glob("*/plugin.yaml"):
 98:     data = yaml.safe_load(manifest.read_text())
 99:     if data.get("type") == "analysis":
100:         analysis_ids.append(data["id"])
101: analysis_ids = sorted(set(analysis_ids))
102: 
103: pipeline = Pipeline(ctx.appdata_root, Path("plugins"), tenant_id=ctx.tenant_id)
104: run_id = pipeline.run(
105:     input_file=None,
106:     plugin_ids=analysis_ids,
107:     settings={},
108:     run_seed=run_seed,
109:     dataset_version_id=dataset_version_id,
110: )
111: run_dir = ctx.tenant_root / "runs" / run_id
112: report = build_report(pipeline.storage, run_id, run_dir, Path("docs/report.schema.json"))
113: write_report(report, run_dir)
114: print(f"RUN_ID={run_id}")
115: print(str(run_dir / "report.md"))
116: "@
117: 
118: Set-Content -Path $tmp -Value $py -Encoding UTF8
119: 
120: function Invoke-WSLRun {
121:   $wsl = Get-Command wsl.exe -ErrorAction SilentlyContinue
122:   if (!$wsl) { throw "WSL not found. Install WSL or provide a native Windows Python." }
123:   $RepoRootWsl = (& wsl.exe wslpath -u $RepoRoot).Trim()
124:   $TmpWsl = (& wsl.exe wslpath -u $tmp).Trim()
125:   if (-not $RepoRootWsl) { throw "Failed to resolve WSL path for $RepoRoot" }
126:   if (-not $TmpWsl) { throw "Failed to resolve WSL path for $tmp" }
127:   $DatasetArg = $DatasetVersionId
128:   $SeedArg = $RunSeed
129:   $bash = @"
130: set -e
131: cd "$RepoRootWsl"
132: if [ ! -d ".venv_wsl" ]; then
133:   python3 -m venv .venv_wsl
134: fi
135: . .venv_wsl/bin/activate
136: python -m pip install -e .
137: STAT_HARNESS_APPDATA="$RepoRootWsl/appdata" DATASET_VERSION_ID="$DatasetArg" RUN_SEED="$SeedArg" python "$TmpWsl"
138: "@
139:   $output = & wsl.exe bash -lc $bash
140:   if (!$output) { throw "WSL run produced no output." }
141:   $runIdLine = ($output | Where-Object { $_ -match "^RUN_ID=" } | Select-Object -First 1)
142:   if (!$runIdLine) { throw "Could not parse RUN_ID from WSL output." }
143:   $runId = $runIdLine -replace "^RUN_ID=", ""
144:   $reportWsl = "$RepoRootWsl/appdata/runs/$runId/report.md"
145:   $reportWin = (& wsl.exe wslpath -w $reportWsl).Trim()
146:   Write-Host $runId
147:   Write-Host $reportWin
148: }
149: 
150: if ($PythonCmd -eq $null) {
151:   Invoke-WSLRun
152:   Remove-Item $tmp -ErrorAction SilentlyContinue
153:   exit 0
154: }
155: 
156: $VenvPython = Join-Path $VenvPath "Scripts\\python.exe"
157: if (!(Test-Path $VenvPython)) {
158:   & $PythonCmd.Exe @($PythonCmd.Args) -m venv $VenvPath
159: }
160: if (!(Test-Path $VenvPython)) {
161:   Write-Host "Native venv not available, falling back to WSL." -ForegroundColor Yellow
162:   Invoke-WSLRun
163:   Remove-Item $tmp -ErrorAction SilentlyContinue
164:   exit 0
165: }
166: 
167: & $VenvPython -m pip install -e $RepoRoot | Out-Host
168: 
169: $env:DATASET_VERSION_ID = $DatasetVersionId
170: $env:RUN_SEED = [string]$RunSeed
171: & $VenvPython $tmp
172: Remove-Item $tmp -ErrorAction SilentlyContinue
````

## File: scripts/run_gauntlet.ps1
````powershell
  1: param(
  2:   [string]$DatasetVersionId = "",
  3:   [int]$RunSeed = 123,
  4:   [string]$PythonExe = "",
  5:   [ValidateSet("console","open","none")]
  6:   [string]$ReportView = "console"
  7: )
  8: 
  9: $ErrorActionPreference = "Stop"
 10: 
 11: function Test-BadPythonPath($exePath) {
 12:   return ($exePath -match "WindowsApps" -or $exePath -match "wsl.exe" -or $exePath -match "^/usr/" -or $exePath -match "\\wsl$")
 13: }
 14: 
 15: function Test-PythonExe {
 16:   param(
 17:     [string]$Exe,
 18:     [string[]]$Args,
 19:     [int]$MinMajor = 3,
 20:     [int]$MinMinor = 11
 21:   )
 22:   try {
 23:     $out = & $Exe @Args -c "import sys; print(f'{sys.version_info[0]}.{sys.version_info[1]}')" 2>$null
 24:     if ($LASTEXITCODE -ne 0) { return $false }
 25:     if ($out -notmatch "^\d+\.\d+$") { return $false }
 26:     $parts = $out.Split(".")
 27:     $maj = [int]$parts[0]
 28:     $min = [int]$parts[1]
 29:     if ($maj -gt $MinMajor) { return $true }
 30:     if ($maj -lt $MinMajor) { return $false }
 31:     return ($min -ge $MinMinor)
 32:   } catch {
 33:     return $false
 34:   }
 35: }
 36: 
 37: function Get-PythonCmd {
 38:   param([string]$Preferred)
 39: 
 40:   if ($Preferred) {
 41:     $Preferred = ($Preferred -replace "[\r\n\t]", "").Trim()
 42:     $Preferred = $Preferred -replace ":\s+\\", ":\\"  # fix accidental newline/space after drive colon
 43:     $Preferred = $Preferred -replace "\\\s+", "\\"    # fix accidental whitespace after backslash
 44: 
 45:     if ($Preferred -eq "py") {
 46:       $cmd = Get-Command py -ErrorAction SilentlyContinue
 47:       if ($cmd) {
 48:         $candidate = @{ Exe = $cmd.Source; Args = @("-3") }
 49:         if (Test-PythonExe $candidate.Exe $candidate.Args) { return $candidate }
 50:       }
 51:       throw "Python launcher 'py' not found on PATH."
 52:     }
 53:     $cmd = Get-Command $Preferred -ErrorAction SilentlyContinue
 54:     if ($cmd) {
 55:       $exe = $cmd.Source
 56:       if (Test-BadPythonPath $exe) {
 57:         Write-Host "Preferred Python resolves to '$exe' (not usable for native Windows). Falling back to WSL." -ForegroundColor Yellow
 58:       } else {
 59:         $candidate = @{ Exe = $exe; Args = @() }
 60:         if (Test-PythonExe $candidate.Exe $candidate.Args) { return $candidate }
 61:       }
 62:     }
 63:     if (Test-Path $Preferred) {
 64:       $exe = (Resolve-Path $Preferred).Path
 65:       if (Test-BadPythonPath $exe) {
 66:         Write-Host "Preferred Python resolves to '$exe' (not usable for native Windows). Falling back to WSL." -ForegroundColor Yellow
 67:       } else {
 68:         $candidate = @{ Exe = $exe; Args = @() }
 69:         if (Test-PythonExe $candidate.Exe $candidate.Args) { return $candidate }
 70:       }
 71:     }
 72:   }
 73: 
 74:   foreach ($name in @("py","python3","python")) {
 75:     $cmd = Get-Command $name -ErrorAction SilentlyContinue
 76:     if (!$cmd) { continue }
 77:     if ($name -eq "py") {
 78:       $candidate = @{ Exe = $cmd.Source; Args = @("-3") }
 79:       if (Test-PythonExe $candidate.Exe $candidate.Args) { return $candidate }
 80:       continue
 81:     }
 82:     $exe = $cmd.Source
 83:     if (Test-BadPythonPath $exe) { continue }
 84:     $candidate = @{ Exe = $exe; Args = @() }
 85:     if (Test-PythonExe $candidate.Exe $candidate.Args) { return $candidate }
 86:   }
 87:   return $null
 88: }
 89: 
 90: function Show-Result {
 91:   param(
 92:     [string[]]$OutputLines
 93:   )
 94:   if (!$OutputLines) { throw "Gauntlet run produced no output." }
 95:   $runIdLine = ($OutputLines | Where-Object { $_ -match "^RUN_ID=" } | Select-Object -First 1)
 96:   $reportLine = ($OutputLines | Where-Object { $_ -match "^REPORT=" } | Select-Object -First 1)
 97:   $truthLine = ($OutputLines | Where-Object { $_ -match "^GROUND_TRUTH=" } | Select-Object -First 1)
 98:   $okLine = ($OutputLines | Where-Object { $_ -match "^GAUNTLET_OK=" } | Select-Object -First 1)
 99:   $failLines = ($OutputLines | Where-Object { $_ -match "^FAILURE=" })
100:   $reportPath = ""
101: 
102:   if ($runIdLine) { Write-Host ($runIdLine -replace "^RUN_ID=", "") }
103:   if ($reportLine) {
104:     $reportPath = ($reportLine -replace "^REPORT=", "")
105:     Write-Host $reportPath
106:   }
107:   if ($truthLine) { Write-Host ("Ground truth: " + ($truthLine -replace "^GROUND_TRUTH=", "")) }
108: 
109:   $okValue = ""
110:   if ($okLine) { $okValue = ($okLine -replace "^GAUNTLET_OK=", "").Trim().ToLowerInvariant() }
111:   $isOk = $okValue -in @("true", "1", "yes", "y")
112:   Write-Host "GAUNTLET RESULT:" -NoNewline
113:   if ($isOk) {
114:     Write-Host " YES" -ForegroundColor Green
115:   } else {
116:     Write-Host " NO" -ForegroundColor Red
117:     foreach ($line in $failLines) {
118:       Write-Host ("- " + ($line -replace "^FAILURE=", ""))
119:     }
120:   }
121: 
122:   return [pscustomobject]@{
123:     ReportPath = $reportPath
124:     IsOk = $isOk
125:   }
126: }
127: 
128: function Show-ReportView {
129:   param(
130:     [string]$ReportPath,
131:     [string]$Mode
132:   )
133:   if ([string]::IsNullOrWhiteSpace($ReportPath)) { return }
134:   if ($Mode -eq "open") {
135:     Start-Process $ReportPath | Out-Null
136:     return
137:   }
138:   if ($Mode -eq "console") {
139:     Write-Host ""
140:     Write-Host "===== REPORT ====="
141:     Get-Content -Path $ReportPath -Raw
142:   }
143: }
144: 
145: $RepoRoot = (Resolve-Path (Join-Path $PSScriptRoot "..")).Path
146: $VenvPath = Join-Path $RepoRoot ".venv"
147: $PythonCmd = $null
148: if ($PythonExe) {
149:   $PythonCmd = Get-PythonCmd -Preferred $PythonExe
150: }
151: 
152: $env:STAT_HARNESS_APPDATA = (Join-Path $RepoRoot "appdata")
153: $LogPath = Join-Path $env:STAT_HARNESS_APPDATA "gauntlet_last.log"
154: if (!(Test-Path $env:STAT_HARNESS_APPDATA)) {
155:   New-Item -ItemType Directory -Force -Path $env:STAT_HARNESS_APPDATA | Out-Null
156: }
157: 
158: function Write-Log {
159:   param([string[]]$Lines)
160:   if ($null -eq $Lines) { $Lines = @() }
161:   Add-Content -Path $LogPath -Value ($Lines -join "`n") -Encoding UTF8
162: }
163: 
164: Set-Content -Path $LogPath -Value ("started_at=" + (Get-Date).ToString("s")) -Encoding UTF8
165: Write-Log -Lines @("repo_root=" + $RepoRoot)
166: 
167: $tmp = Join-Path $env:TEMP "stat_harness_run_gauntlet.py"
168: $py = @"
169: import os
170: import sqlite3
171: from pathlib import Path
172: import yaml
173: 
174: from statistic_harness.core.pipeline import Pipeline
175: from statistic_harness.core.tenancy import get_tenant_context
176: from statistic_harness.core.report import build_report, write_report
177: from statistic_harness.core.evaluation import evaluate_report
178: from statistic_harness.core.utils import now_iso, json_dumps
179: 
180: def latest_dataset_version_id(db_path: Path) -> str | None:
181:     conn = sqlite3.connect(db_path)
182:     conn.row_factory = sqlite3.Row
183:     row = conn.execute(
184:         "SELECT dataset_version_id FROM dataset_versions ORDER BY row_count DESC, created_at DESC LIMIT 1"
185:     ).fetchone()
186:     conn.close()
187:     return row["dataset_version_id"] if row else None
188: 
189: def ground_truth_template(report: dict) -> str:
190:     features = [
191:         f.get("feature")
192:         for f in report.get("plugins", {})
193:         .get("analysis_gaussian_knockoffs", {})
194:         .get("findings", [])
195:     ]
196:     template = {
197:         "strict": False,
198:         "features": [f for f in features if f],
199:         "changepoints": [],
200:         "dependence_shift_pairs": [],
201:         "anomalies": [],
202:         "min_anomaly_hits": 0,
203:         "changepoint_tolerance": 3,
204:     }
205:     return yaml.safe_dump(template)
206: 
207: ctx = get_tenant_context()
208: db_path = ctx.appdata_root / "state.sqlite"
209: 
210: dataset_version_id = os.environ.get("DATASET_VERSION_ID", "")
211: if not dataset_version_id:
212:     dataset_version_id = latest_dataset_version_id(db_path)
213: if not dataset_version_id:
214:     raise SystemExit("No dataset_version_id found. Upload data first.")
215: 
216: run_seed = int(os.environ.get("RUN_SEED", "123"))
217: 
218: pipeline = Pipeline(ctx.appdata_root, Path("plugins"), tenant_id=ctx.tenant_id)
219: ctx_row = pipeline.storage.get_dataset_version_context(dataset_version_id)
220: if not ctx_row:
221:     raise SystemExit(f"Dataset version not found: {dataset_version_id}")
222: 
223: project_id = ctx_row.get("project_id")
224: settings = {"__run_meta": {"plugins": ["all"]}}
225: 
226: columns_meta = pipeline.storage.fetch_dataset_columns(dataset_version_id)
227: columns = [row.get("original_name") for row in columns_meta if row.get("original_name")]
228: colset = set(columns)
229: 
230: def has(name: str) -> bool:
231:     return name in colset
232: 
233: base = {}
234: if has("PROCESS_ID"):
235:     base["process_column"] = "PROCESS_ID"
236: elif has("PROCESS_QUEUE_ID"):
237:     base["process_column"] = "PROCESS_QUEUE_ID"
238: if has("LOCAL_MACHINE_ID"):
239:     base["host_column"] = "LOCAL_MACHINE_ID"
240:     base["server_column"] = "LOCAL_MACHINE_ID"
241: if has("QUEUE_DT"):
242:     base["queue_column"] = "QUEUE_DT"
243: if has("START_DT"):
244:     base["start_column"] = "START_DT"
245:     base["timestamp_column"] = "START_DT"
246: if has("END_DT"):
247:     base["end_column"] = "END_DT"
248: 
249: if base:
250:     settings["analysis_queue_delay_decomposition"] = {
251:         k: base[k]
252:         for k in ("process_column", "queue_column", "start_column", "end_column")
253:         if k in base
254:     }
255:     settings["analysis_close_cycle_contention"] = {
256:         k: base[k]
257:         for k in ("process_column", "server_column", "timestamp_column", "start_column", "end_column")
258:         if k in base
259:     }
260:     settings["analysis_close_cycle_capacity_impact"] = {
261:         k: base[k]
262:         for k in ("process_column", "host_column", "queue_column", "eligible_column", "start_column", "end_column")
263:         if k in base
264:     }
265:     settings["analysis_close_cycle_capacity_model"] = {
266:         k: base[k]
267:         for k in ("process_column", "host_column", "queue_column", "eligible_column", "start_column", "end_column")
268:         if k in base
269:     }
270:     settings["analysis_capacity_scaling"] = {
271:         k: base[k]
272:         for k in ("process_column", "host_column", "queue_column", "eligible_column", "start_column")
273:         if k in base
274:     }
275: 
276: run_id = pipeline.run(
277:     input_file=None,
278:     plugin_ids=["all"],
279:     settings=settings,
280:     run_seed=run_seed,
281:     dataset_version_id=dataset_version_id,
282:     project_id=project_id,
283: )
284: run_dir = ctx.tenant_root / "runs" / run_id
285: report = build_report(pipeline.storage, run_id, run_dir, Path("docs/report.schema.json"))
286: write_report(report, run_dir)
287: 
288: ground_truth = None
289: source = "template"
290: if project_id:
291:     project_row = pipeline.storage.fetch_project(project_id)
292:     if project_row and project_row.get("erp_type"):
293:         erp_type = str(project_row.get("erp_type") or "unknown").strip() or "unknown"
294:         known = pipeline.storage.fetch_known_issues(erp_type, "erp_type")
295:         if known:
296:             payload = {
297:                 "strict": bool(known.get("strict", False)),
298:                 "notes": known.get("notes") or "",
299:                 "expected_findings": known.get("expected_findings") or [],
300:             }
301:             ground_truth = yaml.safe_dump(payload, sort_keys=False)
302:             source = "known"
303: 
304: if ground_truth is None:
305:     ground_truth = ground_truth_template(report)
306:     source = "template"
307: 
308: gt_path = run_dir / "ground_truth.yaml"
309: gt_path.write_text(ground_truth, encoding="utf-8")
310: ok, messages = evaluate_report(run_dir / "report.json", gt_path)
311: eval_payload = {
312:     "evaluated_at": now_iso(),
313:     "result": "passed" if ok else "failed",
314:     "ok": bool(ok),
315:     "messages": messages,
316: }
317: (run_dir / "evaluation.json").write_text(json_dumps(eval_payload), encoding="utf-8")
318: 
319: print(f"RUN_ID={run_id}")
320: print(f"REPORT={run_dir / 'report.md'}")
321: print(f"GROUND_TRUTH={source}")
322: print(f"GAUNTLET_OK={ok}")
323: for msg in messages:
324:     print(f"FAILURE={msg}")
325: "@
326: 
327: Set-Content -Path $tmp -Value $py -Encoding UTF8
328: 
329: function Invoke-WSLRun {
330:   $wsl = Get-Command wsl.exe -ErrorAction SilentlyContinue
331:   if (!$wsl) { throw "WSL not found. Install WSL or provide a native Windows Python." }
332:   function Convert-ToWslPath {
333:     param([string]$Path)
334:     $full = (Resolve-Path $Path).Path
335:     if ($full -match "^([A-Za-z]):\\(.+)$") {
336:       $drive = $Matches[1].ToLowerInvariant()
337:       $rest = $Matches[2] -replace "\\", "/"
338:       return "/mnt/$drive/$rest"
339:     }
340:     return ($full -replace "\\", "/")
341:   }
342:   $RepoRootWsl = Convert-ToWslPath $RepoRoot
343:   $TmpWsl = Convert-ToWslPath $tmp
344:   $RepoRootWsl = $RepoRootWsl.Replace("`r", "").Replace("`n", "")
345:   $TmpWsl = $TmpWsl.Replace("`r", "").Replace("`n", "")
346:   if (-not $RepoRootWsl) { throw "Failed to resolve WSL path for $RepoRoot" }
347:   if (-not $TmpWsl) { throw "Failed to resolve WSL path for $tmp" }
348:   $DatasetArg = $DatasetVersionId
349:   $SeedArg = $RunSeed
350:   $bashTmp = Join-Path $env:TEMP "stat_harness_run_gauntlet.sh"
351:   $bashLines = @(
352:     'set -e',
353:     "cd `"$RepoRootWsl`"",
354:     'PYTHON_BIN=""',
355:     'for candidate in python3.12 python3.11 python3; do',
356:     '  if command -v "$candidate" >/dev/null 2>&1; then',
357:     '    ver=$("$candidate" -c "import sys; print(str(sys.version_info[0]) + \".\" + str(sys.version_info[1]))")',
358:     '    major=${ver%%.*}',
359:     '    minor=${ver#*.}',
360:     '    if [ "$major" -gt 3 ] || { [ "$major" -eq 3 ] && [ "$minor" -ge 11 ]; }; then',
361:     '      PYTHON_BIN="$candidate"',
362:     '      break',
363:     '    fi',
364:     '  fi',
365:     'done',
366:     'if [ -z "$PYTHON_BIN" ]; then echo "WSL_PYTHON_MISSING=python3.11+"; exit 1; fi',
367:     'if [ ! -d ".venv_wsl" ]; then "$PYTHON_BIN" -m venv .venv_wsl; fi',
368:     '. .venv_wsl/bin/activate',
369:     'python -m pip install -e .',
370:     "STAT_HARNESS_APPDATA=`"$RepoRootWsl/appdata`" DATASET_VERSION_ID=`"$DatasetArg`" RUN_SEED=`"$SeedArg`" python `"$TmpWsl`""
371:   )
372:   $bashText = ($bashLines -join "`n")
373:   [System.IO.File]::WriteAllText($bashTmp, $bashText, (New-Object System.Text.UTF8Encoding($false)))
374:   $bashTmpWsl = Convert-ToWslPath $bashTmp
375:   $bash = "bash $bashTmpWsl"
376:   $bash = $bash.Trim()
377:   Write-Log -Lines @(
378:     "wsl_repo_root=" + $RepoRootWsl,
379:     "wsl_tmp=" + $TmpWsl,
380:     "wsl_script=" + $bashTmpWsl,
381:     "dataset_version_id=" + $DatasetArg,
382:     "run_seed=" + $SeedArg,
383:     "wsl_cmd=" + $bash
384:   )
385:   if (-not $bash) { throw "WSL command is empty." }
386:   try {
387:     $output = & wsl.exe bash -lc $bash 2>&1
388:   } catch {
389:     $err = $_ | Out-String
390:     Write-Log -Lines @("WSL_INVOKE_ERROR", $err)
391:     throw
392:   }
393:   Write-Log -Lines $output
394:   if (!$output) { throw "WSL run produced no output." }
395:   $missing = $output | Where-Object { $_ -match "^WSL_PYTHON_MISSING=" } | Select-Object -First 1
396:   if ($missing) {
397:     throw "WSL Python 3.11+ is missing. Install python3.11 (or newer) in your WSL distro and rerun."
398:   }
399:   $hasRunId = $output | Where-Object { $_ -match "^RUN_ID=" } | Select-Object -First 1
400:   if (-not $hasRunId) {
401:     throw ("WSL run did not produce a RUN_ID. Output:`n" + ($output -join "`n"))
402:   }
403:   $reportLine = ($output | Where-Object { $_ -match "^REPORT=" } | Select-Object -First 1)
404:   if ($reportLine) {
405:     $reportWsl = $reportLine -replace "^REPORT=", ""
406:     if ($reportWsl -match "^/mnt/([a-z])/([^:]+)$") {
407:       $drive = $Matches[1].ToUpperInvariant()
408:       $rest = $Matches[2] -replace "/", "\\"
409:       $reportWin = "$drive`:\$rest"
410:     } else {
411:       $reportWin = $reportWsl
412:     }
413:     $output = $output | Where-Object { $_ -notmatch "^REPORT=" }
414:     $output += "REPORT=$reportWin"
415:   }
416:   $result = Show-Result -OutputLines $output
417:   Show-ReportView -ReportPath $result.ReportPath -Mode $ReportView
418:   return $result
419: }
420: 
421: if ($PythonCmd -eq $null) {
422:   Invoke-WSLRun
423:   Remove-Item $tmp -ErrorAction SilentlyContinue
424:   exit 0
425: }
426: 
427: $VenvPython = Join-Path $VenvPath "Scripts\\python.exe"
428: if (!(Test-Path $VenvPython)) {
429:   & $PythonCmd.Exe @($PythonCmd.Args) -m venv $VenvPath
430: }
431: if (!(Test-Path $VenvPython)) {
432:   Write-Host "Native venv not available, falling back to WSL." -ForegroundColor Yellow
433:   Invoke-WSLRun
434:   Remove-Item $tmp -ErrorAction SilentlyContinue
435:   exit 0
436: }
437: if (-not (Test-PythonExe $VenvPython @())) {
438:   Write-Host "Native venv Python did not execute, falling back to WSL." -ForegroundColor Yellow
439:   Invoke-WSLRun
440:   Remove-Item $tmp -ErrorAction SilentlyContinue
441:   exit 0
442: }
443: 
444: & $VenvPython -m pip install -e $RepoRoot | Out-Host
445: 
446: $env:DATASET_VERSION_ID = $DatasetVersionId
447: $env:RUN_SEED = [string]$RunSeed
448: $output = & $VenvPython $tmp 2>&1
449: Write-Log -Lines $output
450: if ($LASTEXITCODE -ne 0 -or -not ($output | Where-Object { $_ -match "^RUN_ID=" } | Select-Object -First 1)) {
451:   Write-Host "Native run failed, falling back to WSL." -ForegroundColor Yellow
452:   Invoke-WSLRun
453:   Remove-Item $tmp -ErrorAction SilentlyContinue
454:   exit 0
455: }
456: $result = Show-Result -OutputLines $output
457: Show-ReportView -ReportPath $result.ReportPath -Mode $ReportView
458: Remove-Item $tmp -ErrorAction SilentlyContinue
````

## File: scripts/run_tests.ps1
````powershell
 1: $ErrorActionPreference = "Stop"
 2: 
 3: $RootDir = Resolve-Path (Join-Path $PSScriptRoot "..")
 4: $VenvDir = Join-Path $RootDir ".venv"
 5: if (-not (Test-Path $VenvDir)) {
 6:     $PythonCmd = Get-Command python -ErrorAction SilentlyContinue
 7:     if (-not $PythonCmd) {
 8:         $PythonCmd = Get-Command py -ErrorAction SilentlyContinue
 9:     }
10:     if (-not $PythonCmd) {
11:         throw "Python not found. Install Python 3 to continue."
12:     }
13:     & $PythonCmd.Source -m venv $VenvDir
14: }
15: 
16: & (Join-Path $VenvDir "Scripts\Activate.ps1")
17: $env:PYTHONPATH = "$RootDir" + ($(if ($env:PYTHONPATH) { ";" + $env:PYTHONPATH } else { "" }))
18: 
19: python -m pytest -q @args
````

## File: scripts/run_tests.sh
````bash
 1: #!/usr/bin/env bash
 2: set -euo pipefail
 3: 
 4: ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
 5: VENV_DIR="$ROOT_DIR/.venv"
 6: 
 7: if [[ ! -d "$VENV_DIR" ]]; then
 8:   PYTHON_BIN="python3"
 9:   if ! command -v "$PYTHON_BIN" >/dev/null 2>&1; then
10:     PYTHON_BIN="python"
11:   fi
12:   "$PYTHON_BIN" -m venv "$VENV_DIR"
13: fi
14: 
15: # shellcheck source=/dev/null
16: . "$VENV_DIR/bin/activate"
17: export PYTHONPATH="$ROOT_DIR${PYTHONPATH:+:$PYTHONPATH}"
18: 
19: python -m pytest -q "$@"
````

## File: scripts/ui_smoke.py
````python
  1: #!/usr/bin/env python3
  2: from __future__ import annotations
  3: 
  4: import argparse
  5: import json
  6: import os
  7: import subprocess
  8: import sys
  9: import time
 10: from pathlib import Path
 11: from typing import Any
 12: from urllib import parse, request
 13: 
 14: DEFAULT_BASE_URL = "http://127.0.0.1:8000"
 15: 
 16: 
 17: def _wait_for_server(base_url: str, timeout: int) -> None:
 18:     deadline = time.time() + timeout
 19:     last_err: Exception | None = None
 20:     while time.time() < deadline:
 21:         try:
 22:             with request.urlopen(f"{base_url}/api/uploads", timeout=5) as resp:
 23:                 if resp.status < 500:
 24:                     return
 25:         except Exception as exc:  # pragma: no cover - waiting loop
 26:             last_err = exc
 27:             time.sleep(0.5)
 28:     raise RuntimeError(f"Server not ready: {last_err}")
 29: 
 30: 
 31: def _start_server(base_url: str) -> subprocess.Popen:
 32:     parsed = parse.urlparse(base_url)
 33:     host = parsed.hostname or "127.0.0.1"
 34:     port = parsed.port or 8000
 35:     root = Path(__file__).resolve().parents[1]
 36:     env = os.environ.copy()
 37:     env["PYTHONPATH"] = str(root / "src") + os.pathsep + env.get("PYTHONPATH", "")
 38:     cmd = [sys.executable, "-m", "statistic_harness.cli", "serve", "--host", host, "--port", str(port)]
 39:     return subprocess.Popen(cmd, cwd=root, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
 40: 
 41: 
 42: def _parse_json(text: str) -> dict[str, Any]:
 43:     return json.loads(text)
 44: 
 45: 
 46: def main() -> int:
 47:     parser = argparse.ArgumentParser(description="UI smoke test (Playwright)")
 48:     parser.add_argument("--base-url", default=DEFAULT_BASE_URL)
 49:     parser.add_argument("--fixture", default="scripts/fixtures/sample.csv")
 50:     parser.add_argument("--timeout", type=int, default=180)
 51:     parser.add_argument("--start-server", action="store_true")
 52:     parser.add_argument("--headed", action="store_true")
 53:     args = parser.parse_args()
 54: 
 55:     base_url = args.base_url.rstrip("/")
 56:     fixture_path = Path(args.fixture)
 57:     if not fixture_path.exists():
 58:         raise SystemExit(f"Fixture not found: {fixture_path}")
 59: 
 60:     try:
 61:         from playwright.sync_api import sync_playwright
 62:     except Exception as exc:  # pragma: no cover - missing dependency
 63:         raise SystemExit(
 64:             "Playwright is not installed. Run: pip install -e .[dev] && python -m playwright install"
 65:         ) from exc
 66: 
 67:     server_proc = None
 68:     try:
 69:         if args.start_server:
 70:             server_proc = _start_server(base_url)
 71:         _wait_for_server(base_url, args.timeout)
 72: 
 73:         with sync_playwright() as p:
 74:             browser = p.chromium.launch(headless=not args.headed)
 75:             page = browser.new_page()
 76: 
 77:             page.goto(f"{base_url}/wizard", wait_until="networkidle")
 78:             page.set_input_files("#upload-form input[type=file]", str(fixture_path))
 79:             page.click("#upload-form button[type=submit]")
 80:             page.wait_for_function(
 81:                 "document.getElementById('upload-result').textContent.includes('upload_id')",
 82:                 timeout=args.timeout * 1000,
 83:             )
 84:             upload_text = page.inner_text("#upload-result")
 85:             upload_payload = _parse_json(upload_text)
 86:             upload_id = upload_payload.get("upload_id")
 87:             if not upload_id:
 88:                 raise RuntimeError("Upload did not return upload_id")
 89: 
 90:             # Ensure strict is unchecked to avoid false-positive enforcement
 91:             if page.is_checked("#strict-flag"):
 92:                 page.click("#strict-flag")
 93: 
 94:             # Exercise quick-add and remove
 95:             page.click("#quick-qemail")
 96:             page.locator(".issue-row").last.locator("button").click()
 97: 
 98:             page.click("#save-issues")
 99:             page.wait_for_function(
100:                 "document.getElementById('save-result').textContent.includes('status')",
101:                 timeout=args.timeout * 1000,
102:             )
103:             save_text = page.inner_text("#save-result")
104:             if "status" not in save_text:
105:                 raise RuntimeError("Known issues save did not return status")
106: 
107:             page.click("#run-auto")
108:             page.wait_for_function(
109:                 "document.getElementById('run-status').textContent.startsWith('completed')",
110:                 timeout=args.timeout * 1000,
111:             )
112:             eval_text = page.inner_text("#eval-result")
113:             eval_payload = _parse_json(eval_text)
114:             if "result" not in eval_payload:
115:                 raise RuntimeError("Evaluation payload missing result")
116: 
117:             # Basic navigation checks
118:             page.goto(f"{base_url}/", wait_until="networkidle")
119:             if "Statistic Harness" not in page.title():
120:                 raise RuntimeError("Home page title missing")
121: 
122:             browser.close()
123: 
124:         print("UI smoke test completed successfully.")
125:         return 0
126:     finally:
127:         if server_proc:
128:             server_proc.terminate()
129:             try:
130:                 server_proc.wait(timeout=10)
131:             except subprocess.TimeoutExpired:
132:                 server_proc.kill()
133: 
134: 
135: if __name__ == "__main__":
136:     raise SystemExit(main())
````

## File: src/statistic_harness/core/auth.py
````python
 1: from __future__ import annotations
 2: 
 3: import base64
 4: import hashlib
 5: import hmac
 6: import secrets
 7: from dataclasses import dataclass
 8: from typing import Any
 9: 
10: PBKDF2_ITERATIONS = 200_000
11: SALT_BYTES = 16
12: SESSION_TOKEN_PREFIX = "shs_"
13: API_KEY_PREFIX = "shk_"
14: 
15: 
16: @dataclass(frozen=True)
17: class AuthUser:
18:     user_id: int
19:     email: str
20:     name: str | None
21:     is_admin: bool
22:     tenant_id: str
23: 
24: 
25: def normalize_email(email: str) -> str:
26:     return email.strip().lower()
27: 
28: 
29: def hash_password(password: str) -> str:
30:     salt = secrets.token_bytes(SALT_BYTES)
31:     dk = hashlib.pbkdf2_hmac(
32:         "sha256", password.encode("utf-8"), salt, PBKDF2_ITERATIONS
33:     )
34:     return "pbkdf2_sha256${}${}${}".format(
35:         PBKDF2_ITERATIONS,
36:         base64.b64encode(salt).decode("ascii"),
37:         base64.b64encode(dk).decode("ascii"),
38:     )
39: 
40: 
41: def verify_password(password: str, encoded: str) -> bool:
42:     try:
43:         scheme, iter_s, salt_b64, hash_b64 = encoded.split("$", 3)
44:     except ValueError:
45:         return False
46:     if scheme != "pbkdf2_sha256":
47:         return False
48:     try:
49:         iterations = int(iter_s)
50:         salt = base64.b64decode(salt_b64.encode("ascii"))
51:         expected = base64.b64decode(hash_b64.encode("ascii"))
52:     except (ValueError, base64.binascii.Error):
53:         return False
54:     computed = hashlib.pbkdf2_hmac(
55:         "sha256", password.encode("utf-8"), salt, iterations
56:     )
57:     return hmac.compare_digest(computed, expected)
58: 
59: 
60: def generate_session_token() -> str:
61:     return SESSION_TOKEN_PREFIX + secrets.token_urlsafe(32)
62: 
63: 
64: def generate_api_key() -> str:
65:     return API_KEY_PREFIX + secrets.token_urlsafe(32)
66: 
67: 
68: def hash_token(token: str) -> str:
69:     return hashlib.sha256(token.encode("utf-8")).hexdigest()
70: 
71: 
72: def is_api_key(token: str) -> bool:
73:     return token.startswith(API_KEY_PREFIX)
74: 
75: 
76: def is_session_token(token: str) -> bool:
77:     return token.startswith(SESSION_TOKEN_PREFIX)
78: 
79: 
80: def auth_user_from_row(row: dict[str, Any]) -> AuthUser:
81:     return AuthUser(
82:         user_id=int(row["user_id"]),
83:         email=str(row["email"]),
84:         name=row.get("name"),
85:         is_admin=bool(row.get("is_admin")),
86:         tenant_id=str(row.get("tenant_id") or ""),
87:     )
````

## File: src/statistic_harness/core/column_inference.py
````python
  1: from __future__ import annotations
  2: 
  3: from dataclasses import dataclass
  4: from typing import Iterable
  5: 
  6: import pandas as pd
  7: 
  8: 
  9: @dataclass(frozen=True)
 10: class TimestampInference:
 11:     parse_ratio: float
 12:     min_year: int | None
 13:     max_year: int | None
 14:     span_days: float | None
 15:     unit: str | None
 16:     score: float
 17:     valid: bool
 18:     notes: tuple[str, ...] = ()
 19: 
 20: 
 21: def _sample_series(series: pd.Series, sample_size: int) -> pd.Series:
 22:     if isinstance(series, pd.DataFrame):
 23:         series = series.iloc[:, 0]
 24:     values = series.dropna()
 25:     if values.empty:
 26:         return values
 27:     if len(values) > sample_size:
 28:         return values.head(sample_size)
 29:     return values
 30: 
 31: 
 32: def _score_parsed(
 33:     parsed: pd.Series,
 34:     parse_ratio: float,
 35:     numeric_ratio: float,
 36:     sep_ratio: float,
 37:     name_hint: str | None,
 38:     unit: str | None,
 39: ) -> TimestampInference:
 40:     if parsed.empty or parse_ratio < 0.6:
 41:         return TimestampInference(0.0, None, None, None, unit, 0.0, False, ("low_parse",))
 42: 
 43:     years = parsed.dt.year.dropna()
 44:     if years.empty:
 45:         return TimestampInference(parse_ratio, None, None, None, unit, 0.0, False, ("no_years",))
 46: 
 47:     min_year = int(years.min())
 48:     max_year = int(years.max())
 49:     out_of_range = ((years < 1970) | (years > 2100)).mean()
 50:     span_seconds = float((parsed.max() - parsed.min()).total_seconds())
 51:     span_days = span_seconds / 86400.0
 52:     unique_days = parsed.dt.date.nunique()
 53:     unique_ts = parsed.nunique()
 54: 
 55:     score = parse_ratio * 2.0
 56:     notes: list[str] = []
 57: 
 58:     if 1990 <= min_year <= 2100 and 1990 <= max_year <= 2100:
 59:         score += 0.5
 60:         notes.append("year_range_ok")
 61:     elif min_year < 1970 or max_year > 2100:
 62:         score -= 0.8
 63:         notes.append("year_range_bad")
 64: 
 65:     if out_of_range > 0.1:
 66:         score -= 0.8
 67:         notes.append("out_of_range")
 68:     if span_seconds < 60.0:
 69:         score -= 0.6
 70:         notes.append("span_too_small")
 71:     elif span_days >= 30.0:
 72:         score += 0.4
 73:         notes.append("span_large")
 74:     if unique_ts < 2:
 75:         score -= 0.4
 76:         notes.append("unique_ts_low")
 77: 
 78:     if sep_ratio >= 0.3:
 79:         score += 0.3
 80:         notes.append("separator_strings")
 81: 
 82:     if numeric_ratio >= 0.9 and sep_ratio < 0.1:
 83:         score -= 0.6
 84:         notes.append("numeric_no_sep")
 85:         if unit:
 86:             score += 0.2
 87:             notes.append("numeric_unit")
 88: 
 89:     if name_hint:
 90:         hint = name_hint.lower()
 91:         if any(token in hint for token in ("time", "date", "timestamp", "dt")):
 92:             score += 0.2
 93:             notes.append("name_hint")
 94: 
 95:     valid = (
 96:         score >= 1.2
 97:         and parse_ratio >= 0.6
 98:         and out_of_range <= 0.1
 99:         and (span_seconds >= 60.0 or unique_ts >= 3)
100:     )
101: 
102:     return TimestampInference(
103:         parse_ratio=parse_ratio,
104:         min_year=min_year,
105:         max_year=max_year,
106:         span_days=span_days,
107:         unit=unit,
108:         score=score,
109:         valid=valid,
110:         notes=tuple(notes),
111:     )
112: 
113: 
114: def _parse_numeric_units(values: pd.Series, units: Iterable[str]) -> list[tuple[str, pd.Series]]:
115:     parsed: list[tuple[str, pd.Series]] = []
116:     for unit in units:
117:         try:
118:             series = pd.to_datetime(values, errors="coerce", unit=unit, utc=False)
119:         except (ValueError, OverflowError):
120:             continue
121:         parsed.append((unit, series))
122:     return parsed
123: 
124: 
125: def _parse_digit_format(values: pd.Series, fmt: str) -> pd.Series | None:
126:     try:
127:         return pd.to_datetime(values, errors="coerce", format=fmt, utc=False)
128:     except (ValueError, TypeError):
129:         return None
130: 
131: 
132: def infer_timestamp_series(
133:     series: pd.Series, name_hint: str | None = None, sample_size: int = 500
134: ) -> TimestampInference:
135:     sample = _sample_series(series, sample_size)
136:     if sample.empty:
137:         return TimestampInference(0.0, None, None, None, None, 0.0, False, ("empty",))
138: 
139:     sample_str = sample.astype(str)
140:     numeric = pd.to_numeric(sample, errors="coerce")
141:     numeric_ratio = float(numeric.notna().mean())
142:     sep_ratio = float(sample_str.str.contains(r"[-/:T]", na=False).mean())
143: 
144:     best: TimestampInference | None = None
145: 
146:     if numeric_ratio >= 0.9:
147:         numeric_values = numeric.dropna()
148:         for unit, parsed in _parse_numeric_units(numeric_values, ("s", "ms", "us", "ns")):
149:             parse_ratio = float(parsed.notna().mean())
150:             candidate = _score_parsed(
151:                 parsed, parse_ratio, numeric_ratio, sep_ratio, name_hint, unit
152:             )
153:             if best is None or candidate.score > best.score:
154:                 best = candidate
155: 
156:         digit_8 = sample_str.str.fullmatch(r"\d{8}", na=False)
157:         if digit_8.mean() >= 0.8:
158:             parsed = _parse_digit_format(sample_str[digit_8], "%Y%m%d")
159:             if parsed is not None:
160:                 candidate = _score_parsed(
161:                     parsed, float(parsed.notna().mean()), numeric_ratio, sep_ratio, name_hint, "ymd"
162:                 )
163:                 if best is None or candidate.score > best.score:
164:                     best = candidate
165: 
166:         digit_14 = sample_str.str.fullmatch(r"\d{14}", na=False)
167:         if digit_14.mean() >= 0.8:
168:             parsed = _parse_digit_format(sample_str[digit_14], "%Y%m%d%H%M%S")
169:             if parsed is not None:
170:                 candidate = _score_parsed(
171:                     parsed, float(parsed.notna().mean()), numeric_ratio, sep_ratio, name_hint, "ymdhms"
172:                 )
173:                 if best is None or candidate.score > best.score:
174:                     best = candidate
175:     else:
176:         parsed = pd.to_datetime(sample, errors="coerce", utc=False)
177:         candidate = _score_parsed(
178:             parsed,
179:             float(parsed.notna().mean()),
180:             numeric_ratio,
181:             sep_ratio,
182:             name_hint,
183:             None,
184:         )
185:         best = candidate
186: 
187:     return best or TimestampInference(0.0, None, None, None, None, 0.0, False, ("no_candidate",))
188: 
189: 
190: def choose_timestamp_column(
191:     df: pd.DataFrame, candidates: Iterable[str], sample_size: int = 500
192: ) -> str | None:
193:     best_col: str | None = None
194:     best_score = -1.0
195:     for col in candidates:
196:         if col not in df.columns:
197:             continue
198:         info = infer_timestamp_series(df[col], name_hint=str(col), sample_size=sample_size)
199:         if info.valid and info.score > best_score:
200:             best_col = col
201:             best_score = info.score
202:     return best_col
203: 
204: 
205: def is_valid_timestamp_series(
206:     series: pd.Series, name_hint: str | None = None, sample_size: int = 500
207: ) -> bool:
208:     info = infer_timestamp_series(series, name_hint=name_hint, sample_size=sample_size)
209:     return info.valid
````

## File: src/statistic_harness/core/plugin_runner.py
````python
  1: from __future__ import annotations
  2: 
  3: import builtins
  4: import io
  5: import json
  6: import os
  7: import subprocess
  8: import sys
  9: import time
 10: import traceback
 11: from dataclasses import asdict, dataclass
 12: from pathlib import Path
 13: from typing import Any, Iterable
 14: import math
 15: 
 16: from .dataset_io import resolve_dataset_accessor
 17: from .plugin_manager import PluginSpec
 18: from .storage import Storage
 19: from .types import PluginArtifact, PluginContext, PluginError, PluginResult
 20: from .utils import ensure_dir, now_iso, read_json, write_json
 21: 
 22: 
 23: _NETWORK_ENV = "STAT_HARNESS_ALLOW_NETWORK"
 24: 
 25: 
 26: def _seed_runtime(run_seed: int) -> None:
 27:     import random
 28: 
 29:     random.seed(run_seed)
 30:     try:
 31:         import numpy as np
 32: 
 33:         np.random.seed(run_seed)
 34:     except Exception:
 35:         pass
 36: 
 37: 
 38: def _deterministic_env(run_seed: int) -> dict[str, str]:
 39:     env = os.environ.copy()
 40:     env["PYTHONHASHSEED"] = str(run_seed)
 41:     env["TZ"] = "UTC"
 42:     env["LC_ALL"] = "C"
 43:     env["LANG"] = "C"
 44:     return env
 45: 
 46: 
 47: def _network_allowed() -> bool:
 48:     return os.environ.get(_NETWORK_ENV, "").lower() in {"1", "true", "yes"}
 49: 
 50: 
 51: def _install_network_guard() -> None:
 52:     import socket
 53: 
 54:     def blocked(*args: Any, **kwargs: Any) -> Any:
 55:         raise RuntimeError("Network disabled by STAT_HARNESS_ALLOW_NETWORK=0")
 56: 
 57:     base_socket = socket.socket
 58: 
 59:     class GuardedSocket(base_socket):
 60:         def __init__(self, *args: Any, **kwargs: Any) -> None:  # type: ignore[override]
 61:             blocked(*args, **kwargs)
 62: 
 63:         def connect(self, *args: Any, **kwargs: Any) -> Any:  # type: ignore[override]
 64:             return blocked(*args, **kwargs)
 65: 
 66:         def connect_ex(self, *args: Any, **kwargs: Any) -> Any:  # type: ignore[override]
 67:             return blocked(*args, **kwargs)
 68: 
 69:     socket.socket = GuardedSocket  # type: ignore[assignment]
 70:     socket.create_connection = blocked  # type: ignore[assignment]
 71: 
 72: 
 73: def _install_eval_guard(root_dir: Path | None = None) -> None:
 74:     import inspect
 75: 
 76:     root = root_dir.resolve() if root_dir else None
 77:     orig_eval = builtins.eval
 78: 
 79:     def guarded_eval(
 80:         expression: Any, globals: dict[str, Any] | None = None, locals: dict[str, Any] | None = None
 81:     ) -> Any:
 82:         frame = inspect.currentframe()
 83:         caller = frame.f_back if frame else None
 84:         if root is not None and caller:
 85:             filename = caller.f_code.co_filename
 86:             if filename:
 87:                 try:
 88:                     caller_path = Path(filename).resolve()
 89:                     try:
 90:                         caller_path.relative_to(root)
 91:                         raise RuntimeError("Eval disabled by policy")
 92:                     except ValueError:
 93:                         pass
 94:                 except RuntimeError:
 95:                     raise
 96:                 except Exception:
 97:                     pass
 98:         if caller:
 99:             if globals is None:
100:                 globals = caller.f_globals
101:             if locals is None:
102:                 locals = caller.f_locals
103:         return orig_eval(expression, globals, locals)
104: 
105:     builtins.eval = guarded_eval  # type: ignore[assignment]
106: 
107: 
108: def _install_pickle_guard() -> None:
109:     import pickle
110: 
111:     def blocked(*args: Any, **kwargs: Any) -> Any:
112:         raise RuntimeError("Pickle disabled by policy")
113: 
114:     pickle.load = blocked  # type: ignore[assignment]
115:     pickle.loads = blocked  # type: ignore[assignment]
116:     pickle.dump = blocked  # type: ignore[assignment]
117:     pickle.dumps = blocked  # type: ignore[assignment]
118:     pickle.Pickler = blocked  # type: ignore[assignment]
119:     pickle.Unpickler = blocked  # type: ignore[assignment]
120: 
121: 
122: def _install_shell_guard() -> None:
123:     import subprocess
124: 
125:     def blocked(*args: Any, **kwargs: Any) -> Any:
126:         raise RuntimeError("Shell disabled by policy")
127: 
128:     os.system = blocked  # type: ignore[assignment]
129:     os.popen = blocked  # type: ignore[assignment]
130:     subprocess.run = blocked  # type: ignore[assignment]
131:     subprocess.call = blocked  # type: ignore[assignment]
132:     subprocess.check_call = blocked  # type: ignore[assignment]
133:     subprocess.check_output = blocked  # type: ignore[assignment]
134:     subprocess.Popen = blocked  # type: ignore[assignment]
135: 
136: 
137: def _apply_resource_limits(budget: dict[str, Any]) -> None:
138:     cpu_limit_ms = budget.get("cpu_limit_ms")
139:     if cpu_limit_ms is None:
140:         return
141:     try:
142:         cpu_seconds = max(1, int(math.ceil(float(cpu_limit_ms) / 1000.0)))
143:     except (TypeError, ValueError):
144:         return
145:     try:
146:         import resource
147: 
148:         resource.setrlimit(resource.RLIMIT_CPU, (cpu_seconds, cpu_seconds))
149:     except Exception:
150:         pass
151: 
152: 
153: class FileSandbox:
154:     def __init__(self, allow_paths: Iterable[str], cwd: Path) -> None:
155:         self._cwd = cwd
156:         self._allow = [self._normalize(Path(p)) for p in allow_paths]
157:         self._readonly_allow = self._library_roots()
158:         self._orig_open = builtins.open
159:         self._orig_io_open = io.open
160:         self._orig_path_open = Path.open
161: 
162:     def _normalize(self, path: Path) -> Path:
163:         if not path.is_absolute():
164:             path = (self._cwd / path).resolve()
165:         else:
166:             path = path.resolve()
167:         return path
168: 
169:     def _library_roots(self) -> list[Path]:
170:         roots: list[Path] = []
171:         for base in {sys.prefix, sys.base_prefix}:
172:             if base:
173:                 roots.append(Path(base).resolve())
174:         return roots
175: 
176:     def _is_readonly_allowed(self, path: Path) -> bool:
177:         target = self._normalize(path)
178:         for root in self._readonly_allow:
179:             try:
180:                 target.relative_to(root)
181:                 return True
182:             except ValueError:
183:                 continue
184:         return False
185: 
186:     def _is_allowed(self, path: Path) -> bool:
187:         target = self._normalize(path)
188:         for allowed in self._allow:
189:             if allowed.is_file():
190:                 if target == allowed:
191:                     return True
192:             else:
193:                 try:
194:                     target.relative_to(allowed)
195:                     return True
196:                 except ValueError:
197:                     continue
198:         return False
199: 
200:     def _guarded_open(self, file: Any, *args: Any, **kwargs: Any) -> Any:
201:         path = Path(file) if not isinstance(file, Path) else file
202:         mode = "r"
203:         if args:
204:             mode = str(args[0])
205:         if "mode" in kwargs:
206:             mode = str(kwargs["mode"])
207:         write_mode = any(token in mode for token in ("w", "a", "x", "+"))
208:         if write_mode:
209:             if not self._is_allowed(path):
210:                 raise PermissionError(f"File write denied: {path}")
211:         else:
212:             if not (self._is_allowed(path) or self._is_readonly_allowed(path)):
213:                 raise PermissionError(f"File read denied: {path}")
214:         return self._orig_open(file, *args, **kwargs)
215: 
216:     def __enter__(self) -> "FileSandbox":
217:         builtins.open = self._guarded_open  # type: ignore[assignment]
218:         io.open = self._guarded_open  # type: ignore[assignment]
219:         sandbox = self
220: 
221:         def _path_open(path_self: Path, *args: Any, **kwargs: Any) -> Any:
222:             return sandbox._guarded_open(path_self, *args, **kwargs)
223: 
224:         Path.open = _path_open  # type: ignore[assignment]
225:         return self
226: 
227:     def __exit__(self, exc_type, exc, tb) -> None:  # type: ignore[override]
228:         builtins.open = self._orig_open  # type: ignore[assignment]
229:         io.open = self._orig_io_open  # type: ignore[assignment]
230:         Path.open = self._orig_path_open  # type: ignore[assignment]
231: 
232: 
233: def _load_plugin(plugin_id: str, entrypoint: str) -> Any:
234:     module_path, class_name = entrypoint.split(":", 1)
235:     if module_path.endswith(".py"):
236:         module_path = module_path[:-3]
237:     module_name = f"plugins.{plugin_id}.{module_path}"
238:     module = __import__(module_name, fromlist=[class_name])
239:     return getattr(module, class_name)()
240: 
241: 
242: def _result_payload(result: PluginResult) -> dict[str, Any]:
243:     return {
244:         "status": result.status,
245:         "summary": result.summary,
246:         "metrics": result.metrics,
247:         "findings": result.findings,
248:         "artifacts": [asdict(a) for a in result.artifacts],
249:         "budget": result.budget,
250:         "error": asdict(result.error) if result.error else None,
251:     }
252: 
253: 
254: def _payload_result(payload: dict[str, Any]) -> PluginResult:
255:     artifacts = [
256:         PluginArtifact(**item)
257:         for item in payload.get("artifacts", [])
258:         if isinstance(item, dict)
259:     ]
260:     error = payload.get("error")
261:     error_obj = None
262:     if isinstance(error, dict):
263:         error_obj = PluginError(
264:             type=error.get("type", "Error"),
265:             message=error.get("message", ""),
266:             traceback=error.get("traceback", ""),
267:         )
268:     return PluginResult(
269:         status=payload.get("status", "error"),
270:         summary=payload.get("summary", ""),
271:         metrics=payload.get("metrics", {}),
272:         findings=payload.get("findings", []),
273:         artifacts=artifacts,
274:         budget=payload.get("budget")
275:         or {
276:             "row_limit": None,
277:             "sampled": False,
278:             "time_limit_ms": None,
279:             "cpu_limit_ms": None,
280:         },
281:         error=error_obj,
282:     )
283: 
284: 
285: @dataclass
286: class RunnerResponse:
287:     result: PluginResult
288:     execution: dict[str, Any]
289:     stdout: str
290:     stderr: str
291:     exit_code: int
292: 
293: 
294: def run_plugin_subprocess(
295:     spec: PluginSpec,
296:     request: dict[str, Any],
297:     run_dir: Path,
298:     cwd: Path,
299: ) -> RunnerResponse:
300:     ensure_dir(run_dir / "logs")
301:     request_path = run_dir / "logs" / f"{spec.plugin_id}_request.json"
302:     response_path = run_dir / "logs" / f"{spec.plugin_id}_response.json"
303:     write_json(request_path, request)
304:     start = time.perf_counter()
305:     run_seed = int(request.get("run_seed", 0))
306:     budget = request.get("budget") or {}
307:     timeout_ms = budget.get("time_limit_ms")
308:     timeout = None
309:     if isinstance(timeout_ms, (int, float)) and timeout_ms > 0:
310:         timeout = float(timeout_ms) / 1000.0
311:     try:
312:         proc = subprocess.run(
313:             [
314:                 sys.executable,
315:                 "-m",
316:                 "statistic_harness.core.plugin_runner",
317:                 str(request_path),
318:                 str(response_path),
319:             ],
320:             capture_output=True,
321:             text=True,
322:             cwd=str(cwd),
323:             env=_deterministic_env(run_seed),
324:             timeout=timeout,
325:         )
326:     except subprocess.TimeoutExpired as exc:
327:         duration_ms = int((time.perf_counter() - start) * 1000)
328:         stdout = (exc.stdout or "")[:4000]
329:         stderr = (exc.stderr or "")[:4000]
330:         result = PluginResult(
331:             status="error",
332:             summary=f"{spec.plugin_id} timed out",
333:             metrics={},
334:             findings=[],
335:             artifacts=[],
336:             error=PluginError(
337:                 type="TimeoutError",
338:                 message=f"Plugin exceeded time limit of {timeout_ms}ms",
339:                 traceback="",
340:             ),
341:         )
342:         execution = {
343:             "started_at": request.get("started_at"),
344:             "completed_at": now_iso(),
345:             "duration_ms": duration_ms,
346:             "cpu_user": None,
347:             "cpu_system": None,
348:             "max_rss": None,
349:             "warnings_count": None,
350:         }
351:         return RunnerResponse(
352:             result=result,
353:             execution=execution,
354:             stdout=stdout,
355:             stderr=stderr,
356:             exit_code=-1,
357:         )
358:     duration_ms = int((time.perf_counter() - start) * 1000)
359:     stdout = (proc.stdout or "")[:4000]
360:     stderr = (proc.stderr or "")[:4000]
361:     execution = {
362:         "started_at": request.get("started_at"),
363:         "completed_at": now_iso(),
364:         "duration_ms": duration_ms,
365:         "cpu_user": None,
366:         "cpu_system": None,
367:         "max_rss": None,
368:         "warnings_count": None,
369:     }
370:     if response_path.exists():
371:         payload = read_json(response_path)
372:         result = _payload_result(payload.get("result", {}))
373:         execution.update(payload.get("execution", {}))
374:     else:
375:         result = PluginResult(
376:             status="error",
377:             summary=f"{spec.plugin_id} failed to execute",
378:             metrics={},
379:             findings=[],
380:             artifacts=[],
381:             error=PluginError(
382:                 type="RunnerError",
383:                 message=f"Missing response for {spec.plugin_id}",
384:                 traceback=stderr or "",
385:             ),
386:         )
387:     return RunnerResponse(
388:         result=result,
389:         execution=execution,
390:         stdout=stdout,
391:         stderr=stderr,
392:         exit_code=proc.returncode,
393:     )
394: 
395: 
396: def _run_request(request: dict[str, Any]) -> dict[str, Any]:
397:     started_at = now_iso()
398:     request["started_at"] = started_at
399:     execution: dict[str, Any] = {"started_at": started_at}
400:     warnings_count = 0
401:     try:
402:         run_seed = int(request.get("run_seed", 0))
403:         os.environ.update(_deterministic_env(run_seed))
404:         try:
405:             time.tzset()
406:         except AttributeError:
407:             pass
408:         _seed_runtime(run_seed)
409:         if request.get("sandbox", {}).get("no_network") and not _network_allowed():
410:             _install_network_guard()
411:         allow_paths = request.get("allow_paths") or []
412:         cwd = Path(request.get("root_dir", ".")).resolve()
413:         run_dir = Path(request["run_dir"]).resolve()
414:         with FileSandbox(allow_paths, cwd):
415:             storage = Storage(
416:                 Path(request["appdata_dir"]) / "state.sqlite",
417:                 request.get("tenant_id"),
418:             )
419:             dataset_version_id = request.get("dataset_version_id")
420:             accessor, _ = resolve_dataset_accessor(storage, dataset_version_id)
421:             budget = request.get("budget") or {}
422: 
423:             def dataset_loader(
424:                 columns: list[str] | None = None, row_limit: int | None = None
425:             ):
426:                 limit = row_limit
427:                 if limit is None:
428:                     limit = budget.get("row_limit")
429:                 return accessor.load(columns=columns, row_limit=limit)
430: 
431:             ctx = PluginContext(
432:                 run_id=request["run_id"],
433:                 run_dir=run_dir,
434:                 settings=request.get("settings", {}),
435:                 run_seed=run_seed,
436:                 logger=lambda msg: _write_log(run_dir, request["plugin_id"], msg),
437:                 storage=storage,
438:                 dataset_loader=dataset_loader,
439:                 budget=budget
440:                 or {
441:                     "row_limit": None,
442:                     "sampled": False,
443:                     "time_limit_ms": None,
444:                     "cpu_limit_ms": None,
445:                 },
446:                 tenant_id=request.get("tenant_id"),
447:                 project_id=request.get("project_id"),
448:                 dataset_id=request.get("dataset_id"),
449:                 dataset_version_id=dataset_version_id,
450:                 input_hash=request.get("input_hash"),
451:             )
452:             start = time.perf_counter()
453:             try:
454:                 import resource
455: 
456:                 start_usage = resource.getrusage(resource.RUSAGE_SELF)
457:             except Exception:  # pragma: no cover - platform specific
458:                 start_usage = None
459: 
460:             try:
461:                 import warnings
462: 
463:                 with warnings.catch_warnings(record=True) as caught:
464:                     _install_eval_guard(cwd)
465:                     _install_pickle_guard()
466:                     _install_shell_guard()
467:                     _apply_resource_limits(budget)
468:                     plugin = _load_plugin(request["plugin_id"], request["entrypoint"])
469:                     result = plugin.run(ctx)
470:                     warnings_count = len(caught)
471:             except Exception as exc:
472:                 tb = traceback.format_exc()
473:                 result = PluginResult(
474:                     status="error",
475:                     summary=f"{request['plugin_id']} failed",
476:                     metrics={},
477:                     findings=[],
478:                     artifacts=[],
479:                     error=PluginError(
480:                         type=type(exc).__name__, message=str(exc), traceback=tb
481:                     ),
482:                 )
483: 
484:             duration_ms = int((time.perf_counter() - start) * 1000)
485:             execution.update(
486:                 {
487:                     "completed_at": now_iso(),
488:                     "duration_ms": duration_ms,
489:                     "warnings_count": warnings_count,
490:                 }
491:             )
492:             try:
493:                 import resource
494: 
495:                 end_usage = resource.getrusage(resource.RUSAGE_SELF)
496:                 if start_usage and end_usage:
497:                     execution.update(
498:                         {
499:                             "cpu_user": end_usage.ru_utime - start_usage.ru_utime,
500:                             "cpu_system": end_usage.ru_stime - start_usage.ru_stime,
501:                             "max_rss": end_usage.ru_maxrss,
502:                         }
503:                     )
504:             except Exception:  # pragma: no cover - platform specific
505:                 pass
506:     except Exception as exc:  # pragma: no cover - runner failure
507:         tb = traceback.format_exc()
508:         result = PluginResult(
509:             status="error",
510:             summary=f"{request.get('plugin_id', 'plugin')} runner failed",
511:             metrics={},
512:             findings=[],
513:             artifacts=[],
514:             error=PluginError(type=type(exc).__name__, message=str(exc), traceback=tb),
515:         )
516:         execution.update({"completed_at": now_iso(), "duration_ms": 0})
517:     return {"result": _result_payload(result), "execution": execution}
518: 
519: 
520: def _write_log(run_dir: Path, plugin_id: str, msg: str) -> None:
521:     log_path = run_dir / "logs" / f"{plugin_id}.log"
522:     log_path.parent.mkdir(parents=True, exist_ok=True)
523:     with log_path.open("a", encoding="utf-8") as handle:
524:         handle.write(msg + "\n")
525: 
526: 
527: def main(argv: list[str] | None = None) -> int:
528:     args = argv or sys.argv[1:]
529:     if len(args) != 2:
530:         raise SystemExit("Usage: plugin_runner <request.json> <response.json>")
531:     request_path = Path(args[0])
532:     response_path = Path(args[1])
533:     request = read_json(request_path)
534:     response = _run_request(request)
535:     write_json(response_path, response)
536:     return 0
537: 
538: 
539: if __name__ == "__main__":  # pragma: no cover
540:     raise SystemExit(main())
````

## File: src/statistic_harness/core/stat_controls.py
````python
 1: from __future__ import annotations
 2: 
 3: from typing import Iterable
 4: 
 5: 
 6: def benjamini_hochberg(p_values: Iterable[float]) -> list[float]:
 7:     values = [float(p) for p in p_values]
 8:     n = len(values)
 9:     if n == 0:
10:         return []
11:     indexed = sorted(enumerate(values), key=lambda item: item[1])
12:     q_values = [0.0] * n
13:     prev = 1.0
14:     for rank, (idx, pval) in enumerate(reversed(indexed), start=1):
15:         adj = min(prev, (n / float(n - rank + 1)) * pval)
16:         prev = adj
17:         q_values[idx] = max(0.0, min(1.0, adj))
18:     return q_values
19: 
20: 
21: def confidence_from_p(p_value: float) -> float:
22:     try:
23:         pval = float(p_value)
24:     except (TypeError, ValueError):
25:         return 0.0
26:     return max(0.0, min(1.0, 1.0 - pval))
27: 
28: 
29: def effect_size_ratio(a: float, b: float) -> float:
30:     if b == 0:
31:         return 0.0
32:     return float(a) / float(b)
````

## File: src/statistic_harness/core/template.py
````python
  1: from __future__ import annotations
  2: 
  3: import hashlib
  4: from typing import Any
  5: 
  6: from .storage import Storage
  7: from .utils import json_dumps, now_iso, quote_identifier
  8: 
  9: 
 10: def mapping_hash(mapping: dict[str, Any]) -> str:
 11:     return hashlib.sha256(json_dumps(mapping).encode("utf-8")).hexdigest()
 12: 
 13: 
 14: def apply_template(
 15:     storage: Storage,
 16:     dataset_version_id: str,
 17:     template_id: int,
 18:     mapping: dict[str, Any],
 19: ) -> int:
 20:     template = storage.fetch_template(template_id)
 21:     if not template:
 22:         raise ValueError("Template not found")
 23:     template_fields = storage.fetch_template_fields(template_id)
 24:     dataset = storage.get_dataset_version(dataset_version_id)
 25:     if not dataset:
 26:         raise ValueError("Dataset version not found")
 27: 
 28:     mapping_h = mapping_hash(mapping)
 29:     storage.upsert_dataset_template(
 30:         dataset_version_id,
 31:         template_id,
 32:         json_dumps(mapping),
 33:         mapping_h,
 34:         "pending",
 35:         now_iso(),
 36:         now_iso(),
 37:     )
 38: 
 39:     row_count = 0
 40:     try:
 41:         with storage.connection() as conn:
 42:             columns = storage.fetch_dataset_columns(dataset_version_id, conn)
 43:             raw_table = dataset["table_name"]
 44:             raw_col_map = {
 45:                 col["original_name"]: col["safe_name"] for col in columns
 46:             }
 47:             template_table = template["table_name"]
 48: 
 49:             conn.execute(
 50:                 f"DELETE FROM {quote_identifier(template_table)} WHERE dataset_version_id = ?",
 51:                 (dataset_version_id,),
 52:             )
 53: 
 54:             select_cols = []
 55:             json_parts = []
 56:             for field in template_fields:
 57:                 field_name = field["name"]
 58:                 raw_name = mapping.get(field_name)
 59:                 if raw_name and raw_name in raw_col_map:
 60:                     safe_raw = quote_identifier(raw_col_map[raw_name])
 61:                     select_cols.append(safe_raw)
 62:                     json_parts.append("?")
 63:                     json_parts.append(safe_raw)
 64:                 else:
 65:                     select_cols.append("NULL")
 66:                     json_parts.append("?")
 67:                     json_parts.append("NULL")
 68:             json_expr = "NULL"
 69:             if json_parts:
 70:                 json_expr = "json_object(" + ", ".join(json_parts) + ")"
 71: 
 72:             template_safe_cols = [quote_identifier(field["safe_name"]) for field in template_fields]
 73:             insert_cols = (
 74:                 "dataset_version_id, row_index, row_json"
 75:                 + (", " + ", ".join(template_safe_cols) if template_safe_cols else "")
 76:             )
 77:             select_list = (
 78:                 "?, row_index, " + json_expr
 79:                 + (", " + ", ".join(select_cols) if select_cols else "")
 80:             )
 81:             sql = (
 82:                 f"INSERT INTO {quote_identifier(template_table)} ({insert_cols}) "
 83:                 f"SELECT {select_list} FROM {quote_identifier(raw_table)} ORDER BY row_index"
 84:             )
 85:             params = [dataset_version_id]
 86:             for field in template_fields:
 87:                 params.append(field["name"])
 88:             conn.execute(sql, params)
 89:             cur = conn.execute(
 90:                 f"SELECT COUNT(*) FROM {quote_identifier(template_table)} WHERE dataset_version_id = ?",
 91:                 (dataset_version_id,),
 92:             )
 93:             row_count = int(cur.fetchone()[0])
 94: 
 95:         storage.upsert_dataset_template(
 96:             dataset_version_id,
 97:             template_id,
 98:             json_dumps(mapping),
 99:             mapping_h,
100:             "ready",
101:             now_iso(),
102:             now_iso(),
103:         )
104:         storage.record_template_conversion(
105:             dataset_version_id,
106:             template_id,
107:             "completed",
108:             now_iso(),
109:             now_iso(),
110:             mapping_h,
111:             row_count=row_count,
112:         )
113:         return row_count
114:     except Exception as exc:  # pragma: no cover - error flow
115:         storage.upsert_dataset_template(
116:             dataset_version_id,
117:             template_id,
118:             json_dumps(mapping),
119:             mapping_h,
120:             "error",
121:             now_iso(),
122:             now_iso(),
123:         )
124:         storage.record_template_conversion(
125:             dataset_version_id,
126:             template_id,
127:             "error",
128:             now_iso(),
129:             now_iso(),
130:             mapping_h,
131:             row_count=row_count,
132:             error={"message": str(exc)},
133:         )
134:         raise
````

## File: src/statistic_harness/ui/templates/admin.html
````html
  1: <!doctype html>
  2: <html lang="en">
  3:   <head>
  4:     <meta charset="utf-8" />
  5:     <title>Admin</title>
  6:     <link rel="stylesheet" href="/static/app.css" />
  7:   </head>
  8:   <body>
  9:     <div class="container">
 10:       <nav class="top-nav">
 11:         <a href="/">Home</a>
 12:         <a href="/projects">Projects</a>
 13:         <a href="/templates">Templates</a>
 14:         <a href="/trace">Trace</a>
 15:         <a href="/vectors">Vectors</a>
 16:         <a href="/admin">Admin</a>
 17:         <a href="/logout">Logout</a>
 18:       </nav>
 19: 
 20:       <h1>Admin</h1>
 21:       <p class="muted">Tenant: {{ tenant_id }}</p>
 22: 
 23:       {% if message %}
 24:       <p class="pill ok">{{ message }}</p>
 25:       {% endif %}
 26:       {% if error %}
 27:       <p class="pill error">{{ error }}</p>
 28:       {% endif %}
 29:       {% if new_api_key %}
 30:       <section class="card">
 31:         <h2>New API Key</h2>
 32:         <p class="muted">Copy this key now. It will not be shown again.</p>
 33:         <pre>{{ new_api_key }}</pre>
 34:       </section>
 35:       {% endif %}
 36: 
 37:       <section class="card">
 38:         <h2>Create User</h2>
 39:         <form action="/admin/users" method="post" class="stack">
 40:           <label>Email</label>
 41:           <input type="email" name="email" required />
 42:           <label>Name</label>
 43:           <input type="text" name="name" />
 44:           <label>Password</label>
 45:           <input type="password" name="password" required />
 46:           <label>Role</label>
 47:           <select name="role">
 48:             <option value="member">member</option>
 49:             <option value="admin">admin</option>
 50:           </select>
 51:           <label><input type="checkbox" name="is_admin" /> Global admin</label>
 52:           <button type="submit">Create user</button>
 53:         </form>
 54:       </section>
 55: 
 56:       <section class="card">
 57:         <h2>Create Tenant</h2>
 58:         <form action="/admin/tenants" method="post" class="stack">
 59:           <label>Tenant ID</label>
 60:           <input type="text" name="tenant_id" placeholder="tenant_alpha" required />
 61:           <label>Name</label>
 62:           <input type="text" name="name" placeholder="Tenant Alpha" />
 63:           <button type="submit">Create tenant</button>
 64:         </form>
 65:       </section>
 66: 
 67:       <section class="card">
 68:         <h2>Users</h2>
 69:         {% if users %}
 70:         <table>
 71:           <thead>
 72:             <tr>
 73:               <th>Email</th>
 74:               <th>Name</th>
 75:               <th>Role</th>
 76:               <th>Admin</th>
 77:               <th>Status</th>
 78:               <th></th>
 79:             </tr>
 80:           </thead>
 81:           <tbody>
 82:             {% for user in users %}
 83:             <tr>
 84:               <td>{{ user.email }}</td>
 85:               <td>{{ user.name or "" }}</td>
 86:               <td>{{ user.role }}</td>
 87:               <td>{{ "yes" if user.is_admin else "no" }}</td>
 88:               <td>{{ "disabled" if user.disabled_at else "active" }}</td>
 89:               <td>
 90:                 {% if not user.disabled_at %}
 91:                 <form action="/admin/users/{{ user.user_id }}/disable" method="post">
 92:                   <button type="submit">Disable</button>
 93:                 </form>
 94:                 {% endif %}
 95:               </td>
 96:             </tr>
 97:             {% endfor %}
 98:           </tbody>
 99:         </table>
100:         {% else %}
101:         <p class="muted">No users in this tenant.</p>
102:         {% endif %}
103:       </section>
104: 
105:       <section class="card">
106:         <h2>Create API Key</h2>
107:         <form action="/admin/api-keys" method="post" class="stack">
108:           <label>User</label>
109:           <select name="user_id">
110:             {% for user in users %}
111:             <option value="{{ user.user_id }}">{{ user.email }}</option>
112:             {% endfor %}
113:           </select>
114:           <label>Name</label>
115:           <input type="text" name="name" placeholder="cli" />
116:           <button type="submit">Create key</button>
117:         </form>
118:       </section>
119: 
120:       <section class="card">
121:         <h2>API Keys</h2>
122:         {% if api_keys %}
123:         <table>
124:           <thead>
125:             <tr>
126:               <th>User</th>
127:               <th>Name</th>
128:               <th>Created</th>
129:               <th>Last used</th>
130:               <th>Status</th>
131:               <th></th>
132:             </tr>
133:           </thead>
134:           <tbody>
135:             {% for key in api_keys %}
136:             <tr>
137:               <td>{{ key.email }}</td>
138:               <td>{{ key.name or "" }}</td>
139:               <td>{{ key.created_at }}</td>
140:               <td>{{ key.last_used_at or "" }}</td>
141:               <td>{{ "revoked" if key.revoked_at else "active" }}</td>
142:               <td>
143:                 {% if not key.revoked_at %}
144:                 <form action="/admin/api-keys/{{ key.key_id }}/revoke" method="post">
145:                   <button type="submit">Revoke</button>
146:                 </form>
147:                 {% endif %}
148:               </td>
149:             </tr>
150:             {% endfor %}
151:           </tbody>
152:         </table>
153:         {% else %}
154:         <p class="muted">No API keys yet.</p>
155:         {% endif %}
156:       </section>
157: 
158:       <section class="card">
159:         <h2>Tenants</h2>
160:         {% if tenants %}
161:         <table>
162:           <thead>
163:             <tr>
164:               <th>ID</th>
165:               <th>Name</th>
166:               <th>Default</th>
167:             </tr>
168:           </thead>
169:           <tbody>
170:             {% for tenant in tenants %}
171:             <tr>
172:               <td>{{ tenant.tenant_id }}</td>
173:               <td>{{ tenant.name or "" }}</td>
174:               <td>{{ "yes" if tenant.is_default else "no" }}</td>
175:             </tr>
176:             {% endfor %}
177:           </tbody>
178:         </table>
179:         {% else %}
180:         <p class="muted">No tenants.</p>
181:         {% endif %}
182:       </section>
183:     </div>
184:   </body>
185: </html>
````

## File: src/statistic_harness/ui/templates/auto_evaluate.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Auto Run Started</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6:   <meta http-equiv="refresh" content="2; url=/runs/{{ run_id }}/evaluate?known=1" />
 7: </head>
 8: <body>
 9:   <h1>Auto Run Started</h1>
10:   <p>Run ID: {{ run_id }}</p>
11:   <p>Redirecting to evaluation...</p>
12:   <p>
13:     <a href="/runs/{{ run_id }}/evaluate?known=1">Go to evaluation now</a> |
14:     <a href="/runs/{{ run_id }}">Run status</a>
15:   </p>
16: </body>
17: </html>
````

## File: src/statistic_harness/ui/templates/bootstrap.html
````html
 1: <!doctype html>
 2: <html lang="en">
 3:   <head>
 4:     <meta charset="utf-8" />
 5:     <title>Bootstrap Admin</title>
 6:     <link rel="stylesheet" href="/static/app.css" />
 7:   </head>
 8:   <body>
 9:     <div class="container">
10:       <h1>Create Admin</h1>
11:       {% if error %}
12:       <p class="error">{{ error }}</p>
13:       {% endif %}
14:       <form method="post" action="/bootstrap">
15:         <label>Email</label>
16:         <input type="email" name="email" required />
17:         <label>Name</label>
18:         <input type="text" name="name" />
19:         <label>Password</label>
20:         <input type="password" name="password" required />
21:         <button type="submit">Create admin</button>
22:       </form>
23:     </div>
24:   </body>
25: </html>
````

## File: src/statistic_harness/ui/templates/evaluate.html
````html
  1: <!DOCTYPE html>
  2: <html>
  3: <head>
  4:   <title>Evaluate Run {{ run_id }}</title>
  5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
  6: </head>
  7: <body>
  8:   <h1>Evaluate Run {{ run_id }}</h1>
  9:   <p>Run status: <span id="run-status">{{ run_status }}</span></p>
 10:   {% if run_status != "completed" %}
 11:     <p id="run-status-note">Run is still in progress. This page will update automatically.</p>
 12:   {% endif %}
 13:   <p>
 14:     <a href="/runs/{{ run_id }}/report">View report</a>
 15:     {% if upload_id %}
 16:       | <a href="/known-issues?upload_id={{ upload_id }}">Edit known issues</a>
 17:       | <a href="/runs/{{ run_id }}/evaluate?known=1">Load known issues</a>
 18:     {% endif %}
 19:   </p>
 20:   <h3>How to fill this</h3>
 21:   <p>Set <code>strict: true</code> to enforce no false positives.</p>
 22:   <p>Then list only the issues you already know are true:</p>
 23:   <ul>
 24:     <li><code>features</code>: column names you expect to be selected.</li>
 25:     <li><code>changepoints</code>: row indexes (0-based), or objects with tolerances.</li>
 26:     <li><code>dependence_shift_pairs</code>: lists like <code>[colA, colB]</code>.</li>
 27:     <li><code>anomalies</code>: row indexes (0-based), or objects with tolerances.</li>
 28:   </ul>
 29:   <pre>strict: true
 30: features:
 31:   - ColumnA
 32: changepoints:
 33:   - index: 120
 34:     tolerance:
 35:       absolute: 2
 36: dependence_shift_pairs:
 37:   - [ColumnX, ColumnY]
 38: anomalies:
 39:   - row_index: 55
 40:     tolerance:
 41:       absolute: 0
 42: min_anomaly_hits: 1</pre>
 43:   {% if ground_truth_source %}
 44:     <p>Ground truth source: <strong>{{ ground_truth_source }}</strong></p>
 45:   {% endif %}
 46:   <form action="/runs/{{ run_id }}/evaluate" method="post">
 47:     <label>Ground Truth YAML:</label>
 48:     <textarea name="ground_truth" rows="16" cols="80">{{ ground_truth }}</textarea>
 49:     <button type="submit" id="evaluate-button">Evaluate</button>
 50:   </form>
 51:   <p>
 52:     <a href="/runs/{{ run_id }}/evaluate?template=1">Generate template</a>
 53:     {% if upload_id %}
 54:       | <a href="/runs/{{ run_id }}/evaluate?known=1">Use known issues</a>
 55:     {% endif %}
 56:   </p>
 57:   {% if result %}
 58:     <h2>Result: {{ result }}</h2>
 59:     {% if evaluated_at %}
 60:       <p>Evaluated at: {{ evaluated_at }}</p>
 61:     {% endif %}
 62:     {% if messages %}
 63:       <ul>
 64:         {% for message in messages %}
 65:           <li>{{ message }}</li>
 66:         {% endfor %}
 67:       </ul>
 68:     {% endif %}
 69:   {% endif %}
 70:   <script>
 71:     const statusEl = document.getElementById("run-status");
 72:     const noteEl = document.getElementById("run-status-note");
 73:     const evalBtn = document.getElementById("evaluate-button");
 74:     const params = new URLSearchParams(window.location.search);
 75:     const templateFlag = params.get("template");
 76:     const knownFlag = params.get("known");
 77:     const reloadKey = `eval-reload:{{ run_id }}:${templateFlag || ""}:${knownFlag || ""}`;
 78:     const alreadyReloaded = sessionStorage.getItem(reloadKey) === "1";
 79:     async function pollStatus() {
 80:       const resp = await fetch(`/api/runs/{{ run_id }}`);
 81:       if (!resp.ok) return;
 82:       const payload = await resp.json();
 83:       const status = payload.run?.status || "";
 84:       if (statusEl) statusEl.textContent = status;
 85:       if (status === "completed") {
 86:         if (noteEl) noteEl.remove();
 87:         if (evalBtn) evalBtn.disabled = false;
 88:         if ((templateFlag || knownFlag) && !alreadyReloaded) {
 89:           sessionStorage.setItem(reloadKey, "1");
 90:           window.location.reload();
 91:         }
 92:         return;
 93:       }
 94:       if (evalBtn) evalBtn.disabled = true;
 95:       setTimeout(pollStatus, 2000);
 96:     }
 97:     pollStatus();
 98:   </script>
 99:   <a href="/runs/{{ run_id }}">Back</a>
100: </body>
101: </html>
````

## File: src/statistic_harness/ui/templates/login.html
````html
 1: <!doctype html>
 2: <html lang="en">
 3:   <head>
 4:     <meta charset="utf-8" />
 5:     <title>Statistic Harness Login</title>
 6:     <link rel="stylesheet" href="/static/app.css" />
 7:   </head>
 8:   <body>
 9:     <div class="container">
10:       <h1>Sign in</h1>
11:       {% if error %}
12:       <p class="error">{{ error }}</p>
13:       {% endif %}
14:       <form method="post" action="/login">
15:         <input type="hidden" name="next" value="{{ next }}" />
16:         <label>Email</label>
17:         <input type="email" name="email" required />
18:         <label>Password</label>
19:         <input type="password" name="password" required />
20:         <button type="submit">Sign in</button>
21:       </form>
22:       {% if bootstrap %}
23:       <p>No users exist yet. <a href="/bootstrap">Create admin</a></p>
24:       {% endif %}
25:     </div>
26:   </body>
27: </html>
````

## File: src/statistic_harness/ui/templates/project_roles.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Role Overrides {{ project_id }}</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Role Overrides</h1>
 9:   <p><a href="/projects/{{ project_id }}">Back to Project</a></p>
10: 
11:   <form action="/projects/{{ project_id }}/roles" method="post">
12:     <label>Dataset version:</label>
13:     <select name="dataset_version_id" onchange="this.form.submit()">
14:       {% for dataset in datasets %}
15:         <option value="{{ dataset.dataset_version_id }}" {% if dataset.dataset_version_id == dataset_version_id %}selected{% endif %}>
16:           {{ dataset.dataset_version_id }} (rows: {{ dataset.row_count }})
17:         </option>
18:       {% endfor %}
19:     </select>
20:   </form>
21: 
22:   <form action="/projects/{{ project_id }}/roles" method="post">
23:     <input type="hidden" name="dataset_version_id" value="{{ dataset_version_id }}" />
24:     <datalist id="column-list">
25:       {% for col in columns %}
26:         <option value="{{ col.original_name }}"></option>
27:       {% endfor %}
28:     </datalist>
29: 
30:     <table>
31:       <thead>
32:         <tr>
33:           <th>Role</th>
34:           <th>Column Name</th>
35:         </tr>
36:       </thead>
37:       <tbody>
38:         {% for role in role_keys %}
39:           <tr>
40:             <td>{{ role }}</td>
41:             <td>
42:               <input type="text" name="{{ role }}" list="column-list" value="{{ overrides.get(role, '') }}" />
43:             </td>
44:           </tr>
45:         {% endfor %}
46:       </tbody>
47:     </table>
48:     <button type="submit">Save Overrides</button>
49:   </form>
50: </body>
51: </html>
````

## File: src/statistic_harness/ui/templates/project_settings.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Project Settings — {{ project.name or project_id }}</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark4" />
 6: </head>
 7: <body>
 8:   <div class="container">
 9:     <nav class="top-nav">
10:       <a href="/">Home</a>
11:       <a href="/projects">Projects</a>
12:       <a href="/templates">Templates</a>
13:       <a href="/trace">Trace</a>
14:       <a href="/vectors">Vectors</a>
15:     </nav>
16:     <header class="hero compact">
17:       <div>
18:         <p class="eyebrow">Plugin Settings</p>
19:         <h1>{{ project.name or project_id }}</h1>
20:         {% if message %}
21:           <p class="status">{{ message }}</p>
22:         {% endif %}
23:       </div>
24:       <div class="hero-panel">
25:         <p class="muted">Edit JSON overrides per plugin. Blank means defaults.</p>
26:       </div>
27:     </header>
28: 
29:     <form action="/projects/{{ project_id }}/settings" method="post" class="stack">
30:       {% for spec in plugin_specs %}
31:         <section class="card">
32:           <h3>{{ spec.plugin_id }}</h3>
33:           <p class="muted">{{ spec.settings.description if spec.settings else "" }}</p>
34:           <label>Overrides JSON</label>
35:           <textarea name="settings__{{ spec.plugin_id }}" rows="6" spellcheck="false">{% if plugin_settings.get(spec.plugin_id) %}{{ plugin_settings.get(spec.plugin_id) | tojson(indent=2) }}{% endif %}</textarea>
36:           {% if spec.settings and spec.settings.defaults %}
37:             <details>
38:               <summary>Defaults</summary>
39:               <pre>{{ spec.settings.defaults | tojson(indent=2) }}</pre>
40:             </details>
41:           {% endif %}
42:         </section>
43:       {% endfor %}
44:       <div class="button-row">
45:         <button type="submit">Save Settings</button>
46:         <a class="button ghost" href="/projects/{{ project_id }}">Back to Project</a>
47:       </div>
48:     </form>
49:   </div>
50: </body>
51: </html>
````

## File: src/statistic_harness/ui/templates/project.html
````html
  1: <!DOCTYPE html>
  2: <html>
  3: <head>
  4:   <title>Project {{ project.name or project_id }}</title>
  5:   <link rel="stylesheet" href="/static/app.css?v=dark4" />
  6: </head>
  7: <body>
  8:   <div class="container">
  9:     <nav class="top-nav">
 10:       <a href="/">Home</a>
 11:       <a href="/projects">Projects</a>
 12:       <a href="/templates">Templates</a>
 13:       <a href="/trace">Trace</a>
 14:       <a href="/vectors">Vectors</a>
 15:     </nav>
 16:     <header class="hero compact">
 17:       <div>
 18:         <p class="eyebrow">Project</p>
 19:         <h1>{{ project.name or project_id }}</h1>
 20:         <p class="lead">ERP: <strong>{{ project.erp_type if project else 'unknown' }}</strong></p>
 21:         {% if message %}
 22:           <p class="status">{{ message }}</p>
 23:         {% endif %}
 24:       </div>
 25:       <div class="hero-panel">
 26:         <form action="/projects/{{ project_id }}/erp" method="post" class="stack">
 27:           <label>ERP type</label>
 28:           <input type="text" name="erp_type" value="{{ project.erp_type if project else 'unknown' }}" />
 29:           <button type="submit">Update ERP</button>
 30:         </form>
 31:         <p class="muted">
 32:           <a href="/projects/{{ project_id }}/roles">Role Overrides</a>
 33:           | <a href="/projects/{{ project_id }}/settings">Plugin Settings</a>
 34:         </p>
 35:       </div>
 36:     </header>
 37: 
 38:     <section class="card">
 39:       <h2>Upload & Run All</h2>
 40:       <p class="muted">Uploads are hashed and deduplicated automatically. This will run every analysis plugin.</p>
 41:       <form id="project-upload-form" class="stack">
 42:         <input type="file" id="project-file" required />
 43:         <button type="submit">Upload & Run All Plugins</button>
 44:       </form>
 45:       <div id="upload-status" class="status"></div>
 46:       <div id="run-status" class="status muted"></div>
 47:     </section>
 48: 
 49:     <div class="grid">
 50:       <section class="card">
 51:         <h2>Known Issues</h2>
 52:         <p class="muted">Stored by ERP type. These are used to evaluate outputs deterministically.</p>
 53:         {% if known_issues and known_issues.expected_findings %}
 54:           <ul class="clean-list">
 55:             {% for issue in known_issues.expected_findings %}
 56:               <li>
 57:                 <strong>{{ issue.title or issue.kind }}</strong>
 58:                 {% if issue.description %}<span class="muted"> — {{ issue.description }}</span>{% endif %}
 59:               </li>
 60:             {% endfor %}
 61:           </ul>
 62:         {% else %}
 63:           <p class="muted">No known issues saved yet.</p>
 64:         {% endif %}
 65:         <a class="button ghost" href="/projects/{{ project_id }}/known-issues">Manage Known Issues</a>
 66:       </section>
 67: 
 68:       <section class="card">
 69:         <h2>Recent Runs</h2>
 70:         {% if runs %}
 71:           <ul class="clean-list">
 72:             {% for run in runs %}
 73:               <li>
 74:                 <a href="/runs/{{ run.run_id }}/report">{{ run.label or run.run_id }}</a>
 75:                 <span class="pill {{ run.status }}" data-run-id="{{ run.run_id }}">{{ run.status }}</span>
 76:                 {% if run.evaluation %}
 77:                   <span class="pill {{ 'ok' if run.evaluation.ok else 'error' }}" data-eval-id="{{ run.run_id }}">{{ run.evaluation.result }}</span>
 78:                 {% endif %}
 79:                 <a class="pill" href="/vectors?collection=report_{{ run.run_id }}">Vectors</a>
 80:                 <span class="muted">{{ run.created_at }}</span>
 81:               </li>
 82:             {% endfor %}
 83:           </ul>
 84:         {% else %}
 85:           <p class="muted">No runs yet.</p>
 86:         {% endif %}
 87:       </section>
 88:     </div>
 89: 
 90:     <section class="card">
 91:       <h2>Datasets</h2>
 92:       {% if datasets %}
 93:         <ul class="clean-list">
 94:           {% for dataset in datasets %}
 95:             <li>
 96:               <strong>{{ dataset.source_filename or dataset.dataset_version_id }}</strong>
 97:               <span class="muted">created: {{ dataset.first_run_at or dataset.created_at }}</span>
 98:               {% if dataset.last_run_at %}
 99:                 <span class="muted">last run: {{ dataset.last_run_at }}</span>
100:               {% endif %}
101:               <span class="muted">rows: {{ dataset.row_count }}, cols: {{ dataset.column_count }}</span>
102:               {% if dataset.raw_format_id %}
103:                 <a href="/raw-formats/{{ dataset.raw_format_id }}">format {{ dataset.raw_format_id }}</a>
104:               {% endif %}
105:               <a href="/api/datasets/{{ dataset.dataset_version_id }}/status" target="_blank">status</a>
106:               <button type="button" class="ghost rerun-btn" data-dataset="{{ dataset.dataset_version_id }}">Rerun All Plugins</button>
107:             </li>
108:           {% endfor %}
109:         </ul>
110:       {% else %}
111:         <p class="muted">No datasets yet.</p>
112:       {% endif %}
113:     </section>
114: 
115:     <p><a href="/projects">Back</a></p>
116:   </div>
117: 
118:   <script>
119:     const form = document.getElementById("project-upload-form");
120:     const fileInput = document.getElementById("project-file");
121:     const uploadStatus = document.getElementById("upload-status");
122:     const runStatus = document.getElementById("run-status");
123:     const projectId = "{{ project_id }}";
124: 
125:     async function pollRun(runId) {
126:       if (!runId) return;
127:       const resp = await fetch(`/api/runs/${runId}`);
128:       if (!resp.ok) return;
129:       const payload = await resp.json();
130:       const status = payload.run?.status || "unknown";
131:       runStatus.textContent = `Run ${runId}: ${status}`;
132:       if (status === "completed") {
133:         runStatus.innerHTML = `Run ${runId}: completed — <a href=\"/runs/${runId}/report\">View report</a>`;
134:         return;
135:       }
136:       setTimeout(() => pollRun(runId), 1500);
137:     }
138: 
139:     if (form) {
140:       form.addEventListener("submit", async (event) => {
141:         event.preventDefault();
142:         if (!fileInput || !fileInput.files || !fileInput.files[0]) {
143:           uploadStatus.textContent = "Select a file first.";
144:           return;
145:         }
146:         uploadStatus.textContent = "Uploading...";
147:         const file = fileInput.files[0];
148:         const uploadResp = await fetch(`/api/upload/raw?filename=${encodeURIComponent(file.name)}`, {
149:           method: "POST",
150:           body: file
151:         });
152:         const uploadPayload = await uploadResp.json();
153:         if (!uploadResp.ok) {
154:           uploadStatus.textContent = uploadPayload.detail || "Upload failed.";
155:           return;
156:         }
157:         uploadStatus.textContent = `Upload complete (sha ${uploadPayload.sha256.slice(0, 8)}...). Running analysis...`;
158: 
159:         const formData = new FormData();
160:         formData.append("upload_id", uploadPayload.upload_id);
161:         formData.append("plugins", "all");
162:         formData.append("project_id", projectId);
163:         const runResp = await fetch("/api/runs", { method: "POST", body: formData });
164:         const runPayload = await runResp.json();
165:         if (!runResp.ok) {
166:           runStatus.textContent = runPayload.detail || "Run failed to start.";
167:           return;
168:         }
169:         runStatus.textContent = `Run ${runPayload.run_id} started...`;
170:         pollRun(runPayload.run_id);
171:       });
172:     }
173: 
174:     async function refreshRunStatus(runId) {
175:       const resp = await fetch(`/api/runs/${runId}`);
176:       if (!resp.ok) return;
177:       const payload = await resp.json();
178:       const status = payload.run?.status || "unknown";
179:       const pill = document.querySelector(`span[data-run-id='${runId}']`);
180:       if (pill) {
181:         pill.textContent = status;
182:         pill.className = `pill ${status}`;
183:       }
184:       if (status === "completed") {
185:         const evalResp = await fetch(`/api/runs/${runId}/evaluation`);
186:         if (evalResp.ok) {
187:           const evalPayload = await evalResp.json();
188:           let evalPill = document.querySelector(`span[data-eval-id='${runId}']`);
189:           if (!evalPill) {
190:             evalPill = document.createElement("span");
191:             evalPill.setAttribute("data-eval-id", runId);
192:             evalPill.className = "pill";
193:             pill?.insertAdjacentElement("afterend", evalPill);
194:           }
195:           evalPill.textContent = evalPayload.result || "evaluated";
196:           evalPill.className = `pill ${evalPayload.ok ? "ok" : "error"}`;
197:         }
198:         return;
199:       }
200:       setTimeout(() => refreshRunStatus(runId), 2000);
201:     }
202: 
203:     document.querySelectorAll("span[data-run-id]").forEach((el) => {
204:       const runId = el.getAttribute("data-run-id");
205:       if (runId && el.textContent !== "completed") {
206:         refreshRunStatus(runId);
207:       }
208:     });
209: 
210:     async function rerunDataset(datasetId) {
211:       const resp = await fetch(`/api/datasets/${datasetId}/rerun`, { method: "POST" });
212:       const payload = await resp.json();
213:       if (!resp.ok) {
214:         uploadStatus.textContent = payload.detail || "Rerun failed.";
215:         return;
216:       }
217:       runStatus.textContent = `Run ${payload.run_id} started...`;
218:       pollRun(payload.run_id);
219:     }
220: 
221:     document.querySelectorAll(".rerun-btn").forEach((btn) => {
222:       btn.addEventListener("click", () => {
223:         const datasetId = btn.getAttribute("data-dataset");
224:         if (datasetId) rerunDataset(datasetId);
225:       });
226:     });
227:   </script>
228: </body>
229: </html>
````

## File: src/statistic_harness/ui/templates/projects.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Projects</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark4" />
 6: </head>
 7: <body>
 8:   <div class="container">
 9:     <nav class="top-nav">
10:       <a href="/">Home</a>
11:       <a href="/projects">Projects</a>
12:       <a href="/templates">Templates</a>
13:       <a href="/trace">Trace</a>
14:       <a href="/vectors">Vectors</a>
15:     </nav>
16:     <header class="hero compact">
17:       <div>
18:         <p class="eyebrow">Projects</p>
19:         <h1>All Projects</h1>
20:         <p class="lead">Projects group uploads, runs, and known issues by ERP.</p>
21:       </div>
22:       <div class="hero-panel">
23:         <form action="/projects" method="post" class="stack">
24:           <label>Project name</label>
25:           <input type="text" name="name" placeholder="New project" required />
26:           <label>ERP type</label>
27:           <input type="text" name="erp_type" value="unknown" />
28:           <button type="submit">Create Project</button>
29:         </form>
30:       </div>
31:     </header>
32: 
33:     <section class="card">
34:       {% if projects %}
35:         <div class="grid">
36:           {% for project in projects %}
37:             <div class="card subtle">
38:               <h3><a href="/projects/{{ project.project_id }}">{{ project.name or project.project_id }}</a></h3>
39:               <p class="muted">ERP: {{ project.erp_type or 'unknown' }}</p>
40:               <p class="muted">Datasets: {{ project.dataset_count }}</p>
41:             </div>
42:           {% endfor %}
43:         </div>
44:       {% else %}
45:         <p class="muted">No projects created yet.</p>
46:       {% endif %}
47:     </section>
48: 
49:     <p><a href="/">Back</a></p>
50:   </div>
51: </body>
52: </html>
````

## File: src/statistic_harness/ui/templates/raw_format.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Raw Format {{ format_id }}</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Raw Format {{ format_id }}</h1>
 9:   <h2>Notes</h2>
10:   <ul>
11:     {% for note in notes %}
12:       <li>{{ note.note }} ({{ note.created_at }})</li>
13:     {% endfor %}
14:   </ul>
15:   <form action="/raw-formats/{{ format_id }}/notes" method="post">
16:     <label>New note:</label>
17:     <input type="text" name="note" required />
18:     <button type="submit">Add</button>
19:   </form>
20:   <h2>Mapping Presets</h2>
21:   <ul>
22:     {% for mapping in mappings %}
23:       <li>
24:         template {{ mapping.template_id }} | {{ mapping.mapping_hash }}
25:         {% if mapping.notes %} - {{ mapping.notes }}{% endif %}
26:       </li>
27:     {% endfor %}
28:   </ul>
29:   <form action="/raw-formats/{{ format_id }}/mappings" method="post">
30:     <label>Template ID:</label>
31:     <input type="number" name="template_id" required />
32:     <label>Mapping JSON:</label>
33:     <textarea name="mapping_json" rows="6" cols="60" required>{"field":"raw_column"}</textarea>
34:     <label>Notes:</label>
35:     <input type="text" name="notes" />
36:     <button type="submit">Save Mapping</button>
37:   </form>
38:   <a href="/raw-formats">Back</a>
39: </body>
40: </html>
````

## File: src/statistic_harness/ui/templates/raw_formats.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Raw Formats</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Raw Formats</h1>
 9:   <ul>
10:     {% for fmt in formats %}
11:       <li>
12:         <strong><a href="/raw-formats/{{ fmt.format_id }}">{{ fmt.format_id }}</a></strong>
13:         {{ fmt.fingerprint }}
14:         (datasets: {{ fmt.dataset_count }})
15:       </li>
16:     {% endfor %}
17:   </ul>
18:   <a href="/">Back</a>
19: </body>
20: </html>
````

## File: src/statistic_harness/ui/templates/row_trace.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Row Trace</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Row Trace</h1>
 9:   <p>Dataset Version: {{ dataset_version_id }}</p>
10:   <p>Row Index: {{ row_index }}</p>
11:   {% if source_dataset_version_id %}
12:     <p>Source Dataset Version: {{ source_dataset_version_id }}</p>
13:   {% endif %}
14:   {% if payload.rows %}
15:     {% for row in payload.rows %}
16:       <section>
17:         <h2>Row {{ row.row_index }}</h2>
18:         <p>Dataset Version: {{ row.dataset_version_id }}</p>
19:         <p>Row ID: {{ row.row_id }}</p>
20:         {% if row.row_json %}
21:           <details>
22:             <summary>Row JSON</summary>
23:             <pre>{{ row.row_json }}</pre>
24:           </details>
25:         {% endif %}
26:         <h3>Values</h3>
27:         <ul>
28:           {% for key, value in row.values.items() %}
29:             <li>{{ key }}: {{ value }}</li>
30:           {% endfor %}
31:         </ul>
32:         {% if row.parameters %}
33:           <h3>Parameters</h3>
34:           <ul>
35:             {% for param in row.parameters %}
36:               <li>
37:                 {{ param.canonical }}
38:                 {% if param.kv %}
39:                   <ul>
40:                     {% for pair in param.kv %}
41:                       <li>{{ pair.key }}={{ pair.value }}</li>
42:                     {% endfor %}
43:                   </ul>
44:                 {% endif %}
45:               </li>
46:             {% endfor %}
47:           </ul>
48:         {% endif %}
49:       </section>
50:     {% endfor %}
51:   {% else %}
52:     <p>No rows found.</p>
53:   {% endif %}
54:   <a href="/trace">Back to Trace</a>
55: </body>
56: </html>
````

## File: src/statistic_harness/ui/templates/template_results.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Template {{ template_id }} Results</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Template {{ template_id }} Combined Results</h1>
 9:   <p>Dataset Version: {{ dataset_version_id }}</p>
10:   <h2>Filters</h2>
11:   <form action="/templates/{{ template_id }}/results" method="get">
12:     <label>Project IDs (comma separated):</label>
13:     <input type="text" name="project_ids" value="{{ filter_params.project_ids }}" />
14:     <label>Dataset IDs (comma separated):</label>
15:     <input type="text" name="dataset_ids" value="{{ filter_params.dataset_ids }}" />
16:     <label>Dataset Version IDs (comma separated):</label>
17:     <input type="text" name="dataset_version_ids" value="{{ filter_params.dataset_version_ids }}" />
18:     <label>Raw Format IDs (comma separated):</label>
19:     <input type="text" name="raw_format_ids" value="{{ filter_params.raw_format_ids }}" />
20:     <label>Created After (ISO timestamp):</label>
21:     <input type="text" name="created_after" value="{{ filter_params.created_after }}" />
22:     <label>Created Before (ISO timestamp):</label>
23:     <input type="text" name="created_before" value="{{ filter_params.created_before }}" />
24:     <button type="submit">Apply Filters</button>
25:   </form>
26:   {% if filters %}
27:     <p>Active filters: {{ filters }}</p>
28:   {% endif %}
29:   <ul>
30:     {% for row in results %}
31:       <li>
32:         <strong>{{ row.plugin_id }}</strong> ({{ row.status }})
33:         - {{ row.summary }}
34:       </li>
35:     {% endfor %}
36:   </ul>
37:   <a href="/templates/{{ template_id }}">Back</a>
38: </body>
39: </html>
````

## File: src/statistic_harness/ui/templates/template.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Template {{ template.name }}</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Template {{ template.name }}</h1>
 9:   <h2>Fields</h2>
10:   <ul>
11:     {% for field in fields %}
12:       <li>{{ field.name }} ({{ field.dtype or 'text' }})</li>
13:     {% endfor %}
14:   </ul>
15: 
16:   <h2>Map Dataset to Template</h2>
17:   <form action="/templates/{{ template.template_id }}/map" method="post">
18:     <label>Dataset Version ID:</label>
19:     <input type="text" name="dataset_version_id" required />
20:     <label>Mapping JSON:</label>
21:     <textarea name="mapping_json" rows="6" cols="60" required>{"field":"raw_column"}</textarea>
22:     <button type="submit">Apply Mapping</button>
23:   </form>
24: 
25:   <h2>Run Combined Analysis</h2>
26:   <form action="/templates/{{ template.template_id }}/run" method="post">
27:     <label>Plugins (comma separated):</label>
28:     <input type="text" name="plugins" placeholder="auto or plugin_id,plugin_id" />
29:     <label>Settings JSON:</label>
30:     <textarea name="settings_json" rows="4" cols="60"></textarea>
31:     <label>Project IDs (comma separated):</label>
32:     <input type="text" name="project_ids" />
33:     <label>Dataset IDs (comma separated):</label>
34:     <input type="text" name="dataset_ids" />
35:     <label>Dataset Version IDs (comma separated):</label>
36:     <input type="text" name="dataset_version_ids" />
37:     <label>Raw Format IDs (comma separated):</label>
38:     <input type="text" name="raw_format_ids" />
39:     <label>Created After (ISO timestamp):</label>
40:     <input type="text" name="created_after" placeholder="2026-01-01T00:00:00Z" />
41:     <label>Created Before (ISO timestamp):</label>
42:     <input type="text" name="created_before" placeholder="2026-12-31T23:59:59Z" />
43:     <label>Run seed:</label>
44:     <input type="number" name="run_seed" value="0" />
45:     <button type="submit">Run</button>
46:   </form>
47:   <p><a href="/templates/{{ template.template_id }}/results">View Combined Results</a></p>
48: 
49:   <a href="/templates">Back</a>
50: </body>
51: </html>
````

## File: src/statistic_harness/ui/templates/templates.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Templates</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Templates</h1>
 9:   <ul>
10:     {% for template in templates %}
11:       <li>
12:         <strong><a href="/templates/{{ template.template_id }}">{{ template.name }}</a></strong>
13:         (id: {{ template.template_id }})
14:       </li>
15:     {% endfor %}
16:   </ul>
17:   <a href="/">Back</a>
18: </body>
19: </html>
````

## File: src/statistic_harness/ui/templates/trace.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Trace</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Trace</h1>
 9:   <form method="get" action="/trace">
10:     <label>Entity Type:</label>
11:     <input type="text" name="entity_type" value="{{ entity_type }}" required />
12:     <label>Key:</label>
13:     <input type="text" name="key" value="{{ key }}" required />
14:     <label>Max Depth:</label>
15:     <input type="number" name="max_depth" value="{{ max_depth }}" />
16:     <button type="submit">Trace</button>
17:   </form>
18:   {% if result %}
19:     <h2>Result</h2>
20:     <pre>{{ result }}</pre>
21:   {% endif %}
22:   <a href="/">Back</a>
23: </body>
24: </html>
````

## File: src/statistic_harness/ui/templates/vectors.html
````html
  1: <!doctype html>
  2: <html lang="en">
  3:   <head>
  4:     <meta charset="utf-8" />
  5:     <title>Vector Search</title>
  6:     <link rel="stylesheet" href="/static/app.css" />
  7:   </head>
  8:   <body>
  9:     <div class="container">
 10:       <nav class="top-nav">
 11:         <a href="/">Home</a>
 12:         <a href="/projects">Projects</a>
 13:         <a href="/templates">Templates</a>
 14:         <a href="/trace">Trace</a>
 15:         <a href="/vectors">Vectors</a>
 16:       </nav>
 17: 
 18:       <h1>Vector Search</h1>
 19:       {% if error %}
 20:       <p class="pill error">{{ error }}</p>
 21:       {% endif %}
 22: 
 23:       <section class="card">
 24:         <h2>Query</h2>
 25:         <form action="/vectors/query" method="post" class="stack">
 26:           <label>Collection</label>
 27:           {% if collections %}
 28:             <select name="collection" required>
 29:               {% for col in collections %}
 30:                 {% set selected = query.collection == col.name %}
 31:                 <option value="{{ col.name }}" {% if selected %}selected{% endif %}>
 32:                   {{ col.name }} ({{ col.dimensions }})
 33:                 </option>
 34:               {% endfor %}
 35:             </select>
 36:           {% else %}
 37:             <input type="text" name="collection" value="{{ query.collection or '' }}" required />
 38:           {% endif %}
 39: 
 40:           <label>Text query</label>
 41:           <textarea name="text" rows="3" placeholder="queue delay summary...">{{ query.text or '' }}</textarea>
 42: 
 43:           <label>Vector (comma-separated)</label>
 44:           <textarea name="vector" rows="3" placeholder="0.1, 0.0, ...">{{ query.vector or '' }}</textarea>
 45: 
 46:           <label>k (page size)</label>
 47:           <input type="number" name="k" value="{{ query.k or 10 }}" min="1" />
 48:           <input type="hidden" name="offset" value="{{ query.offset or 0 }}" />
 49:           <div class="pill-row">
 50:             <button class="pill" type="submit" name="k" value="10">Top 10</button>
 51:             <button class="pill" type="submit" name="k" value="25">Top 25</button>
 52:             <button class="pill" type="submit" name="k" value="50">Top 50</button>
 53:             <button class="pill" type="submit" name="k" value="100">Top 100</button>
 54:           </div>
 55: 
 56:           <label>Dimensions (optional)</label>
 57:           <input type="number" name="dimensions" value="{{ query.dimensions or '' }}" />
 58: 
 59:           <button type="submit">Search</button>
 60:         </form>
 61:       </section>
 62: 
 63:       <section class="card">
 64:         <h2>Suggestions</h2>
 65:         {% if default_collection %}
 66:         <div class="pill-row">
 67:           {% set col = default_collection %}
 68:           <a class="pill" href="/vectors?collection={{ col }}&text=queue%20delay%20summary">Queue delay summary</a>
 69:           <a class="pill" href="/vectors?collection={{ col }}&text=top%20findings">Top findings</a>
 70:           <a class="pill" href="/vectors?collection={{ col }}&text=tail%20latency">Tail latency</a>
 71:           <a class="pill" href="/vectors?collection={{ col }}&text=anomaly%20drivers">Anomaly drivers</a>
 72:         </div>
 73:         {% else %}
 74:         <p class="muted">Select a collection to see suggestions.</p>
 75:         {% endif %}
 76:       </section>
 77: 
 78:       <section class="card">
 79:         <h2>Collections</h2>
 80:         {% if collections %}
 81:         <table>
 82:           <thead>
 83:             <tr>
 84:               <th>Name</th>
 85:               <th>Dimensions</th>
 86:               <th>Created</th>
 87:             </tr>
 88:           </thead>
 89:           <tbody>
 90:             {% for col in collections %}
 91:             <tr>
 92:               <td>{{ col.name }}</td>
 93:               <td>{{ col.dimensions }}</td>
 94:               <td>{{ col.created_at or '' }}</td>
 95:             </tr>
 96:             {% endfor %}
 97:           </tbody>
 98:         </table>
 99:         {% else %}
100:         <p class="muted">No collections indexed yet.</p>
101:         {% endif %}
102:       </section>
103: 
104:       <section class="card">
105:         <h2>Results</h2>
106:         {% if results is not none %}
107:           {% if results %}
108:           {% if pagination %}
109:           <div class="pill-row">
110:             <span class="muted">Offset {{ pagination.offset }} · Page size {{ pagination.page_size }}</span>
111:             {% if pagination.has_prev %}
112:               <form action="/vectors/query" method="post">
113:                 <input type="hidden" name="collection" value="{{ query.collection }}" />
114:                 <input type="hidden" name="text" value="{{ query.text }}" />
115:                 <input type="hidden" name="vector" value="{{ query.vector }}" />
116:                 <input type="hidden" name="k" value="{{ pagination.page_size }}" />
117:                 <input type="hidden" name="offset" value="{{ pagination.prev_offset }}" />
118:                 <input type="hidden" name="dimensions" value="{{ query.dimensions or '' }}" />
119:                 <button class="pill" type="submit">Prev</button>
120:               </form>
121:             {% endif %}
122:             {% if pagination.has_next %}
123:               <form action="/vectors/query" method="post">
124:                 <input type="hidden" name="collection" value="{{ query.collection }}" />
125:                 <input type="hidden" name="text" value="{{ query.text }}" />
126:                 <input type="hidden" name="vector" value="{{ query.vector }}" />
127:                 <input type="hidden" name="k" value="{{ pagination.page_size }}" />
128:                 <input type="hidden" name="offset" value="{{ pagination.next_offset }}" />
129:                 <input type="hidden" name="dimensions" value="{{ query.dimensions or '' }}" />
130:                 <button class="pill" type="submit">Next</button>
131:               </form>
132:             {% endif %}
133:           </div>
134:           {% endif %}
135:           <table>
136:             <thead>
137:               <tr>
138:                 <th>Item</th>
139:                 <th>Summary</th>
140:                 <th>Distance</th>
141:                 <th>Payload</th>
142:               </tr>
143:             </thead>
144:             <tbody>
145:               {% for row in results %}
146:               <tr>
147:                 <td>{{ row.item_id }}</td>
148:                 <td>
149:                   {% if row.payload is mapping %}
150:                     {% set plugin_id = row.payload.get('plugin_id') %}
151:                     {% set kind = row.payload.get('kind') %}
152:                     {% set ptype = row.payload.get('type') %}
153:                     {% set idx = row.payload.get('index') %}
154:                     {% if plugin_id %}<strong>{{ plugin_id }}</strong>{% endif %}
155:                     {% if ptype %}<span class="muted">/ {{ ptype }}</span>{% endif %}
156:                     {% if kind %}<span class="muted">/ {{ kind }}</span>{% endif %}
157:                     {% if idx is not none %}<span class="muted">#{{ idx }}</span>{% endif %}
158:                   {% elif row.payload %}
159:                     {{ row.payload }}
160:                   {% else %}
161:                     <span class="muted">—</span>
162:                   {% endif %}
163:                 </td>
164:                 <td>{{ "%.4f"|format(row.distance) if row.distance is not none else "" }}</td>
165:                 <td><pre>{{ row.payload | tojson(indent=2) }}</pre></td>
166:               </tr>
167:               {% endfor %}
168:             </tbody>
169:           </table>
170:           <details>
171:             <summary>Raw JSON</summary>
172:             <pre>{{ results | tojson(indent=2) }}</pre>
173:           </details>
174:           {% else %}
175:           <p class="muted">No matches found.</p>
176:           {% endif %}
177:         {% else %}
178:           <p class="muted">Run a query to see results.</p>
179:         {% endif %}
180:       </section>
181:     </div>
182:   </body>
183: </html>
````

## File: src/statistic_harness/ui/templates/wizard.html
````html
  1: <!DOCTYPE html>
  2: <html>
  3: <head>
  4:   <title>Statistic Harness Wizard</title>
  5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
  6: </head>
  7: <body>
  8:   <h1>Wizard: Upload → Known Issues → Run</h1>
  9:   <p><a href="/">Home</a> | <a href="/known-issues">Known Issues</a></p>
 10: 
 11:   <section>
 12:     <h2>Step 1: Upload</h2>
 13:     <form id="upload-form" action="/api/upload" method="post" enctype="multipart/form-data">
 14:       <label>Upload file:</label>
 15:       <input type="file" name="file" required />
 16:       <button type="submit">Upload</button>
 17:     </form>
 18:     <pre id="upload-result"></pre>
 19:   </section>
 20: 
 21:   <section>
 22:     <h2>Step 2: Known Issues</h2>
 23:     <label>Upload:</label>
 24:     <select id="upload-select">
 25:       <option value="">Select an upload</option>
 26:       {% for upload in uploads %}
 27:         <option value="{{ upload.upload_id }}">{{ upload.filename }} ({{ upload.size_bytes }} bytes)</option>
 28:       {% endfor %}
 29:     </select>
 30:     <label>ERP type:</label>
 31:     <input type="text" id="erp-type" value="{{ erp_type or 'unknown' }}" />
 32: 
 33:     <label><input type="checkbox" id="strict-flag" checked /> Strict (no false positives)</label>
 34:     <label>Notes:</label>
 35:     <textarea id="notes" rows="3"></textarea>
 36:     <label><input type="checkbox" id="autosave-flag" checked /> Auto-save</label>
 37:     <p>Where/Contains format: one <code>key=value</code> per line. Use commas for lists.</p>
 38: 
 39:     <div id="issues"></div>
 40: 
 41:     <button type="button" id="add-issue">Add Issue</button>
 42:     <button type="button" id="quick-qemail">Quick add: qemail close-cycle contention</button>
 43:     <button type="button" id="save-issues">Save Known Issues</button>
 44:   </section>
 45: 
 46:   <section>
 47:     <h2>Step 3: Run</h2>
 48:     <button type="button" id="run-auto">Run Auto + Evaluate</button>
 49:     <p>Run status: <span id="run-status">idle</span></p>
 50:     <pre id="eval-result"></pre>
 51:   </section>
 52: 
 53:   <section>
 54:     <h2>Ground Truth Preview (read-only)</h2>
 55:     <textarea id="ground-truth" rows="16" readonly></textarea>
 56:   </section>
 57: 
 58:   <pre id="save-result"></pre>
 59: 
 60:   <script id="plugin-ids-json" type="application/json">{{ plugin_ids | tojson }}</script>
 61:   <script>
 62:     const uploadForm = document.getElementById("upload-form");
 63:     const uploadResult = document.getElementById("upload-result");
 64:     const uploadSelect = document.getElementById("upload-select");
 65:     const strictFlag = document.getElementById("strict-flag");
 66:     const notesEl = document.getElementById("notes");
 67:     const autosaveFlag = document.getElementById("autosave-flag");
 68:     const issuesContainer = document.getElementById("issues");
 69:     const groundTruthEl = document.getElementById("ground-truth");
 70:     const saveResult = document.getElementById("save-result");
 71:     const runStatus = document.getElementById("run-status");
 72:     const evalResult = document.getElementById("eval-result");
 73:     const erpTypeEl = document.getElementById("erp-type");
 74:     const pluginIds = JSON.parse(document.getElementById("plugin-ids-json").textContent || "[]");
 75: 
 76:     function kvToText(obj) {
 77:       if (!obj) return "";
 78:       const lines = [];
 79:       for (const key of Object.keys(obj)) {
 80:         const value = obj[key];
 81:         if (Array.isArray(value)) {
 82:           lines.push(`${key}=${value.join(",")}`);
 83:         } else {
 84:           lines.push(`${key}=${value}`);
 85:         }
 86:       }
 87:       return lines.join("\n");
 88:     }
 89: 
 90:     function parseKV(text) {
 91:       const out = {};
 92:       if (!text) return out;
 93:       const lines = text.split("\n");
 94:       for (const line of lines) {
 95:         const trimmed = line.trim();
 96:         if (!trimmed) continue;
 97:         const idx = trimmed.indexOf("=");
 98:         if (idx === -1) continue;
 99:         const key = trimmed.slice(0, idx).trim();
100:         let raw = trimmed.slice(idx + 1).trim();
101:         if (!key) continue;
102:         let value = raw;
103:         if (raw.includes(",")) {
104:           value = raw.split(",").map((v) => v.trim()).filter((v) => v.length);
105:         } else if (raw.toLowerCase() === "true" || raw.toLowerCase() === "false") {
106:           value = raw.toLowerCase() === "true";
107:         } else if (!Number.isNaN(Number(raw)) && raw !== "") {
108:           value = Number(raw);
109:         }
110:         out[key] = value;
111:       }
112:       return out;
113:     }
114: 
115:     function addIssueRow(issue) {
116:       const row = document.createElement("div");
117:       row.className = "issue-row";
118:       row.style.border = "1px solid #2a3140";
119:       row.style.padding = "0.5rem";
120:       row.style.marginBottom = "0.5rem";
121:       const pluginSelect = document.createElement("select");
122:       const emptyOption = document.createElement("option");
123:       emptyOption.value = "";
124:       emptyOption.textContent = "(any plugin)";
125:       pluginSelect.appendChild(emptyOption);
126:       for (const pid of pluginIds) {
127:         const opt = document.createElement("option");
128:         opt.value = pid;
129:         opt.textContent = pid;
130:         pluginSelect.appendChild(opt);
131:       }
132:       pluginSelect.value = issue.plugin_id || "";
133: 
134:       const kindInput = document.createElement("input");
135:       kindInput.type = "text";
136:       kindInput.placeholder = "kind (required)";
137:       kindInput.value = issue.kind || "";
138: 
139:       const titleInput = document.createElement("input");
140:       titleInput.type = "text";
141:       titleInput.placeholder = "title (optional)";
142:       titleInput.value = issue.title || "";
143: 
144:       const whereInput = document.createElement("textarea");
145:       whereInput.rows = 3;
146:       whereInput.placeholder = "where: key=value per line";
147:       whereInput.value = kvToText(issue.where || {});
148: 
149:       const containsInput = document.createElement("textarea");
150:       containsInput.rows = 3;
151:       containsInput.placeholder = "contains: key=value per line";
152:       containsInput.value = kvToText(issue.contains || {});
153: 
154:       const minCount = document.createElement("input");
155:       minCount.type = "number";
156:       minCount.placeholder = "min";
157:       if (issue.min_count !== undefined && issue.min_count !== null) {
158:         minCount.value = issue.min_count;
159:       }
160: 
161:       const maxCount = document.createElement("input");
162:       maxCount.type = "number";
163:       maxCount.placeholder = "max";
164:       if (issue.max_count !== undefined && issue.max_count !== null) {
165:         maxCount.value = issue.max_count;
166:       }
167: 
168:       const removeBtn = document.createElement("button");
169:       removeBtn.type = "button";
170:       removeBtn.textContent = "Remove";
171:       removeBtn.addEventListener("click", () => row.remove());
172: 
173:       row.appendChild(pluginSelect);
174:       row.appendChild(kindInput);
175:       row.appendChild(titleInput);
176:       row.appendChild(whereInput);
177:       row.appendChild(containsInput);
178:       row.appendChild(minCount);
179:       row.appendChild(maxCount);
180:       row.appendChild(removeBtn);
181: 
182:       issuesContainer.appendChild(row);
183:     }
184: 
185:     function collectIssues() {
186:       const issues = [];
187:       const rows = document.querySelectorAll(".issue-row");
188:       rows.forEach((row) => {
189:         const selects = row.getElementsByTagName("select");
190:         const inputs = row.getElementsByTagName("input");
191:         const areas = row.getElementsByTagName("textarea");
192:         const plugin_id = selects[0]?.value || "";
193:         const kind = inputs[0]?.value || "";
194:         const title = inputs[1]?.value || "";
195:         const min_count = inputs[2]?.value;
196:         const max_count = inputs[3]?.value;
197:         const where = parseKV(areas[0]?.value || "");
198:         const contains = parseKV(areas[1]?.value || "");
199:         const issue = { kind };
200:         if (plugin_id) issue.plugin_id = plugin_id;
201:         if (title) issue.title = title;
202:         if (Object.keys(where).length) issue.where = where;
203:         if (Object.keys(contains).length) issue.contains = contains;
204:         if (min_count !== undefined && min_count !== "") issue.min_count = Number(min_count);
205:         if (max_count !== undefined && max_count !== "") issue.max_count = Number(max_count);
206:         if (kind) issues.push(issue);
207:       });
208:       return issues;
209:     }
210: 
211:     function refreshGroundTruth(yamlText) {
212:       groundTruthEl.value = yamlText || "";
213:     }
214: 
215:     let autosaveTimer = null;
216:     function scheduleAutosave() {
217:       if (!autosaveFlag || !autosaveFlag.checked) return;
218:       if (autosaveTimer) clearTimeout(autosaveTimer);
219:       autosaveTimer = setTimeout(() => {
220:         saveKnownIssues(true);
221:       }, 800);
222:     }
223: 
224:     async function saveKnownIssues(isAuto = false) {
225:       const uploadId = uploadSelect.value;
226:       if (!uploadId) {
227:         saveResult.textContent = "Select an upload first.";
228:         return null;
229:       }
230:       const payload = {
231:         strict: strictFlag.checked,
232:         notes: notesEl.value || "",
233:         expected_findings: collectIssues()
234:       };
235:       saveResult.textContent = isAuto ? "Auto-saving..." : "Saving...";
236:       const resp = await fetch("/api/known-issues", {
237:         method: "POST",
238:         headers: { "Content-Type": "application/json" },
239:         body: JSON.stringify({
240:           upload_id: uploadId,
241:           erp_type: erpTypeEl ? erpTypeEl.value : "",
242:           known_issues: payload
243:         })
244:       });
245:       const data = await resp.json();
246:       saveResult.textContent = JSON.stringify(data, null, 2);
247:       if (data.ground_truth_yaml) {
248:         refreshGroundTruth(data.ground_truth_yaml);
249:       }
250:       return data;
251:     }
252: 
253:     function ensureUploadOption(payload) {
254:       if (!uploadSelect || !payload.upload_id) return;
255:       const id = payload.upload_id;
256:       let option = uploadSelect.querySelector(`option[value="${id}"]`);
257:       const displayName = payload.filename || payload.uploaded_filename || "upload";
258:       const dedupeNote = payload.deduplicated ? " (deduped)" : "";
259:       const label = `${displayName} (${payload.sha256.slice(0, 8)}...)${dedupeNote}`;
260:       if (!option) {
261:         option = document.createElement("option");
262:         option.value = id;
263:         uploadSelect.insertBefore(option, uploadSelect.firstChild);
264:       }
265:       option.textContent = label;
266:       uploadSelect.value = id;
267:     }
268: 
269:     async function loadKnownIssues(uploadId) {
270:       if (!uploadId) return;
271:       const erpType = erpTypeEl ? erpTypeEl.value || "unknown" : "unknown";
272:       const params = new URLSearchParams({
273:         upload_id: uploadId,
274:         erp_type: erpType
275:       });
276:       const resp = await fetch(`/api/known-issues?${params.toString()}`);
277:       const data = await resp.json();
278:       const payload = data.known_issues || { strict: true, notes: "", expected_findings: [] };
279:       strictFlag.checked = !!payload.strict;
280:       notesEl.value = payload.notes || "";
281:       issuesContainer.innerHTML = "";
282:       const issues = payload.expected_findings || [];
283:       if (issues.length) {
284:         issues.forEach(addIssueRow);
285:       } else {
286:         addIssueRow({});
287:       }
288:       refreshGroundTruth(data.ground_truth_yaml || "");
289:     }
290: 
291:     if (uploadForm) {
292:       uploadForm.addEventListener("submit", async (event) => {
293:         event.preventDefault();
294:         uploadResult.textContent = "Uploading...";
295:         const fileInput = uploadForm.querySelector('input[type="file"]');
296:         if (!fileInput || !fileInput.files || !fileInput.files[0]) {
297:           uploadResult.textContent = "Select a file first.";
298:           return;
299:         }
300:         const file = fileInput.files[0];
301:         const response = await fetch(`/api/upload/raw?filename=${encodeURIComponent(file.name)}`, {
302:           method: "POST",
303:           body: file
304:         });
305:         const payload = await response.json();
306:         uploadResult.textContent = JSON.stringify(payload, null, 2);
307:         if (payload.upload_id) {
308:           ensureUploadOption(payload);
309:           await loadKnownIssues(payload.upload_id);
310:         }
311:       });
312:     }
313: 
314:     if (uploadSelect) {
315:       uploadSelect.addEventListener("change", () => {
316:         const id = uploadSelect.value;
317:         if (id) {
318:           loadKnownIssues(id);
319:         }
320:       });
321:     }
322:     if (erpTypeEl) {
323:       erpTypeEl.addEventListener("change", () => {
324:         const id = uploadSelect.value;
325:         if (id) {
326:           loadKnownIssues(id);
327:         }
328:       });
329:     }
330: 
331:     document.getElementById("add-issue").addEventListener("click", () => addIssueRow({}));
332:     document.getElementById("quick-qemail").addEventListener("click", () => {
333:       addIssueRow({
334:         plugin_id: "analysis_close_cycle_contention",
335:         kind: "close_cycle_contention",
336:         where: { process: "qemail" },
337:         min_count: 1,
338:         max_count: 1
339:       });
340:     });
341: 
342:     document.getElementById("save-issues").addEventListener("click", async () => {
343:       await saveKnownIssues(false);
344:     });
345: 
346:     async function pollRun(runId) {
347:       if (runStatus) runStatus.textContent = "running";
348:       async function tick() {
349:         const resp = await fetch(`/api/runs/${runId}`);
350:         if (!resp.ok) return;
351:         const payload = await resp.json();
352:         const status = payload.run?.status || "";
353:         if (runStatus) runStatus.textContent = status || "unknown";
354:         if (status !== "completed") {
355:           setTimeout(tick, 2000);
356:           return;
357:         }
358:         if (runStatus) runStatus.textContent = "completed (evaluating)";
359:         const evalResp = await fetch(`/api/runs/${runId}/evaluate`, {
360:           method: "POST",
361:           headers: { "Content-Type": "application/json" },
362:           body: JSON.stringify({ mode: "auto" })
363:         });
364:         const evalPayload = await evalResp.json();
365:         if (evalResult) {
366:           evalResult.textContent = JSON.stringify(evalPayload, null, 2);
367:         }
368:         if (runStatus) runStatus.textContent = "completed";
369:       }
370:       tick();
371:     }
372: 
373:     document.getElementById("run-auto").addEventListener("click", async () => {
374:       const uploadId = uploadSelect.value;
375:       if (!uploadId) {
376:         saveResult.textContent = "Select an upload first.";
377:         return;
378:       }
379:       await saveKnownIssues(false);
380:       const formData = new FormData();
381:       formData.set("upload_id", uploadId);
382:       const resp = await fetch("/api/runs/auto-evaluate", {
383:         method: "POST",
384:         body: formData
385:       });
386:       const data = await resp.json();
387:       if (data.run_id) {
388:         if (runStatus) runStatus.textContent = `queued (${data.run_id})`;
389:         pollRun(data.run_id);
390:       } else {
391:         saveResult.textContent = JSON.stringify(data, null, 2);
392:       }
393:     });
394: 
395:     document.addEventListener("input", (event) => {
396:       if (!event.target) return;
397:       if (event.target.closest(".issue-row") || event.target === notesEl || event.target === strictFlag) {
398:         scheduleAutosave();
399:       }
400:     });
401: 
402:     if (autosaveFlag) {
403:       autosaveFlag.addEventListener("change", () => {
404:         if (autosaveFlag.checked) {
405:           scheduleAutosave();
406:         }
407:       });
408:     }
409: 
410:     if (uploadSelect && uploadSelect.value) {
411:       loadKnownIssues(uploadSelect.value);
412:     } else if (uploadSelect && uploadSelect.options.length > 1) {
413:       uploadSelect.value = uploadSelect.options[1].value;
414:       loadKnownIssues(uploadSelect.value);
415:     } else {
416:       addIssueRow({});
417:     }
418:   </script>
419: </body>
420: </html>
````

## File: tests/fixtures/db/generate_golden_dbs.py
````python
  1: from __future__ import annotations
  2: 
  3: from pathlib import Path
  4: import json
  5: import sqlite3
  6: 
  7: from statistic_harness.core.migrations import MIGRATIONS
  8: from statistic_harness.core.utils import DEFAULT_TENANT_ID, now_iso, json_dumps
  9: 
 10: 
 11: FIXTURE_DIR = Path(__file__).resolve().parent
 12: 
 13: 
 14: def apply_migrations(conn: sqlite3.Connection, up_to: int) -> None:
 15:     for version, migration in enumerate(MIGRATIONS[:up_to], start=1):
 16:         migration(conn)
 17:         conn.execute(
 18:             "INSERT OR IGNORE INTO schema_migrations (version, applied_at) VALUES (?, ?)",
 19:             (version, now_iso()),
 20:         )
 21:         conn.execute(f"PRAGMA user_version = {version}")
 22:     conn.commit()
 23: 
 24: 
 25: def seed_base(conn: sqlite3.Connection, version: int) -> None:
 26:     run_id = f"run_v{version}"
 27:     if version >= 16:
 28:         conn.execute(
 29:             "INSERT INTO runs (run_id, tenant_id, created_at, status, upload_id, input_filename, canonical_path, settings_json, error_json) "
 30:             "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
 31:             (
 32:                 run_id,
 33:                 DEFAULT_TENANT_ID,
 34:                 now_iso(),
 35:                 "completed",
 36:                 "upload",
 37:                 "file.csv",
 38:                 "path.csv",
 39:                 "{}",
 40:                 None,
 41:             ),
 42:         )
 43:     else:
 44:         conn.execute(
 45:             "INSERT INTO runs (run_id, created_at, status, upload_id, input_filename, canonical_path, settings_json, error_json) "
 46:             "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
 47:             (
 48:                 run_id,
 49:                 now_iso(),
 50:                 "completed",
 51:                 "upload",
 52:                 "file.csv",
 53:                 "path.csv",
 54:                 "{}",
 55:                 None,
 56:             ),
 57:         )
 58:     conn.execute(
 59:         "INSERT INTO plugin_results (run_id, plugin_id, status, summary, metrics_json, findings_json, artifacts_json, error_json) "
 60:         "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
 61:         (run_id, "analysis_dp_gmm", "ok", "seed", "{}", "[]", "[]", None),
 62:     )
 63: 
 64:     if version >= 2:
 65:         if version >= 16:
 66:             conn.execute(
 67:                 "INSERT INTO projects (project_id, tenant_id, fingerprint, name, created_at) VALUES (?, ?, ?, ?, ?)",
 68:                 ("project_seed", DEFAULT_TENANT_ID, "fp_seed", "Project Seed", now_iso()),
 69:             )
 70:             conn.execute(
 71:                 "INSERT INTO datasets (dataset_id, tenant_id, project_id, fingerprint, created_at) VALUES (?, ?, ?, ?, ?)",
 72:                 (
 73:                     "dataset_seed",
 74:                     DEFAULT_TENANT_ID,
 75:                     "project_seed",
 76:                     "fp_seed",
 77:                     now_iso(),
 78:                 ),
 79:             )
 80:             conn.execute(
 81:                 "INSERT INTO dataset_versions (dataset_version_id, tenant_id, dataset_id, created_at, table_name, data_hash, row_count, column_count) "
 82:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
 83:                 (
 84:                     "dv_seed",
 85:                     DEFAULT_TENANT_ID,
 86:                     "dataset_seed",
 87:                     now_iso(),
 88:                     "dataset_seed_table",
 89:                     "hash",
 90:                     1,
 91:                     1,
 92:                 ),
 93:             )
 94:             conn.execute(
 95:                 "INSERT INTO dataset_columns (dataset_version_id, tenant_id, column_id, safe_name, original_name, dtype, role) "
 96:                 "VALUES (?, ?, ?, ?, ?, ?, ?)",
 97:                 ("dv_seed", DEFAULT_TENANT_ID, 1, "c1", "col1", "float", None),
 98:             )
 99:         else:
100:             conn.execute(
101:                 "INSERT INTO projects (project_id, fingerprint, name, created_at) VALUES (?, ?, ?, ?)",
102:                 ("project_seed", "fp_seed", "Project Seed", now_iso()),
103:             )
104:             conn.execute(
105:                 "INSERT INTO datasets (dataset_id, project_id, fingerprint, created_at) VALUES (?, ?, ?, ?)",
106:                 ("dataset_seed", "project_seed", "fp_seed", now_iso()),
107:             )
108:             conn.execute(
109:                 "INSERT INTO dataset_versions (dataset_version_id, dataset_id, created_at, table_name, data_hash, row_count, column_count) "
110:                 "VALUES (?, ?, ?, ?, ?, ?, ?)",
111:                 ("dv_seed", "dataset_seed", now_iso(), "dataset_seed_table", "hash", 1, 1),
112:             )
113:             conn.execute(
114:                 "INSERT INTO dataset_columns (dataset_version_id, column_id, safe_name, original_name, dtype, role) "
115:                 "VALUES (?, ?, ?, ?, ?, ?)",
116:                 ("dv_seed", 1, "c1", "col1", "float", None),
117:             )
118: 
119:     if version >= 3:
120:         if version >= 16:
121:             conn.execute(
122:                 "INSERT INTO plugin_results_v2 (run_id, tenant_id, plugin_id, plugin_version, executed_at, code_hash, settings_hash, dataset_hash, status, summary, metrics_json, findings_json, artifacts_json, error_json) "
123:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
124:                 (
125:                     run_id,
126:                     DEFAULT_TENANT_ID,
127:                     "analysis_dp_gmm",
128:                     "0.1.0",
129:                     now_iso(),
130:                     None,
131:                     None,
132:                     None,
133:                     "ok",
134:                     "seed",
135:                     "{}",
136:                     "[]",
137:                     "[]",
138:                     None,
139:                 ),
140:             )
141:         else:
142:             conn.execute(
143:                 "INSERT INTO plugin_results_v2 (run_id, plugin_id, plugin_version, executed_at, code_hash, settings_hash, dataset_hash, status, summary, metrics_json, findings_json, artifacts_json, error_json) "
144:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
145:                 (
146:                     run_id,
147:                     "analysis_dp_gmm",
148:                     "0.1.0",
149:                     now_iso(),
150:                     None,
151:                     None,
152:                     None,
153:                     "ok",
154:                     "seed",
155:                     "{}",
156:                     "[]",
157:                     "[]",
158:                     None,
159:                 ),
160:             )
161: 
162:     if version >= 4:
163:         conn.execute(
164:             "INSERT INTO parameter_entities (canonical_text) VALUES (?)",
165:             ("k=v",),
166:         )
167:         conn.execute(
168:             "INSERT INTO parameter_kv (entity_id, key, value) VALUES (?, ?, ?)",
169:             (1, "k", "v"),
170:         )
171:         conn.execute(
172:             "INSERT INTO row_parameter_link (dataset_version_id, row_index, entity_id) VALUES (?, ?, ?)",
173:             ("dv_seed", 0, 1),
174:         )
175:         conn.execute(
176:             "INSERT INTO entities (type, key) VALUES (?, ?)",
177:             ("dataset_version", "dv_seed"),
178:         )
179:         conn.execute(
180:             "INSERT INTO entities (type, key) VALUES (?, ?)",
181:             ("parameter", "k=v"),
182:         )
183:         conn.execute(
184:             "INSERT INTO edges (src_entity_id, dst_entity_id, kind, evidence_json, score) VALUES (?, ?, ?, ?, ?)",
185:             (1, 2, "uses_parameter", json.dumps({"column": "col1"}), None),
186:         )
187: 
188:     if version >= 5:
189:         if version >= 16:
190:             conn.execute(
191:                 "INSERT INTO analysis_jobs (dataset_version_id, tenant_id, plugin_id, plugin_version, code_hash, settings_hash, run_seed, status, created_at) "
192:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
193:                 (
194:                     "dv_seed",
195:                     DEFAULT_TENANT_ID,
196:                     "analysis_dp_gmm",
197:                     "0.1.0",
198:                     None,
199:                     None,
200:                     0,
201:                     "queued",
202:                     now_iso(),
203:                 ),
204:             )
205:             conn.execute(
206:                 "INSERT INTO deliveries (project_id, tenant_id, dataset_version_id, plugin_id, plugin_version, code_hash, dataset_hash, delivered_at, notes) "
207:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
208:                 (
209:                     "project_seed",
210:                     DEFAULT_TENANT_ID,
211:                     "dv_seed",
212:                     "analysis_dp_gmm",
213:                     "0.1.0",
214:                     None,
215:                     "hash",
216:                     now_iso(),
217:                     "note",
218:                 ),
219:             )
220:         else:
221:             conn.execute(
222:                 "INSERT INTO analysis_jobs (dataset_version_id, plugin_id, plugin_version, code_hash, settings_hash, run_seed, status, created_at) "
223:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
224:                 (
225:                     "dv_seed",
226:                     "analysis_dp_gmm",
227:                     "0.1.0",
228:                     None,
229:                     None,
230:                     0,
231:                     "queued",
232:                     now_iso(),
233:                 ),
234:             )
235:             conn.execute(
236:                 "INSERT INTO deliveries (project_id, dataset_version_id, plugin_id, plugin_version, code_hash, dataset_hash, delivered_at, notes) "
237:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
238:                 (
239:                     "project_seed",
240:                     "dv_seed",
241:                     "analysis_dp_gmm",
242:                     "0.1.0",
243:                     None,
244:                     "hash",
245:                     now_iso(),
246:                     "note",
247:                 ),
248:             )
249: 
250:     if version >= 6:
251:         if version >= 16:
252:             conn.execute(
253:                 "INSERT INTO raw_formats (fingerprint, tenant_id, name, created_at) VALUES (?, ?, ?, ?)",
254:                 ("raw_fp", DEFAULT_TENANT_ID, "Raw Format", now_iso()),
255:             )
256:             conn.execute(
257:                 "INSERT INTO raw_format_notes (format_id, tenant_id, note, created_at) VALUES (?, ?, ?, ?)",
258:                 (1, DEFAULT_TENANT_ID, "note", now_iso()),
259:             )
260:         else:
261:             conn.execute(
262:                 "INSERT INTO raw_formats (fingerprint, name, created_at) VALUES (?, ?, ?)",
263:                 ("raw_fp", "Raw Format", now_iso()),
264:             )
265:             conn.execute(
266:                 "INSERT INTO raw_format_notes (format_id, note, created_at) VALUES (?, ?, ?)",
267:                 (1, "note", now_iso()),
268:             )
269:         conn.execute(
270:             "UPDATE dataset_versions SET raw_format_id = ? WHERE dataset_version_id = ?",
271:             (1, "dv_seed"),
272:         )
273:         table_name = "template_seed"
274:         if version >= 16:
275:             conn.execute(
276:                 "INSERT INTO templates (name, tenant_id, description, version, created_at, table_name) VALUES (?, ?, ?, ?, ?, ?)",
277:                 ("Template Seed", DEFAULT_TENANT_ID, "desc", "v1", now_iso(), table_name),
278:             )
279:             conn.execute(
280:                 "INSERT INTO template_fields (template_id, tenant_id, field_id, safe_name, name, dtype, role, required) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
281:                 (1, DEFAULT_TENANT_ID, 1, "f1", "value", "float", None, 0),
282:             )
283:             conn.execute(
284:                 "INSERT INTO dataset_templates (dataset_version_id, tenant_id, template_id, mapping_json, mapping_hash, status, created_at, updated_at) "
285:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
286:                 (
287:                     "dv_seed",
288:                     DEFAULT_TENANT_ID,
289:                     1,
290:                     json_dumps({"value": "col1"}),
291:                     "hash",
292:                     "ready",
293:                     now_iso(),
294:                     now_iso(),
295:                 ),
296:             )
297:             conn.execute(
298:                 "INSERT INTO template_conversions (dataset_version_id, tenant_id, template_id, status, started_at, completed_at, error_json, mapping_hash, row_count) "
299:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
300:                 (
301:                     "dv_seed",
302:                     DEFAULT_TENANT_ID,
303:                     1,
304:                     "completed",
305:                     now_iso(),
306:                     now_iso(),
307:                     None,
308:                     "hash",
309:                     1,
310:                 ),
311:             )
312:         else:
313:             conn.execute(
314:                 "INSERT INTO templates (name, description, version, created_at, table_name) VALUES (?, ?, ?, ?, ?)",
315:                 ("Template Seed", "desc", "v1", now_iso(), table_name),
316:             )
317:             conn.execute(
318:                 "INSERT INTO template_fields (template_id, field_id, safe_name, name, dtype, role, required) VALUES (?, ?, ?, ?, ?, ?, ?)",
319:                 (1, 1, "f1", "value", "float", None, 0),
320:             )
321:             conn.execute(
322:                 "INSERT INTO dataset_templates (dataset_version_id, template_id, mapping_json, mapping_hash, status, created_at, updated_at) "
323:                 "VALUES (?, ?, ?, ?, ?, ?, ?)",
324:                 (
325:                     "dv_seed",
326:                     1,
327:                     json_dumps({"value": "col1"}),
328:                     "hash",
329:                     "ready",
330:                     now_iso(),
331:                     now_iso(),
332:                 ),
333:             )
334:             conn.execute(
335:                 "INSERT INTO template_conversions (dataset_version_id, template_id, status, started_at, completed_at, error_json, mapping_hash, row_count) "
336:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
337:                 ("dv_seed", 1, "completed", now_iso(), now_iso(), None, "hash", 1),
338:             )
339:         conn.execute(
340:             "CREATE TABLE IF NOT EXISTS template_seed (row_id INTEGER PRIMARY KEY AUTOINCREMENT, dataset_version_id TEXT, row_index INTEGER, row_json TEXT, f1 TEXT)"
341:         )
342: 
343:     if version >= 7:
344:         if version >= 16:
345:             conn.execute(
346:                 "INSERT INTO raw_format_mappings (format_id, tenant_id, template_id, mapping_json, mapping_hash, notes, created_at) "
347:                 "VALUES (?, ?, ?, ?, ?, ?, ?)",
348:                 (
349:                     1,
350:                     DEFAULT_TENANT_ID,
351:                     1,
352:                     json_dumps({"value": "col1"}),
353:                     "hash",
354:                     "note",
355:                     now_iso(),
356:                 ),
357:             )
358:         else:
359:             conn.execute(
360:                 "INSERT INTO raw_format_mappings (format_id, template_id, mapping_json, mapping_hash, notes, created_at) "
361:                 "VALUES (?, ?, ?, ?, ?, ?)",
362:                 (1, 1, json_dumps({"value": "col1"}), "hash", "note", now_iso()),
363:             )
364: 
365:     if version >= 8:
366:         if version >= 16:
367:             conn.execute(
368:                 "INSERT INTO plugin_executions (run_id, tenant_id, plugin_id, plugin_version, started_at, completed_at, duration_ms, status, exit_code, cpu_user, cpu_system, max_rss, warnings_count, stdout, stderr) "
369:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
370:                 (
371:                     run_id,
372:                     DEFAULT_TENANT_ID,
373:                     "analysis_dp_gmm",
374:                     "0.1.0",
375:                     now_iso(),
376:                     now_iso(),
377:                     1,
378:                     "ok",
379:                     0,
380:                     0.1,
381:                     0.1,
382:                     123,
383:                     0,
384:                     "out",
385:                     "",
386:                 ),
387:             )
388:         else:
389:             conn.execute(
390:                 "INSERT INTO plugin_executions (run_id, plugin_id, plugin_version, started_at, completed_at, duration_ms, status, exit_code, cpu_user, cpu_system, max_rss, warnings_count, stdout, stderr) "
391:                 "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
392:                 (
393:                     run_id,
394:                     "analysis_dp_gmm",
395:                     "0.1.0",
396:                     now_iso(),
397:                     now_iso(),
398:                     1,
399:                     "ok",
400:                     0,
401:                     0.1,
402:                     0.1,
403:                     123,
404:                     0,
405:                     "out",
406:                     "",
407:                 ),
408:             )
409: 
410:     if version >= 10:
411:         if version >= 16:
412:             conn.execute(
413:                 "INSERT INTO known_issue_sets (sha256, tenant_id, upload_id, strict, notes, created_at, updated_at) "
414:                 "VALUES (?, ?, ?, ?, ?, ?, ?)",
415:                 (
416:                     "seedhash",
417:                     DEFAULT_TENANT_ID,
418:                     "upload_seed",
419:                     1,
420:                     "seed notes",
421:                     now_iso(),
422:                     now_iso(),
423:                 ),
424:             )
425:         else:
426:             conn.execute(
427:                 "INSERT INTO known_issue_sets (sha256, upload_id, strict, notes, created_at, updated_at) "
428:                 "VALUES (?, ?, ?, ?, ?, ?)",
429:                 ("seedhash", "upload_seed", 1, "seed notes", now_iso(), now_iso()),
430:             )
431:         conn.execute(
432:             "INSERT INTO known_issues (set_id, title, plugin_id, kind, where_json, contains_json, min_count, max_count, created_at, updated_at) "
433:             "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
434:             (
435:                 1,
436:                 "qemail close cycle",
437:                 "analysis_close_cycle_contention",
438:                 "close_cycle_contention",
439:                 json_dumps({"process": "qemail"}),
440:                 None,
441:                 1,
442:                 1,
443:                 now_iso(),
444:                 now_iso(),
445:             ),
446:         )
447: 
448:     if version >= 11:
449:         if version >= 16:
450:             conn.execute(
451:                 "INSERT INTO dataset_role_candidates (dataset_version_id, tenant_id, column_id, role, score, reasons_json, created_at) "
452:                 "VALUES (?, ?, ?, ?, ?, ?, ?)",
453:                 (
454:                     "dv_seed",
455:                     DEFAULT_TENANT_ID,
456:                     1,
457:                     "start_time",
458:                     3.0,
459:                     json_dumps(["name_match"]),
460:                     now_iso(),
461:                 ),
462:             )
463:             conn.execute(
464:                 "INSERT INTO project_role_overrides (project_id, tenant_id, role, column_name, created_at, updated_at) "
465:                 "VALUES (?, ?, ?, ?, ?, ?)",
466:                 (
467:                     "project_seed",
468:                     DEFAULT_TENANT_ID,
469:                     "queue_time",
470:                     "col1",
471:                     now_iso(),
472:                     now_iso(),
473:                 ),
474:             )
475:         else:
476:             conn.execute(
477:                 "INSERT INTO dataset_role_candidates (dataset_version_id, column_id, role, score, reasons_json, created_at) "
478:                 "VALUES (?, ?, ?, ?, ?, ?)",
479:                 (
480:                     "dv_seed",
481:                     1,
482:                     "start_time",
483:                     3.0,
484:                     json_dumps(["name_match"]),
485:                     now_iso(),
486:                 ),
487:             )
488:             conn.execute(
489:                 "INSERT INTO project_role_overrides (project_id, role, column_name, created_at, updated_at) "
490:                 "VALUES (?, ?, ?, ?, ?)",
491:                 ("project_seed", "queue_time", "col1", now_iso(), now_iso()),
492:             )
493: 
494:     conn.commit()
495: 
496: 
497: def main() -> None:
498:     FIXTURE_DIR.mkdir(parents=True, exist_ok=True)
499:     for version in range(1, len(MIGRATIONS) + 1):
500:         path = FIXTURE_DIR / f"v{version}.sqlite"
501:         if path.exists():
502:             path.unlink()
503:         conn = sqlite3.connect(path)
504:         apply_migrations(conn, version)
505:         seed_base(conn, version)
506:         conn.close()
507: 
508: 
509: if __name__ == "__main__":
510:     main()
````

## File: tests/fixtures/enertia_eventlog.csv
````
 1: CASE_ID,PROCESS_CODE,MODULE_CODE,USER_NAME,HOST_NAME,QUEUED_AT,ELIGIBLE_AT,START_TIME,END_TIME,DURATION_SEC,DEPENDENCY_ID,CHAIN_ID,PARAMS
 2: C3000,alpha,m1,user0,h1,2026-01-15T00:00:00,2026-01-15T00:01:00,2026-01-15T00:03:00,2026-01-15T00:06:00,180,,chain_a,queue=start
 3: C3001,beta,m2,user1,h2,2026-01-15T00:04:00,2026-01-15T00:04:00,2026-01-15T00:05:00,2026-01-15T00:09:00,240,C3000,chain_a,queue=start
 4: C3002,alpha,m1,user2,h2,2026-01-15T00:08:00,2026-01-15T00:09:00,2026-01-15T00:10:00,2026-01-15T00:15:00,300,C3001,chain_a,queue=start
 5: C3003,beta,m2,user3,h1,2026-01-15T00:12:00,2026-01-15T00:12:00,2026-01-15T00:14:00,2026-01-15T00:17:00,180,C3002,chain_a,queue=start
 6: C3004,alpha,m1,user0,h2,2026-01-15T00:16:00,2026-01-15T00:17:00,2026-01-15T00:18:00,2026-01-15T00:22:00,240,C3003,chain_a,queue=start
 7: C3005,beta,m2,user1,h2,2026-01-15T00:20:00,2026-01-15T00:20:00,2026-01-15T00:21:00,2026-01-15T00:26:00,300,C3004,chain_a,queue=start
 8: C3006,alpha,m1,user2,h1,2026-01-15T00:24:00,2026-01-15T00:25:00,2026-01-15T00:27:00,2026-01-15T00:30:00,180,C3005,chain_a,queue=start
 9: C3007,beta,m2,user3,h2,2026-01-15T00:28:00,2026-01-15T00:28:00,2026-01-15T00:29:00,2026-01-15T00:33:00,240,C3006,chain_a,queue=start
10: C3008,alpha,m1,user0,h2,2026-01-15T00:32:00,2026-01-15T00:33:00,2026-01-15T00:34:00,2026-01-15T00:39:00,300,,chain_b,queue=start
11: C3009,beta,m2,user1,h1,2026-01-15T00:36:00,2026-01-15T00:36:00,2026-01-15T00:38:00,2026-01-15T00:41:00,180,C3008,chain_b,queue=start
12: C3010,alpha,m1,user2,h2,2026-01-15T00:40:00,2026-01-15T00:41:00,2026-01-15T00:42:00,2026-01-15T00:46:00,240,C3009,chain_b,queue=start
13: C3011,beta,m2,user3,h2,2026-01-15T00:44:00,2026-01-15T00:44:00,2026-01-15T00:45:00,2026-01-15T00:50:00,300,C3010,chain_b,queue=start
14: C3012,alpha,m1,user0,h1,2026-01-15T00:48:00,2026-01-15T00:49:00,2026-01-15T00:51:00,2026-01-15T00:54:00,180,C3011,chain_b,queue=start
15: C3013,beta,m2,user1,h2,2026-01-15T00:52:00,2026-01-15T00:52:00,2026-01-15T00:53:00,2026-01-15T00:57:00,240,C3012,chain_b,queue=start
16: C3014,alpha,m1,user2,h2,2026-01-15T00:56:00,2026-01-15T00:57:00,2026-01-15T00:58:00,2026-01-15T01:03:00,300,C3013,chain_b,queue=start
17: C3015,beta,m2,user3,h1,2026-01-15T01:00:00,2026-01-15T01:00:00,2026-01-15T01:02:00,2026-01-15T01:05:00,180,C3014,chain_b,queue=start
````

## File: tests/fixtures/ground_truth_tolerance.yaml
````yaml
 1: features:
 2:   - x1
 3: changepoints:
 4:   - 100
 5: changepoint_tolerance:
 6:   absolute: 5
 7:   relative: 0.2
 8: anomalies:
 9:   - 50
10: anomaly_tolerance:
11:   absolute: 0
12:   relative: 0.1
13: min_anomaly_hits: 1
````

## File: tests/fixtures/quorum_close_cycle.csv
````
 1: CASE_ID,PROCESS_NAME,QUEUE_DT,START_DT,END_DT,DURATION_SEC,SERVER,USER_ID,PARAMS
 2: 1000,qemail,2026-01-20T01:00:00,2026-01-20T01:01:00,2026-01-20T01:01:30,30,qpec1,user0,queue=start
 3: 1001,qemail,2026-01-20T01:01:00,2026-01-20T01:02:00,2026-01-20T01:02:30,30,qpec1,user1,queue=start
 4: 1002,qemail,2026-01-20T01:02:00,2026-01-20T01:03:00,2026-01-20T01:03:30,30,qpec1,user2,queue=start
 5: 1003,qpec,2026-01-20T02:00:00,2026-01-20T02:01:00,2026-01-20T02:02:40,100,qpec2,system,batch=close
 6: 1004,qemail,2026-01-21T01:00:00,2026-01-21T01:01:00,2026-01-21T01:01:30,30,qpec1,user0,queue=start
 7: 1005,qemail,2026-01-21T01:01:00,2026-01-21T01:02:00,2026-01-21T01:02:30,30,qpec1,user1,queue=start
 8: 1006,qemail,2026-01-21T01:02:00,2026-01-21T01:03:00,2026-01-21T01:03:30,30,qpec1,user2,queue=start
 9: 1007,qemail,2026-01-21T01:03:00,2026-01-21T01:04:00,2026-01-21T01:04:30,30,qpec1,user0,queue=start
10: 1008,qpec,2026-01-21T02:00:00,2026-01-21T02:01:00,2026-01-21T02:03:00,120,qpec2,system,batch=close
11: 1009,qemail,2026-01-22T01:00:00,2026-01-22T01:01:00,2026-01-22T01:01:30,30,qpec1,user0,queue=start
12: 1010,qemail,2026-01-22T01:01:00,2026-01-22T01:02:00,2026-01-22T01:02:30,30,qpec1,user1,queue=start
13: 1011,qemail,2026-01-22T01:02:00,2026-01-22T01:03:00,2026-01-22T01:03:30,30,qpec1,user2,queue=start
14: 1012,qemail,2026-01-22T01:03:00,2026-01-22T01:04:00,2026-01-22T01:04:30,30,qpec1,user0,queue=start
15: 1013,qemail,2026-01-22T01:04:00,2026-01-22T01:05:00,2026-01-22T01:05:30,30,qpec1,user1,queue=start
16: 1014,qpec,2026-01-22T02:00:00,2026-01-22T02:01:00,2026-01-22T02:03:20,140,qpec2,system,batch=close
17: 1015,qemail,2026-01-23T01:00:00,2026-01-23T01:01:00,2026-01-23T01:01:30,30,qpec1,user0,queue=start
18: 1016,qemail,2026-01-23T01:01:00,2026-01-23T01:02:00,2026-01-23T01:02:30,30,qpec1,user1,queue=start
19: 1017,qemail,2026-01-23T01:02:00,2026-01-23T01:03:00,2026-01-23T01:03:30,30,qpec1,user2,queue=start
20: 1018,qemail,2026-01-23T01:03:00,2026-01-23T01:04:00,2026-01-23T01:04:30,30,qpec1,user0,queue=start
21: 1019,qemail,2026-01-23T01:04:00,2026-01-23T01:05:00,2026-01-23T01:05:30,30,qpec1,user1,queue=start
22: 1020,qemail,2026-01-23T01:05:00,2026-01-23T01:06:00,2026-01-23T01:06:30,30,qpec1,user2,queue=start
23: 1021,qpec,2026-01-23T02:00:00,2026-01-23T02:01:00,2026-01-23T02:03:40,160,qpec2,system,batch=close
24: 1022,qemail,2026-01-24T01:00:00,2026-01-24T01:01:00,2026-01-24T01:01:30,30,qpec1,user0,queue=start
25: 1023,qemail,2026-01-24T01:01:00,2026-01-24T01:02:00,2026-01-24T01:02:30,30,qpec1,user1,queue=start
26: 1024,qemail,2026-01-24T01:02:00,2026-01-24T01:03:00,2026-01-24T01:03:30,30,qpec1,user2,queue=start
27: 1025,qemail,2026-01-24T01:03:00,2026-01-24T01:04:00,2026-01-24T01:04:30,30,qpec1,user0,queue=start
28: 1026,qemail,2026-01-24T01:04:00,2026-01-24T01:05:00,2026-01-24T01:05:30,30,qpec1,user1,queue=start
29: 1027,qemail,2026-01-24T01:05:00,2026-01-24T01:06:00,2026-01-24T01:06:30,30,qpec1,user2,queue=start
30: 1028,qemail,2026-01-24T01:06:00,2026-01-24T01:07:00,2026-01-24T01:07:30,30,qpec1,user0,queue=start
31: 1029,qpec,2026-01-24T02:00:00,2026-01-24T02:01:00,2026-01-24T02:04:00,180,qpec2,system,batch=close
32: 1030,qemail,2026-01-10T01:00:00,2026-01-10T01:01:00,2026-01-10T01:01:30,30,qpec1,user0,queue=start
33: 1031,qemail,2026-01-10T01:01:00,2026-01-10T01:02:00,2026-01-10T01:02:30,30,qpec1,user1,queue=start
34: 1032,qpec,2026-01-10T02:00:00,2026-01-10T02:01:00,2026-01-10T02:01:50,50,qpec2,system,batch=open
35: 1033,qemail,2026-01-11T01:00:00,2026-01-11T01:01:00,2026-01-11T01:01:30,30,qpec1,user0,queue=start
36: 1034,qemail,2026-01-11T01:01:00,2026-01-11T01:02:00,2026-01-11T01:02:30,30,qpec1,user1,queue=start
37: 1035,qpec,2026-01-11T02:00:00,2026-01-11T02:01:00,2026-01-11T02:01:50,50,qpec2,system,batch=open
38: 1036,qemail,2026-01-12T01:00:00,2026-01-12T01:01:00,2026-01-12T01:01:30,30,qpec1,user0,queue=start
39: 1037,qemail,2026-01-12T01:01:00,2026-01-12T01:02:00,2026-01-12T01:02:30,30,qpec1,user1,queue=start
40: 1038,qpec,2026-01-12T02:00:00,2026-01-12T02:01:00,2026-01-12T02:01:50,50,qpec2,system,batch=open
41: 1039,qemail,2026-01-13T01:00:00,2026-01-13T01:01:00,2026-01-13T01:01:30,30,qpec1,user0,queue=start
42: 1040,qemail,2026-01-13T01:01:00,2026-01-13T01:02:00,2026-01-13T01:02:30,30,qpec1,user1,queue=start
43: 1041,qpec,2026-01-13T02:00:00,2026-01-13T02:01:00,2026-01-13T02:01:50,50,qpec2,system,batch=open
44: 1042,qemail,2026-01-14T01:00:00,2026-01-14T01:01:00,2026-01-14T01:01:30,30,qpec1,user0,queue=start
45: 1043,qemail,2026-01-14T01:01:00,2026-01-14T01:02:00,2026-01-14T01:02:30,30,qpec1,user1,queue=start
46: 1044,qpec,2026-01-14T02:00:00,2026-01-14T02:01:00,2026-01-14T02:01:50,50,qpec2,system,batch=open
````

## File: tests/plugins/test_attribution.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_attribution.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_attribution_emits_dimension_findings(run_dir):
10:     rows = [
11:         {
12:             "process": "alpha",
13:             "module": "m1",
14:             "user": "u1",
15:             "queue_time": dt.datetime(2026, 1, 1, 8, 0, 0),
16:             "start_time": dt.datetime(2026, 1, 1, 8, 2, 0),
17:         },
18:         {
19:             "process": "alpha",
20:             "module": "m1",
21:             "user": "u2",
22:             "queue_time": dt.datetime(2026, 1, 1, 9, 0, 0),
23:             "start_time": dt.datetime(2026, 1, 1, 9, 3, 0),
24:         },
25:         {
26:             "process": "beta",
27:             "module": "m2",
28:             "user": "u3",
29:             "queue_time": dt.datetime(2026, 1, 1, 10, 0, 0),
30:             "start_time": dt.datetime(2026, 1, 1, 10, 0, 10),
31:         },
32:     ]
33:     df = pd.DataFrame(rows)
34:     for col in ["queue_time", "start_time"]:
35:         df[col] = df[col].astype(str)
36: 
37:     ctx = make_context(run_dir, df, {"wait_threshold_seconds": 60})
38:     result = Plugin().run(ctx)
39: 
40:     assert result.status == "ok"
41:     findings = [
42:         f
43:         for f in result.findings
44:         if f.get("kind") == "attribution" and f.get("dimension") == "process"
45:     ]
46:     assert findings
47:     assert findings[0]["key"] == "alpha"
````

## File: tests/plugins/test_capacity_scaling.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_capacity_scaling.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_capacity_scaling_models_wait(run_dir):
10:     rows = [
11:         {
12:             "HOST": "h1",
13:             "QUEUE_DT": dt.datetime(2026, 1, 1, 0, 0, 0),
14:             "START_DT": dt.datetime(2026, 1, 1, 0, 1, 0),
15:         },
16:         {
17:             "HOST": "h1",
18:             "QUEUE_DT": dt.datetime(2026, 1, 1, 0, 2, 0),
19:             "START_DT": dt.datetime(2026, 1, 1, 0, 4, 0),
20:         },
21:     ]
22:     df = pd.DataFrame(rows)
23:     df["QUEUE_DT"] = df["QUEUE_DT"].astype(str)
24:     df["START_DT"] = df["START_DT"].astype(str)
25: 
26:     ctx = make_context(run_dir, df, {})
27:     result = Plugin().run(ctx)
28: 
29:     assert result.status == "ok"
30:     assert result.metrics["scale_factor"] == 2.0
31:     assert result.findings
32:     assert result.findings[0]["measurement_type"] == "modeled"
33:     assert "assumptions" in result.findings[0]
````

## File: tests/plugins/test_chain_makespan.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_chain_makespan.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_chain_makespan_outputs_gaps(run_dir):
10:     rows = [
11:         {
12:             "MASTER_ID": "chain_a",
13:             "START_DT": dt.datetime(2026, 1, 1, 0, 0, 0),
14:             "END_DT": dt.datetime(2026, 1, 1, 0, 5, 0),
15:         },
16:         {
17:             "MASTER_ID": "chain_a",
18:             "START_DT": dt.datetime(2026, 1, 1, 0, 10, 0),
19:             "END_DT": dt.datetime(2026, 1, 1, 0, 15, 0),
20:         },
21:         {
22:             "MASTER_ID": "chain_b",
23:             "START_DT": dt.datetime(2026, 1, 1, 0, 0, 0),
24:             "END_DT": dt.datetime(2026, 1, 1, 0, 3, 0),
25:         },
26:     ]
27:     df = pd.DataFrame(rows)
28:     df["START_DT"] = df["START_DT"].astype(str)
29:     df["END_DT"] = df["END_DT"].astype(str)
30: 
31:     ctx = make_context(run_dir, df, {})
32:     result = Plugin().run(ctx)
33: 
34:     assert result.status == "ok"
35:     assert result.metrics["chains"] == 2
36:     findings = [f for f in result.findings if f.get("kind") == "chain_makespan"]
37:     assert findings
38:     chain_a = [f for f in findings if f.get("sequence_id") == "chain_a"][0]
39:     assert chain_a["idle_gap_seconds"] > 0
40:     assert chain_a["measurement_type"] == "measured"
````

## File: tests/plugins/test_close_cycle_capacity_impact.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_close_cycle_capacity_impact.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def _make_rows(days, hosts, duration_sec, process):
10:     rows = []
11:     for day in days:
12:         for host in hosts:
13:             start = dt.datetime(2026, 1, day, 0, 0, 0)
14:             end = start + dt.timedelta(seconds=duration_sec)
15:             rows.append(
16:                 {
17:                     "PROCESS_ID": process,
18:                     "LOCAL_MACHINE_ID": host,
19:                     "START_DT": start.isoformat(),
20:                     "END_DT": end.isoformat(),
21:                 }
22:             )
23:     return rows
24: 
25: 
26: def _base_settings():
27:     return {
28:         "close_window_mode": "override",
29:         "close_cycle_start_day": 20,
30:         "close_cycle_end_day": 24,
31:         "bucket_size": "day",
32:         "min_bucket_rows": 1,
33:         "min_buckets_per_group": 2,
34:         "min_buckets_per_month": 1,
35:         "min_months": 1,
36:         "bootstrap_samples": 200,
37:         "alpha": 0.2,
38:     }
39: 
40: 
41: def test_capacity_impact_not_applicable_without_third_host(run_dir):
42:     rows = _make_rows(range(20, 25), ["h1", "h2"], 10, "PROC")
43:     df = pd.DataFrame(rows)
44: 
45:     settings = _base_settings()
46:     ctx = make_context(run_dir, df, settings)
47:     result = Plugin().run(ctx)
48: 
49:     assert result.status == "ok"
50:     assert result.findings
51:     assert all(item["decision"] == "not_applicable" for item in result.findings)
52: 
53: 
54: def test_capacity_impact_detects_reduction_with_third_host(run_dir):
55:     rows = []
56:     rows += _make_rows([20, 21, 22], ["h1", "h2", "h3"], 7, "PROC")
57:     rows += _make_rows([23, 24], ["h1", "h2"], 10, "PROC")
58:     df = pd.DataFrame(rows)
59: 
60:     settings = _base_settings()
61:     settings.update(
62:         {
63:             "target_reduction": 0.30,
64:             "tolerance": 0.2,
65:             "max_js_divergence": 0.9,
66:         }
67:     )
68:     ctx = make_context(run_dir, df, settings)
69:     result = Plugin().run(ctx)
70: 
71:     assert result.status == "ok"
72:     detected = [item for item in result.findings if item["decision"] == "detected"]
73:     assert detected
74: 
75: 
76: def test_capacity_impact_confounded_by_process_mix(run_dir):
77:     rows = []
78:     rows += _make_rows([20, 21, 22], ["h1", "h2", "h3"], 7, "PROC_A")
79:     rows += _make_rows([23, 24], ["h1", "h2"], 10, "PROC_B")
80:     df = pd.DataFrame(rows)
81: 
82:     settings = _base_settings()
83:     settings.update(
84:         {
85:             "target_reduction": 0.30,
86:             "tolerance": 0.2,
87:             "max_js_divergence": 0.1,
88:         }
89:     )
90:     ctx = make_context(run_dir, df, settings)
91:     result = Plugin().run(ctx)
92: 
93:     assert result.status == "ok"
94:     assert all(item["decision"] == "not_applicable" for item in result.findings)
````

## File: tests/plugins/test_close_cycle_capacity_model.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_close_cycle_capacity_model.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def _make_rows(days, hosts, process="PROC"):
10:     rows = []
11:     for day in days:
12:         for host in hosts:
13:             start = dt.datetime(2026, 1, day, 0, 0, 0)
14:             end = start + dt.timedelta(hours=1)
15:             queue = start - dt.timedelta(hours=2)
16:             eligible = start - dt.timedelta(hours=1)
17:             rows.append(
18:                 {
19:                     "PROCESS_ID": process,
20:                     "LOCAL_MACHINE_ID": host,
21:                     "START_DT": start.isoformat(),
22:                     "END_DT": end.isoformat(),
23:                     "QUEUE_DT": queue.isoformat(),
24:                     "ELIGIBLE_DT": eligible.isoformat(),
25:                 }
26:             )
27:     return rows
28: 
29: 
30: def _base_settings():
31:     return {
32:         "close_window_mode": "override",
33:         "close_cycle_start_day": 20,
34:         "close_cycle_end_day": 24,
35:         "bucket_size": "day",
36:         "min_bucket_rows": 1,
37:         "min_buckets_per_group": 2,
38:         "min_months": 1,
39:         "baseline_host_count": 2,
40:         "added_hosts": 1,
41:         "target_reduction": 0.30,
42:         "tolerance": 0.15,
43:     }
44: 
45: 
46: def test_capacity_model_emits_modeled_findings(run_dir):
47:     rows = _make_rows(range(20, 25), ["h1", "h2"])
48:     df = pd.DataFrame(rows)
49: 
50:     ctx = make_context(run_dir, df, _base_settings())
51:     result = Plugin().run(ctx)
52: 
53:     assert result.status == "ok"
54:     modeled = [
55:         item
56:         for item in result.findings
57:         if item.get("decision") == "modeled" and item.get("metric_type") == "queue_to_end"
58:     ]
59:     assert modeled
60:     assert any(item.get("target_met") for item in modeled)
61: 
62: 
63: def test_capacity_model_handles_missing_queue_columns(run_dir):
64:     rows = []
65:     for day in range(20, 25):
66:         for host in ["h1", "h2"]:
67:             start = dt.datetime(2026, 1, day, 0, 0, 0)
68:             end = start + dt.timedelta(hours=1)
69:             rows.append(
70:                 {
71:                     "PROCESS_ID": "PROC",
72:                     "LOCAL_MACHINE_ID": host,
73:                     "START_DT": start.isoformat(),
74:                     "END_DT": end.isoformat(),
75:                 }
76:             )
77:     df = pd.DataFrame(rows)
78: 
79:     ctx = make_context(run_dir, df, _base_settings())
80:     result = Plugin().run(ctx)
81: 
82:     assert result.status == "ok"
83:     not_applicable = [
84:         item
85:         for item in result.findings
86:         if item.get("metric_type") in {"queue_to_end", "eligible_to_end"}
87:     ]
88:     assert not_applicable
89:     assert all(item.get("decision") == "not_applicable" for item in not_applicable)
90: 
91:     ttc_modeled = [
92:         item
93:         for item in result.findings
94:         if item.get("metric_type") == "ttc" and item.get("decision") == "modeled"
95:     ]
96:     assert ttc_modeled
````

## File: tests/plugins/test_close_cycle_contention.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_close_cycle_contention.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def _add_day(rows, date, qemail_count, other_duration):
10:     for idx in range(10):
11:         rows.append(
12:             {
13:                 "process": "qpec_job",
14:                 "timestamp": date + dt.timedelta(minutes=idx),
15:                 "duration": other_duration,
16:                 "server": "qpec1",
17:                 "params": "job=main",
18:             }
19:         )
20:     for idx in range(qemail_count):
21:         rows.append(
22:             {
23:                 "process": "qemail",
24:                 "timestamp": date + dt.timedelta(minutes=60 + idx),
25:                 "duration": 1,
26:                 "server": "qpec1" if idx % 2 == 0 else "qpec2",
27:                 "params": "noop=true",
28:             }
29:         )
30: 
31: 
32: def test_close_cycle_contention_detects_qemail(run_dir):
33:     rows = []
34:     # Open cycle days: Jan 10-19, Feb 6-10
35:     for day in range(10, 20):
36:         date = dt.datetime(2026, 1, day, 8, 0, 0)
37:         _add_day(rows, date, qemail_count=2, other_duration=10)
38:     for day in range(6, 11):
39:         date = dt.datetime(2026, 2, day, 8, 0, 0)
40:         _add_day(rows, date, qemail_count=2, other_duration=10)
41: 
42:     # Close cycle days: Jan 20-31, Feb 1-5 with varying load
43:     for day in range(20, 32):
44:         date = dt.datetime(2026, 1, day, 8, 0, 0)
45:         qemail_count = 2 + (day % 4)
46:         other_duration = 10 + qemail_count * 4
47:         _add_day(rows, date, qemail_count=qemail_count, other_duration=other_duration)
48:     for day in range(1, 6):
49:         date = dt.datetime(2026, 2, day, 8, 0, 0)
50:         qemail_count = 2 + (day % 4)
51:         other_duration = 10 + qemail_count * 4
52:         _add_day(rows, date, qemail_count=qemail_count, other_duration=other_duration)
53: 
54:     df = pd.DataFrame(rows)
55:     df["timestamp"] = df["timestamp"].astype(str)
56:     ctx = make_context(run_dir, df, {})
57:     result = Plugin().run(ctx)
58:     assert result.status == "ok"
59:     contention = [f for f in result.findings if f.get("kind") == "close_cycle_contention"]
60:     assert contention
61:     assert any(f.get("process") == "qemail" for f in contention)
````

## File: tests/plugins/test_close_cycle_revenue_compression.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_close_cycle_revenue_compression.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def _make_rows(days, process, queue_wait_days=7, service_days=1):
10:     rows = []
11:     for day in days:
12:         queue = dt.datetime(2026, 1, day, 0, 0, 0)
13:         start = queue + dt.timedelta(days=queue_wait_days)
14:         end = start + dt.timedelta(days=service_days)
15:         rows.append(
16:             {
17:                 "PROCESS": process,
18:                 "QUEUE_DT": queue.isoformat(),
19:                 "START_DT": start.isoformat(),
20:                 "END_DT": end.isoformat(),
21:                 "HOST": "h1",
22:             }
23:         )
24:     return rows
25: 
26: 
27: def test_revenue_compression_model_required_scale(run_dir):
28:     rows = []
29:     rows += _make_rows([20, 21], "revenue")
30:     df = pd.DataFrame(rows)
31: 
32:     settings = {
33:         "close_window_mode": "override",
34:         "close_cycle_start_day": 20,
35:         "close_cycle_end_day": 27,
36:         "target_days": 7,
37:         "max_scale": 5,
38:         "revenue_process_patterns": ["revenue"],
39:         "min_month_rows": 1,
40:     }
41:     ctx = make_context(run_dir, df, settings)
42:     result = Plugin().run(ctx)
43: 
44:     assert result.status == "ok"
45:     assert result.findings
46:     finding = result.findings[0]
47:     assert finding["decision"] == "modeled"
48:     required_scale = finding.get("worst_month_required_scale")
49:     assert required_scale is not None
50:     assert 1.15 < required_scale < 1.2
51: 
52: 
53: def test_revenue_compression_not_applicable_when_no_match(run_dir):
54:     rows = []
55:     rows += _make_rows([20, 21], "other")
56:     df = pd.DataFrame(rows)
57: 
58:     settings = {
59:         "close_window_mode": "override",
60:         "close_cycle_start_day": 20,
61:         "close_cycle_end_day": 27,
62:         "target_days": 7,
63:         "max_scale": 5,
64:         "revenue_process_patterns": ["revenue"],
65:     }
66:     ctx = make_context(run_dir, df, settings)
67:     result = Plugin().run(ctx)
68: 
69:     assert result.status == "ok"
70:     assert result.findings
71:     assert result.findings[0]["decision"] == "not_applicable"
````

## File: tests/plugins/test_concurrency_reconstruction.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_concurrency_reconstruction.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_concurrency_reconstruction_peaks(run_dir):
10:     rows = [
11:         {
12:             "HOST": "h1",
13:             "START_DT": dt.datetime(2026, 1, 1, 0, 0, 0),
14:             "END_DT": dt.datetime(2026, 1, 1, 0, 10, 0),
15:         },
16:         {
17:             "HOST": "h1",
18:             "START_DT": dt.datetime(2026, 1, 1, 0, 5, 0),
19:             "END_DT": dt.datetime(2026, 1, 1, 0, 15, 0),
20:         },
21:         {
22:             "HOST": "h1",
23:             "START_DT": dt.datetime(2026, 1, 1, 0, 12, 0),
24:             "END_DT": dt.datetime(2026, 1, 1, 0, 20, 0),
25:         },
26:     ]
27:     df = pd.DataFrame(rows)
28:     df["START_DT"] = df["START_DT"].astype(str)
29:     df["END_DT"] = df["END_DT"].astype(str)
30: 
31:     ctx = make_context(run_dir, df, {})
32:     result = Plugin().run(ctx)
33: 
34:     assert result.status == "ok"
35:     findings = [
36:         f
37:         for f in result.findings
38:         if f.get("kind") == "concurrency_summary" and f.get("host") == "h1"
39:     ]
40:     assert findings
41:     assert findings[0]["peak_concurrency"] == 2
````

## File: tests/plugins/test_dependency_resolution_join.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_dependency_resolution_join.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_dependency_resolution_join_emits_lag_summary(run_dir):
 8:     df = pd.DataFrame(
 9:         [
10:             {
11:                 "dep_id": "ROOT",
12:                 "process_id": "A",
13:                 "start_ts": "2026-01-01 00:00:00",
14:                 "end_ts": "2026-01-01 00:10:00",
15:             },
16:             {
17:                 "dep_id": "A",
18:                 "process_id": "B",
19:                 "start_ts": "2026-01-01 00:10:00",
20:                 "end_ts": "2026-01-01 00:20:00",
21:             },
22:             {
23:                 "dep_id": "A",
24:                 "process_id": "C",
25:                 "start_ts": "2026-01-01 00:15:00",
26:                 "end_ts": "2026-01-01 00:25:00",
27:             },
28:         ]
29:     )
30:     df = df[["process_id", "dep_id", "start_ts", "end_ts"]]
31: 
32:     ctx = make_context(run_dir, df, {})
33:     result = Plugin().run(ctx)
34: 
35:     assert result.status == "ok"
36:     assert result.findings
37:     finding = result.findings[0]
38:     assert finding["kind"] == "dependency_lag_summary"
39:     assert finding["dependency_rows"] == 2
40:     assert finding["near_zero_ratio"] >= 0.5
````

## File: tests/plugins/test_determinism_discipline.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_determinism_discipline.plugin import Plugin
 4: from statistic_harness.core.types import PluginResult
 5: from statistic_harness.core.utils import now_iso
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_determinism_discipline_flags_missing_measurement(run_dir):
10:     df = pd.DataFrame({"value": [1]})
11:     ctx = make_context(run_dir, df, {})
12: 
13:     ctx.storage.create_run(
14:         run_id=ctx.run_id,
15:         created_at=now_iso(),
16:         status="running",
17:         upload_id="local",
18:         input_filename="fixture.csv",
19:         canonical_path=str(ctx.run_dir / "dataset" / "canonical.csv"),
20:         settings={},
21:         error=None,
22:         run_seed=ctx.run_seed,
23:         project_id=ctx.project_id,
24:         dataset_id=ctx.dataset_id,
25:         dataset_version_id=ctx.dataset_version_id,
26:         input_hash=ctx.input_hash,
27:     )
28: 
29:     ctx.storage.save_plugin_result(
30:         ctx.run_id,
31:         "analysis_fake_missing",
32:         None,
33:         now_iso(),
34:         None,
35:         None,
36:         None,
37:         PluginResult(
38:             "ok",
39:             "missing measurement",
40:             {},
41:             [{"kind": "fake"}],
42:             [],
43:             None,
44:         ),
45:     )
46:     ctx.storage.save_plugin_result(
47:         ctx.run_id,
48:         "analysis_fake_modeled",
49:         None,
50:         now_iso(),
51:         None,
52:         None,
53:         None,
54:         PluginResult(
55:             "ok",
56:             "modeled missing assumption",
57:             {},
58:             [{"kind": "fake", "measurement_type": "modeled"}],
59:             [],
60:             None,
61:         ),
62:     )
63:     ctx.storage.save_plugin_result(
64:         ctx.run_id,
65:         "analysis_fake_good",
66:         None,
67:         now_iso(),
68:         None,
69:         None,
70:         None,
71:         PluginResult(
72:             "ok",
73:             "measured",
74:             {},
75:             [{"kind": "fake", "measurement_type": "measured"}],
76:             [],
77:             None,
78:         ),
79:     )
80: 
81:     result = Plugin().run(ctx)
82: 
83:     assert result.status == "ok"
84:     assert result.findings
85:     summary = result.findings[0]
86:     assert summary["kind"] == "determinism_discipline_summary"
87:     assert summary["missing_measurement_type"] >= 1
88:     assert summary["modeled_missing_assumption"] >= 1
89:     violations = [f for f in result.findings if f.get("kind") == "determinism_violation"]
90:     assert any(v.get("issue") == "missing_measurement_type" for v in violations)
91:     assert any(v.get("issue") == "modeled_missing_assumption" for v in violations)
````

## File: tests/plugins/test_eventlog_fixture_smoke.py
````python
 1: from pathlib import Path
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_attribution.plugin import Plugin as Attribution
 6: from plugins.analysis_capacity_scaling.plugin import Plugin as CapacityScaling
 7: from plugins.analysis_chain_makespan.plugin import Plugin as ChainMakespan
 8: from plugins.analysis_concurrency_reconstruction.plugin import Plugin as Concurrency
 9: from plugins.analysis_dependency_resolution_join.plugin import Plugin as DependencyJoin
10: from plugins.analysis_percentile_analysis.plugin import Plugin as Percentile
11: from plugins.analysis_process_sequence.plugin import Plugin as ProcessSequence
12: from plugins.analysis_sequence_classification.plugin import Plugin as SequenceClassification
13: from plugins.analysis_tail_isolation.plugin import Plugin as TailIsolation
14: from tests.conftest import make_context
15: 
16: 
17: def test_quorum_fixture_smoke_for_eventlog_plugins(run_dir):
18:     df = pd.read_csv(Path("tests/fixtures/quorum_close_cycle.csv"))
19:     ctx = make_context(run_dir, df, {})
20: 
21:     plugins = [
22:         Attribution,
23:         CapacityScaling,
24:         ChainMakespan,
25:         Concurrency,
26:         DependencyJoin,
27:         Percentile,
28:         ProcessSequence,
29:         SequenceClassification,
30:         TailIsolation,
31:     ]
32: 
33:     for plugin_cls in plugins:
34:         result = plugin_cls().run(ctx)
35:         assert result.status in {"ok", "skipped"}
````

## File: tests/plugins/test_percentile_analysis.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_percentile_analysis.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_percentile_analysis_outputs_stats(run_dir):
10:     rows = [
11:         {
12:             "process": "alpha",
13:             "module": "m1",
14:             "queue_time": dt.datetime(2026, 1, 1, 8, 0, 0),
15:             "start_time": dt.datetime(2026, 1, 1, 8, 0, 30),
16:             "end_time": dt.datetime(2026, 1, 1, 8, 2, 0),
17:         },
18:         {
19:             "process": "alpha",
20:             "module": "m1",
21:             "queue_time": dt.datetime(2026, 1, 1, 9, 0, 0),
22:             "start_time": dt.datetime(2026, 1, 1, 9, 1, 0),
23:             "end_time": dt.datetime(2026, 1, 1, 9, 4, 0),
24:         },
25:         {
26:             "process": "beta",
27:             "module": "m2",
28:             "queue_time": dt.datetime(2026, 1, 1, 10, 0, 0),
29:             "start_time": dt.datetime(2026, 1, 1, 10, 0, 10),
30:             "end_time": dt.datetime(2026, 1, 1, 10, 0, 40),
31:         },
32:     ]
33:     df = pd.DataFrame(rows)
34:     for col in ["queue_time", "start_time", "end_time"]:
35:         df[col] = df[col].astype(str)
36: 
37:     ctx = make_context(run_dir, df, {})
38:     result = Plugin().run(ctx)
39: 
40:     assert result.status == "ok"
41:     stats = [f for f in result.findings if f.get("kind") == "percentile_stats"]
42:     assert stats
43:     assert any(f.get("process") == "alpha" for f in stats)
44:     assert all("eligible_wait_p95" in f for f in stats)
````

## File: tests/plugins/test_process_sequence.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_process_sequence.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_process_sequence_plugin(run_dir):
 8:     df = pd.DataFrame(
 9:         {
10:             "case_id": [1, 1, 2, 2],
11:             "activity": ["A", "B", "A", "C"],
12:             "timestamp": [1, 2, 1, 2],
13:         }
14:     )
15:     ctx = make_context(run_dir, df, {})
16:     result = Plugin().run(ctx)
17:     assert result.status == "ok"
18:     assert any(f["kind"] == "process_variant" for f in result.findings)
````

## File: tests/plugins/test_profile_eventlog.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.profile_eventlog.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_profile_eventlog_role_inference(run_dir):
 8:     df = pd.DataFrame(
 9:         {
10:             "QUEUE_DT": [
11:                 "2026-01-20T01:00:00",
12:                 "2026-01-20T02:00:00",
13:                 "2026-01-21T01:30:00",
14:             ],
15:             "START_DT": [
16:                 "2026-01-20T01:01:00",
17:                 "2026-01-20T02:05:00",
18:                 "2026-01-21T01:45:00",
19:             ],
20:             "END_DT": [
21:                 "2026-01-20T01:04:00",
22:                 "2026-01-20T02:08:00",
23:                 "2026-01-21T01:50:00",
24:             ],
25:             "PROCESS_ID": ["qemail", "qpec", "qpec"],
26:             "USER_ID": ["u1", "u2", "u1"],
27:         }
28:     )
29:     ctx = make_context(run_dir, df, {})
30:     result = Plugin().run(ctx)
31:     assert result.status == "ok"
32:     candidates = ctx.storage.fetch_dataset_role_candidates(ctx.dataset_version_id)
33:     assert any(c["role"] == "start_time" for c in candidates)
34:     columns = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
35:     assert any(col.get("role") in {"queue_time", "start_time", "end_time"} for col in columns)
````

## File: tests/plugins/test_queue_delay_decomposition.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_queue_delay_decomposition.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_queue_delay_decomposition_targets_qemail(run_dir):
10:     rows = []
11:     # Close cycle days
12:     for day in range(20, 23):
13:         queue_ts = dt.datetime(2026, 1, day, 8, 0, 0)
14:         for idx in range(3):
15:             rows.append(
16:                 {
17:                     "process": "qemail",
18:                     "queue_dt": queue_ts + dt.timedelta(minutes=idx),
19:                     "start_dt": queue_ts + dt.timedelta(minutes=65 + idx),
20:                     "dep_id": None,
21:                 }
22:             )
23:     # Open cycle days
24:     for day in range(10, 12):
25:         queue_ts = dt.datetime(2026, 1, day, 8, 0, 0)
26:         for idx in range(2):
27:             rows.append(
28:                 {
29:                     "process": "qemail",
30:                     "queue_dt": queue_ts + dt.timedelta(minutes=idx),
31:                     "start_dt": queue_ts + dt.timedelta(minutes=10 + idx),
32:                     "dep_id": None,
33:                 }
34:             )
35:     # Another process
36:     rows.append(
37:         {
38:             "process": "other",
39:             "queue_dt": dt.datetime(2026, 1, 20, 9, 0, 0),
40:             "start_dt": dt.datetime(2026, 1, 20, 9, 5, 0),
41:             "dep_id": None,
42:         }
43:     )
44: 
45:     df = pd.DataFrame(rows)
46:     df["queue_dt"] = df["queue_dt"].astype(str)
47:     df["start_dt"] = df["start_dt"].astype(str)
48: 
49:     ctx = make_context(run_dir, df, {})
50:     result = Plugin().run(ctx)
51:     assert result.status == "ok"
52: 
53:     findings = [
54:         f
55:         for f in result.findings
56:         if f.get("kind") == "eligible_wait_process_stats"
57:         and f.get("process") == "qemail"
58:     ]
59:     assert findings
60:     impact = [f for f in result.findings if f.get("kind") == "eligible_wait_impact"]
61:     assert impact
````

## File: tests/plugins/test_sequence_classification.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_sequence_classification.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_sequence_classification_detects_dependency_rows(run_dir):
 8:     df = pd.DataFrame(
 9:         {
10:             "process": ["alpha", "alpha", "beta", "beta"],
11:             "dep_process_queue_id": [None, "123", None, "999"],
12:         }
13:     )
14:     ctx = make_context(run_dir, df, {})
15:     result = Plugin().run(ctx)
16:     assert result.status == "ok"
17:     findings = [f for f in result.findings if f.get("kind") == "sequence_classification"]
18:     assert findings
19:     assert any(f.get("process") == "alpha" for f in findings)
````

## File: tests/plugins/test_tail_isolation.py
````python
 1: import datetime as dt
 2: 
 3: import pandas as pd
 4: 
 5: from plugins.analysis_tail_isolation.plugin import Plugin
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_tail_isolation_process_dimension(run_dir):
10:     rows = [
11:         {
12:             "process": "alpha",
13:             "queue_time": dt.datetime(2026, 1, 1, 8, 0, 0),
14:             "start_time": dt.datetime(2026, 1, 1, 8, 2, 0),
15:         },
16:         {
17:             "process": "alpha",
18:             "queue_time": dt.datetime(2026, 1, 1, 9, 0, 0),
19:             "start_time": dt.datetime(2026, 1, 1, 9, 3, 0),
20:         },
21:         {
22:             "process": "beta",
23:             "queue_time": dt.datetime(2026, 1, 1, 10, 0, 0),
24:             "start_time": dt.datetime(2026, 1, 1, 10, 0, 30),
25:         },
26:     ]
27:     df = pd.DataFrame(rows)
28:     df["queue_time"] = df["queue_time"].astype(str)
29:     df["start_time"] = df["start_time"].astype(str)
30: 
31:     ctx = make_context(run_dir, df, {"wait_threshold_seconds": 60})
32:     result = Plugin().run(ctx)
33: 
34:     assert result.status == "ok"
35:     assert result.metrics["tail_rows"] > 0
36:     findings = [
37:         f
38:         for f in result.findings
39:         if f.get("kind") == "tail_isolation" and f.get("dimension") == "process"
40:     ]
41:     assert findings
42:     assert findings[0]["key"] == "alpha"
````

## File: tests/plugins/test_transform_normalize_mixed.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.transform_normalize_mixed.plugin import Plugin
 4: from statistic_harness.core.utils import quote_identifier
 5: from tests.conftest import make_context
 6: 
 7: 
 8: def test_transform_normalize_mixed_basic(run_dir):
 9:     df = pd.DataFrame(
10:         {
11:             "Name": [" Foo ", "Bar"],
12:             "Amount": ["1,000", "2"],
13:             "ID": ["001", "002"],
14:         }
15:     )
16:     ctx = make_context(run_dir, df, {})
17:     result = Plugin().run(ctx)
18:     assert result.status == "ok"
19: 
20:     dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
21:     assert dataset_template
22:     assert dataset_template["status"] == "ready"
23:     template = ctx.storage.fetch_template(int(dataset_template["template_id"]))
24:     assert template
25:     fields = ctx.storage.fetch_template_fields(int(dataset_template["template_id"]))
26:     assert len(fields) == 3
27: 
28:     name_col = fields[0]["safe_name"]
29:     amount_col = fields[1]["safe_name"]
30:     id_col = fields[2]["safe_name"]
31: 
32:     with ctx.storage.connection() as conn:
33:         cur = conn.execute(
34:             f"""
35:             SELECT {quote_identifier(name_col)}, {quote_identifier(amount_col)}, {quote_identifier(id_col)}
36:             FROM {quote_identifier(template['table_name'])}
37:             WHERE dataset_version_id = ? AND row_index = ?
38:             """,
39:             (ctx.dataset_version_id, 0),
40:         )
41:         row = cur.fetchone()
42:         assert row is not None
43:         assert row[0] == "foo"
44:         assert isinstance(row[1], (int, float))
45:         assert row[2] == "001"
````

## File: tests/test_append_only.py
````python
 1: import sqlite3
 2: 
 3: import pytest
 4: 
 5: from statistic_harness.core.storage import Storage
 6: from statistic_harness.core.utils import quote_identifier
 7: 
 8: 
 9: def test_append_only_triggers(tmp_path):
10:     storage = Storage(tmp_path / "state.sqlite")
11:     table_name = "dataset_test"
12:     columns = [
13:         {
14:             "column_id": 1,
15:             "safe_name": "c1",
16:             "original_name": "a",
17:             "dtype": "int64",
18:             "sqlite_type": "INTEGER",
19:         }
20:     ]
21: 
22:     with storage.connection() as conn:
23:         storage.create_dataset_table(table_name, columns, conn)
24:         storage.add_append_only_triggers(table_name, conn)
25:         storage.insert_dataset_rows(table_name, ["c1"], [(0, None, 1)], conn)
26: 
27:         with pytest.raises(sqlite3.IntegrityError):
28:             conn.execute(
29:                 f"DELETE FROM {quote_identifier(table_name)} WHERE row_index = 0"
30:             )
31: 
32:         with pytest.raises(sqlite3.IntegrityError):
33:             conn.execute(
34:                 f"UPDATE {quote_identifier(table_name)} SET c1 = 2 WHERE row_index = 0"
35:             )
````

## File: tests/test_auth.py
````python
 1: from statistic_harness.core.auth import (
 2:     generate_api_key,
 3:     generate_session_token,
 4:     hash_password,
 5:     hash_token,
 6:     verify_password,
 7: )
 8: from statistic_harness.core.storage import Storage
 9: from statistic_harness.core.utils import now_iso
10: 
11: 
12: def test_password_hash_roundtrip():
13:     hashed = hash_password("secret")
14:     assert verify_password("secret", hashed)
15:     assert not verify_password("nope", hashed)
16: 
17: 
18: def test_auth_storage_records(tmp_path):
19:     storage = Storage(tmp_path / "state.sqlite")
20:     user_id = storage.create_user("user@example.com", hash_password("pw"), None, True, now_iso())
21:     storage.ensure_membership(user_id, "admin", now_iso())
22: 
23:     session_token = generate_session_token()
24:     storage.create_session(user_id, hash_token(session_token), now_iso(), now_iso())
25:     session = storage.fetch_session_by_hash(hash_token(session_token))
26:     assert session is not None
27:     assert int(session["user_id"]) == user_id
28: 
29:     api_key = generate_api_key()
30:     key_id = storage.create_api_key(user_id, hash_token(api_key), "cli", now_iso())
31:     key_row = storage.fetch_api_key_by_hash(hash_token(api_key))
32:     assert key_row is not None
33:     assert int(key_row["key_id"]) == key_id
````

## File: tests/test_dedupe_runs.py
````python
 1: from pathlib import Path
 2: 
 3: import pandas as pd
 4: 
 5: from statistic_harness.core.pipeline import Pipeline
 6: 
 7: 
 8: def test_dedupe_by_content(tmp_path, monkeypatch):
 9:     appdata = tmp_path / "appdata"
10:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
11:     pipeline = Pipeline(appdata, Path("plugins"))
12: 
13:     run_id_1 = pipeline.run(
14:         Path("tests/fixtures/synth_linear.csv"), ["profile_basic"], {}, 42
15:     )
16:     run_id_2 = pipeline.run(
17:         Path("tests/fixtures/synth_linear.csv"), ["profile_basic"], {}, 42
18:     )
19: 
20:     run_1 = pipeline.storage.fetch_run(run_id_1)
21:     run_2 = pipeline.storage.fetch_run(run_id_2)
22: 
23:     assert run_1
24:     assert run_2
25:     assert run_1["project_id"] == run_2["project_id"]
26:     assert run_1["dataset_id"] == run_2["dataset_id"]
27:     assert run_1["dataset_version_id"] == run_2["dataset_version_id"]
28: 
29:     other_path = tmp_path / "other.csv"
30:     pd.DataFrame({"a": [1, 2], "b": [3, 5]}).to_csv(other_path, index=False)
31:     run_id_3 = pipeline.run(other_path, ["profile_basic"], {}, 42)
32:     run_3 = pipeline.storage.fetch_run(run_id_3)
33:     assert run_3
34:     assert run_3["dataset_id"] != run_1["dataset_id"]
````

## File: tests/test_determinism.py
````python
 1: import json
 2: from pathlib import Path
 3: 
 4: from statistic_harness.core.pipeline import Pipeline
 5: 
 6: 
 7: def test_report_determinism(tmp_path, monkeypatch):
 8:     appdata_1 = tmp_path / "appdata_1"
 9:     appdata_2 = tmp_path / "appdata_2"
10: 
11:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata_1))
12:     pipeline_1 = Pipeline(appdata_1, Path("plugins"))
13:     run_id_1 = pipeline_1.run(
14:         Path("tests/fixtures/synth_linear.csv"), ["profile_basic"], {}, 123
15:     )
16:     report_1 = json.loads(
17:         (appdata_1 / "runs" / run_id_1 / "report.json").read_text(encoding="utf-8")
18:     )
19: 
20:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata_2))
21:     pipeline_2 = Pipeline(appdata_2, Path("plugins"))
22:     run_id_2 = pipeline_2.run(
23:         Path("tests/fixtures/synth_linear.csv"), ["profile_basic"], {}, 123
24:     )
25:     report_2 = json.loads(
26:         (appdata_2 / "runs" / run_id_2 / "report.json").read_text(encoding="utf-8")
27:     )
28: 
29:     assert report_1["input"] == report_2["input"]
30:     assert report_1["plugins"] == report_2["plugins"]
````

## File: tests/test_feature_flags.py
````python
 1: from pathlib import Path
 2: 
 3: from statistic_harness.core.tenancy import get_tenant_context, tenancy_enabled
 4: from statistic_harness.core.utils import auth_enabled, vector_store_enabled
 5: 
 6: 
 7: def test_default_flags_disabled(monkeypatch):
 8:     monkeypatch.delenv("STAT_HARNESS_ENABLE_AUTH", raising=False)
 9:     monkeypatch.delenv("STAT_HARNESS_ENABLE_VECTOR_STORE", raising=False)
10:     monkeypatch.delenv("STAT_HARNESS_ENABLE_TENANCY", raising=False)
11:     assert not auth_enabled()
12:     assert not vector_store_enabled()
13:     assert not tenancy_enabled()
14: 
15: 
16: def test_auth_flag_enabled(monkeypatch):
17:     monkeypatch.setenv("STAT_HARNESS_ENABLE_AUTH", "1")
18:     assert auth_enabled()
19: 
20: 
21: def test_vector_store_flag_enabled(monkeypatch):
22:     monkeypatch.setenv("STAT_HARNESS_ENABLE_VECTOR_STORE", "true")
23:     assert vector_store_enabled()
24: 
25: 
26: def test_tenant_context_paths(monkeypatch, tmp_path):
27:     monkeypatch.delenv("STAT_HARNESS_ENABLE_TENANCY", raising=False)
28:     ctx = get_tenant_context("alpha", tmp_path)
29:     assert ctx.tenant_root == tmp_path
30: 
31:     monkeypatch.setenv("STAT_HARNESS_ENABLE_TENANCY", "1")
32:     ctx = get_tenant_context("alpha", tmp_path)
33:     assert ctx.tenant_root == Path(tmp_path) / "tenants" / "alpha"
````

## File: tests/test_migrations.py
````python
 1: from pathlib import Path
 2: import shutil
 3: import sqlite3
 4: 
 5: from statistic_harness.core.migrations import MIGRATIONS, run_migrations
 6: 
 7: 
 8: FIXTURE_DIR = Path("tests/fixtures/db")
 9: 
10: 
11: def _expected_tables(version: int) -> dict[str, int]:
12:     expected = {
13:         "runs": 1,
14:         "plugin_results": 1,
15:         "plugin_results_v2": 1,
16:     }
17:     if version >= 2:
18:         expected.update(
19:             {
20:                 "projects": 1,
21:                 "datasets": 1,
22:                 "dataset_versions": 1,
23:                 "dataset_columns": 1,
24:             }
25:         )
26:     if version >= 4:
27:         expected.update(
28:             {
29:                 "parameter_entities": 1,
30:                 "parameter_kv": 1,
31:                 "row_parameter_link": 1,
32:                 "entities": 2,
33:                 "edges": 1,
34:             }
35:         )
36:     if version >= 5:
37:         expected.update(
38:             {
39:                 "analysis_jobs": 1,
40:                 "deliveries": 1,
41:             }
42:         )
43:     if version >= 6:
44:         expected.update(
45:             {
46:                 "raw_formats": 1,
47:                 "raw_format_notes": 1,
48:                 "templates": 1,
49:                 "template_fields": 1,
50:                 "dataset_templates": 1,
51:                 "template_conversions": 1,
52:             }
53:         )
54:     if version >= 7:
55:         expected.update({"raw_format_mappings": 1})
56:     if version >= 8:
57:         expected.update({"plugin_executions": 1})
58:     if version >= 10:
59:         expected.update({"known_issue_sets": 1, "known_issues": 1})
60:     if version >= 11:
61:         expected.update({"dataset_role_candidates": 1, "project_role_overrides": 1})
62:     if version >= 13:
63:         expected.update({"project_plugin_settings": 0})
64:     if version >= 15:
65:         expected.update({"pii_salts": 0, "pii_entities": 0})
66:     if version >= 16:
67:         expected.update({"tenants": 1})
68:     if version >= 17:
69:         expected.update(
70:             {
71:                 "users": 0,
72:                 "tenant_memberships": 0,
73:                 "user_sessions": 0,
74:                 "api_keys": 0,
75:             }
76:         )
77:     if version >= 18:
78:         expected.update({"vector_collections": 0})
79:     return expected
80: 
81: 
82: def test_migrations_from_golden(tmp_path):
83:     total_versions = len(MIGRATIONS)
84:     for version in range(1, total_versions + 1):
85:         fixture = FIXTURE_DIR / f"v{version}.sqlite"
86:         assert fixture.exists()
87:         db_path = tmp_path / f"v{version}.sqlite"
88:         shutil.copy(fixture, db_path)
89:         conn = sqlite3.connect(db_path)
90:         run_migrations(conn)
91:         cur = conn.execute("PRAGMA user_version")
92:         assert int(cur.fetchone()[0]) == total_versions
93:         expected = _expected_tables(version)
94:         for table, min_count in expected.items():
95:             cur = conn.execute(f"SELECT COUNT(*) FROM {table}")
96:             count = int(cur.fetchone()[0])
97:             assert count >= min_count
98:         conn.close()
````

## File: tests/test_missing_plugins.py
````python
 1: from pathlib import Path
 2: from dataclasses import replace
 3: 
 4: from statistic_harness.core.pipeline import Pipeline
 5: 
 6: 
 7: def _filtered_specs(pipeline: Pipeline, exclude: set[str]) -> list:
 8:     specs = pipeline.manager.discover()
 9:     return [spec for spec in specs if spec.plugin_id not in exclude]
10: 
11: 
12: def test_missing_planner_records_error(tmp_path, monkeypatch):
13:     appdata = tmp_path / "appdata"
14:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
15:     pipeline = Pipeline(appdata, Path("plugins"))
16:     filtered = _filtered_specs(pipeline, {"planner_basic"})
17:     monkeypatch.setattr(pipeline.manager, "discover", lambda: filtered)
18:     run_id = pipeline.run(
19:         Path("tests/fixtures/synth_linear.csv"), [], {}, 123
20:     )
21:     results = pipeline.storage.fetch_plugin_results(run_id)
22:     planner = next(
23:         row for row in results if row["plugin_id"] == "planner_basic"
24:     )
25:     assert planner["status"] == "error"
26: 
27: 
28: def test_missing_report_still_writes_report(tmp_path, monkeypatch):
29:     appdata = tmp_path / "appdata"
30:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
31:     pipeline = Pipeline(appdata, Path("plugins"))
32:     filtered = _filtered_specs(pipeline, {"report_bundle"})
33:     monkeypatch.setattr(pipeline.manager, "discover", lambda: filtered)
34:     run_id = pipeline.run(
35:         Path("tests/fixtures/synth_linear.csv"), [], {}, 123
36:     )
37:     run_dir = appdata / "runs" / run_id
38:     assert (run_dir / "report.json").exists()
39:     assert (run_dir / "report.md").exists()
40:     results = pipeline.storage.fetch_plugin_results(run_id)
41:     report = next(
42:         row for row in results if row["plugin_id"] == "report_bundle"
43:     )
44:     assert report["status"] == "error"
45: 
46: 
47: def test_report_failure_still_writes_report(tmp_path, monkeypatch):
48:     appdata = tmp_path / "appdata"
49:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
50:     pipeline = Pipeline(appdata, Path("plugins"))
51:     specs = pipeline.manager.discover()
52:     broken_specs = []
53:     for spec in specs:
54:         if spec.plugin_id == "report_bundle":
55:             spec = replace(spec, entrypoint="missing.py:Plugin")
56:         broken_specs.append(spec)
57:     monkeypatch.setattr(pipeline.manager, "discover", lambda: broken_specs)
58:     run_id = pipeline.run(Path("tests/fixtures/synth_linear.csv"), [], {}, 123)
59:     run_dir = appdata / "runs" / run_id
60:     assert (run_dir / "report.json").exists()
61:     assert (run_dir / "report.md").exists()
````

## File: tests/test_parameter_normalization.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.profile_basic.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_parameter_normalization(run_dir):
 8:     df = pd.DataFrame(
 9:         {
10:             "params": ["a=1; b=2", "b=2; a=1"],
11:             "value": [10, 12],
12:         }
13:     )
14:     ctx = make_context(run_dir, df, {})
15:     result = Plugin().run(ctx)
16:     assert result.status == "ok"
17: 
18:     with ctx.storage.connection() as conn:
19:         entities = conn.execute("SELECT COUNT(*) FROM parameter_entities").fetchone()
20:         assert entities and int(entities[0]) == 1
21:         links = conn.execute(
22:             "SELECT COUNT(*) FROM row_parameter_link WHERE dataset_version_id = ?",
23:             (ctx.dataset_version_id,),
24:         ).fetchone()
25:         assert links and int(links[0]) == 2
26:         kv_rows = conn.execute(
27:             "SELECT COUNT(*) FROM parameter_kv WHERE key IN ('a', 'b')"
28:         ).fetchone()
29:         assert kv_rows and int(kv_rows[0]) >= 2
````

## File: tests/test_performance_smoke.py
````python
 1: import os
 2: import time
 3: from pathlib import Path
 4: 
 5: from statistic_harness.core.pipeline import Pipeline
 6: 
 7: 
 8: def test_performance_smoke(tmp_path, monkeypatch):
 9:     appdata = tmp_path / "appdata"
10:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
11:     pipeline = Pipeline(appdata, Path("plugins"))
12: 
13:     start = time.perf_counter()
14:     pipeline.run(Path("tests/fixtures/synth_linear.csv"), ["profile_basic"], {}, 123)
15:     elapsed = time.perf_counter() - start
16: 
17:     max_seconds = float(os.environ.get("STAT_HARNESS_PERF_MAX_SECONDS", "10"))
18:     assert elapsed < max_seconds
````

## File: tests/test_plugin_manifest_schema.py
````python
 1: from pathlib import Path
 2: 
 3: import pytest
 4: from jsonschema import ValidationError
 5: 
 6: from statistic_harness.core.plugin_manager import PluginManager
 7: 
 8: 
 9: def test_manifest_schema_validation(tmp_path: Path) -> None:
10:     schema_src = Path("docs/plugin_manifest.schema.json")
11:     schema_dst = tmp_path / "docs" / "plugin_manifest.schema.json"
12:     schema_dst.parent.mkdir(parents=True, exist_ok=True)
13:     schema_dst.write_text(schema_src.read_text(encoding="utf-8"), encoding="utf-8")
14: 
15:     plugins_dir = tmp_path / "plugins"
16:     plugin_dir = plugins_dir / "bad_plugin"
17:     plugin_dir.mkdir(parents=True, exist_ok=True)
18:     (plugin_dir / "config.schema.json").write_text("{}", encoding="utf-8")
19:     (plugin_dir / "output.schema.json").write_text("{}", encoding="utf-8")
20:     (plugin_dir / "plugin.yaml").write_text(
21:         """id: bad_plugin
22: name: Bad Plugin
23: version: 0.1.0
24: type: analysis
25: entrypoint: plugin.py:Plugin
26: """,
27:         encoding="utf-8",
28:     )
29: 
30:     manager = PluginManager(plugins_dir)
31:     specs = manager.discover()
32:     assert specs == []
33:     assert manager.discovery_errors
34:     assert any(
35:         "Invalid manifest" in err.message for err in manager.discovery_errors
36:     )
37: 
38: 
39: def test_plugin_config_and_output_validation() -> None:
40:     manager = PluginManager(Path("plugins"))
41:     specs = {spec.plugin_id: spec for spec in manager.discover()}
42:     ingest = specs["ingest_tabular"]
43: 
44:     manager.validate_config(
45:         ingest,
46:         {"encoding": "utf-8", "delimiter": None, "sheet_name": None, "chunk_size": 10},
47:     )
48:     with pytest.raises(ValidationError):
49:         manager.validate_config(ingest, {"chunk_size": "bad"})
50: 
51:     manager.validate_output(
52:         ingest,
53:         {
54:             "status": "ok",
55:             "summary": "ok",
56:             "metrics": {},
57:             "findings": [],
58:             "artifacts": [],
59:             "budget": {
60:                 "row_limit": None,
61:                 "sampled": False,
62:                 "time_limit_ms": None,
63:                 "cpu_limit_ms": None,
64:             },
65:             "error": None,
66:         },
67:     )
````

## File: tests/test_raw_format_mappings.py
````python
 1: import hashlib
 2: 
 3: from statistic_harness.core.storage import Storage
 4: from statistic_harness.core.utils import json_dumps, now_iso
 5: 
 6: 
 7: def test_raw_format_mapping_storage(tmp_path):
 8:     storage = Storage(tmp_path / "state.sqlite")
 9:     format_id = storage.ensure_raw_format(
10:         fingerprint="format-fp",
11:         name="Format A",
12:         created_at=now_iso(),
13:     )
14:     template_id = storage.create_template(
15:         name="Template A",
16:         fields=[{"name": "x", "dtype": "float"}],
17:         description=None,
18:         version=None,
19:         created_at=now_iso(),
20:     )
21:     mapping = {"x": "col_x"}
22:     mapping_hash = hashlib.sha256(
23:         json_dumps(mapping).encode("utf-8")
24:     ).hexdigest()
25:     storage.add_raw_format_mapping(
26:         format_id,
27:         template_id,
28:         json_dumps(mapping),
29:         mapping_hash,
30:         "note",
31:         now_iso(),
32:     )
33:     mappings = storage.list_raw_format_mappings(format_id)
34:     assert any(m["mapping_hash"] == mapping_hash for m in mappings)
````

## File: tests/test_row_trace.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.profile_basic.plugin import Plugin as ProfilePlugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_row_trace_with_parameters(run_dir):
 8:     df = pd.DataFrame({"params": ["k=v", "x=1"], "value": [1, 2]})
 9:     ctx = make_context(run_dir, df, {})
10:     ProfilePlugin().run(ctx)
11: 
12:     payload = ctx.storage.fetch_row_trace(ctx.dataset_version_id, 0)
13:     assert payload["rows"]
14:     row = payload["rows"][0]
15:     assert row["row_index"] == 0
16:     assert "params" in row["values"]
17:     assert row["parameters"]
````

## File: tests/test_sandbox.py
````python
  1: import socket
  2: import subprocess
  3: 
  4: import pytest
  5: 
  6: from statistic_harness.core.plugin_runner import (
  7:     FileSandbox,
  8:     _install_eval_guard,
  9:     _install_network_guard,
 10:     _install_pickle_guard,
 11:     _install_shell_guard,
 12: )
 13: 
 14: 
 15: def test_file_sandbox_blocks_disallowed_paths(tmp_path):
 16:     allowed_dir = tmp_path / "allowed"
 17:     allowed_dir.mkdir()
 18:     allowed_file = allowed_dir / "ok.txt"
 19:     allowed_file.write_text("ok", encoding="utf-8")
 20: 
 21:     blocked_dir = tmp_path / "blocked"
 22:     blocked_dir.mkdir()
 23:     blocked_file = blocked_dir / "no.txt"
 24:     blocked_file.write_text("no", encoding="utf-8")
 25: 
 26:     with FileSandbox([str(allowed_dir)], cwd=tmp_path):
 27:         assert allowed_file.read_text(encoding="utf-8") == "ok"
 28:         allowed_file.write_text("updated", encoding="utf-8")
 29:         with pytest.raises(PermissionError):
 30:             blocked_file.read_text(encoding="utf-8")
 31:         with pytest.raises(PermissionError):
 32:             blocked_file.write_text("nope", encoding="utf-8")
 33: 
 34: 
 35: def test_network_guard_blocks_socket():
 36:     orig_socket = socket.socket
 37:     orig_create = socket.create_connection
 38:     try:
 39:         _install_network_guard()
 40:         with pytest.raises(RuntimeError):
 41:             socket.socket()
 42:     finally:
 43:         socket.socket = orig_socket
 44:         socket.create_connection = orig_create
 45: 
 46: 
 47: def test_eval_guard_blocks_eval():
 48:     import builtins
 49:     from pathlib import Path
 50: 
 51:     orig_eval = builtins.eval
 52:     try:
 53:         root = Path(__file__).resolve().parents[1]
 54:         _install_eval_guard(root)
 55:         with pytest.raises(RuntimeError):
 56:             eval("1 + 1")  # noqa: S307 - intentional for guard test
 57:     finally:
 58:         builtins.eval = orig_eval
 59: 
 60: 
 61: 
 62: def test_pickle_guard_blocks_pickling():
 63:     import pickle
 64: 
 65:     orig_load = pickle.load
 66:     orig_loads = pickle.loads
 67:     orig_dump = pickle.dump
 68:     orig_dumps = pickle.dumps
 69:     orig_pickler = pickle.Pickler
 70:     orig_unpickler = pickle.Unpickler
 71:     try:
 72:         _install_pickle_guard()
 73:         with pytest.raises(RuntimeError):
 74:             pickle.dumps({"a": 1})
 75:         with pytest.raises(RuntimeError):
 76:             pickle.loads(b"test")
 77:     finally:
 78:         pickle.load = orig_load
 79:         pickle.loads = orig_loads
 80:         pickle.dump = orig_dump
 81:         pickle.dumps = orig_dumps
 82:         pickle.Pickler = orig_pickler
 83:         pickle.Unpickler = orig_unpickler
 84: 
 85: 
 86: def test_shell_guard_blocks_subprocess_and_os():
 87:     import os
 88: 
 89:     orig_system = os.system
 90:     orig_popen = os.popen
 91:     orig_run = subprocess.run
 92:     orig_call = subprocess.call
 93:     orig_check_call = subprocess.check_call
 94:     orig_check_output = subprocess.check_output
 95:     orig_popen_cls = subprocess.Popen
 96:     try:
 97:         _install_shell_guard()
 98:         with pytest.raises(RuntimeError):
 99:             os.system("echo blocked")
100:         with pytest.raises(RuntimeError):
101:             os.popen("echo blocked")
102:         with pytest.raises(RuntimeError):
103:             subprocess.run(["echo", "blocked"])  # noqa: S603 - intentional
104:     finally:
105:         os.system = orig_system
106:         os.popen = orig_popen
107:         subprocess.run = orig_run
108:         subprocess.call = orig_call
109:         subprocess.check_call = orig_check_call
110:         subprocess.check_output = orig_check_output
111:         subprocess.Popen = orig_popen_cls
````

## File: tests/test_template_combined_run.py
````python
 1: from pathlib import Path
 2: 
 3: from statistic_harness.core.pipeline import Pipeline
 4: from statistic_harness.core.utils import now_iso
 5: 
 6: 
 7: def test_template_combined_run(tmp_path, monkeypatch):
 8:     appdata = tmp_path / "appdata"
 9:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
10:     pipeline = Pipeline(appdata, Path("plugins"))
11: 
12:     run_id = pipeline.run(
13:         Path("tests/fixtures/synth_linear.csv"), ["profile_basic"], {}, 0
14:     )
15:     run_row = pipeline.storage.fetch_run(run_id)
16:     assert run_row
17:     dataset_version_id = run_row["dataset_version_id"]
18: 
19:     template_id = pipeline.storage.create_template(
20:         name="Linear",
21:         fields=[{"name": "x1", "dtype": "float"}, {"name": "y", "dtype": "float"}],
22:         description=None,
23:         version=None,
24:         created_at=now_iso(),
25:     )
26:     pipeline.run(
27:         None,
28:         ["transform_template"],
29:         {"transform_template": {"template_id": template_id, "mapping": {"x1": "x1", "y": "y"}}},
30:         0,
31:         dataset_version_id=dataset_version_id,
32:     )
33: 
34:     aggregate_id = pipeline.storage.ensure_template_aggregate_dataset(
35:         template_id, now_iso()
36:     )
37:     combined_run = pipeline.run(
38:         None, ["profile_basic"], {}, 0, dataset_version_id=aggregate_id
39:     )
40:     run_dir = appdata / "runs" / combined_run
41:     assert (run_dir / "report.json").exists()
````

## File: tests/test_template_conversion.py
````python
 1: from pathlib import Path
 2: 
 3: import pandas as pd
 4: 
 5: from statistic_harness.core.pipeline import Pipeline
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_template_conversion(run_dir):
10:     df = pd.DataFrame({"a": [1, 2], "b": ["x", "y"]})
11:     ctx = make_context(run_dir, df, {})
12: 
13:     template_id = ctx.storage.create_template(
14:         name="Basic",
15:         fields=[
16:             {"name": "alpha", "dtype": "int"},
17:             {"name": "beta", "dtype": "text"},
18:         ],
19:         description=None,
20:         version=None,
21:         created_at="now",
22:     )
23: 
24:     mapping = {"alpha": "a", "beta": "b"}
25:     pipeline = Pipeline(run_dir, Path("plugins"))
26:     run_id = pipeline.run(
27:         None,
28:         ["transform_template"],
29:         {"transform_template": {"template_id": template_id, "mapping": mapping}},
30:         0,
31:         dataset_version_id=ctx.dataset_version_id,
32:     )
33:     assert run_id
34:     dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
35:     assert dataset_template
36:     assert dataset_template["status"] == "ready"
````

## File: tests/test_template_filters.py
````python
 1: import json
 2: 
 3: import pandas as pd
 4: 
 5: from statistic_harness.core.dataset_io import resolve_dataset_accessor
 6: from statistic_harness.core.storage import Storage
 7: from statistic_harness.core.utils import now_iso
 8: 
 9: 
10: def test_template_filters_by_project(tmp_path):
11:     storage = Storage(tmp_path / "state.sqlite")
12:     template_id = storage.create_template(
13:         name="FilterTemplate",
14:         fields=[{"name": "value", "dtype": "float"}],
15:         description=None,
16:         version=None,
17:         created_at=now_iso(),
18:     )
19:     template = storage.fetch_template(template_id)
20:     assert template
21:     table_name = template["table_name"]
22:     fields = storage.fetch_template_fields(template_id)
23:     safe_cols = [field["safe_name"] for field in fields]
24: 
25:     project_a = "project_a"
26:     project_b = "project_b"
27:     storage.ensure_project(project_a, "fp_a", now_iso())
28:     storage.ensure_project(project_b, "fp_b", now_iso())
29:     storage.ensure_dataset("dataset_a", project_a, "fp_a", now_iso())
30:     storage.ensure_dataset("dataset_b", project_b, "fp_b", now_iso())
31:     storage.ensure_dataset_version("dv_a", "dataset_a", now_iso(), "dataset_dv_a", "hash_a")
32:     storage.ensure_dataset_version("dv_b", "dataset_b", now_iso(), "dataset_dv_b", "hash_b")
33: 
34:     rows = [
35:         ("dv_a", 0, json.dumps({"value": 1.0}), 1.0),
36:         ("dv_b", 0, json.dumps({"value": 2.0}), 2.0),
37:     ]
38:     storage.insert_template_rows(table_name, safe_cols, rows)
39: 
40:     aggregate_id = storage.ensure_template_aggregate_dataset(
41:         template_id, now_iso(), filters={"project_ids": [project_a]}
42:     )
43:     accessor, _ = resolve_dataset_accessor(storage, aggregate_id)
44:     df = accessor.load()
45:     assert isinstance(df, pd.DataFrame)
46:     assert len(df) == 1
47:     assert df["dataset_version_id"].iloc[0] == "dv_a"
````

## File: tests/test_tenant_isolation.py
````python
  1: from statistic_harness.core.storage import Storage
  2: from statistic_harness.core.tenancy import scope_identifier
  3: from statistic_harness.core.utils import now_iso
  4: 
  5: 
  6: def test_tenant_isolation_storage(tmp_path):
  7:     db_path = tmp_path / "state.sqlite"
  8:     tenant_a = "tenant_a"
  9:     tenant_b = "tenant_b"
 10:     storage_a = Storage(db_path, tenant_a)
 11:     storage_b = Storage(db_path, tenant_b)
 12: 
 13:     created_at = now_iso()
 14:     project_a = scope_identifier(tenant_a, "project")
 15:     dataset_a = scope_identifier(tenant_a, "dataset")
 16:     version_a = scope_identifier(tenant_a, "dv")
 17:     storage_a.ensure_project(project_a, project_a, created_at)
 18:     storage_a.ensure_dataset(dataset_a, project_a, dataset_a, created_at)
 19:     storage_a.ensure_dataset_version(
 20:         version_a, dataset_a, created_at, "table_a", "hash_a"
 21:     )
 22: 
 23:     project_b = scope_identifier(tenant_b, "project")
 24:     dataset_b = scope_identifier(tenant_b, "dataset")
 25:     version_b = scope_identifier(tenant_b, "dv")
 26:     storage_b.ensure_project(project_b, project_b, created_at)
 27:     storage_b.ensure_dataset(dataset_b, project_b, dataset_b, created_at)
 28:     storage_b.ensure_dataset_version(
 29:         version_b, dataset_b, created_at, "table_b", "hash_b"
 30:     )
 31: 
 32:     assert {row["project_id"] for row in storage_a.list_projects()} == {project_a}
 33:     assert {row["project_id"] for row in storage_b.list_projects()} == {project_b}
 34: 
 35:     assert {row["dataset_version_id"] for row in storage_a.list_dataset_versions()} == {
 36:         version_a
 37:     }
 38:     assert {row["dataset_version_id"] for row in storage_b.list_dataset_versions()} == {
 39:         version_b
 40:     }
 41: 
 42: 
 43: def test_tenant_isolation_uploads_runs(tmp_path):
 44:     db_path = tmp_path / "state.sqlite"
 45:     tenant_a = "tenant_a"
 46:     tenant_b = "tenant_b"
 47:     storage_a = Storage(db_path, tenant_a)
 48:     storage_b = Storage(db_path, tenant_b)
 49: 
 50:     created_at = now_iso()
 51:     project_a = scope_identifier(tenant_a, "project")
 52:     dataset_a = scope_identifier(tenant_a, "dataset")
 53:     version_a = scope_identifier(tenant_a, "dv")
 54:     storage_a.ensure_project(project_a, project_a, created_at)
 55:     storage_a.ensure_dataset(dataset_a, project_a, dataset_a, created_at)
 56:     storage_a.ensure_dataset_version(
 57:         version_a, dataset_a, created_at, "table_a", "hash_a"
 58:     )
 59: 
 60:     project_b = scope_identifier(tenant_b, "project")
 61:     dataset_b = scope_identifier(tenant_b, "dataset")
 62:     version_b = scope_identifier(tenant_b, "dv")
 63:     storage_b.ensure_project(project_b, project_b, created_at)
 64:     storage_b.ensure_dataset(dataset_b, project_b, dataset_b, created_at)
 65:     storage_b.ensure_dataset_version(
 66:         version_b, dataset_b, created_at, "table_b", "hash_b"
 67:     )
 68: 
 69:     upload_id_a = "upload_a"
 70:     upload_id_b = "upload_b"
 71:     storage_a.create_upload(upload_id_a, "a.csv", 10, "sha_a", created_at)
 72:     storage_b.create_upload(upload_id_b, "b.csv", 12, "sha_b", created_at)
 73: 
 74:     run_id_a = "run_a"
 75:     run_id_b = "run_b"
 76:     storage_a.create_run(
 77:         run_id=run_id_a,
 78:         created_at=created_at,
 79:         status="completed",
 80:         upload_id=upload_id_a,
 81:         input_filename="a.csv",
 82:         canonical_path="path_a",
 83:         settings={},
 84:         error=None,
 85:         run_seed=0,
 86:         project_id=project_a,
 87:         dataset_id=dataset_a,
 88:         dataset_version_id=version_a,
 89:         input_hash="sha_a",
 90:     )
 91:     storage_b.create_run(
 92:         run_id=run_id_b,
 93:         created_at=created_at,
 94:         status="completed",
 95:         upload_id=upload_id_b,
 96:         input_filename="b.csv",
 97:         canonical_path="path_b",
 98:         settings={},
 99:         error=None,
100:         run_seed=0,
101:         project_id=project_b,
102:         dataset_id=dataset_b,
103:         dataset_version_id=version_b,
104:         input_hash="sha_b",
105:     )
106: 
107:     assert storage_a.fetch_upload(upload_id_b) is None
108:     assert storage_b.fetch_upload(upload_id_a) is None
109:     assert {row["upload_id"] for row in storage_a.list_uploads()} == {upload_id_a}
110:     assert {row["upload_id"] for row in storage_b.list_uploads()} == {upload_id_b}
111: 
112:     assert storage_a.fetch_run(run_id_b) is None
113:     assert storage_b.fetch_run(run_id_a) is None
114:     assert {row["run_id"] for row in storage_a.list_runs_by_project(project_a)} == {
115:         run_id_a
116:     }
117:     assert {row["run_id"] for row in storage_b.list_runs_by_project(project_b)} == {
118:         run_id_b
119:     }
````

## File: tests/test_ui_template_results.py
````python
 1: import asyncio
 2: import importlib
 3: 
 4: from starlette.requests import Request
 5: 
 6: 
 7: def test_template_results_filters(monkeypatch, tmp_path):
 8:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(tmp_path / "appdata"))
 9:     from statistic_harness.ui import server as server_mod
10: 
11:     server_mod = importlib.reload(server_mod)
12:     template_id = server_mod.pipeline.storage.create_template(
13:         name="FilterUI",
14:         fields=[{"name": "value", "dtype": "float"}],
15:         description=None,
16:         version=None,
17:         created_at=server_mod.now_iso(),
18:     )
19: 
20:     query = (
21:         b"project_ids=b,a&raw_format_ids=2,1&created_after=2026-01-01T00:00:00Z"
22:     )
23:     scope = {
24:         "type": "http",
25:         "method": "GET",
26:         "path": f"/templates/{template_id}/results",
27:         "query_string": query,
28:         "headers": [],
29:     }
30: 
31:     async def render_body() -> str:
32:         request = Request(scope)
33:         response = await server_mod.template_results(request, template_id)
34:         return response.template.render(response.context)
35: 
36:     body = asyncio.run(render_body())
37:     assert "Active filters" in body
38:     assert "project_ids" in body
39:     assert "raw_format_ids" in body
40:     assert "created_after" in body
41:     assert 'value="b,a"' in body
42:     assert 'value="2,1"' in body
43:     assert 'value="2026-01-01T00:00:00Z"' in body
````

## File: tests/test_ui_vectors_api.py
````python
 1: import asyncio
 2: import importlib
 3: 
 4: from starlette.requests import Request
 5: 
 6: 
 7: def test_vectors_api_disabled(monkeypatch, tmp_path):
 8:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(tmp_path / "appdata"))
 9:     monkeypatch.delenv("STAT_HARNESS_ENABLE_VECTOR_STORE", raising=False)
10:     from statistic_harness.ui import server as server_mod
11: 
12:     server_mod = importlib.reload(server_mod)
13: 
14:     scope = {
15:         "type": "http",
16:         "method": "GET",
17:         "path": "/api/vectors/collections",
18:         "query_string": b"",
19:         "headers": [],
20:     }
21: 
22:     async def call_api() -> int:
23:         request = Request(scope)
24:         try:
25:             await server_mod.vector_collections_api(request)
26:         except Exception as exc:
27:             return getattr(exc, "status_code", 0)
28:         return 200
29: 
30:     status = asyncio.run(call_api())
31:     assert status == 400
````

## File: tests/test_ui_vectors.py
````python
 1: import asyncio
 2: import importlib
 3: 
 4: from starlette.requests import Request
 5: 
 6: 
 7: def test_vectors_page_disabled(monkeypatch, tmp_path):
 8:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(tmp_path / "appdata"))
 9:     monkeypatch.delenv("STAT_HARNESS_ENABLE_VECTOR_STORE", raising=False)
10:     from statistic_harness.ui import server as server_mod
11: 
12:     server_mod = importlib.reload(server_mod)
13: 
14:     scope = {
15:         "type": "http",
16:         "method": "GET",
17:         "path": "/vectors",
18:         "query_string": b"",
19:         "headers": [],
20:     }
21: 
22:     async def render_body() -> str:
23:         request = Request(scope)
24:         response = await server_mod.vectors_view(request)
25:         return response.template.render(response.context)
26: 
27:     body = asyncio.run(render_body())
28:     assert "Vector store is disabled" in body
29: 
30: 
31: def test_vectors_page_prefill(monkeypatch, tmp_path):
32:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(tmp_path / "appdata"))
33:     monkeypatch.delenv("STAT_HARNESS_ENABLE_VECTOR_STORE", raising=False)
34:     from statistic_harness.ui import server as server_mod
35: 
36:     server_mod = importlib.reload(server_mod)
37: 
38:     scope = {
39:         "type": "http",
40:         "method": "GET",
41:         "path": "/vectors",
42:         "query_string": b"collection=report_run&text=queue%20delay",
43:         "headers": [],
44:     }
45: 
46:     async def render_body() -> str:
47:         request = Request(scope)
48:         response = await server_mod.vectors_view(request, collection="report_run", text="queue delay")
49:         return response.template.render(response.context)
50: 
51:     body = asyncio.run(render_body())
52:     assert "queue delay" in body
````

## File: tests/test_upload_limits.py
````python
 1: from statistic_harness.core.utils import file_size_limit, max_upload_bytes
 2: 
 3: 
 4: def test_max_upload_bytes_env(monkeypatch):
 5:     monkeypatch.delenv("STAT_HARNESS_MAX_UPLOAD_BYTES", raising=False)
 6:     assert max_upload_bytes() is None
 7: 
 8:     monkeypatch.setenv("STAT_HARNESS_MAX_UPLOAD_BYTES", "0")
 9:     assert max_upload_bytes() is None
10: 
11:     monkeypatch.setenv("STAT_HARNESS_MAX_UPLOAD_BYTES", "not-a-number")
12:     assert max_upload_bytes() is None
13: 
14:     monkeypatch.setenv("STAT_HARNESS_MAX_UPLOAD_BYTES", "100")
15:     assert max_upload_bytes() == 100
16: 
17: 
18: def test_file_size_limit(tmp_path):
19:     path = tmp_path / "sample.bin"
20:     path.write_bytes(b"x" * 10)
21:     file_size_limit(path, 10)
22:     try:
23:         file_size_limit(path, 9)
24:     except ValueError:
25:         return
26:     raise AssertionError("Expected size limit to raise")
````

## File: tests/test_vector_store.py
````python
 1: import pytest
 2: 
 3: from statistic_harness.core.vector_store import VectorStore
 4: 
 5: 
 6: def _make_store(tmp_path, monkeypatch):
 7:     monkeypatch.setenv("STAT_HARNESS_ENABLE_VECTOR_STORE", "1")
 8:     try:
 9:         return VectorStore(tmp_path / "state.sqlite", tenant_id="tenant_a")
10:     except RuntimeError as exc:
11:         pytest.skip(str(exc))
12: 
13: 
14: def test_vector_store_add_query_delete(tmp_path, monkeypatch):
15:     store = _make_store(tmp_path, monkeypatch)
16:     store.add(
17:         "demo",
18:         [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]],
19:         item_ids=["a", "b"],
20:         payloads=[{"label": "a"}, {"label": "b"}],
21:     )
22:     results = store.query("demo", [1.0, 0.0, 0.0], k=2)
23:     assert results
24:     assert results[0]["item_id"] == "a"
25: 
26:     store.delete("demo", ["a"], dimensions=3)
27:     results = store.query("demo", [1.0, 0.0, 0.0], k=2)
28:     assert all(row["item_id"] != "a" for row in results)
29: 
30: 
31: def test_vector_store_deterministic_order(tmp_path, monkeypatch):
32:     store = _make_store(tmp_path, monkeypatch)
33:     store.add(
34:         "ties",
35:         [[1.0, 0.0], [1.0, 0.0]],
36:         item_ids=["b", "a"],
37:         payloads=[None, None],
38:     )
39:     results = store.query("ties", [1.0, 0.0], k=2)
40:     assert [row["item_id"] for row in results] == ["a", "b"]
41: 
42: 
43: def test_vector_store_as_of_filter(tmp_path, monkeypatch):
44:     store = _make_store(tmp_path, monkeypatch)
45:     import statistic_harness.core.vector_store as vector_store
46: 
47:     monkeypatch.setattr(
48:         vector_store, "now_iso", lambda: "2026-02-02T00:00:00+00:00"
49:     )
50:     store.add("asof", [[1.0, 0.0]], item_ids=["a"], payloads=[None])
51:     past = store.query(
52:         "asof", [1.0, 0.0], k=1, as_of="2026-02-01T00:00:00+00:00"
53:     )
54:     future = store.query(
55:         "asof", [1.0, 0.0], k=1, as_of="2026-02-03T00:00:00+00:00"
56:     )
57:     assert past == []
58:     assert [row["item_id"] for row in future] == ["a"]
59: 
60: 
61: def test_vector_store_tenant_isolation(tmp_path, monkeypatch):
62:     monkeypatch.setenv("STAT_HARNESS_ENABLE_VECTOR_STORE", "1")
63:     try:
64:         store_a = VectorStore(tmp_path / "state.sqlite", tenant_id="tenant_a")
65:         store_b = VectorStore(tmp_path / "state.sqlite", tenant_id="tenant_b")
66:     except RuntimeError as exc:
67:         pytest.skip(str(exc))
68: 
69:     store_a.add("demo", [[1.0, 0.0]], item_ids=["a"], payloads=[None])
70:     store_b.add("demo", [[1.0, 0.0]], item_ids=["b"], payloads=[None])
71: 
72:     results_a = store_a.query("demo", [1.0, 0.0], k=2)
73:     results_b = store_b.query("demo", [1.0, 0.0], k=2)
74:     assert [row["item_id"] for row in results_a] == ["a"]
75:     assert [row["item_id"] for row in results_b] == ["b"]
````

## File: tools/sitecustomize.py
````python
 1: from __future__ import annotations
 2: 
 3: import errno
 4: import os
 5: import shutil
 6: 
 7: 
 8: if os.environ.get("STAT_HARNESS_SAFE_RENAME") == "1":
 9:     _ORIG_RENAME = os.rename
10:     _ORIG_REPLACE = os.replace
11: 
12:     def _copy_fallback(src: str, dst: str) -> None:
13:         if os.path.isdir(src):
14:             shutil.copytree(src, dst, dirs_exist_ok=True)
15:             shutil.rmtree(src)
16:         else:
17:             shutil.copy2(src, dst)
18:             os.unlink(src)
19: 
20:     def _safe_rename(src: str, dst: str) -> None:
21:         try:
22:             _ORIG_RENAME(src, dst)
23:         except OSError as exc:
24:             if exc.errno != errno.EXDEV:
25:                 raise
26:             _copy_fallback(src, dst)
27: 
28:     def _safe_replace(src: str, dst: str) -> None:
29:         try:
30:             _ORIG_REPLACE(src, dst)
31:         except OSError as exc:
32:             if exc.errno != errno.EXDEV:
33:                 raise
34:             _copy_fallback(src, dst)
35: 
36:     os.rename = _safe_rename
37:     os.replace = _safe_replace
````

## File: .gitkeep
````
1: 
````

## File: 1-32-2026-recommendations-plan.md
````markdown
  1: # Plan: 1-32-2026 Recommendations Implementation
  2: 
  3: **Generated**: January 31, 2026
  4: **Estimated Complexity**: High
  5: 
  6: ## Overview
  7: Implement all recommendations R1–R20 to move the harness to a fully SQLite‑backed, deterministic, append‑only, auditable pipeline with project/dataset lineage, explicit evidence for findings, AutoPlanner selection, backfill for new plugins, and a hardened UI that exposes projects, traces, and method history. All analysis must operate on data stored in SQLite (not the uploaded file) and align with the four pillars: **performance**, **security**, **accuracy**, and **citeability** in balanced priority. The plan prioritizes schema/migration foundations, streaming ingest with safe column canonicalization, dataset/project dedupe, versioned results, lineage graph + parameter normalization, deterministic planning, sequence mining, statistical controls, delivery tracking with hashes, and robust UI/test coverage.
  8: 
  9: ## Prerequisites
 10: - Python 3.11+ and existing dependencies from `pyproject.toml`.
 11: - SQLite with JSON1 and FTS5 enabled (standard in Python builds).
 12: - Ability to run `python -m pytest -q` locally.
 13: 
 14: ## Sprint 1: Storage & Migration Foundations
 15: **Goal**: Establish robust, versioned schema with migrations and thread‑safe DB access (R14, R15 baseline).
 16: **Demo/Validation**:
 17: - `python -m pytest -q` (new migration and concurrency tests).
 18: - Start UI and run two parallel runs without DB errors.
 19: 
 20: ### Task 1.1: Introduce migration framework
 21: - **Location**: `src/statistic_harness/core/storage.py`, new `src/statistic_harness/core/migrations.py`, `tests/test_migrations.py`
 22: - **Description**: Add schema versioning via `PRAGMA user_version`, a `schema_migrations` table, and ordered migration functions. Migrate from existing schema without data loss.
 23: - **Dependencies**: None
 24: - **Acceptance Criteria**:
 25:   - New DB initializes at latest version.
 26:   - Existing `runs` and `plugin_results` data preserved after migration.
 27: - **Validation**:
 28:   - Unit tests covering empty DB and legacy DB migration path.
 29: 
 30: ### Task 1.2: Thread‑safe DB access + WAL
 31: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/ui/server.py`
 32: - **Description**: Refactor `Storage` to open per‑operation or per‑request connections, enable WAL and foreign keys, remove global shared connection usage in FastAPI app.
 33: - **Dependencies**: Task 1.1
 34: - **Acceptance Criteria**:
 35:   - Multiple background runs complete without `sqlite3.ProgrammingError` or locking issues.
 36: - **Validation**:
 37:   - Concurrency test spawns parallel runs and verifies consistency.
 38: 
 39: ### Task 1.3: Core schema scaffolding for projects/datasets/runs/uploads
 40: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`
 41: - **Description**: Add base tables for `projects`, `datasets`, `dataset_versions`, `uploads`, and update `runs` to reference dataset_version and project IDs.
 42: - **Dependencies**: Task 1.1
 43: - **Acceptance Criteria**:
 44:   - New tables created and referenced with FK constraints.
 45: - **Validation**:
 46:   - Migration tests assert presence of new tables/columns.
 47: 
 48: ## Sprint 2: Full‑Dataset Ingest & SQLite Row Storage
 49: **Goal**: Stream ingest without truncation, store all rows in SQLite, and canonicalize columns safely (R1, R4, R20).
 50: **Demo/Validation**:
 51: - Integration test with >10k rows stored fully in SQLite.
 52: - `report.json` uses DB‑derived row counts.
 53: 
 54: ### Task 2.1: Streaming ingest for CSV/XLSX/JSON
 55: - **Location**: `plugins/ingest_tabular/plugin.py`, `src/statistic_harness/core/dataset_io.py`
 56: - **Description**: Replace `head(max_rows)` with streaming/batched read; compute row counts and persist to DB in chunks. Keep optional canonical CSV artifact for debugging only.
 57: - **Dependencies**: Sprint 1 schema in place
 58: - **Acceptance Criteria**:
 59:   - Ingest handles >10k rows without truncation.
 60:   - Ingest does not load entire dataset into memory when possible.
 61: - **Validation**:
 62:   - Unit test with large fixture asserts exact row count.
 63: 
 64: ### Task 2.2: Column canonicalization + safe identifiers
 65: - **Location**: `plugins/ingest_tabular/plugin.py`, new helper in `src/statistic_harness/core/utils.py`
 66: - **Description**: Map original headers to safe internal IDs; store mapping in `dataset_columns` table; handle duplicates/reserved words; enforce parameterized SQL.
 67: - **Dependencies**: Task 2.1
 68: - **Acceptance Criteria**:
 69:   - Fuzzed headers ingest without SQL errors.
 70: - **Validation**:
 71:   - New fuzz test for hostile headers.
 72: 
 73: ### Task 2.3: SQLite raw row storage design
 74: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/dataset_io.py`
 75: - **Description**: Implement per‑dataset wide table for typed columns plus a `row_json` column for round‑trip fidelity; store `row_index` and `dataset_version_id` for traceability.
 76: - **Dependencies**: Task 2.1, 2.2
 77: - **Acceptance Criteria**:
 78:   - Round‑trip test CSV → SQLite → export preserves row count and column mapping.
 79: - **Validation**:
 80:   - Property test on synthetic dataset fixtures.
 81: 
 82: ### Task 2.4: DB‑first dataset access
 83: - **Location**: `src/statistic_harness/core/dataset_io.py`, `src/statistic_harness/core/pipeline.py`
 84: - **Description**: Replace file‑based `DatasetAccessor` with SQLite‑backed access (chunked row iteration and column metadata); prohibit analysis plugins from reading raw upload paths.
 85: - **Dependencies**: Task 2.3
 86: - **Acceptance Criteria**:
 87:   - All plugins can operate using DB access only.
 88:   - Canonical CSV (if retained) is not used for analysis.
 89: - **Validation**:
 90:   - Integration test asserts DB row counts used in report.
 91: 
 92: ## Sprint 3: Project/Dataset Dedup + Append‑Only Guarantees
 93: **Goal**: Deduplicate uploads, reuse datasets across runs, and enforce append‑only raw data (R2, R3, R5).
 94: **Demo/Validation**:
 95: - Re‑uploading identical bytes reuses dataset_id.
 96: - Attempts to UPDATE/DELETE raw rows fail.
 97: 
 98: ### Task 3.1: Content fingerprinting + upload records
 99: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/core/storage.py`
100: - **Description**: Compute SHA‑256 at upload; create `uploads` entry; map to project/dataset by fingerprint.
101: - **Dependencies**: Sprint 1 schema
102: - **Acceptance Criteria**:
103:   - Same bytes with different filenames map to same project.
104: - **Validation**:
105:   - Unit test for dedupe behavior.
106: 
107: ### Task 3.2: Project/dataset resolution in pipeline
108: - **Location**: `src/statistic_harness/core/pipeline.py`, `src/statistic_harness/core/storage.py`
109: - **Description**: On run start, resolve `project_id` and `dataset_id` by fingerprint; create new dataset_version if needed; store IDs on runs.
110: - **Dependencies**: Task 3.1
111: - **Acceptance Criteria**:
112:   - New run reuses existing dataset when file unchanged.
113: - **Validation**:
114:   - Integration test: rerun with same file creates new run but same dataset_id.
115: 
116: ### Task 3.3: Append‑only enforcement
117: - **Location**: `src/statistic_harness/core/migrations.py`
118: - **Description**: Add SQLite triggers to prevent DELETE/UPDATE on raw dataset tables; ensure storage layer avoids destructive SQL.
119: - **Dependencies**: Task 2.3
120: - **Acceptance Criteria**:
121:   - DELETE/UPDATE raises SQLite error.
122: - **Validation**:
123:   - DB tests attempt UPDATE/DELETE and assert failure.
124: 
125: ## Sprint 4: Versioned Results + Evidence‑First Reports
126: **Goal**: Never overwrite results, attach evidence to every finding, and update report schema (R6, R11, R13) to maximize citeability.
127: **Demo/Validation**:
128: - Re‑running a plugin stores a new result record.
129: - Report schema validates with evidence fields.
130: 
131: ### Task 4.1: Versioned plugin result storage + hashes
132: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/pipeline.py`
133: - **Description**: Replace PK `(run_id, plugin_id)` with `result_id` and store `plugin_version`, `executed_at`, `code_hash`, `settings_hash`, `dataset_hash`; add “latest” query helpers to detect stale results when data or plugin changes.
134: - **Dependencies**: Sprint 1 migrations
135: - **Acceptance Criteria**:
136:   - Two runs of same plugin create two records.
137: - **Validation**:
138:   - Unit test for historical results retention.
139: 
140: ### Task 4.2: Evidence schema + report updates
141: - **Location**: `src/statistic_harness/core/types.py`, `src/statistic_harness/core/report.py`, `docs/report.schema.json`
142: - **Description**: Require `evidence` entries per finding (dataset_id, row_ids, column_ids, query snippet); update report builder and JSON schema.
143: - **Dependencies**: Task 4.1
144: - **Acceptance Criteria**:
145:   - Report generation fails if findings lack evidence.
146: - **Validation**:
147:   - Schema validation tests for evidence fields.
148: 
149: ### Task 4.3: Resource budget reporting (unlimited defaults)
150: - **Location**: `src/statistic_harness/core/types.py`, `src/statistic_harness/core/pipeline.py`
151: - **Description**: Add budget metadata to context and results; default to “unlimited” but report whether sampling/approximation was used.
152: - **Dependencies**: Task 4.1
153: - **Acceptance Criteria**:
154:   - Report includes budget usage fields (even when unlimited).
155: - **Validation**:
156:   - Unit test ensures default budget metadata present.
157: 
158: ## Sprint 5: Parameter Normalization + Lineage Graph + Indexes
159: **Goal**: Normalize parameter entities, build association graph, and enable trace queries (R7, R8, R17).
160: **Demo/Validation**:
161: - Trace endpoint returns deterministic graph for fixture.
162: - Parameter text variants normalize to same entity.
163: 
164: ### Task 5.1: Parameter entity schema + normalization
165: - **Location**: `src/statistic_harness/core/storage.py`, `plugins/profile_basic/plugin.py`
166: - **Description**: Add tables `parameter_entities`, `parameter_kv`, `row_parameter_link`; use heuristic extraction to normalize parameter fields into key/value entities (no fixed schema).
167: - **Dependencies**: Sprint 2 row storage
168: - **Acceptance Criteria**:
169:   - Different textual forms map to same entity_id.
170: - **Validation**:
171:   - Fuzz tests on whitespace/order/case.
172: 
173: ### Task 5.2: Column role inference (heuristics)
174: - **Location**: `plugins/profile_basic/plugin.py`, new helper in `src/statistic_harness/core/utils.py`
175: - **Description**: Infer probable roles (case/process id, activity, timestamp, parameter blob, numeric measures) using heuristics; persist roles in DB for AutoPlanner and analysis plugins.
176: - **Dependencies**: Task 5.1
177: - **Acceptance Criteria**:
178:   - Role inference is deterministic for the same dataset.
179: - **Validation**:
180:   - Unit tests with varied fixtures.
181: 
182: ### Task 5.3: Lineage graph tables + APIs
183: - **Location**: `src/statistic_harness/core/storage.py`, new `src/statistic_harness/core/lineage.py`
184: - **Description**: Add `entities` and `edges` tables, helper APIs, and recursive CTE trace queries.
185: - **Dependencies**: Task 5.1, 5.2
186: - **Acceptance Criteria**:
187:   - Trace query returns reachable nodes for fixture graph.
188: - **Validation**:
189:   - Golden test for trace results.
190: 
191: ### Task 5.4: Index strategy (FTS + key/value)
192: - **Location**: `src/statistic_harness/core/migrations.py`
193: - **Description**: Add indexes on entity key/type, edge src/dst, parameter_kv; optional FTS for raw parameter text.
194: - **Dependencies**: Task 5.1, 5.3
195: - **Acceptance Criteria**:
196:   - EXPLAIN shows index usage for trace/search.
197: - **Validation**:
198:   - Performance regression test with synthetic scale.
199: 
200: ## Sprint 6: Deterministic AutoPlanner + Plugin Capability Tags
201: **Goal**: Automatically select plugins based on dataset profile, deterministically (R9).
202: **Demo/Validation**:
203: - Planner selects expected plugins for numeric vs event‑log fixtures.
204: 
205: ### Task 6.1: Capability tags in plugin manifests
206: - **Location**: `plugins/*/plugin.yaml`, `src/statistic_harness/core/plugin_manager.py`
207: - **Description**: Extend plugin manifests with tags (needs_numeric, needs_timestamp, needs_eventlog, etc.) and load into specs.
208: - **Dependencies**: None
209: - **Acceptance Criteria**:
210:   - Plugin specs expose capability tags.
211: - **Validation**:
212:   - Unit test on plugin discovery.
213: 
214: ### Task 6.2: AutoPlanner phase in pipeline
215: - **Location**: `src/statistic_harness/core/pipeline.py`, new `src/statistic_harness/core/planner.py`
216: - **Description**: Add deterministic planner after ingest/profile to select plugins; log plan and store in DB.
217: - **Dependencies**: Task 6.1, Sprint 2, Sprint 5
218: - **Acceptance Criteria**:
219:   - Same dataset always yields identical plan.
220: - **Validation**:
221:   - Snapshot tests for planner output.
222: 
223: ## Sprint 7: Sequence Mining + Evidence‑Rich Findings
224: **Goal**: Add process/sequence mining plugin with evidence and parameter linkage (R10).
225: **Demo/Validation**:
226: - Synthetic event‑log fixture yields known variants and transitions.
227: 
228: ### Task 7.1: Event‑log detection utilities
229: - **Location**: `src/statistic_harness/core/utils.py`, `plugins/analysis_process_sequence/plugin.py` (new)
230: - **Description**: Detect case/activity/timestamp columns from profile metadata and dataset schema.
231: - **Dependencies**: Sprint 2, Sprint 5
232: - **Acceptance Criteria**:
233:   - Event‑log columns detected deterministically for fixtures.
234: - **Validation**:
235:   - Unit tests for detection heuristics.
236: 
237: ### Task 7.2: Sequence mining plugin implementation
238: - **Location**: `plugins/analysis_process_sequence/*`
239: - **Description**: Compute frequent variants (n‑grams + full sequences), transition matrices, rare path anomalies; store findings with evidence row pointers.
240: - **Dependencies**: Task 7.1
241: - **Acceptance Criteria**:
242:   - Findings include evidence with row_ids and parameter entity references.
243: - **Validation**:
244:   - Plugin unit tests with synthetic fixtures.
245: 
246: ## Sprint 8: Statistical Controls Library
247: **Goal**: Reduce false discoveries across plugins (R12).
248: **Demo/Validation**:
249: - Null datasets produce low finding counts.
250: 
251: ### Task 8.1: Add statistical controls module
252: - **Location**: new `src/statistic_harness/core/stat_controls.py`
253: - **Description**: Implement effect sizes, multiple‑testing corrections, confidence scoring utilities.
254: - **Dependencies**: None
255: - **Acceptance Criteria**:
256:   - Functions deterministic with fixed seeds.
257: - **Validation**:
258:   - Unit tests on known distributions.
259: 
260: ### Task 8.2: Integrate controls into analysis plugins
261: - **Location**: `plugins/analysis_*/*`
262: - **Description**: Apply scoring and corrections to plugin outputs; include confidence in findings.
263: - **Dependencies**: Task 8.1
264: - **Acceptance Criteria**:
265:   - Findings include score/confidence fields.
266: - **Validation**:
267:   - Regression tests on synthetic null data.
268: 
269: ## Sprint 9: Backfill + Job Queue + Delivery Tracking
270: **Goal**: Backrun new plugins over old datasets and track executed vs pending vs delivered (R18) using hashes for data/plugin changes.
271: **Demo/Validation**:
272: - Adding new plugin schedules jobs for existing datasets.
273: 
274: ### Task 9.1: Analysis job queue in SQLite
275: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`
276: - **Description**: Add `analysis_jobs` table; create enqueue/dequeue helpers; idempotent scheduling per dataset/plugin/version.
277: - **Dependencies**: Sprint 4 (versioned results)
278: - **Acceptance Criteria**:
279:   - Jobs are not duplicated for same dataset/plugin/version.
280: - **Validation**:
281:   - Unit test for enqueue idempotency.
282: 
283: ### Task 9.2: Delivery state + hash tracking
284: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/migrations.py`
285: - **Description**: Add tables for `deliveries` (project_id, dataset_version_id, plugin_id, plugin_hash, dataset_hash, delivered_at, notes). Determine “needs re-run” when dataset hash or plugin hash changes since delivery.
286: - **Dependencies**: Task 4.1
287: - **Acceptance Criteria**:
288:   - Delivered plugins remain marked delivered unless data or plugin hash changes.
289: - **Validation**:
290:   - Unit tests covering hash changes.
291: 
292: ### Task 9.3: Backfill CLI command
293: - **Location**: `src/statistic_harness/cli.py`
294: - **Description**: Implement `backfill --plugin <id> --all-projects` to schedule jobs; worker processes jobs sequentially.
295: - **Dependencies**: Task 9.1
296: - **Acceptance Criteria**:
297:   - Backfill schedules expected jobs and produces results.
298: - **Validation**:
299:   - Integration test with fixture DB.
300: 
301: ## Sprint 10: UI/UX Hardening + Project/Trace Views
302: **Goal**: Bulletproof UI with project index, upload → run flow, trace views, and method history (R16).
303: **Demo/Validation**:
304: - Upload triggers run automatically and shows progress + method list.
305: - Project list and trace endpoints work with fixtures.
306: 
307: ### Task 10.1: Upload flow + validation
308: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`
309: - **Description**: Validate file types/sizes; compute hash at upload; auto‑create run; return run_id without manual upload_id.
310: - **Dependencies**: Sprint 3 (upload records)
311: - **Acceptance Criteria**:
312:   - Upload returns run_id immediately.
313: - **Validation**:
314:   - FastAPI TestClient smoke tests.
315: 
316: ### Task 10.2: Project list + dataset views
317: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`
318: - **Description**: Add `/projects` list, project detail, dataset metadata, and global search by parameter/process/hash.
319: - **Dependencies**: Sprint 5 indexes
320: - **Acceptance Criteria**:
321:   - UI renders project list with pagination.
322: - **Validation**:
323:   - UI tests with TestClient.
324: 
325: ### Task 10.3: Trace endpoint and method history
326: - **Location**: `src/statistic_harness/ui/server.py`, templates
327: - **Description**: Add `/trace` endpoint returning lineage graph; show methods run vs pending vs delivered per dataset (based on job queue, results, and deliveries).
328: - **Dependencies**: Sprint 5, Sprint 9
329: - **Acceptance Criteria**:
330:   - Method history shows executed timestamps and pending plugin list.
331: - **Validation**:
332:   - Deterministic response tests.
333: 
334: ### Task 10.4: Delivery controls in UI
335: - **Location**: `src/statistic_harness/ui/server.py`, templates
336: - **Description**: Add UI controls to mark project delivered (all selected plugins) and per‑plugin delivered; display stale/due when plugin or data hash changes.
337: - **Dependencies**: Task 9.2
338: - **Acceptance Criteria**:
339:   - User can mark delivery at project or plugin level.
340: - **Validation**:
341:   - UI tests for delivery state toggles.
342: 
343: ## Sprint 11: PII Tagging + Governance
344: **Goal**: Identify PII columns and propagate tags without exporting raw data (R19).
345: **Demo/Validation**:
346: - PII fixture columns detected and tagged in report.
347: 
348: ### Task 11.1: PII detection in profile plugin
349: - **Location**: `plugins/profile_basic/plugin.py`, `src/statistic_harness/core/storage.py`
350: - **Description**: Add regex/pattern heuristics for PII (email, phone, SSN, address); tag columns and store in DB.
351: - **Dependencies**: Sprint 2 schema
352: - **Acceptance Criteria**:
353:   - PII tags appear in report metadata.
354: - **Validation**:
355:   - PII fixture tests.
356: 
357: ### Task 11.2: Redaction policy for reports
358: - **Location**: `src/statistic_harness/core/report.py`, `docs/report.schema.json`
359: - **Description**: Ensure reports do not include raw PII; only tags and aggregated insights.
360: - **Dependencies**: Task 11.1
361: - **Acceptance Criteria**:
362:   - Report omits raw PII values.
363: - **Validation**:
364:   - Report schema + regression tests.
365: 
366: ## Sprint 12: Test Harness Expansion
367: **Goal**: Comprehensive unit/integration tests + evaluator harness updates (AGENTS.md).
368: **Demo/Validation**:
369: - `python -m pytest -q` green with new tests.
370: 
371: ### Task 12.1: Update evaluator harness
372: - **Location**: `src/statistic_harness/core/evaluation.py`, `tests/test_evaluation.py`
373: - **Description**: Align evaluator with new report/evidence schema and new findings structure.
374: - **Dependencies**: Sprint 4
375: - **Acceptance Criteria**:
376:   - Ground truth YAML checks pass with updated schema.
377: - **Validation**:
378:   - Evaluator tests pass.
379: 
380: ### Task 12.2: Integration tests for full pipeline
381: - **Location**: `tests/test_pipeline_integration.py`, new fixtures in `tests/fixtures/*`
382: - **Description**: Run full pipeline against dataset fixtures; assert report outputs exist and validate; assert backfill works.
383: - **Dependencies**: Sprints 1–11
384: - **Acceptance Criteria**:
385:   - Reports generated and validate against schema.
386: - **Validation**:
387:   - Integration tests pass.
388: 
389: ## Testing Strategy
390: - Unit tests per plugin for deterministic outputs and evidence presence (accuracy + citeability).
391: - Integration tests that run full pipeline and validate `report.json` against updated schema.
392: - Migration tests for legacy → latest schema (data preserved).
393: - Performance tests for large CSV ingestion and trace query latency.
394: - Security tests for path traversal prevention and safe SQL identifier handling.
395: - Concurrency tests for multi‑run parallel execution.
396: 
397: ## Potential Risks & Gotchas
398: - Schema migrations must preserve historical data; failure risks data loss.
399: - Wide per‑dataset tables can be large; ensure batching and index strategy to avoid slow ingest.
400: - Deterministic AutoPlanner must avoid non‑deterministic ordering from dict/SQL.
401: - Sequence mining on very large datasets may be heavy; consider staged computation with DB‑side aggregation.
402: - UI trace/query endpoints must avoid exposing raw PII values.
403: - Delivery state can drift if hashes are computed inconsistently; define canonical hashing inputs and version them.
404: 
405: ## Rollback Plan
406: - Keep migration steps reversible where possible; provide a downgrade script for schema version N → N‑1.
407: - Gate new behavior behind config flags if needed (AutoPlanner on/off).
408: - Maintain read compatibility for legacy report schema during transition.
````

## File: CHANGELOG.md
````markdown
 1: # Changelog
 2: 
 3: All notable changes to this project will be documented in this file.
 4: 
 5: ## [Unreleased]
 6: 
 7: ### Changed
 8: - Normalized line endings across the repository for consistent Windows/WSL compatibility.
 9: 
10: ### Fixed
11: - Added a safe-rename hook for WSL editable installs plus one-command dev install scripts and README guidance.
12: - Corrected plugin loading for `plugin.py:Plugin` entrypoints, Excel ingest sheet handling, and JSON serialization of numpy bools.
13: 
14: ### Added
15: - Added a `make dev` target for WSL-friendly setup.
````

## File: close-cycle-third-server-impact-plan.md
````markdown
  1: # Plan: Close-Cycle Third-Server Impact Detection
  2: 
  3: **Generated**: 2026-02-02
  4: **Estimated Complexity**: High
  5: 
  6: ## Overview
  7: Assess whether the existing plugins can *prove or refute* the claim: “adding a 3rd process server decreases time‑to‑completion by ~30% during close windows,” with **no false positives** (only false negatives). If current plugins cannot reach the conclusion, design a new **dynamic**, **conservative** plugin that detects this effect in future datasets without pre‑known data or columns. The effect will be defined explicitly as a **relative median time‑to‑completion (start→end)** reduction with a conservative CI gate, while also tracking **queue→end** and **eligible→end** as supporting metrics.
  8: 
  9: The plan follows four pillars:
 10: - **Precision**: zero false positives (strong evidence gates).
 11: - **Recall**: detect real ≥30% improvements when data supports it.
 12: - **Robustness**: dynamic column inference, varying data sizes/shapes.
 13: - **Explainability**: human‑readable reasons and artifacts.
 14: 
 15: ## Prerequisites
 16: - Access to the uploaded dataset(s) in `appdata/state.sqlite`.
 17: - Working Python environment (venv in `/tmp/stat_harness_venv` or equivalent).
 18: - Known close‑window definition (default day 20–5 unless overridden).
 19: - Confirmation of time‑to‑completion definition (start→end vs queue→end).
 20: 
 21: ## Sprint 1: Evaluate With Existing Plugins (No New Code)
 22: **Goal**: Use current plugins to either reach the conclusion or document precisely why they cannot.
 23: **Demo/Validation**:
 24: - Run selected plugins on the dataset and capture report artifacts.
 25: - Verify whether any plugin yields a *measured* close‑window improvement tied to **host count** or **capacity**.
 26: 
 27: ### Task 1.1: Column mapping and host variation check
 28: - **Location**: `appdata/state.sqlite`, dataset columns, plugin settings
 29: - **Description**: Identify host/server column, start/end columns, queue/eligible columns (if any) **dynamically** using column inference. Compute whether close windows ever include ≥3 distinct hosts (current data may only have 2).
 30: - **Complexity**: 4
 31: - **Dependencies**: None
 32: - **Acceptance Criteria**:
 33:   - Confirm which columns represent server/host and time‑to‑completion (start→end).
 34:   - Record a decision note that **time‑to‑completion = start→end** (primary), with queue→end and eligible→end as secondary diagnostics.
 35:   - Confirm whether host count varies (2 vs 3+) during close windows.
 36: **Validation**:
 37: - Record inferred columns, close‑window definition (inferred vs override), and plugin versions in a run artifact or notes.
 38: - Simple exploratory script or plugin log output captured in report notes.
 39: 
 40: ### Task 1.2: Run `analysis_concurrency_reconstruction`
 41: - **Location**: `plugins/analysis_concurrency_reconstruction/`
 42: - **Description**: Produce host‑level concurrency summary and host count statistics using both **concurrent active hosts** and **unique active hosts** per bucket; check for evidence of a 3rd server in close windows.
 43: - **Complexity**: 3
 44: - **Dependencies**: Task 1.1
 45: - **Acceptance Criteria**:
 46:   - Artifact shows host count and concurrency timeline.
 47:   - If host count never reaches 3 in close windows, document “insufficient evidence.”
 48: - **Validation**:
 49:   - Report artifact in run directory.
 50: 
 51: ### Task 1.3: Run `analysis_capacity_scaling` and `analysis_queue_delay_decomposition`
 52: - **Location**: `plugins/analysis_capacity_scaling/`, `plugins/analysis_queue_delay_decomposition/`
 53: - **Description**: Use modeled capacity scaling and eligible‑wait decomposition to see if any modeled ≈30% reduction exists during close windows (as supporting evidence only; not proof).
 54: - **Complexity**: 4
 55: - **Dependencies**: Task 1.1
 56: - **Acceptance Criteria**:
 57:   - Clear statement that these are **modeled** (not measured) and **non‑decisive** for “no false positives.”
 58: - **Validation**:
 59:   - Report artifacts with summaries; note limitation if only modeled.
 60: 
 61: ### Task 1.4: Run `analysis_close_cycle_duration_shift`
 62: - **Location**: `plugins/analysis_close_cycle_duration_shift/`
 63: - **Description**: Check for measured close‑window duration changes by process to rule out that the “improvement” is actually per‑process latency change rather than capacity.
 64: - **Complexity**: 3
 65: - **Dependencies**: Task 1.1
 66: - **Acceptance Criteria**:
 67:   - If no measured per‑process duration drop is detected, record that existing plugins do not prove the claim.
 68: - **Validation**:
 69:   - Report artifact in run directory.
 70: 
 71: ## Sprint 2: Design a New Conservative Plugin (Dynamic, No False Positives)
 72: **Goal**: Define a new plugin that detects **measured** close‑window improvement linked to host count, with strong anti‑false‑positive guards.
 73: **Demo/Validation**:
 74: - Design doc + config schema + output schema ready for implementation.
 75: 
 76: ### Task 2.1: Define detection logic & thresholds
 77: - **Location**: new design notes in `docs/` or plugin README
 78: - **Description**: Specify the statistical test and guards:
 79:   - **Effect definition**: `effect = median_ttc(host>=3) / median_ttc(host<=2) - 1`. Require CI entirely ≤ **-0.30 ± tolerance** (configurable).
 80:   - **Time‑to‑completion** is **start→end** (primary); also compute queue→end and eligible→end as secondary diagnostics.
 81:   - **Bucketization**: configurable `bucket_size` (default daily) with minimum items per bucket and minimum buckets per group.
 82:   - **Host count metrics**: compute both **concurrent active hosts** (primary classifier) and **unique active hosts** (secondary diagnostic) per bucket; report both.
 83:   - Compare buckets with `host_count >= 3` vs `host_count <= 2`.
 84:   - Use a conservative test (bootstrap CI or permutation) with low α (e.g., 0.01).
 85:   - Confounding guards: process‑mix divergence threshold **and** workload volume parity (e.g., total rows or queue backlog similarity).
 86:   - Time‑trend guard: optionally difference‑in‑differences across close‑window weeks.
 87:   - **Close‑window detection**: infer real monthly close windows dynamically (e.g., detect compressed windows and holidays). If a user override is supplied, it **supersedes** inference.
 88: - **Complexity**: 6
 89: - **Dependencies**: Sprint 1 findings
 90: - **Acceptance Criteria**:
 91:   - Formal definition of “time‑to‑completion.”
 92:   - Defined bucket size and minimum bucket counts per group.
 93:   - Documented confounding controls and negative control strategy.
 94:   - Documented close‑window inference method with defaults and overrides.
 95: - **Validation**:
 96:   - Written spec + configuration options.
 97: 
 98: ### Task 2.2: Define plugin interface and artifacts
 99: - **Location**: `plugins/analysis_close_cycle_capacity_impact/`
100: - **Description**: Specify config schema, output schema, and artifact formats:
101:   - `results.md` with “Detected / Suppressed / Not‑Applicable”
102:   - `results.csv` detail table
103:   - `results.json` full metrics and diagnostics
104:   - Explicit classification policy: when to emit **Detected** vs **Suppressed** vs **Not Applicable**
105:   - Surface both host‑count metrics and all three time‑to‑completion measures.
106: - **Complexity**: 5
107: - **Dependencies**: Task 2.1
108: - **Acceptance Criteria**:
109:   - Schemas enforce strict config validation.
110:   - Output includes reason summary and evidence fields (rows, columns, bucket stats).
111: - **Validation**:
112:   - Schema review + example output JSON.
113: 
114: ## Sprint 3: Implement Plugin + Tests (No False Positives)
115: **Goal**: Implement the plugin and verify conservative behavior with synthetic and real datasets.
116: **Demo/Validation**:
117: - Tests pass; plugin returns “not applicable” unless evidence is strong.
118: 
119: ### Task 3.1: Implement plugin code
120: - **Location**: `plugins/analysis_close_cycle_capacity_impact/plugin.py`
121: - **Description**:
122:   - Dynamic column inference (host, start, end, queue/eligible optional).
123:   - Close‑window bucketization; **concurrent host count** and **unique host count** computation per bucket.
124:   - Median completion time (start→end primary); compare 3+ vs 2‑ host groups.
125:   - Secondary diagnostics: queue→end and eligible→end effect sizes and CIs.
126:   - Conservative statistical test + effect size threshold.
127:   - Confounding guards (process mix, workload parity, time‑trend).
128:   - Data‑quality gates (missing timestamps, negative durations, inconsistent host IDs).
129:   - Human‑readable summary + suppression reasons.
130: - **Complexity**: 7
131: - **Dependencies**: Sprint 2
132: - **Acceptance Criteria**:
133:   - Finding emitted only with strong evidence; otherwise “not applicable.”
134:   - Fully dynamic across datasets/columns.
135: - **Validation**:
136:   - Unit tests (see Task 3.2), manual run on provided dataset.
137: 
138: ### Task 3.2: Add tests for no‑false‑positive behavior
139: - **Location**: `tests/plugins/test_close_cycle_capacity_impact.py`
140: - **Description**:
141:   - Synthetic dataset with **no host count variation** → no findings.
142:   - Synthetic dataset with **3rd host** and ≥30% improvement → finding.
143:   - Confounded dataset (host count change + process mix shift) → no finding.
144:   - Negative control (shuffle host labels or compare non‑close windows) → no finding.
145: - **Complexity**: 6
146: - **Dependencies**: Task 3.1
147: - **Acceptance Criteria**:
148:   - All tests pass, no false positives.
149: - **Validation**:
150:   - `pytest -q` with `PYTHONPATH` set.
151: 
152: ### Task 3.3: Wire into evaluation/known issues
153: - **Location**: `appdata/known_issues/*.yaml`
154: - **Description**: Add/adjust known‑issue rule to expect the new plugin’s finding **only for datasets that meet evidence gates** (e.g., ≥3‑host periods). Otherwise require “not applicable.”
155: - **Complexity**: 3
156: - **Dependencies**: Task 3.1
157: - **Acceptance Criteria**:
158:   - Known‑issue entry matches plugin output.
159: - **Validation**:
160:   - `stat-harness eval` passes against the known issue.
161: 
162: ## Testing Strategy
163: - **Unit tests**: synthetic cases for true positive, false positive, confounded.
164: - **Integration**: run plugin on the proc log dataset; confirm either:
165:   - A measured ≥30% improvement is detected, or
166:   - It is flagged as “not applicable/insufficient evidence” because only 2 hosts exist now.
167: - **Sensitivity**: rerun with alternative bucket sizes to ensure conclusion stability.
168: - **Regression**: ensure other plugins still pass current test suite.
169: 
170: ## Potential Risks & Gotchas
171: - **No host count variation** in close windows → cannot prove the claim (false negative).
172: - **Process mix or volume confounding** can mimic improvement; must reject when mix diverges.
173: - **Time‑to‑completion definition ambiguity** (start→end vs queue→end) or timezone drift.
174: - **Host column ambiguity** or inconsistent server IDs.
175: - **Sparse buckets** (few close‑window days/hours) → insufficient power.
176: 
177: ## Rollback Plan
178: - Remove new plugin directory and schema files.
179: - Revert any known‑issues entry referencing the new plugin.
180: - Delete plugin references in docs or tests if added.
````

## File: cohesive-plan-docs-phase2-plan.md
````markdown
  1: # Plan: Cohesive Harness Roadmap (Plans + Phase 2)
  2: 
  3: **Generated**: February 2, 2026
  4: **Estimated Complexity**: High
  5: 
  6: ## Overview
  7: This roadmap consolidates the three plan docs (recommendations, kernel/plugin stability, queue-delay plugins) into one cohesive sequence and adds Phase 2 deliverables (multi-tenant + auth + vector store). The repo already implements many items from those plans; this plan focuses on remaining deltas, verification hardening, and the Phase 2 buildout. Phase 2 work is gated on completing the queue/capacity suite and a fully passing test run. Emphasis stays balanced across performance, security, accuracy, and citeability.
  8: 
  9: ## Prerequisites
 10: - Python 3.11+, `python -m pytest -q` passes on the current baseline.
 11: - Phase 2 tenancy model: shared DB with `tenant_id` columns (confirmed).
 12: - Auth model: local users + session tokens AND API keys (confirmed).
 13: - SQLite vector store: `sqlite-vec` available as a builtin extension.
 14: - Queue/capacity plugin suite must complete and all tests must pass before Phase 2 work begins.
 15: - Quorum Upstream fixture available (real fixture).
 16: 
 17: ## Sprint 1: Determinism + Kernel Hardening Deltas
 18: **Goal**: Close remaining determinism gaps and security guardrails not fully covered by existing code/tests.
 19: **Demo/Validation**:
 20: - Run pipeline twice with same seed; `report.json` is byte-stable.
 21: - Uploads enforce file size limits; offline guardrails remain intact.
 22: 
 23: ### Task 1.1: Deterministic runtime seeding in subprocess runner
 24: - **Location**: `src/statistic_harness/core/plugin_runner.py`, `src/statistic_harness/core/pipeline.py`
 25: - **Description**: Ensure deterministic runtime by seeding `random`, NumPy (if available), and setting `PYTHONHASHSEED`, `TZ=UTC`, `LC_ALL=C` for subprocess execution. Pass a sanitized env into `subprocess.run`.
 26: - **Complexity**: 6
 27: - **Dependencies**: None
 28: - **Acceptance Criteria**:
 29:   - Identical runs on the same dataset produce byte-identical `report.json`.
 30: - **Validation**:
 31:   - New determinism test that runs the same pipeline twice and diffs output.
 32: 
 33: ### Task 1.2: Stable ordering + canonical JSON rounding policy
 34: - **Location**: `src/statistic_harness/core/utils.py`, `src/statistic_harness/core/report.py`, `src/statistic_harness/core/pipeline.py`
 35: - **Description**: Add a canonical JSON helper that stabilizes float formatting (fixed precision) and ensure findings/metrics are consistently ordered in report output.
 36: - **Complexity**: 5
 37: - **Dependencies**: Task 1.1
 38: - **Acceptance Criteria**:
 39:   - Report ordering is stable across runs even with unordered inputs.
 40: - **Validation**:
 41:   - Snapshot test for report ordering and canonical JSON output.
 42: 
 43: ### Task 1.3: Upload size limits + explicit guardrails
 44: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/core/utils.py`, `tests/test_security_paths.py`
 45: - **Description**: Enforce max upload size (configurable), and add explicit tests for size violations and traversal attempts. Use existing `file_size_limit` helper.
 46: - **Complexity**: 4
 47: - **Dependencies**: None
 48: - **Acceptance Criteria**:
 49:   - Oversized uploads fail with a deterministic error.
 50: - **Validation**:
 51:   - New/extended security tests for file size enforcement.
 52: 
 53: ### Task 1.4: Performance smoke regression test
 54: - **Location**: `tests/test_performance_smoke.py`
 55: - **Description**: Add a lightweight benchmark that runs a small fixture through the subprocess path and checks elapsed time within a loose threshold (enforced in CI).
 56: - **Complexity**: 3
 57: - **Dependencies**: Task 1.1
 58: - **Acceptance Criteria**:
 59:   - Performance smoke test is stable on local dev machines.
 60: - **Validation**:
 61:   - `python -m pytest -q` passes including the performance smoke test.
 62: 
 63: ## Sprint 2: Resource Budgets + PII Governance
 64: **Goal**: Add explicit resource budget metadata and PII tagging/anonymization to improve citeability and governance.
 65: **Demo/Validation**:
 66: - Reports include budget metadata (even when unlimited).
 67: - PII tags appear; any cloud/API egress (LLM or external service calls) is anonymized via entity hash table.
 68: 
 69: ### Task 2.1: Resource budget metadata
 70: - **Location**: `src/statistic_harness/core/types.py`, `src/statistic_harness/core/report.py`, `docs/report.schema.json`, `plugins/*/output.schema.json`
 71: - **Description**: Add budget metadata to `PluginContext` and `PluginResult` (row limits, sampling flags, time/CPU budget) with defaults of “unlimited”. Emit in report output and update plugin output schemas.
 72: - **Complexity**: 5
 73: - **Dependencies**: None
 74: - **Acceptance Criteria**:
 75:   - All plugin outputs include budget fields with defaults.
 76: - **Validation**:
 77:   - Schema + unit tests for budget defaults in report output.
 78: 
 79: ### Task 2.2: PII detection + tagging
 80: - **Location**: `plugins/profile_basic/plugin.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`
 81: - **Description**: Add PII heuristics (email/phone/SSN/address) and store tags plus entity hashes. Introduce a `pii_entities` table that records raw value, deterministic hash, type, and tenant_id. Use a per-tenant salt so hashes are stable within a tenant and non-linkable across tenants.
 82: - **Complexity**: 6
 83: - **Dependencies**: Task 2.1
 84: - **Acceptance Criteria**:
 85:   - PII columns are tagged deterministically for fixtures.
 86: - **Validation**:
 87:   - New tests with a PII fixture.
 88: 
 89: ### Task 2.3: PII anonymization policy for cloud egress
 90: - **Location**: `plugins/llm_prompt_builder/*`, `src/statistic_harness/core/report.py`, `docs/report.schema.json`
 91: - **Description**: Define “offsystem egress” as explicit cloud/API calls (LLM or other external services). Before any such call, replace raw PII values with entity hashes and type labels using the `pii_entities` table. Preserve raw PII internally and in local reports.
 92: - **Complexity**: 4
 93: - **Dependencies**: Task 2.2
 94: - **Acceptance Criteria**:
 95:   - LLM prompt builder never emits raw PII in outbound payloads.
 96: - **Validation**:
 97:   - Tests that scan outbound payloads for raw PII patterns on fixtures.
 98: 
 99: ### Task 2.4: LLM egress safety checks
100: - **Location**: `plugins/llm_prompt_builder/*`, `tests/test_llm_prompt_builder.py`
101: - **Description**: Add unit/integration tests ensuring LLM prompt payloads are anonymized via entity hashes and include PII type labels when applicable.
102: - **Complexity**: 4
103: - **Dependencies**: Task 2.3
104: - **Acceptance Criteria**:
105:   - Tests fail if any raw PII appears in outbound payloads.
106: - **Validation**:
107:   - `python -m pytest -q` passes with new LLM egress checks.
108: 
109: ## Sprint 3: Statistical Controls Library
110: **Goal**: Reduce false discoveries across analysis plugins and improve confidence scoring.
111: **Demo/Validation**:
112: - Null datasets yield low false-positive findings.
113: 
114: ### Task 3.1: Implement statistical controls helpers
115: - **Location**: `src/statistic_harness/core/stat_controls.py`
116: - **Description**: Add effect size calculations, multiple-testing corrections (BH/FDR), and confidence scoring utilities with deterministic behavior.
117: - **Complexity**: 6
118: - **Dependencies**: None
119: - **Acceptance Criteria**:
120:   - Deterministic outputs for fixed inputs and seeds.
121: - **Validation**:
122:   - Unit tests on known distributions.
123: 
124: ### Task 3.2: Integrate controls into analysis plugins
125: - **Location**: `plugins/analysis_*/*`, `plugins/*/output.schema.json`
126: - **Description**: Apply corrections and add confidence/score fields to findings and metrics; update schemas accordingly.
127: - **Complexity**: 7
128: - **Dependencies**: Task 3.1
129: - **Acceptance Criteria**:
130:   - Findings include confidence fields; tests remain deterministic.
131: - **Validation**:
132:   - Plugin regression tests with null datasets.
133: 
134: ## Sprint 4: Queue/Delay Plugin Suite (Core Set)
135: **Goal**: Complete the missing queue/capacity plugins for the core queue-delay decomposition set.
136: **Demo/Validation**:
137: - New plugins run on a synthetic event-log fixture and emit expected findings with evidence.
138: 
139: ### Task 4.1: Dependency-resolution join plugin
140: - **Location**: `plugins/analysis_dependency_resolution_join/`, `plugins/analysis_dependency_resolution_join/*.schema.json`
141: - **Description**: Join dependency end timestamps to compute START-DEP_END and emit lag distribution stats.
142: - **Complexity**: 6
143: - **Dependencies**: Sprint 2 (role inference already present)
144: - **Acceptance Criteria**:
145:   - Emits deterministic lag distribution metrics.
146: - **Validation**:
147:   - Unit test with synthetic dependency fixture.
148: 
149: ### Task 4.2: Tail isolation plugin
150: - **Location**: `plugins/analysis_tail_isolation/`
151: - **Description**: Filter eligible-wait > threshold and attribute tail to process/module/user/sequence with evidence.
152: - **Complexity**: 5
153: - **Dependencies**: Task 4.1
154: - **Acceptance Criteria**:
155:   - Tail findings include evidence row IDs.
156: - **Validation**:
157:   - Plugin test with threshold fixture.
158: 
159: ### Task 4.3: Percentile analysis plugin
160: - **Location**: `plugins/analysis_percentile_analysis/`
161: - **Description**: Compute p50/p95/p99 for eligible wait and completion time per process/module.
162: - **Complexity**: 4
163: - **Dependencies**: Task 4.1
164: - **Acceptance Criteria**:
165:   - Percentiles are deterministic and tolerance-checked.
166: - **Validation**:
167:   - Numeric tolerance tests in evaluation.
168: 
169: ### Task 4.4: Attribution analysis plugin
170: - **Location**: `plugins/analysis_attribution/`
171: - **Description**: Aggregate eligible wait and tail counts by PROCESS_ID, MODULE_CD, USER_ID with deterministic ranking.
172: - **Complexity**: 4
173: - **Dependencies**: Task 4.1
174: - **Acceptance Criteria**:
175:   - Deterministic ranking with evidence slices.
176: - **Validation**:
177:   - Fixture test for top contributor stability.
178: 
179: ### Task 4.5: Determinism discipline plugin
180: - **Location**: `plugins/analysis_determinism_discipline/`
181: - **Description**: Inspect plugin outputs for measurement_type usage and emit violations if missing or inconsistent.
182: - **Complexity**: 5
183: - **Dependencies**: Sprint 2
184: - **Acceptance Criteria**:
185:   - Emits a summary finding and violations for mis-labeled outputs.
186: - **Validation**:
187:   - Unit test with mocked plugin outputs.
188: 
189: ### Task 4.6: Update planner capability tags for new plugins
190: - **Location**: `plugins/*/plugin.yaml`, `src/statistic_harness/core/planner.py`
191: - **Description**: Add capability tags for newly added queue plugins and ensure deterministic planner selection.
192: - **Complexity**: 3
193: - **Dependencies**: Tasks 4.1–4.5
194: - **Acceptance Criteria**:
195:   - Planner selects queue plugins only when event-log roles are present.
196: - **Validation**:
197:   - Planner selection test on event-log vs numeric fixtures.
198: 
199: ## Sprint 5: Queue/Capacity Modeling + Parity Suite
200: **Goal**: Implement advanced modeled plugins and parity checks with Quorum-style fixtures.
201: **Demo/Validation**:
202: - Full queue plugin suite passes evaluation parity on the Quorum fixture.
203: 
204: ### Task 5.1: Chain makespan plugin
205: - **Location**: `plugins/analysis_chain_makespan/`
206: - **Description**: Compute sequence makespan, idle gaps, and critical path effects with evidence.
207: - **Complexity**: 7
208: - **Dependencies**: Sprint 4
209: - **Acceptance Criteria**:
210:   - Outputs per-sequence makespan metrics.
211: - **Validation**:
212:   - Synthetic fixture with known makespan.
213: 
214: ### Task 5.2: Concurrency reconstruction plugin
215: - **Location**: `plugins/analysis_concurrency_reconstruction/`
216: - **Description**: Reconstruct concurrency via START/END overlaps; output distribution and peak concurrency.
217: - **Complexity**: 7
218: - **Dependencies**: Sprint 4
219: - **Acceptance Criteria**:
220:   - Deterministic concurrency metrics with evidence.
221: - **Validation**:
222:   - Fixture with known overlaps.
223: 
224: ### Task 5.3: Capacity scaling model plugin
225: - **Location**: `plugins/analysis_capacity_scaling/`
226: - **Description**: Apply modeled scaling to eligible-wait bucket; emit modeled reduction stats with explicit assumptions.
227: - **Complexity**: 6
228: - **Dependencies**: Tasks 5.1–5.2
229: - **Acceptance Criteria**:
230:   - Modeled outputs labeled with assumptions and measurement_type.
231: - **Validation**:
232:   - Evaluation checks modeled metrics within tolerance.
233: 
234: ### Task 5.4: Quorum parity evaluation suite
235: - **Location**: `tests/fixtures/`, `tests/test_quorum_evaluation.py`, `tests/plugins/*`
236: - **Description**: Add/extend fixture and expected metrics/findings to enforce parity for queue plugins.
237: - **Complexity**: 6
238: - **Dependencies**: Sprint 4–5 plugins
239: - **Acceptance Criteria**:
240:   - Evaluation passes only when metrics are within tolerance.
241: - **Validation**:
242: - `python -m pytest -q` passes with parity suite.
243: 
244: ## Phase Gate: Phase 1 Completion
245: **Requirement**: Do not begin Sprint 6 until Sprint 1–5 are complete and `python -m pytest -q` passes with the queue/capacity parity suite enabled.
246: 
247: ## Sprint 6: Phase 2 Tenancy + Auth Foundation
248: **Goal**: Implement multi-tenant isolation and authentication while preserving Phase 1 local-only constraints.
249: **Demo/Validation**:
250: - Two tenants can run analyses without data leakage.
251: - Auth gates UI/API and CLI actions deterministically.
252: 
253: ### Task 6.1: Tenancy model + migration plan
254: - **Location**: `src/statistic_harness/core/tenancy.py`, `src/statistic_harness/core/migrations.py`, `docs/references.md`
255: - **Description**: Implement shared-DB tenancy with `tenant_id` columns. Add a TenantContext (tenant_id, appdata_root, db_path) and migration path for existing data.
256: - **Complexity**: 8
257: - **Dependencies**: None
258: - **Acceptance Criteria**:
259:   - Default tenant created for existing data; tenant context resolves DB + appdata paths deterministically.
260: - **Validation**:
261:   - Migration test that upgrades an existing DB into tenant-aware state.
262: 
263: ### Task 6.2: Tenant-aware storage and appdata isolation
264: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/utils.py`, `src/statistic_harness/core/pipeline.py`
265: - **Description**: Scope all storage queries and file paths by tenant (tenant_id column or per-tenant DB + appdata root). Update pipeline initialization accordingly.
266: - **Complexity**: 8
267: - **Dependencies**: Task 6.1
268: - **Acceptance Criteria**:
269:   - Tenant A cannot access Tenant B datasets or files.
270: - **Validation**:
271:   - New tests for tenant isolation and path scoping.
272: 
273: ### Task 6.3: Auth tables + middleware
274: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`, `src/statistic_harness/cli.py`
275: - **Description**: Add users/tenants/memberships and token/API key storage; implement login + session auth and API key auth for UI/API and CLI operations (with strong password hashing and session/token rotation).
276: - **Complexity**: 8
277: - **Dependencies**: Tasks 6.1, 6.2
278: - **Acceptance Criteria**:
279:   - Auth required for protected routes; unauthenticated requests fail.
280: - **Validation**:
281:   - UI/API auth tests with valid/invalid tokens.
282: 
283: ### Task 6.4: Tenant-aware uploads + runs
284: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/core/storage.py`
285: - **Description**: Ensure uploads, runs, and artifacts are stored under tenant-specific namespaces and recorded with tenant_id.
286: - **Complexity**: 7
287: - **Dependencies**: Task 6.2
288: - **Acceptance Criteria**:
289:   - Upload and run records are isolated by tenant.
290: - **Validation**:
291:   - Integration tests with two tenants performing uploads and runs.
292: 
293: ### Task 6.5: Admin bootstrap + tenant/user management flows
294: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`, `src/statistic_harness/cli.py`
295: - **Description**: Add bootstrap flow for initial admin creation, tenant creation, user invites, token/API key issuance, and revocation/reset workflows.
296: - **Complexity**: 7
297: - **Dependencies**: Task 6.3
298: - **Acceptance Criteria**:
299:   - New tenants/users can be created and revoked without DB edits.
300: - **Validation**:
301:   - UI/CLI tests for admin bootstrap and token revocation.
302: 
303: ### Task 6.6: Phase 2 feature flags
304: - **Location**: `src/statistic_harness/core/utils.py`, `src/statistic_harness/ui/server.py`, `src/statistic_harness/cli.py`
305: - **Description**: Add config flags to gate tenancy/auth/vector store behavior and preserve Phase 1 defaults. Wire flags through UI/API/CLI and add tests for flag on/off behavior.
306: - **Complexity**: 5
307: - **Dependencies**: Task 6.1
308: - **Acceptance Criteria**:
309:   - Phase 1 behavior remains unchanged when flags are off.
310: - **Validation**:
311:   - Tests covering flag-disabled and flag-enabled modes.
312: 
313: ## Sprint 7: Phase 2 Vector Store
314: **Goal**: Implement a local vector store for embeddings with tenant isolation and deterministic queries.
315: **Demo/Validation**:
316: - Add/query/delete vectors locally with deterministic nearest-neighbor results.
317: 
318: ### Task 7.1: Vector store interface + sqlite-vec backend
319: - **Location**: `src/statistic_harness/core/vector_store.py`, `src/statistic_harness/core/migrations.py`
320: - **Description**: Replace the placeholder with a concrete API (`add`, `query`, `delete`) backed by builtin `sqlite-vec`. Store vectors per tenant and enforce deterministic ordering.
321: - **Complexity**: 7
322: - **Dependencies**: Sprint 6
323: - **Acceptance Criteria**:
324:   - Vector store supports add/query/delete with deterministic ordering.
325: - **Validation**:
326:   - Unit tests for vector operations and ordering.
327: 
328: ### Task 7.2: Embedding pipeline integration
329: - **Location**: `plugins/llm_prompt_builder/*`, `plugins/*/plugin.yaml`, optional new `plugins/analysis_vector_index/`
330: - **Description**: Add a plugin (or extend existing) that generates local embeddings (offline; e.g., TF-IDF or deterministic hashing) and indexes them in the vector store for retrieval.
331: - **Complexity**: 6
332: - **Dependencies**: Task 7.1
333: - **Acceptance Criteria**:
334:   - Vector index is created and queryable for a run.
335: - **Validation**:
336:   - Integration test that indexes findings and retrieves expected neighbors.
337: 
338: ### Task 7.3: Tenant isolation for vector store
339: - **Location**: `src/statistic_harness/core/vector_store.py`, `tests/test_vector_store.py`
340: - **Description**: Enforce tenant scoping in vector store queries and deletes.
341: - **Complexity**: 5
342: - **Dependencies**: Task 7.1
343: - **Acceptance Criteria**:
344:   - Tenant A cannot query or delete Tenant B vectors.
345: - **Validation**:
346:   - Isolation tests for vector operations.
347: 
348: ## Sprint 8: Documentation + Regression Suite
349: **Goal**: Update documentation and ensure all new capabilities are covered by tests.
350: **Demo/Validation**:
351: - `python -m pytest -q` green; docs describe Phase 2 and queue plugins.
352: 
353: ### Task 8.1: Documentation updates
354: - **Location**: `README.md`, `docs/plugin_dev_guide.md`, `docs/references.md`
355: - **Description**: Document determinism, budgets, PII policy, queue plugins, multi-tenant/auth model, and vector store usage.
356: - **Complexity**: 3
357: - **Dependencies**: Sprints 1–7
358: - **Acceptance Criteria**:
359:   - Docs match actual behavior and configs.
360: - **Validation**:
361:   - Manual doc review; spot-check with example commands.
362: 
363: ### Task 8.2: Integration/regression tests expansion
364: - **Location**: `tests/*`
365: - **Description**: Add integration tests that cover queue plugin suite, tenancy isolation, vector store, and PII redaction.
366: - **Complexity**: 6
367: - **Dependencies**: Sprints 2–7
368: - **Acceptance Criteria**:
369:   - New tests pass and cover Phase 2 behavior.
370: - **Validation**:
371:   - `python -m pytest -q` passes.
372: 
373: ### Task 8.3: Update migration fixtures for new schema versions
374: - **Location**: `tests/fixtures/db/*`, `tests/test_migrations.py`
375: - **Description**: Generate new golden DB fixtures and update expectations for added tables/columns (PII tags, tenancy, vector store).
376: - **Complexity**: 4
377: - **Dependencies**: Sprints 2, 6, 7
378: - **Acceptance Criteria**:
379:   - Migration tests pass for all versions including new schemas.
380: - **Validation**:
381:   - `python -m pytest -q` passes `test_migrations.py`.
382: 
383: ## Testing Strategy
384: - Determinism tests: run pipeline twice with identical seeds and diff outputs.
385: - Security tests: upload size limits, path traversal, offline network guards.
386: - Queue/capacity plugin tests: synthetic fixtures + Quorum parity evaluation.
387: - Governance tests: PII tagging plus cloud/API egress anonymization via entity hash table; budget metadata presence.
388: - Tenancy/auth tests: tenant isolation across uploads, runs, vector queries.
389: - Vector store tests: add/query/delete with deterministic ordering and isolation using `sqlite-vec`.
390: 
391: ## Potential Risks & Gotchas
392: - Determinism may be violated by third-party libs; enforce seeding and canonical ordering.
393: - Queue/capacity plugins rely on inferred roles; ambiguous inference should emit warnings and not fail.
394: - PII anonymization must avoid leaking raw values via cloud/API egress; enforce prompt sanitization and audit tests.
395: - `sqlite-vec` behavior/performance may vary by build; keep queries bounded and deterministic.
396: - Tenant-scoped hashing must be stable for each tenant and salted to avoid cross-tenant linkage.
397: 
398: ## Rollback Plan
399: - Gate Phase 2 features behind config flags to keep Phase 1 behavior stable.
400: - Keep migrations additive and reversible where feasible (avoid destructive schema changes).
401: - Retain legacy code paths for report generation until new schema changes are validated.
````

## File: cohesive-plan-docs-phase2-todos-plan.md
````markdown
  1: # Plan: Cohesive Roadmap (3 Plans + Phase 2 TODOs)
  2: 
  3: **Generated**: February 2, 2026
  4: **Estimated Complexity**: High
  5: 
  6: ## Overview
  7: This plan unifies the three existing plans (kernel/plugin stability, queue/capacity plugins + evaluation, and 1‑32‑2026 recommendations) into one sequenced roadmap and adds explicit Phase 2 TODO/Not‑Implemented closure. It prioritizes the four pillars (performance, security, accuracy, citeability) equally, enforces Phase 1 offline constraints, and gates Phase 2 on a fully passing test suite. It also encodes user decisions: shared DB tenancy via `tenant_id`, auth via both session tokens and API keys, sqlite-vec builtin for vectors, no dataset row truncation (streaming ingest for million‑row sheets), loose performance thresholds, and PII anonymization via entity hashes before any explicit cloud/API egress.
  8: 
  9: ## Prerequisites
 10: - Python 3.11+ (`python3` is available) and `python -m pytest -q` passes on baseline before changes.
 11: - SQLite with JSON1 and builtin `sqlite-vec` extension.
 12: - Real Quorum‑style fixture for queue/capacity parity validation.
 13: - Phase 1 must remain local‑only (no runtime network calls).
 14: - Default top‑K vector query target is 10 (configurable only if explicitly required).
 15: 
 16: ## Sprint 1: Determinism + Guardrails Baseline
 17: **Goal**: Close determinism gaps and security guardrails before feature expansion.
 18: **Demo/Validation**:
 19: - Run the same pipeline twice with the same seed → byte‑stable `report.json`.
 20: - Oversized uploads and traversal attempts are rejected deterministically.
 21: 
 22: ### Task 1.1: Deterministic seeding + canonical JSON
 23: - **Location**: `src/statistic_harness/core/plugin_runner.py`, `src/statistic_harness/core/pipeline.py`, `src/statistic_harness/core/utils.py`, `src/statistic_harness/core/report.py`
 24: - **Description**: Seed `random`/NumPy (if available), set stable env (`PYTHONHASHSEED`, `TZ=UTC`, `LC_ALL=C`), and add canonical JSON formatting for report output ordering and float precision.
 25: - **Complexity**: 6
 26: - **Dependencies**: None
 27: - **Acceptance Criteria**:
 28:   - Identical runs on the same dataset produce byte‑identical `report.json`.
 29: - **Validation**:
 30:   - New determinism test runs pipeline twice and diffs output.
 31: 
 32: ### Task 1.2: Offline + path + upload guardrails
 33: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/core/utils.py`, `src/statistic_harness/core/storage.py`, `tests/test_security_paths.py`, `tests/test_offline.py`
 34: - **Description**: Enforce max upload size and path traversal protections; keep offline network denial in UI + plugin runner.
 35: - **Complexity**: 5
 36: - **Dependencies**: None
 37: - **Acceptance Criteria**:
 38:   - Oversized uploads fail with a deterministic error.
 39:   - Path traversal attempts fail.
 40: - **Validation**:
 41:   - Security tests for size + traversal + offline access.
 42: 
 43: ### Task 1.3: Performance smoke test (loose thresholds)
 44: - **Location**: `tests/test_performance_smoke.py`
 45: - **Description**: Add a small benchmark with loose thresholds to catch regressions without blocking normal runs.
 46: - **Complexity**: 3
 47: - **Dependencies**: Task 1.1
 48: - **Acceptance Criteria**:
 49:   - Performance smoke test is stable on local dev.
 50: - **Validation**:
 51:   - `python -m pytest -q` passes including the smoke test.
 52: 
 53: ### Task 1.4: Kernel boundary security guardrails
 54: - **Location**: `src/statistic_harness/core/utils.py`, `src/statistic_harness/core/storage.py`, `src/statistic_harness/ui/server.py`, `tests/test_security_paths.py`
 55: - **Description**: Enforce file type/size validation, block `pickle`/`eval`/shelling in analysis paths, and add tests for each guardrail.
 56: - **Complexity**: 5
 57: - **Dependencies**: Task 1.2
 58: - **Acceptance Criteria**:
 59:   - Disallowed file types, sizes, and dangerous operations are rejected deterministically.
 60: - **Validation**:
 61:   - Security tests cover each guardrail.
 62: 
 63: ## Sprint 2: Plugin Contracts + Isolation
 64: **Goal**: Harden plugin interfaces and enforce isolation via subprocess execution.
 65: **Demo/Validation**:
 66: - `stat-harness list-plugins` shows validated manifest metadata.
 67: - Plugins run in subprocesses with audit entries for every execution.
 68: 
 69: ### Task 2.1: Plugin manifest schema validation
 70: - **Location**: `docs/plugin_manifest.schema.json`, `src/statistic_harness/core/plugin_manager.py`, `plugins/*/plugin.yaml`
 71: - **Description**: Define required manifest fields (id, version, type, entrypoint, schemas, sandbox flags) and validate on load.
 72: - **Complexity**: 6
 73: - **Dependencies**: None
 74: - **Acceptance Criteria**:
 75:   - Invalid manifests fail fast with clear errors.
 76: - **Validation**:
 77:   - Unit tests for manifest validation.
 78: 
 79: ### Task 2.2: Per‑plugin config/output schemas
 80: - **Location**: `plugins/*/config.schema.json`, `plugins/*/output.schema.json`
 81: - **Description**: Add minimal deterministic schemas for plugin inputs/outputs (including measurement labels and evidence fields).
 82: - **Complexity**: 7
 83: - **Dependencies**: Task 2.1
 84: - **Acceptance Criteria**:
 85:   - Kernel validates config and output per plugin run.
 86: - **Validation**:
 87:   - Schema validation tests.
 88: 
 89: ### Task 2.3: Subprocess runner + sandbox + audit log
 90: - **Location**: `src/statistic_harness/core/plugin_runner.py`, `src/statistic_harness/core/pipeline.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`
 91: - **Description**: Execute plugins in subprocesses with deterministic I/O, block network, restrict FS allowlist, and record `plugin_executions` metrics.
 92: - **Complexity**: 8
 93: - **Dependencies**: Task 2.1
 94: - **Acceptance Criteria**:
 95:   - Every plugin execution writes an audit row.
 96:   - Network/file access outside allowlist fails.
 97: - **Validation**:
 98:   - Integration tests with a “malicious” plugin fixture.
 99: 
100: ### Task 2.4: Deterministic parallel scheduling
101: - **Location**: `src/statistic_harness/core/pipeline.py`, `src/statistic_harness/core/report.py`
102: - **Description**: Run independent plugins in parallel by DAG layer and preserve deterministic ordering in report assembly.
103: - **Complexity**: 6
104: - **Dependencies**: Task 2.3
105: - **Acceptance Criteria**:
106:   - Parallel execution yields identical output to sequential runs.
107: - **Validation**:
108:   - Integration test comparing sequential vs parallel outputs.
109: 
110: ### Task 2.5: Kernel minimization + plugin‑only steps
111: - **Location**: `src/statistic_harness/core/pipeline.py`, `src/statistic_harness/core/plugin_manager.py`, `plugins/planner_basic/*`, `plugins/transform_template/*`
112: - **Description**: Remove kernel fallback logic so planner/transform/analysis/report/llm steps are plugin‑only; fail closed with error summaries when missing.
113: - **Complexity**: 6
114: - **Dependencies**: Task 2.1
115: - **Acceptance Criteria**:
116:   - Auto‑runs require planner plugin output; transform runs only via plugin.
117: - **Validation**:
118:   - Unit tests for missing‑plugin error summaries.
119: 
120: ## Sprint 3: Storage + Ingest Completeness
121: **Goal**: Store all rows in SQLite (no truncation), dedupe uploads, and enforce append‑only raw data.
122: **Demo/Validation**:
123: - >1M‑row sheet ingests without truncation.
124: - Re‑uploading identical data reuses dataset IDs.
125: 
126: ### Task 3.1: Migration framework + thread‑safe DB access
127: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`, `src/statistic_harness/ui/server.py`, `tests/test_migrations.py`
128: - **Description**: Ensure versioned migrations, WAL, and per‑request connections; add missing migrations if needed.
129: - **Complexity**: 6
130: - **Dependencies**: None
131: - **Acceptance Criteria**:
132:   - Legacy DB upgrades to latest schema without data loss.
133: - **Validation**:
134:   - Migration tests pass for all fixtures.
135: 
136: ### Task 3.2: Streaming ingest (no truncation)
137: - **Location**: `plugins/ingest_tabular/plugin.py`, `src/statistic_harness/core/dataset_io.py`
138: - **Description**: Stream CSV/XLSX/JSON in chunks, compute full row counts, and persist all rows to SQLite without caps.
139: - **Complexity**: 7
140: - **Dependencies**: Task 3.1
141: - **Acceptance Criteria**:
142:   - Large fixtures ingest without truncation and without full‑file memory load.
143: - **Validation**:
144:   - Large‑fixture test asserting exact row count.
145: 
146: ### Task 3.3: Column canonicalization + safe identifiers
147: - **Location**: `src/statistic_harness/core/utils.py`, `plugins/ingest_tabular/plugin.py`, `src/statistic_harness/core/storage.py`
148: - **Description**: Map raw headers to safe IDs, store mapping in `dataset_columns`, and guard against SQL injection in identifiers.
149: - **Complexity**: 5
150: - **Dependencies**: Task 3.2
151: - **Acceptance Criteria**:
152:   - Hostile headers ingest without SQL errors.
153: - **Validation**:
154:   - Fuzz tests for header normalization.
155: 
156: ### Task 3.4: Append‑only enforcement + dedupe
157: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`, `src/statistic_harness/ui/server.py`
158: - **Description**: Add triggers to block UPDATE/DELETE on raw tables, hash uploads for dedupe, and reuse dataset IDs for identical content.
159: - **Complexity**: 6
160: - **Dependencies**: Task 3.2
161: - **Acceptance Criteria**:
162:   - UPDATE/DELETE fails for raw row tables.
163:   - Duplicate uploads map to the same dataset.
164: - **Validation**:
165:   - Tests for append‑only enforcement and dedupe behavior.
166: 
167: ### Task 3.5: DB‑first dataset access
168: - **Location**: `src/statistic_harness/core/dataset_io.py`, `src/statistic_harness/core/pipeline.py`
169: - **Description**: Ensure analysis plugins read datasets exclusively from SQLite (no raw upload path access), with chunked iteration helpers.
170: - **Complexity**: 5
171: - **Dependencies**: Task 3.2
172: - **Acceptance Criteria**:
173:   - Analysis plugins operate without reading original upload files.
174: - **Validation**:
175:   - Integration test asserts DB row counts are used in report.
176: 
177: ### Task 3.6: Golden DB fixtures + migration safety
178: - **Location**: `tests/fixtures/db/*`, `tests/test_migrations.py`
179: - **Description**: Generate a small SQLite fixture per schema version and verify upgrades preserve core data (runs/results/templates).
180: - **Complexity**: 5
181: - **Dependencies**: Task 3.1
182: - **Acceptance Criteria**:
183:   - Migrations pass across all versions with preserved data.
184: - **Validation**:
185:   - `python -m pytest -q` passes `test_migrations.py`.
186: 
187: ## Sprint 4: Evidence‑First Reporting + Evaluation
188: **Goal**: Guarantee citeable outputs with evidence links, evaluator parity, and measurement labeling.
189: **Demo/Validation**:
190: - `report.md` and `report.json` always generated and validate against schema.
191: 
192: ### Task 4.1: Versioned results + evidence schema
193: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/report.py`, `docs/report.schema.json`
194: - **Description**: Store versioned plugin results with hashes, require evidence entries per finding (row/column references, query snippets), and always emit `report.md` + `report.json` even if plugins fail.
195: - **Complexity**: 6
196: - **Dependencies**: Sprint 3
197: - **Acceptance Criteria**:
198:   - Re‑running a plugin creates a new result record.
199:   - Reports fail validation if evidence is missing.
200: - **Validation**:
201:   - Schema validation tests.
202: 
203: ### Task 4.2: Evaluator harness + expected metrics
204: - **Location**: `src/statistic_harness/core/evaluation.py`, `docs/evaluation.md`, `tests/test_evaluation.py`
205: - **Description**: Add numeric expected metrics with abs/rel tolerance; default strict evaluation (unexpected findings fail unless allowed).
206: - **Complexity**: 5
207: - **Dependencies**: Task 4.1
208: - **Acceptance Criteria**:
209:   - Evaluation fails when metrics drift beyond tolerance.
210: - **Validation**:
211:   - Unit tests with known expected metrics.
212: 
213: ### Task 4.3: Measurement labeling contract
214: - **Location**: `docs/report.schema.json`, `src/statistic_harness/core/report.py`, `plugins/*/output.schema.json`
215: - **Description**: Require `measurement_type` (measured/modeled/not_applicable/error) in findings and metrics.
216: - **Complexity**: 4
217: - **Dependencies**: Task 4.2
218: - **Acceptance Criteria**:
219:   - All plugin outputs validate with measurement labels.
220: - **Validation**:
221:   - Schema tests + plugin output tests.
222: 
223: ### Task 4.4: AutoPlanner + capability tags
224: - **Location**: `plugins/*/plugin.yaml`, `src/statistic_harness/core/planner.py`, `src/statistic_harness/core/pipeline.py`
225: - **Description**: Add capability tags (needs_eventlog, needs_timestamp, etc.) and deterministic planner selection based on profile metadata.
226: - **Complexity**: 6
227: - **Dependencies**: Task 2.1, Sprint 3
228: - **Acceptance Criteria**:
229:   - Same dataset yields identical plan; event‑log datasets select queue plugins.
230: - **Validation**:
231:   - Snapshot tests for planner output.
232: 
233: ### Task 4.5: Parameter entities + normalization
234: - **Location**: `src/statistic_harness/core/storage.py`, `plugins/profile_basic/plugin.py`
235: - **Description**: Add `parameter_entities`, `parameter_kv`, and `row_parameter_link` tables; normalize parameter text into deterministic entities.
236: - **Complexity**: 6
237: - **Dependencies**: Sprint 3
238: - **Acceptance Criteria**:
239:   - Textual variants map to the same entity_id.
240: - **Validation**:
241:   - Fuzz tests on whitespace/case/order variants.
242: 
243: ### Task 4.6: Column role inference (general)
244: - **Location**: `plugins/profile_basic/plugin.py`, `src/statistic_harness/core/utils.py`
245: - **Description**: Infer probable roles (case/process id/activity/timestamp/measure) for non‑eventlog datasets to feed AutoPlanner.
246: - **Complexity**: 5
247: - **Dependencies**: Task 4.5
248: - **Acceptance Criteria**:
249:   - Role inference is deterministic for fixtures.
250: - **Validation**:
251:   - Unit tests for role assignment.
252: 
253: ### Task 4.7: Lineage graph tables + trace queries
254: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/lineage.py`, `src/statistic_harness/core/migrations.py`
255: - **Description**: Add `entities`/`edges` tables, recursive CTE trace queries, and indexes for graph traversal.
256: - **Complexity**: 6
257: - **Dependencies**: Task 4.5
258: - **Acceptance Criteria**:
259:   - Trace query returns deterministic reachable nodes.
260: - **Validation**:
261:   - Golden test for trace results.
262: 
263: ## Sprint 5: Queue/Capacity Plugin Suite + Parity
264: **Goal**: Implement queue/capacity plugins with deterministic parity against real fixtures.
265: **Demo/Validation**:
266: - Quorum fixture parity suite passes.
267: 
268: ### Task 5.1: Field inference + ERP scoping
269: - **Location**: `plugins/profile_eventlog/`, `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/ui/server.py`
270: - **Description**: Infer event‑log roles with confidence scores; store candidates; scope known‑issues by ERP type (default “unknown”).
271: - **Complexity**: 7
272: - **Dependencies**: Sprint 3
273: - **Acceptance Criteria**:
274:   - Deterministic role map with confidence scores and warnings for low confidence.
275: - **Validation**:
276:   - Unit tests on synthetic event‑log fixtures.
277: 
278: ### Task 5.2: Core queue plugins (decomposition → attribution)
279: - **Location**: `plugins/analysis_queue_delay_decomposition/`, `plugins/analysis_dependency_resolution_join/`, `plugins/analysis_sequence_classification/`, `plugins/analysis_tail_isolation/`, `plugins/analysis_percentile_analysis/`, `plugins/analysis_attribution/`, `plugins/analysis_determinism_discipline/`
280: - **Description**: Implement measured findings with evidence and measurement labels; skip gracefully when roles are missing.
281: - **Complexity**: 8
282: - **Dependencies**: Task 5.1
283: - **Acceptance Criteria**:
284:   - Deterministic metrics and labeled outputs for each plugin.
285: - **Validation**:
286:   - Unit tests per plugin with synthetic fixtures.
287: 
288: ### Task 5.3: Modeled plugins (sequence + concurrency + scaling)
289: - **Location**: `plugins/analysis_chain_makespan/`, `plugins/analysis_concurrency_reconstruction/`, `plugins/analysis_capacity_scaling/`
290: - **Description**: Add modeled outputs with explicit assumptions and evidence, separating measured vs modeled results.
291: - **Complexity**: 7
292: - **Dependencies**: Task 5.2
293: - **Acceptance Criteria**:
294:   - Modeled outputs include assumptions and are labeled correctly.
295: - **Validation**:
296:   - Plugin tests with known fixtures.
297: 
298: ### Task 5.4: Real‑fixture parity suite
299: - **Location**: `tests/fixtures/`, `tests/test_quorum_evaluation.py`, `tests/plugins/*`
300: - **Description**: Add the real Quorum fixture and expected metrics/findings with loose thresholds.
301: - **Complexity**: 6
302: - **Dependencies**: Tasks 5.1–5.3
303: - **Acceptance Criteria**:
304:   - Parity suite passes only when metrics match within tolerance.
305: - **Validation**:
306:   - `python -m pytest -q` passes with parity suite.
307: 
308: ### Task 5.5: Process sequence mining plugin
309: - **Location**: `plugins/analysis_process_sequence/*`, `src/statistic_harness/core/utils.py`
310: - **Description**: Implement sequence mining (variants, transitions, anomalies) with evidence row references and parameter entity linkage.
311: - **Complexity**: 6
312: - **Dependencies**: Sprint 3, Task 4.1
313: - **Acceptance Criteria**:
314:   - Findings include evidence and are deterministic for fixtures.
315: - **Validation**:
316:   - Plugin unit tests with synthetic event‑log fixtures.
317: 
318: ### Task 5.6: Statistical controls module
319: - **Location**: `src/statistic_harness/core/stat_controls.py`
320: - **Description**: Add effect sizes, multiple‑testing corrections (BH/FDR), and confidence scoring utilities with deterministic outputs.
321: - **Complexity**: 5
322: - **Dependencies**: None
323: - **Acceptance Criteria**:
324:   - Deterministic results for fixed inputs.
325: - **Validation**:
326:   - Unit tests on known distributions.
327: 
328: ### Task 5.7: Integrate statistical controls into analysis plugins
329: - **Location**: `plugins/analysis_*/*`, `plugins/*/output.schema.json`
330: - **Description**: Apply corrections and add confidence/score fields to findings and metrics across analysis plugins.
331: - **Complexity**: 6
332: - **Dependencies**: Task 5.6
333: - **Acceptance Criteria**:
334:   - Findings include confidence fields; results remain deterministic.
335: - **Validation**:
336:   - Regression tests on synthetic null datasets.
337: 
338: ### Task 5.8: Manual role overrides (optional)
339: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`
340: - **Description**: Allow per‑project role overrides when inference is ambiguous; overrides are deterministic and never required to run.
341: - **Complexity**: 5
342: - **Dependencies**: Task 5.1
343: - **Acceptance Criteria**:
344:   - Overrides take precedence over inferred roles when present.
345: - **Validation**:
346:   - Unit test verifying override precedence.
347: 
348: ## Sprint 6: UI + Ops Workflow (Phase 1)
349: **Goal**: Deliver end‑to‑end project/dataset visibility and operational controls.
350: **Demo/Validation**:
351: - Upload → run flow is deterministic; project list and trace view render.
352: 
353: ### Task 6.1: Upload flow improvements
354: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`
355: - **Description**: Validate file types/sizes, compute hash at upload, auto‑create run, and return run_id immediately.
356: - **Complexity**: 5
357: - **Dependencies**: Sprint 3
358: - **Acceptance Criteria**:
359:   - Upload returns a run_id without manual steps.
360: - **Validation**:
361:   - FastAPI TestClient upload tests.
362: 
363: ### Task 6.2: Project/dataset views + trace UI
364: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`, `src/statistic_harness/core/lineage.py`
365: - **Description**: Add project index, dataset metadata, and a trace endpoint/UI using lineage graph queries.
366: - **Complexity**: 6
367: - **Dependencies**: Task 4.7
368: - **Acceptance Criteria**:
369:   - Trace view returns deterministic graph results.
370: - **Validation**:
371:   - UI smoke tests for project list and trace endpoints.
372: 
373: ### Task 6.3: Mapping presets + combined analysis filters
374: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`
375: - **Description**: Persist mapping presets per raw format and add filtered combined runs (project/format/date range).
376: - **Complexity**: 6
377: - **Dependencies**: Sprint 3
378: - **Acceptance Criteria**:
379:   - Presets can be saved/selected; combined runs respect filters.
380: - **Validation**:
381:   - UI tests for presets + filter fields.
382: 
383: ### Task 6.4: Backfill + delivery tracking
384: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`, `src/statistic_harness/cli.py`, `src/statistic_harness/ui/server.py`
385: - **Description**: Add job queue + delivery tables, backfill CLI, and UI controls to mark delivered/stale based on hashes.
386: - **Complexity**: 7
387: - **Dependencies**: Task 4.1
388: - **Acceptance Criteria**:
389:   - Backfill schedules jobs deterministically; delivery state updates correctly.
390: - **Validation**:
391:   - Integration tests for backfill + delivery state.
392: 
393: ## Phase Gate: Phase 1 Completion
394: **Requirement**: Do not start Phase 2 until Sprints 1–6 are complete and `python -m pytest -q` passes fully.
395: 
396: ## Sprint 7: Phase 2 Tenancy + Auth (and TODO Closure)
397: **Goal**: Add tenant isolation and auth while preserving local‑only behavior by default.
398: **Demo/Validation**:
399: - Two tenants can run analyses without data leakage.
400: - Auth gates UI/API/CLI deterministically.
401: 
402: ### Task 7.1: Tenant context + migrations
403: - **Location**: `src/statistic_harness/core/tenancy.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`
404: - **Description**: Implement shared‑DB tenancy via `tenant_id` columns, tenant‑scoped appdata paths, and migration of legacy data into default tenant.
405: - **Complexity**: 7
406: - **Dependencies**: Phase Gate
407: - **Acceptance Criteria**:
408:   - Tenant A cannot access Tenant B data.
409: - **Validation**:
410:   - Tenant isolation tests (uploads, runs, vectors).
411: 
412: ### Task 7.2: Auth tables + middleware + admin flows
413: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/*`, `src/statistic_harness/cli.py`, `src/statistic_harness/core/auth.py`
414: - **Description**: Add user/tenant/membership tables, session tokens and API keys, login + bootstrap admin, and CLI user/key management.
415: - **Complexity**: 8
416: - **Dependencies**: Task 7.1
417: - **Acceptance Criteria**:
418:   - Protected routes require auth; CLI respects API keys.
419: - **Validation**:
420:   - UI/API auth tests with valid/invalid tokens.
421: 
422: ### Task 7.3: Phase 2 flags + TODO closure
423: - **Location**: `src/statistic_harness/core/utils.py`, `src/statistic_harness/ui/server.py`, `src/statistic_harness/cli.py`, `codex/PROJECT_SPEC.md`, `README.md`
424: - **Description**: Add feature flags for tenancy/auth/vector store and remove “Phase 2 stub only” language; scan repo for TODO/NotImplemented markers and resolve or document them.
425: - **Complexity**: 4
426: - **Dependencies**: Task 7.2
427: - **Acceptance Criteria**:
428:   - Phase 1 behavior is unchanged when flags are off.
429:   - Phase 2 TODOs are resolved or explicitly tracked.
430: - **Validation**:
431:   - Flag‑on/off tests; doc update review.
432: 
433: ## Sprint 8: Phase 2 Vector Store + Stateless Flow
434: **Goal**: Provide a local vector store with deterministic, stateless, repeatable query flow.
435: **Demo/Validation**:
436: - Add/query/delete vectors with deterministic ordering and tenant isolation.
437: 
438: ### Task 8.1: sqlite‑vec backend + registry
439: - **Location**: `src/statistic_harness/core/vector_store.py`, `src/statistic_harness/core/migrations.py`
440: - **Description**: Implement add/query/delete on sqlite‑vec, store collection metadata in DB, and enforce tenant scoping.
441: - **Complexity**: 6
442: - **Dependencies**: Sprint 7
443: - **Acceptance Criteria**:
444:   - Vector queries return deterministic ordering.
445: - **Validation**:
446:   - Unit tests for add/query/delete and isolation.
447: 
448: ### Task 8.2: Deterministic embedding/indexing plugin
449: - **Location**: `plugins/report_bundle/plugin.py` (or new `plugins/analysis_vector_index/`)
450: - **Description**: Create deterministic embeddings (hash‑based or TF‑IDF) and index findings/notes into the vector store.
451: - **Complexity**: 5
452: - **Dependencies**: Task 8.1
453: - **Acceptance Criteria**:
454:   - Index is created and queryable for a run.
455: - **Validation**:
456:   - Integration test that indexes and retrieves expected neighbors.
457: 
458: ### Task 8.3: Two‑step stateless query flow (repeatable)
459: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/vectors.html`, `src/statistic_harness/cli.py`
460: - **Description**: Step 1 creates a query snapshot (as_of timestamp + signed cursor). Step 2 paginates via cursor only, ensuring repeatable results and deterministic ordering. Default top‑K is 10; allow override only when explicitly configured.
461: - **Complexity**: 6
462: - **Dependencies**: Task 8.1
463: - **Acceptance Criteria**:
464:   - Cursor‑only pagination works without server‑side state.
465:   - Results are repeatable using the same cursor.
466: - **Validation**:
467:   - API/UI tests for cursor pagination and repeatability.
468: 
469: ### Task 8.4: Ultra‑flexible vector collections
470: - **Location**: `src/statistic_harness/core/vector_store.py`, `src/statistic_harness/core/storage.py`
471: - **Description**: Support multiple collections per run, metadata filters, and future embedding variants without schema changes.
472: - **Complexity**: 4
473: - **Dependencies**: Task 8.1
474: - **Acceptance Criteria**:
475:   - Metadata filters restrict queries deterministically.
476: - **Validation**:
477:   - Unit tests for filtered queries.
478: 
479: ## Sprint 9: Phase 2 Governance + Docs + Regression
480: **Goal**: Add governance metadata and finalize documentation/tests.
481: **Demo/Validation**:
482: - `python -m pytest -q` passes with Phase 2 features enabled.
483: 
484: ### Task 9.1: Resource budget metadata
485: - **Location**: `src/statistic_harness/core/types.py`, `src/statistic_harness/core/report.py`, `docs/report.schema.json`, `plugins/*/output.schema.json`
486: - **Description**: Add budget metadata with defaults of “unlimited” and include in report output.
487: - **Complexity**: 4
488: - **Dependencies**: Sprint 7
489: - **Acceptance Criteria**:
490:   - Budget fields appear in all plugin outputs.
491: - **Validation**:
492:   - Schema tests for budget defaults.
493: 
494: ### Task 9.2: PII tagging + cloud‑egress anonymization
495: - **Location**: `plugins/profile_basic/plugin.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`, `plugins/llm_prompt_builder/*`, `tests/test_llm_prompt_builder.py`
496: - **Description**: Detect PII columns, store entity hashes in DB, and anonymize raw PII only when sending to explicit cloud/API egress (never during local analysis).
497: - **Complexity**: 6
498: - **Dependencies**: Sprint 7
499: - **Acceptance Criteria**:
500:   - Outbound payloads contain no raw PII; local reports still store full data internally.
501: - **Validation**:
502:   - Tests scan outbound payloads for PII patterns.
503: 
504: ### Task 9.3: Docs + migration fixtures + regression tests
505: - **Location**: `README.md`, `docs/*`, `tests/fixtures/db/*`, `tests/test_migrations.py`, `tests/*`
506: - **Description**: Update docs to reflect Phase 2 features and regenerate migration fixtures; expand regression coverage for tenancy, auth, vector store, and PII.
507: - **Complexity**: 5
508: - **Dependencies**: Sprint 7–8
509: - **Acceptance Criteria**:
510:   - Docs match actual behavior; migration fixtures pass.
511: - **Validation**:
512:   - `python -m pytest -q` passes with updated fixtures.
513: 
514: ## Testing Strategy
515: - Determinism: run pipeline twice; diff outputs.
516: - Security: upload size limits, traversal protections, offline network denial.
517: - AutoPlanner + lineage: snapshot planner output and trace query results.
518: - Queue/capacity: synthetic + real fixture parity with loose thresholds.
519: - Statistical controls: null dataset regression for low false positives.
520: - Tenancy/auth: isolation tests across uploads, runs, vectors, and API.
521: - Vector store: add/query/delete with deterministic ordering, cursor pagination, and isolation.
522: - PII governance: ensure anonymization only for explicit cloud/API egress.
523: 
524: ## Potential Risks & Gotchas
525: - Determinism drift from third‑party libs; enforce seeding and stable ordering.
526: - Large datasets may stress SQLite; rely on chunked inserts and indexes.
527: - Stateless cursors can be large; mitigate by compact encoding + HMAC signing.
528: - Role inference ambiguity can cause false results; emit warnings and not‑applicable findings.
529: - PII anonymization must never regress into outbound payloads; keep tests strict.
530: 
531: ## Rollback Plan
532: - Keep feature flags for Phase 2 to preserve Phase 1 behavior.
533: - Migrations remain additive; avoid destructive schema changes.
534: - Subprocess runner can fall back to in‑process execution for debugging (dev‑only).
````

## File: kernel-plugin-stability-plan.md
````markdown
  1: # Plan: Kernel-Plugin Stability & Isolation
  2: 
  3: **Generated**: January 31, 2026
  4: **Estimated Complexity**: High
  5: 
  6: ## Overview
  7: Implement the 10 stability recommendations with a plugin-first architecture: the kernel only orchestrates logging, plugin management, UI, and storage while all default logic/config lives in plugins (ingest, profile/validate, planner, transform, analysis, report, llm/offline prompt builder). Add OS-process isolation for plugins, strict schema validation, sandboxing, audit logging, deterministic parallel execution, template mapping presets, filtered combined analysis runs, and explicit determinism controls (run_seed propagation, canonical JSON, stable ordering). Enforce Phase 1 offline execution and citeable outputs (report.md + report.json schema validation, evaluator harness). Emphasize the four pillars equally: performance, security, accuracy, citeability.
  8: 
  9: ## Prerequisites
 10: - Python 3.11+ (existing project).
 11: - SQLite with JSON1 (already in use).
 12: - Ability to run `python -m pytest -q` locally.
 13: - Local-only runtime: no network calls in Phase 1 (plugins and UI).
 14: 
 15: ## Sprint 1: Plugin Contracts & Registry Hardening
 16: **Goal**: Make plugin metadata/schema mandatory and validated; establish stable contracts for inputs/outputs.
 17: **Demo/Validation**:
 18: - `python -m pytest -q` with new validation tests.
 19: - `stat-harness list-plugins` shows schema metadata.
 20: 
 21: ### Task 1.1: Define plugin manifest schema + required fields
 22: - **Location**: `docs/plugin_manifest.schema.json` (new), `src/statistic_harness/core/plugin_manager.py`
 23: - **Description**: Add a JSON schema for plugin manifests (`plugin.yaml`) requiring fields: `id`, `name`, `version`, `type` (ingest/profile/analysis/report/llm/planner/transform), `entrypoint`, `capabilities`, `config_schema`, `output_schema`, `sandbox` (no-network, fs-allowlist). Update `PluginManager` to validate against schema.
 24: - **Complexity**: 6
 25: - **Dependencies**: None
 26: - **Acceptance Criteria**:
 27:   - Invalid manifests fail fast with clear errors.
 28:   - All existing plugins include required fields.
 29: - **Validation**:
 30:   - Unit test for manifest validation.
 31: 
 32: ### Task 1.2: Add per-plugin config/output schemas
 33: - **Location**: `plugins/*/config.schema.json` (new), `plugins/*/output.schema.json` (new)
 34: - **Description**: Add JSON schemas for plugin input config and output structure. Keep schemas minimal but explicit for determinism/citeability (e.g., finding kinds, evidence and citation fields, stable ordering expectations).
 35: - **Complexity**: 7
 36: - **Dependencies**: Task 1.1
 37: - **Acceptance Criteria**:
 38:   - Kernel validates config and output for each plugin run.
 39: - **Validation**:
 40:   - Tests cover schema validation failure cases.
 41: 
 42: ### Task 1.3: Update plugin manifests to reference schemas
 43: - **Location**: `plugins/*/plugin.yaml`
 44: - **Description**: Add `config_schema` and `output_schema` references to each plugin manifest, plus sandbox flags.
 45: - **Complexity**: 5
 46: - **Dependencies**: Task 1.1, 1.2
 47: - **Acceptance Criteria**:
 48:   - All manifests pass validation.
 49: - **Validation**:
 50:   - `test_plugin_discovery` + new manifest tests.
 51: 
 52: ## Sprint 2: OS-Process Plugin Runner + Sandbox
 53: **Goal**: Run each plugin in a separate OS process with deterministic I/O, strict sandbox, and audit logging.
 54: **Demo/Validation**:
 55: - Plugins run via subprocess and return identical results to current flow.
 56: - Plugins cannot access network or files outside allowlist.
 57: 
 58: ### Task 2.1: Implement plugin runner subprocess
 59: - **Location**: `src/statistic_harness/core/plugin_runner.py` (new), `src/statistic_harness/core/pipeline.py`
 60: - **Description**: Add a subprocess runner that reads a request JSON (run context + settings), executes plugin entrypoint in a separate Python process, and writes response JSON. Kernel orchestrates runs via subprocess, not in-process.
 61: - **Complexity**: 8
 62: - **Dependencies**: Sprint 1 schemas
 63: - **Acceptance Criteria**:
 64:   - All plugins run in isolated processes.
 65: - **Validation**:
 66:   - Integration test runs pipeline and verifies reports.
 67: 
 68: ### Task 2.2: Sandboxing (no network + FS allowlist)
 69: - **Location**: `src/statistic_harness/core/plugin_runner.py`
 70: - **Description**: Block network by monkeypatching sockets/HTTP before plugin load; restrict file access to run dir + appdata + plugins dir by guarding `open()`/`Path` usage where feasible and validating file paths on kernel API boundaries.
 71: - **Complexity**: 7
 72: - **Dependencies**: Task 2.1
 73: - **Acceptance Criteria**:
 74:   - Network calls fail.
 75:   - Writes outside allowlist fail.
 76: - **Validation**:
 77:   - New tests with a "malicious" plugin fixture.
 78: 
 79: ### Task 2.3: Execution audit table + runtime metrics
 80: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/plugin_runner.py`
 81: - **Description**: Add `plugin_executions` table capturing: started/ended timestamps, duration, CPU user/system, max RSS, rows read/written, rows output, warnings count, exit status, stdout/stderr (truncated). Populate from runner and context wrappers.
 82: - **Complexity**: 8
 83: - **Dependencies**: Task 2.1
 84: - **Acceptance Criteria**:
 85:   - Every plugin run writes an audit row.
 86: - **Validation**:
 87:   - Tests verify audit rows exist and are deterministic.
 88: 
 89: ### Task 2.4: Deterministic execution context + run_seed propagation
 90: - **Location**: `src/statistic_harness/core/plugin_runner.py`, `src/statistic_harness/core/pipeline.py`, `src/statistic_harness/core/types.py`
 91: - **Description**: Propagate `run_seed` to all plugin runs; seed `random` and optional NumPy/Scipy if present; set `PYTHONHASHSEED`, `TZ=UTC`, `LC_ALL=C`, and stable locale; canonicalize JSON serialization (sorted keys, stable float rounding); ensure deterministic glob/file ordering in any kernel helpers.
 92: - **Complexity**: 7
 93: - **Dependencies**: Task 2.1
 94: - **Acceptance Criteria**:
 95:   - Identical runs on the same dataset produce byte-stable `report.json`.
 96: - **Validation**:
 97:   - Determinism tests run pipeline twice and diff outputs.
 98: 
 99: ### Task 2.5: Phase 1 offline enforcement (kernel + UI)
100: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/core/plugin_runner.py`, `tests/test_offline.py` (new)
101: - **Description**: Bind UI to localhost only, prevent external assets, and add kernel-level network deny checks so no runtime code can reach the network. Enforce hard-fail by default with a dev-only override flag (default off). Add tests that attempts to access external URLs fail.
102: - **Complexity**: 6
103: - **Dependencies**: Task 2.1
104: - **Acceptance Criteria**:
105:   - Network calls fail in plugins and UI runtime paths.
106: - **Validation**:
107:   - Offline tests pass under `python -m pytest -q`.
108: 
109: ## Sprint 3: Deterministic Parallel Execution
110: **Goal**: Run independent plugins in parallel while keeping deterministic ordering and results.
111: **Demo/Validation**:
112: - Multiple analysis plugins run concurrently without altering report ordering or contents.
113: 
114: ### Task 3.1: DAG-aware parallel scheduling
115: - **Location**: `src/statistic_harness/core/pipeline.py`
116: - **Description**: Group plugins by dependency level (toposort layers) and run each layer in parallel via subprocess runner; store results sorted by plugin_id in report output.
117: - **Complexity**: 7
118: - **Dependencies**: Sprint 2 runner
119: - **Acceptance Criteria**:
120:   - Parallel execution matches sequential outputs.
121: - **Validation**:
122:   - Integration test compares outputs between sequential and parallel modes.
123: 
124: ### Task 3.2: Deterministic output ordering
125: - **Location**: `src/statistic_harness/core/report.py`
126: - **Description**: Ensure report assembly orders plugins deterministically (sorted by id) and evidence rows sorted consistently.
127: - **Complexity**: 4
128: - **Dependencies**: Task 3.1
129: - **Acceptance Criteria**:
130:   - `report.json` stable across runs.
131: - **Validation**:
132:   - Snapshot test on report outputs.
133: 
134: ### Task 3.3: Performance regression benchmarks (lightweight)
135: - **Location**: `tests/test_performance_smoke.py` (new), `tools/benchmarks/*` (optional)
136: - **Description**: Add a small performance smoke test to measure subprocess overhead and parallel speedups on a tiny fixture. Keep thresholds loose to catch regressions without blocking normal runs.
137: - **Complexity**: 3
138: - **Dependencies**: Task 3.1
139: - **Acceptance Criteria**:
140:   - Performance smoke test is stable on local dev.
141: - **Validation**:
142:   - `python -m pytest -q` passes with the smoke test enabled.
143: 
144: ## Sprint 4: Kernel Minimization & Plugin-First Defaults
145: **Goal**: Move default logic/config to plugins; kernel only orchestrates.
146: **Demo/Validation**:
147: - Auto runs depend on planner plugin; transform mappings always via transform plugin.
148: 
149: ### Task 4.1: Planner plugin is the only auto-selection path
150: - **Location**: `src/statistic_harness/core/pipeline.py`, `plugins/planner_basic/*`
151: - **Description**: Remove kernel fallback planner logic; auto runs require planner plugin to produce selection. Manual selection still works.
152: - **Complexity**: 5
153: - **Dependencies**: Sprint 1
154: - **Acceptance Criteria**:
155:   - Auto plan uses planner plugin output.
156: - **Validation**:
157:   - Unit test confirms planner output controls selection.
158: 
159: ### Task 4.2: Transform template is a plugin-only step
160: - **Location**: `plugins/transform_template/*`, `src/statistic_harness/core/template.py`
161: - **Description**: Ensure template conversions are only triggered via transform plugin (kernel does not call apply_template directly).
162: - **Complexity**: 4
163: - **Dependencies**: Existing transform plugin
164: - **Acceptance Criteria**:
165:   - Mapping requests run through transform plugin.
166: - **Validation**:
167:   - Template conversion tests pass.
168: 
169: ### Task 4.3: Enforce plugin-only pipeline steps
170: - **Location**: `src/statistic_harness/core/pipeline.py`, `src/statistic_harness/core/plugin_manager.py`, `plugins/*/plugin.yaml`
171: - **Description**: Ensure ingest, profile/validate, analysis, report, and llm steps are always resolved through plugins; kernel rejects missing step types and records errors in the report rather than crashing.
172: - **Complexity**: 6
173: - **Dependencies**: Task 1.1
174: - **Acceptance Criteria**:
175:   - Pipeline fails closed with an error summary if a required step plugin is missing.
176: - **Validation**:
177:   - Unit test stubs missing plugin and asserts error reporting.
178: 
179: ### Task 4.4: Security guardrails at kernel boundaries
180: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/ui/server.py`, `src/statistic_harness/core/utils.py`
181: - **Description**: Add path traversal protection for downloads/artifacts, file type/size validation for uploads, and explicit bans on `pickle`, `eval`, and shelling out in analysis paths. Add tests for each guardrail.
182: - **Complexity**: 7
183: - **Dependencies**: Sprint 1
184: - **Acceptance Criteria**:
185:   - Invalid paths and file types are rejected deterministically.
186: - **Validation**:
187:   - Security tests cover traversal, size, and type checks.
188: 
189: ## Sprint 5: Mapping Presets + Raw Format Learning
190: **Goal**: Persist mapping presets per raw format; allow user to choose among presets.
191: **Demo/Validation**:
192: - Raw format view shows mapping presets and notes.
193: 
194: ### Task 5.1: Raw format mapping preset storage
195: - **Location**: `src/statistic_harness/core/migrations.py`, `src/statistic_harness/core/storage.py`
196: - **Description**: Add `raw_format_mappings` table and CRUD helpers; hash mapping JSON for determinism and dedupe.
197: - **Complexity**: 5
198: - **Dependencies**: Sprint 1
199: - **Acceptance Criteria**:
200:   - Multiple presets saved per format.
201: - **Validation**:
202:   - Unit tests for preset save/list.
203: 
204: ### Task 5.2: UI for mapping preset selection
205: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/raw_format.html`
206: - **Description**: Add form to store presets; list presets per format; allow copy/paste into template mapping.
207: - **Complexity**: 4
208: - **Dependencies**: Task 5.1
209: - **Acceptance Criteria**:
210:   - User can add/view presets in UI.
211: - **Validation**:
212:   - UI smoke tests (FastAPI TestClient).
213: 
214: ## Sprint 6: Combined Analysis Filters (Project/Format/Date)
215: **Goal**: Allow filtered cross-project analysis without merging all data indiscriminately.
216: **Demo/Validation**:
217: - Combined analysis runs scoped by project or raw format.
218: 
219: ### Task 6.1: Aggregate dataset filters
220: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/dataset_io.py`, `src/statistic_harness/core/template.py`
221: - **Description**: Extend aggregate datasets to carry filters (project_ids, dataset_ids, raw_format_ids, date range) in mapping_json; TemplateAccessor applies SQL WHERE filters deterministically.
222: - **Complexity**: 7
223: - **Dependencies**: Sprint 4
224: - **Acceptance Criteria**:
225:   - Aggregate runs only include filtered datasets.
226: - **Validation**:
227:   - New tests verifying filters.
228: 
229: ### Task 6.2: UI for filtered combined runs
230: - **Location**: `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/template.html`
231: - **Description**: Add filter fields (project_ids, raw_format_ids, date range) to combined run form.
232: - **Complexity**: 4
233: - **Dependencies**: Task 6.1
234: - **Acceptance Criteria**:
235:   - Combined run respects filters.
236: - **Validation**:
237:   - UI test + integration test.
238: 
239: ## Sprint 7: Golden DB Fixtures + Migration Safety
240: **Goal**: Full migration coverage with golden DB fixtures.
241: **Demo/Validation**:
242: - Migrations from v1...vN pass and preserve data.
243: 
244: ### Task 7.1: Golden DB generator and fixtures
245: - **Location**: `tests/fixtures/db/*`, `tests/test_migrations.py` (new)
246: - **Description**: Create small SQLite fixture DBs for each schema version; test upgrades to latest preserving runs/results/templates.
247: - **Complexity**: 6
248: - **Dependencies**: All migrations
249: - **Acceptance Criteria**:
250:   - Migrations pass across all versions.
251: - **Validation**:
252:   - `python -m pytest -q`.
253: 
254: ## Sprint 8: Reporting + Evaluator Non-Negotiables
255: **Goal**: Always emit `report.md` and `report.json`, validate against schema, and add evaluator harness.
256: **Demo/Validation**:
257: - Integration test asserts both report files exist and `report.json` validates.
258: - Evaluator harness checks `ground_truth.yaml` against report output.
259: 
260: ### Task 8.1: Report artifacts + schema validation
261: - **Location**: `src/statistic_harness/core/report.py`, `docs/report.schema.json`, `tests/test_report_outputs.py` (new)
262: - **Description**: Ensure every run produces `report.md` and `report.json` and validate JSON against schema. If a plugin fails, include error summaries and still write reports.
263: - **Complexity**: 6
264: - **Dependencies**: Sprint 1
265: - **Acceptance Criteria**:
266:   - Reports are always generated, even on plugin errors.
267: - **Validation**:
268:   - Tests assert report files exist and schema validation passes.
269: 
270: ### Task 8.2: Evaluator harness with `ground_truth.yaml`
271: - **Location**: `src/statistic_harness/core/evaluator.py` (new), `tests/test_evaluator.py` (new)
272: - **Description**: Implement evaluator that reads `ground_truth.yaml`, compares required attributes to `report.json` within tolerances (support absolute and relative), and fails deterministically.
273: - **Complexity**: 5
274: - **Dependencies**: Task 8.1
275: - **Acceptance Criteria**:
276:   - Evaluator test passes on synthetic fixture.
277: - **Validation**:
278:   - Integration test runs evaluator as part of pipeline test.
279: 
280: ## Testing Strategy
281: - Unit tests for plugin manifest schema validation.
282: - Isolation tests for filesystem/network restrictions.
283: - Integration tests for parallel execution and deterministic output.
284: - Migration golden tests.
285: - Report artifact + schema validation tests.
286: - Evaluator harness tests with `ground_truth.yaml`.
287: 
288: ## Potential Risks & Gotchas
289: - OS-level sandboxing may not fully prevent network if plugins import low-level modules; Python-level guards mitigate but are not foolproof.
290: - Parallel subprocess execution increases load; deterministic ordering and JSON serialization must be enforced.
291: - Schema adoption requires updating all existing plugins; rollout must be coordinated to avoid blocked runs.
292: - Offline enforcement must cover UI assets and any helper utilities.
293: - Determinism needs consistent run_seed propagation, RNG seeding, and locale/timezone control.
294: 
295: ## Rollback Plan
296: - Keep runner feature flag to fall back to in-process execution during transition (dev-only).
297: - Preserve legacy plugin manifests in a migration branch if needed.
298: - Maintain previous schema migration steps; do not drop legacy tables.
````

## File: Makefile
````makefile
1: SHELL := /bin/bash
2: PYTHON ?= python3
3: 
4: .PHONY: dev
5: dev:
6: 	$(PYTHON) -m venv .venv
7: 	. .venv/bin/activate && ./scripts/install_dev.sh
````

## File: queue-delay-plugins-evaluation-plan.md
````markdown
  1: # Plan: Queue/Capacity Plugins + Evaluation Parity
  2: 
  3: **Generated**: 2026-02-01
  4: **Estimated Complexity**: High
  5: 
  6: ## Overview
  7: Implement 10 separate, deterministic analysis plugins that reproduce the working queue/capacity methods on arbitrary ERP event-log data, without pre-known column names. Add dynamic field inference, measured vs modeled labeling, and evaluation parity checks so the harness can validate that plugin outputs match expected results for Quorum Upstream (and later Enertia). All plugins must be independent, skip gracefully when required fields cannot be inferred, and emit explicit "not applicable" + "error detail" findings instead of hard-failing.
  8: 
  9: ## Decisions (from user)
 10: - Role inference: auto-select best match and proceed (no manual override required to run).
 11: - Evaluation strictness: fail on unexpected findings by default.
 12: - ERP default: "unknown" with its own known-issues set; can be updated later.
 13: 
 14: ## Prerequisites
 15: - Current schema + plugin system (src/ + plugins/ layouts)
 16: - Existing dynamic dataset ingestion into SQLite
 17: - Ability to run tests: `python -m pytest -q`
 18: - Sample event-log fixture(s) to validate outputs (Quorum Upstream test file)
 19: 
 20: ## Sprint 1: Field Inference + Evaluation Enhancements
 21: **Goal**: Infer event-log roles from arbitrary headers and extend evaluation so numeric outputs can be checked for parity.
 22: **Demo/Validation**:
 23: - Run the new inference plugin on a fixture and see inferred roles with confidence.
 24: - Run evaluation on a report using a new expected-metrics payload and see pass/fail reasons.
 25: 
 26: ### Task 1.1: Add event-log field inference plugin
 27: - **Location**: `plugins/profile_eventlog/`, `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/migrations.py`
 28: - **Description**: Create a profile plugin that scans dataset columns and stores ranked role candidates (queue time, start time, end time, process id/name, module code, user id, dependency id, master/sequence id, host/worker id, status). Use name heuristics + value pattern checks (timestamp parse rate, cardinality, monotonicity hints) and write results to a new `dataset_role_candidates` table plus set `dataset_columns.role` for the best-scoring role per field.
 29: - **Complexity**: 8
 30: - **Dependencies**: None
 31: - **Acceptance Criteria**:
 32:   - Plugin outputs a deterministic role map with scores + reasons.
 33:   - If roles are ambiguous, output includes multiple candidates and marks confidence below threshold.
 34: - **Validation**:
 35:   - Unit tests with synthetic columns; assert deterministic role assignments.
 36: 
 37: ### Task 1.2: Add ERP type metadata + known-issues scoping
 38: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/ui/server.py`, `src/statistic_harness/ui/templates/known_issues.html`, `src/statistic_harness/ui/templates/wizard.html`
 39: - **Description**: Add `erp_type` to projects and allow known-issues sets to be scoped by ERP type (default "unknown"). When evaluating, prefer known-issues for the project's ERP type; fall back to upload hash if ERP type is missing.
 40: - **Complexity**: 6
 41: - **Dependencies**: None
 42: - **Acceptance Criteria**:
 43:   - Known issues can be saved/loaded by ERP type and project.
 44:   - Evaluation picks ERP-scoped issues automatically when available.
 45: - **Validation**:
 46:   - Storage tests for ERP-scoped sets; UI smoke test shows ERP type.
 47: 
 48: ### Task 1.3: Extend evaluation with numeric expected metrics
 49: - **Location**: `src/statistic_harness/core/evaluation.py`, `docs/evaluation.md`, `docs/report.schema.json`
 50: - **Description**: Add `expected_metrics` to ground truth: list of {plugin_id, metric, value, tolerance}. Support numeric tolerance (abs/rel). Validate plugin metrics by exact key path (e.g., `eligible_wait.p95`). Default evaluation behavior is strict: unexpected findings fail unless explicitly disabled in ground truth.
 51: - **Complexity**: 5
 52: - **Dependencies**: None
 53: - **Acceptance Criteria**:
 54:   - Evaluation fails when a metric is missing or outside tolerance.
 55:   - Clear messages identify plugin/metric mismatches.
 56: - **Validation**:
 57:   - Unit tests for metric comparisons and tolerance behavior.
 58: 
 59: ### Task 1.4: Introduce measured vs modeled labeling contract
 60: - **Location**: `docs/report.schema.json`, `src/statistic_harness/core/report.py`, `plugins/*/output.schema.json`
 61: - **Description**: Standardize a `measurement_type` field for findings/metrics (values: `measured`, `modeled`, `not_applicable`, `error`). Require each plugin to label outputs accordingly.
 62: - **Complexity**: 4
 63: - **Dependencies**: None
 64: - **Acceptance Criteria**:
 65:   - Schema validates all plugin outputs with measurement labels.
 66: - **Validation**:
 67:   - Schema tests + plugin unit tests with both measured and modeled cases.
 68: 
 69: ### Task 1.5: Add manual role overrides (optional per project)
 70: - **Location**: `src/statistic_harness/core/storage.py`, `src/statistic_harness/core/migrations.py`, `src/statistic_harness/ui/templates/`, `src/statistic_harness/ui/server.py`
 71: - **Description**: Provide a per-project override table so you can explicitly map key roles (queue/start/end/process/dependency/master/user/module/host) when inference is ambiguous. Overrides must be deterministic and preserved in DB.
 72: - **Complexity**: 6
 73: - **Dependencies**: Task 1.1
 74: - **Acceptance Criteria**:
 75:   - UI can save overrides and they are used by analysis plugins.
 76: - **Validation**:
 77:   - Unit test: overrides take precedence over inferred roles.
 78: 
 79: ## Sprint 2: Core Queue/Delay Plugins (1-5, 9, 10)
 80: **Goal**: Implement the first seven plugins and verify deterministic parity on the Quorum test file.
 81: **Demo/Validation**:
 82: - Run the plugin suite on fixture; evaluation passes with expected metrics/findings.
 83: 
 84: ### Task 2.1: Queue delay decomposition plugin
 85: - **Location**: `plugins/analysis_queue_delay_decomposition/`
 86: - **Description**: Compute total wait (START-QUEUE), split into prereq wait (DEP_END-QUEUE) and eligible wait (START-DEP_END). Emit measured findings per process/module/user and tail stats.
 87: - **Complexity**: 7
 88: - **Dependencies**: Task 1.1
 89: - **Acceptance Criteria**:
 90:   - Produces eligible/prereq buckets with deterministic counts and percentiles.
 91:   - Emits not_applicable when dependency or timestamp roles missing.
 92: - **Validation**:
 93:   - Unit test fixture with known splits.
 94: 
 95: ### Task 2.2: Dependency-resolution join plugin
 96: - **Location**: `plugins/analysis_dependency_resolution_join/`
 97: - **Description**: Infer dependency key, join to dependency end time, compute START-DEP_END. Report near-zero starts to show dependency dominance.
 98: - **Complexity**: 6
 99: - **Dependencies**: Task 1.1
100: - **Acceptance Criteria**:
101:   - Emits distribution of dependency lag and proportion <=2s.
102: - **Validation**:
103:   - Synthetic fixture test verifying lag distribution.
104: 
105: ### Task 2.3: Standalone vs sequence classification plugin
106: - **Location**: `plugins/analysis_sequence_classification/`
107: - **Description**: Classify rows by presence of dependency/master pointers. Output counts and eligible-wait stats per class.
108: - **Complexity**: 5
109: - **Dependencies**: Task 1.1
110: - **Acceptance Criteria**:
111:   - Emits counts/percentiles per class with evidence.
112: - **Validation**:
113:   - Unit test on synthetic rows with/without deps.
114: 
115: ### Task 2.4: Threshold-based tail isolation plugin
116: - **Location**: `plugins/analysis_tail_isolation/`
117: - **Description**: Filter eligible-wait > configurable threshold (default 60s). Attribute tail to process/module/user/sequence.
118: - **Complexity**: 5
119: - **Dependencies**: Task 2.1
120: - **Acceptance Criteria**:
121:   - Emits tail segments with evidence row IDs.
122: - **Validation**:
123:   - Fixture test verifying threshold effect.
124: 
125: ### Task 2.5: Percentile analysis plugin
126: - **Location**: `plugins/analysis_percentile_analysis/`
127: - **Description**: Compute p50/p95/p99 for eligible wait and completion time per process/module.
128: - **Complexity**: 4
129: - **Dependencies**: Task 2.1
130: - **Acceptance Criteria**:
131:   - Emits percentiles + dominant tail contributors.
132: - **Validation**:
133:   - Numeric tolerance tests in evaluation.
134: 
135: ### Task 2.6: Attribution analysis plugin
136: - **Location**: `plugins/analysis_attribution/`
137: - **Description**: Aggregate eligible wait and tail counts by PROCESS_ID, MODULE_CD, USER_ID with deterministic ranking.
138: - **Complexity**: 4
139: - **Dependencies**: Task 2.1
140: - **Acceptance Criteria**:
141:   - Deterministic ranking with evidence slices.
142: - **Validation**:
143:   - Fixture test ensures top contributor is stable.
144: 
145: ### Task 2.7: Determinism discipline plugin
146: - **Location**: `plugins/analysis_determinism_discipline/`
147: - **Description**: Inspect all plugin outputs and emit a summary finding validating measurement_type usage. Flags any modeled output lacking explicit assumptions or any missing label.
148: - **Complexity**: 5
149: - **Dependencies**: Task 1.4
150: - **Acceptance Criteria**:
151:   - Emits one summary finding and error findings for violations.
152: - **Validation**:
153:   - Unit test with mocked plugin outputs.
154: 
155: ## Sprint 3: Sequence + Concurrency + Capacity Modeling (6-8)
156: **Goal**: Implement the remaining plugins and ensure modeled outputs are clearly separated from measured facts.
157: **Demo/Validation**:
158: - Run all plugins on the Quorum fixture and pass evaluation.
159: 
160: ### Task 3.1: Chain makespan analysis plugin
161: - **Location**: `plugins/analysis_chain_makespan/`
162: - **Description**: Use MASTER_PROCESS_QUEUE_ID (or inferred sequence id) to compute makespan, runtime sum, idle gaps, and critical path effects. Emit evidence rows.
163: - **Complexity**: 7
164: - **Dependencies**: Task 1.1
165: - **Acceptance Criteria**:
166:   - Outputs per-sequence makespan and idle-gap metrics.
167: - **Validation**:
168:   - Synthetic fixture with known makespan.
169: 
170: ### Task 3.2: Concurrency reconstruction plugin
171: - **Location**: `plugins/analysis_concurrency_reconstruction/`
172: - **Description**: Reconstruct concurrent running counts using START/END overlap per host/QPEC. Output observed concurrency distribution vs configured caps if present.
173: - **Complexity**: 7
174: - **Dependencies**: Task 1.1
175: - **Acceptance Criteria**:
176:   - Emits concurrency time series summary and peak concurrency.
177: - **Validation**:
178:   - Fixture with known overlaps.
179: 
180: ### Task 3.3: Capacity-scaling scenario modeling plugin
181: - **Location**: `plugins/analysis_capacity_scaling/`
182: - **Description**: Apply proportional scaling to the eligible-wait bucket only (modeled). Support configurable scale factor; default derived from host count detected. Output modeled reduction stats.
183: - **Complexity**: 6
184: - **Dependencies**: Task 2.1, Task 3.2
185: - **Acceptance Criteria**:
186:   - Modeled outputs are labeled and include explicit assumptions.
187: - **Validation**:
188:   - Evaluation checks modeled metrics with tolerance.
189: 
190: ### Task 3.4: Evaluation parity suite for Quorum Upstream
191: - **Location**: `tests/fixtures/`, `tests/test_evaluation.py`, `tests/plugins/` for new plugin tests
192: - **Description**: Add a Quorum-style fixture and expected metrics/findings that represent the known results (qemail close-cycle contention, dependency waits, etc.). Ensure evaluation passes only when plugins reproduce them.
193: - **Complexity**: 6
194: - **Dependencies**: Sprint 2, Sprint 3
195: - **Acceptance Criteria**:
196:   - Evaluation passes with expected metrics and fails when metrics drift.
197: - **Validation**:
198:   - `python -m pytest -q` passes with new fixtures.
199: 
200: ## Testing Strategy
201: - Unit tests per new plugin using synthetic fixtures.
202: - Integration test running the full pipeline on a Quorum-style fixture and validating report schema + evaluation parity.
203: - Migration tests for ERP-scoped known issues and role candidates tables.
204: 
205: ## Potential Risks & Gotchas
206: - **Ambiguous column inference** could cause mis-classification. Mitigation: store multiple candidates with confidence, auto-select best match but emit a warning finding when confidence is below threshold.
207: - **Mixed timezones/invalid timestamps** could distort waits. Mitigation: parse failures tracked; emit a warning finding with counts; allow per-project override later.
208: - **Huge datasets** could make overlap reconstruction expensive. Mitigation: chunked SQL with indexes; report when sampling is used.
209: 
210: ## Rollback Plan
211: - Disable new plugins by removing them from the autoplanner/selection list and leave existing pipeline intact.
212: - Revert migration by leaving new tables unused (no destructive changes to existing tables).
````

## File: codex/PROJECT_SPEC.md
````markdown
  1: # PROJECT SPEC — Statistic Harness (Phase 1: local-only)
  2: 
  3: Repository: `ninjra/statistic_harness`
  4: 
  5: ## 0) Objective
  6: Build a local-only, plugin-first data analysis harness that:
  7: 1) Accepts uploaded tabular files (CSV/TSV/TXT delimited, XLSX, JSON).
  8: 2) Runs a configurable set of statistical analysis plugins to detect hidden patterns.
  9: 3) Optionally builds an OFFLINE LLM prompt artifact (no network calls) to summarize findings.
 10: 4) Stores state in SQLite and artifacts on disk (self-contained).
 11: 5) Produces both:
 12:    - human report: `report.md`
 13:    - machine report: `report.json` validating against `docs/report.schema.json`
 14: 6) Includes a test suite that passes (`pytest -q`) with synthetic fixtures and an evaluator harness.
 15: 
 16: ## 1) Phase constraints
 17: ### Phase 1 (default behavior)
 18: - Local-only by default; auth and multi-tenant are off unless explicitly enabled.
 19: - No external services: no Qdrant, no server DBs.
 20: - No network calls during analysis runs.
 21: - Portable across Windows 11 + WSL2 (code must be OS-agnostic; scripts provided for PS + bash).
 22: 
 23: ### Phase 2 (optional, behind flags)
 24: - Multi-tenant + auth are supported when enabled via feature flags.
 25: - Tenant-aware storage namespaces are enforced when tenancy is enabled.
 26: 
 27: ## 2) Non-negotiables
 28: - All pipeline steps must be plugins/modules, including ingest + report.
 29: - Plugin system must support:
 30:   - Python plugins (executable)
 31:   - Markdown plugins (static report sections/templates)
 32: - All plugins are configurable:
 33:   - enable/disable per run
 34:   - per-plugin settings via JSON/YAML
 35: - Deterministic runs:
 36:   - a `run_seed` is assigned and recorded
 37:   - all RNG usage must derive from it
 38: - Every run produces a `report.md` and `report.json` even if some plugins fail.
 39: 
 40: ## 3) Repo structure (must match exactly)
 41: Use `src/` layout.
 42: 
 43: ```
 44: 
 45: .
 46: ├─ AGENTS.md
 47: ├─ README.md
 48: ├─ LICENSE (MIT)
 49: ├─ pyproject.toml
 50: ├─ .gitignore
 51: ├─ config/
 52: │  └─ app.yaml
 53: ├─ docs/
 54: │  ├─ report.schema.json
 55: │  ├─ plugin_dev_guide.md
 56: │  ├─ evaluation.md
 57: │  └─ references.md
 58: ├─ scripts/
 59: │  ├─ bootstrap.ps1
 60: │  ├─ bootstrap.sh
 61: │  ├─ run_ui.ps1
 62: │  ├─ run_ui.sh
 63: │  ├─ run_cli_example.ps1
 64: │  └─ run_cli_example.sh
 65: ├─ src/statistic_harness/
 66: │  ├─ **init**.py
 67: │  ├─ cli.py
 68: │  ├─ core/
 69: │  │  ├─ **init**.py
 70: │  │  ├─ types.py
 71: │  │  ├─ plugin_manager.py
 72: │  │  ├─ pipeline.py
 73: │  │  ├─ storage.py
 74: │  │  ├─ dataset_io.py
 75: │  │  ├─ evaluation.py
 76: │  │  ├─ report.py
 77: │  │  ├─ utils.py
 78: │  │  ├─ vector_store.py
 79: │  │  └─ tenancy.py  (stub only)
 80: │  └─ ui/
 81: │     ├─ **init**.py
 82: │     ├─ server.py
 83: │     ├─ templates/
 84: │     │  ├─ index.html
 85: │     │  ├─ run.html
 86: │     │  └─ plugins.html
 87: │     └─ static/
 88: │        └─ app.css
 89: ├─ plugins/
 90: │  ├─ ingest_tabular/
 91: │  │  ├─ plugin.yaml
 92: │  │  ├─ plugin.py
 93: │  │  └─ README.md
 94: │  ├─ profile_basic/
 95: │  │  ├─ plugin.yaml
 96: │  │  ├─ plugin.py
 97: │  │  └─ README.md
 98: │  ├─ analysis_conformal_feature_prediction/
 99: │  ├─ analysis_online_conformal_changepoint/
100: │  ├─ analysis_gaussian_knockoffs/
101: │  ├─ analysis_knockoff_wrapper_rf/
102: │  ├─ analysis_notears_linear/
103: │  ├─ analysis_bocpd_gaussian/
104: │  ├─ analysis_scan_statistics/
105: │  ├─ analysis_graph_topology_curves/
106: │  ├─ analysis_dp_gmm/
107: │  ├─ analysis_gaussian_copula_shift/
108: │  ├─ report_bundle/
109: │  └─ llm_prompt_builder/
110: │
111: └─ tests/
112: ├─ test_plugin_discovery.py
113: ├─ test_pipeline_integration.py
114: ├─ test_report_schema.py
115: ├─ test_security_paths.py
116: ├─ plugins/
117: │  ├─ test_conformal_feature_prediction.py
118: │  ├─ test_online_conformal_changepoint.py
119: │  ├─ test_gaussian_knockoffs.py
120: │  ├─ test_knockoff_wrapper_rf.py
121: │  ├─ test_notears_linear.py
122: │  ├─ test_bocpd_gaussian.py
123: │  ├─ test_scan_statistics.py
124: │  ├─ test_graph_topology_curves.py
125: │  ├─ test_dp_gmm.py
126: │  └─ test_gaussian_copula_shift.py
127: └─ fixtures/
128: ├─ make_synth_data.py
129: ├─ synth_linear.csv
130: ├─ synth_timeseries.csv
131: ├─ synth_clusters.csv
132: ├─ synth_shift_corr.csv
133: └─ ground_truth_synth.yaml
134: 
135: ```
136: 
137: ## 4) Dependencies (pyproject.toml)
138: Must be Windows-friendly wheels and common.
139: Required runtime deps:
140: - fastapi
141: - uvicorn
142: - jinja2
143: - python-multipart
144: - pydantic
145: - pyyaml
146: - numpy
147: - pandas
148: - openpyxl
149: - scikit-learn
150: - jsonschema
151: 
152: Dev deps:
153: - pytest
154: - ruff (optional)
155: - mypy (optional)
156: 
157: NOTE: no external DB services.
158: 
159: ## 5) Data + storage model
160: ### 5.1 App data directory (gitignored)
161: - `./appdata/state.sqlite`
162: - `./appdata/uploads/<upload_id>/<original_filename>`
163: - `./appdata/runs/<run_id>/`
164:   - `dataset/canonical.csv`
165:   - `artifacts/<plugin_id>/...`
166:   - `report.json`
167:   - `report.md`
168:   - `logs/run.log`
169: 
170: ### 5.2 SQLite schema (core/storage.py)
171: Tables:
172: - `runs`:
173:   - run_id TEXT PK
174:   - created_at TEXT (ISO8601)
175:   - status TEXT (queued|running|completed|failed)
176:   - upload_id TEXT
177:   - input_filename TEXT
178:   - canonical_path TEXT
179:   - settings_json TEXT
180:   - error_json TEXT NULL
181: - `plugin_results`:
182:   - run_id TEXT
183:   - plugin_id TEXT
184:   - status TEXT (ok|skipped|error)
185:   - summary TEXT
186:   - metrics_json TEXT
187:   - findings_json TEXT
188:   - artifacts_json TEXT
189:   - error_json TEXT NULL
190:   - PRIMARY KEY (run_id, plugin_id)
191: 
192: All JSON stored as TEXT.
193: 
194: ## 6) Plugin system (core/plugin_manager.py)
195: ### 6.1 Plugin manifest
196: Each plugin directory contains `plugin.yaml`:
197: 
198: Fields:
199: - id: string (unique)
200: - name: string
201: - version: semver string
202: - type: one of: ingest|profile|analysis|report|llm|markdown
203: - entrypoint: `plugin.py:Plugin` (class name)
204: - depends_on: list of plugin ids
205: - settings:
206:   - description: string
207:   - schema: JSON Schema object OR pydantic import path
208:   - defaults: dict
209: 
210: ### 6.2 Plugin API
211: Define a base protocol in `core/types.py`:
212: 
213: - `PluginContext`:
214:   - run_id, run_dir, artifacts_dir(plugin_id)
215:   - sqlite connection handle (or Storage object)
216:   - dataset accessor (loads canonical.csv lazily)
217:   - run_seed
218:   - logger
219:   - settings dict (validated)
220:   - helpers to write artifacts
221: 
222: - `PluginResult`:
223:   - status
224:   - summary (short human text)
225:   - metrics: dict[str, float|int|str]
226:   - findings: list[dict]  (MACHINE fields; stable)
227:   - artifacts: list[{path,type,description}]
228:   - error: optional {type,message,traceback}
229: 
230: Plugin class must implement:
231: - `run(ctx: PluginContext) -> PluginResult`
232: 
233: ### 6.3 Plugin discovery
234: - Scan `./plugins/*/plugin.yaml`
235: - Validate manifests.
236: - Import entrypoint.
237: - Return a registry list for UI and CLI.
238: 
239: ## 7) Pipeline execution (core/pipeline.py)
240: Pipeline responsibilities:
241: - Create run_id + run_dir
242: - Call ingest plugin first to create canonical dataset
243: - Execute selected plugins in dependency order
244: - Always run report plugin at end
245: - Continue execution even if a plugin errors (record error and proceed)
246: - Ensure report files exist regardless of plugin errors
247: 
248: ## 8) CLI (src/statistic_harness/cli.py)
249: Implement with argparse.
250: 
251: Commands:
252: - `stat-harness list-plugins`
253: - `stat-harness serve --host 127.0.0.1 --port 8000`
254: - `stat-harness run --file <path> --plugins <csv> --settings <json_or_yaml_path> --run-seed <int optional>`
255: - `stat-harness eval --report <report.json> --ground-truth <ground_truth.yaml>`
256: - `stat-harness make-ground-truth-template --report <report.json> -o <yaml>`
257: 
258: All commands must work on Windows + WSL.
259: 
260: ## 9) UI (FastAPI + Jinja2)
261: Endpoints:
262: - GET `/` upload + plugin selection UI
263: - GET `/plugins` shows plugin list + descriptions
264: - POST `/api/upload` returns upload_id
265: - POST `/api/runs` create a run from upload_id + selected plugins + settings
266: - GET `/runs/{run_id}` status page (polling)
267: - GET `/api/runs/{run_id}/report.json`
268: - GET `/api/runs/{run_id}/report.md`
269: - GET `/api/runs/{run_id}/artifacts/{plugin_id}/{path}` (MUST be safe against traversal)
270: 
271: Run execution should be background thread; store status updates in sqlite.
272: 
273: ## 10) Report format
274: ### 10.1 report.json
275: Must conform to `docs/report.schema.json`.
276: 
277: Minimum fields:
278: - run_id, created_at, status
279: - input: {filename, rows, cols, inferred_types}
280: - plugins: object keyed by plugin_id, each has:
281:   - status, summary, metrics, findings, artifacts, error
282: 
283: ### 10.2 report.md
284: Human readable:
285: - dataset summary
286: - executed plugins + summaries
287: - top findings list with links to artifacts
288: - error section (if any)
289: 
290: ## 11) Implement the Phase 1 plugin catalog (100% required)
291: 
292: ### ingest_tabular (type=ingest)
293: Goal: parse CSV/TSV/TXT/XLSX/JSON into pandas DataFrame and write canonical.csv.
294: Settings:
295: - delimiter: null|string
296: - sheet_name: null|string (xlsx)
297: - encoding: default utf-8
298: - max_rows: int (safety)
299: Outputs:
300: - dataset/canonical.csv
301: - dataset/schema.json: column names + inferred dtype + missingness
302: 
303: Tests:
304: - ingest csv fixture
305: - ingest xlsx fixture (generate on the fly in test)
306: 
307: ### profile_basic (type=profile)
308: Compute:
309: - per-column: dtype, missing %, unique count, min/max/mean/std for numeric
310: - correlation matrix (numeric only) if <= N columns
311: Outputs:
312: - artifacts/profile_basic/columns.json
313: - artifacts/profile_basic/correlation.csv (optional)
314: Tests:
315: - validate basic stats exist
316: 
317: ### analysis_conformal_feature_prediction (type=analysis)
318: Split conformal anomaly detection per numeric column:
319: - For each selected column y (default: all numeric up to max_cols):
320:   - Fit model y ~ X (Ridge regression)
321:   - Split train/calib/test by index (deterministic)
322:   - q = quantile(|resid_calib|, ceil((n+1)*(1-alpha))/n)
323:   - interval = pred ± q
324:   - flag anomalies in test
325: Settings:
326: - alpha: float (default 0.1)
327: - max_target_cols: int
328: - model: "ridge"
329: Outputs:
330: - artifacts/.../anomalies.csv
331: - artifacts/.../anomalies.json
332: Findings:
333: - list of {kind:"anomaly", column, row_index, score, lower, upper}
334: Test:
335: - synthetic linear data with injected outliers must be detected.
336: 
337: ### analysis_online_conformal_changepoint (type=analysis)
338: Online conformal scores on a specified value column:
339: - rolling-mean forecast + calibration window quantile
340: - compute anomaly indicator
341: - detect change-point when anomaly-rate in last W exceeds threshold
342: Settings:
343: - time_column, value_column
344: - alpha, forecast_window, calib_window
345: - alarm_rate_window, alarm_rate_threshold
346: Outputs:
347: - artifacts/.../alerts.json
348: Findings:
349: - {kind:"changepoint", index, time, score}
350: Test:
351: - mean-shift series must produce changepoint within tolerance.
352: 
353: ### analysis_gaussian_knockoffs (type=analysis)
354: Gaussian Model-X knockoffs (estimated covariance) + Lasso importance:
355: - generate knockoff features
356: - compute W_j = |beta_j| - |beta_j_knock|
357: - apply knockoff threshold for FDR q
358: Settings:
359: - target_column
360: - fdr_q (default 0.1)
361: - lasso_alpha (default 0.01)
362: Outputs:
363: - artifacts/.../selection.json
364: Findings:
365: - {kind:"feature_discovery", feature, score, selected:true/false}
366: Test:
367: - synthetic with 2 true features must select them.
368: 
369: ### analysis_knockoff_wrapper_rf (type=analysis)
370: Same knockoffs generation, but compute importance via RandomForestRegressor:
371: - W_j = imp_j - imp_j_knock
372: Settings:
373: - target_column, fdr_q, n_estimators
374: Outputs:
375: - selection.json
376: Test:
377: - same synthetic; must recover true features.
378: 
379: ### analysis_notears_linear (type=analysis)
380: Implement simplified NOTEARS-style linear DAG learning:
381: - minimize (1/2n)||X - XW||^2 + lambda*|W| with acyclicity constraint
382: - small d cap (default <= 20 numeric cols)
383: Settings:
384: - max_cols
385: - lambda_l1
386: - max_iter, lr
387: - weight_threshold
388: Outputs:
389: - artifacts/.../graph.json with:
390:   - nodes: [col names]
391:   - edges: [{source, target, weight}] AND also edges_compact: [[i,j,weight]]
392: Findings:
393: - {kind:"graph_edge", source, target, weight}
394: Test:
395: - 3-node chain synthetic should recover edges (within tolerance).
396: 
397: ### analysis_bocpd_gaussian (type=analysis)
398: Bayesian Online Changepoint Detection (Adams & MacKay style):
399: - univariate series value_column
400: - constant hazard H
401: - conjugate normal prior for mean (known variance estimated)
402: - compute P(r_t=0) and detect peaks above threshold
403: Settings:
404: - value_column, time_column (optional)
405: - hazard (default 1/200)
406: - peak_threshold
407: Outputs:
408: - changepoints.json
409: Findings:
410: - {kind:"changepoint", index, prob}
411: Test:
412: - mean-shift series must be detected.
413: 
414: ### analysis_scan_statistics (type=analysis)
415: 1D scan over windows:
416: - search over window lengths [Lmin, Lmax]
417: - statistic = standardized mean difference
418: - p-value via permutation test (seeded)
419: Settings:
420: - value_column
421: - min_window, max_window
422: - n_permutations
423: Outputs:
424: - results.json includes top windows with score + p_value
425: Findings:
426: - {kind:"cluster", start, end, score, p_value}
427: Test:
428: - injected elevated segment must be top-ranked.
429: 
430: ### analysis_graph_topology_curves (type=analysis)
431: Approx persistent “Betti-like” curves from threshold graphs:
432: - sample up to max_points points from numeric feature space
433: - compute pairwise distances
434: - for thresholds eps list:
435:   - build graph edges where dist <= eps
436:   - compute beta0 via union-find (components)
437:   - compute beta1 approx via E - V + C
438: Settings:
439: - max_points
440: - n_thresholds
441: Outputs:
442: - curves.json {eps:[], beta0:[], beta1:[]}
443: Findings:
444: - {kind:"topology", metric:"beta1_peak", value:...}
445: Test:
446: - circle-like synthetic should show beta1 peak > 0.
447: 
448: ### analysis_dp_gmm (type=analysis)
449: DP Gaussian mixture (collapsed Gibbs), diagonal / spherical known variance:
450: - implement CRP assignment sampling with concentration alpha
451: - compute predictive log probs using conjugate normal prior for mean
452: Settings:
453: - alpha
454: - sigma2 (optional; estimate if null)
455: - n_iter
456: - burn_in
457: Outputs:
458: - assignments.csv
459: - summary.json with cluster sizes + means
460: Findings:
461: - {kind:"cluster", cluster_id, size}
462: Test:
463: - 2-component mixture must yield >=2 clusters and purity threshold.
464: 
465: ### analysis_gaussian_copula_shift (type=analysis)
466: Dependence shift detection via Gaussian copula:
467: - split data into two segments (first half vs second half) OR rolling
468: - rank-transform each variable -> U in (0,1)
469: - probit transform -> Z
470: - compute corr matrices, delta = max abs diff
471: - identify top changed pairs; optional permutation p-value (seeded)
472: Settings:
473: - segment: "halves" (phase1)
474: - max_pairs
475: - n_permutations
476: Outputs:
477: - summary.json
478: Findings:
479: - {kind:"dependence_shift", pair:[a,b], delta, p_value}
480: Test:
481: - synthetic with correlation introduced in second half must flag that pair.
482: 
483: ### report_bundle (type=report)
484: - Reads sqlite plugin_results + artifacts
485: - Writes report.json + report.md
486: - Validates JSON against schema; if invalid, fail test
487: Test:
488: - pipeline integration test ensures report files exist.
489: 
490: ### llm_prompt_builder (type=llm)
491: OFFLINE ONLY:
492: - Reads report.json
493: - Writes:
494:   - artifacts/llm_prompt_builder/prompt.md (structured prompt)
495:   - artifacts/llm_prompt_builder/brief.md (short)
496: No network calls.
497: Test:
498: - prompt files exist and contain required sections.
499: 
500: ## 12) Evaluator harness (core/evaluation.py)
501: Implement evaluation against a ground truth YAML schema:
502: - expected features discovered (by knockoffs plugins)
503: - expected changepoints within tolerance
504: - expected dependency shift pairs
505: - expected anomaly detection (at least K of known anomalies)
506: Provide:
507: - docs/evaluation.md
508: - tests/fixtures/ground_truth_synth.yaml
509: - a CLI `stat-harness eval ...` returning non-zero exit on failure.
510: 
511: ## 13) Security regression tests
512: - Ensure artifact endpoint cannot read outside run directory (path traversal attempts).
513: - Ensure uploads are stored only inside `appdata/uploads`.
514: 
515: ## 14) Scripts
516: Provide scripts for Windows PowerShell and bash:
517: - `scripts/bootstrap.*`:
518:   - create venv `.venv`
519:   - install deps `pip install -e ".[dev]"`
520: - `scripts/run_ui.*`:
521:   - run local server
522: - `scripts/run_cli_example.*`:
523:   - run an example analysis on a fixture dataset
524: 
525: ## 15) Docs
526: Create `docs/references.md` listing conceptual sources (no code copying):
527: - Conformal prediction: https://arxiv.org/abs/2107.07511
528: - Model-X knockoffs: https://academic.oup.com/jrsssb/article/80/3/551/7048447
529: - NOTEARS: https://arxiv.org/abs/1803.01422
530: - BOCPD: https://arxiv.org/abs/0710.3742
531: - Spatial scan statistic: https://www.satscan.org/papers/k-cstm1997.pdf
532: - DP mixture MCMC: https://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/DirichletProc/NealDirichletJCGS2000.pdf
533: - TDA survey example: https://www.frontiersin.org/articles/10.3389/frai.2021.681108/full
534: - Copula shift detection example: https://www.tandfonline.com/doi/full/10.1080/08982112.2025.2577437
535: 
536: ## 16) Definition of Done (must meet)
537: - `python -m pytest -q` passes.
538: - `stat-harness list-plugins` shows all plugins above.
539: - UI:
540:   - upload CSV
541:   - select plugins
542:   - run completes
543:   - run page shows results
544:   - report downloads work
545: - Integration pipeline always writes report.md + report.json even if a plugin errors.
546: - report.json validates against docs/report.schema.json.
547: 
548: ## 17) Implementation instruction (what Codex must do)
549: 1) Create all files/directories above.
550: 2) Implement core framework + UI + CLI.
551: 3) Implement every plugin listed (Phase 1) with deterministic behavior and tests.
552: 4) Run tests and fix until green.
553: 5) Keep runtime offline (no network calls).
554: 6) Do not copy code from the internet; implement algorithms from scratch based on the ideas only.
````

## File: config/app.yaml
````yaml
1: appdata_dir: appdata
2: max_upload_size_mb: 50
````

## File: docs/evaluation.md
````markdown
 1: # Evaluation Harness
 2: 
 3: The evaluator compares `report.json` against a ground truth YAML file to assert
 4: expected findings (feature discovery, changepoints, dependence shifts, and
 5: anomalies) within configured tolerances. `strict` defaults to `true`; set
 6: `strict: false` in the YAML to allow unexpected findings.
 7: 
 8: You can also specify `expected_findings` for plugin-specific assertions:
 9: 
10: ```
11: expected_findings:
12:   - plugin_id: analysis_process_sequence
13:     kind: process_variant
14:     contains:
15:       variant: qemail
16:     min_count: 1
17: ```
18: 
19: `where` enforces exact matches, `contains` checks substring/list membership.
20: 
21: You can also specify expected numeric metrics:
22: 
23: ```
24: expected_metrics:
25:   - plugin_id: analysis_queue_delay_decomposition
26:     metric: eligible_wait.p95
27:     value: 120.0
28:     tolerance:
29:       absolute: 5
30:       relative: 0.1
31: ```
32: 
33: Tolerance supports `absolute` and `relative` (fractional) thresholds. When `strict`
34: is true, unexpected findings for a known kind will fail evaluation.
````

## File: docs/plugin_dev_guide.md
````markdown
 1: # Plugin Development Guide
 2: 
 3: Plugins live in `plugins/<plugin_id>` and provide a `plugin.yaml` manifest with
 4: an entrypoint class that implements `run(ctx)`.
 5: 
 6: ## Manifest + Schemas
 7: 
 8: Each plugin must ship:
 9: 
10: - `plugin.yaml` (validated against `docs/plugin_manifest.schema.json`)
11: - `config.schema.json` (defaults validated on load)
12: - `output.schema.json` (validated after execution)
13: 
14: Manifests define `type`, `entrypoint`, `depends_on`, `capabilities`, and `sandbox`
15: (`no_network` + `fs_allowlist`).
16: 
17: ## Output Requirements
18: 
19: Findings must include:
20: 
21: - `measurement_type`: `measured | modeled | not_applicable | error`
22: - `evidence`: dataset_id, dataset_version_id, row_ids, column_ids, and query
23: 
24: All plugins should emit a `budget` object (row_limit/sample flags/time/CPU budgets)
25: even when defaults are “unlimited”.
````

## File: docs/references.md
````markdown
 1: # References
 2: 
 3: - Conformal prediction: https://arxiv.org/abs/2107.07511
 4: - Model-X knockoffs: https://academic.oup.com/jrsssb/article/80/3/551/7048447
 5: - NOTEARS: https://arxiv.org/abs/1803.01422
 6: - BOCPD: https://arxiv.org/abs/0710.3742
 7: - Spatial scan statistic: https://www.satscan.org/papers/k-cstm1997.pdf
 8: - DP mixture MCMC: https://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/DirichletProc/NealDirichletJCGS2000.pdf
 9: - TDA survey example: https://www.frontiersin.org/articles/10.3389/frai.2021.681108/full
10: - Copula shift detection example: https://www.tandfonline.com/doi/full/10.1080/08982112.2025.2577437
````

## File: plugins/analysis_bocpd_gaussian/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_bocpd_gaussian/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import numpy as np
 4: 
 5: from statistic_harness.core.types import PluginArtifact, PluginResult
 6: from statistic_harness.core.utils import write_json
 7: 
 8: 
 9: class Plugin:
10:     def run(self, ctx) -> PluginResult:
11:         df = ctx.dataset_loader()
12:         value_col = ctx.settings.get("value_column")
13:         if not value_col:
14:             numeric = df.select_dtypes(include="number")
15:             if numeric.empty:
16:                 return PluginResult("skipped", "No numeric columns", {}, [], [], None)
17:             value_col = numeric.columns[0]
18:         series = df[value_col].to_numpy()
19:         n = len(series)
20:         if n < 2:
21:             return PluginResult("skipped", "Not enough data", {}, [], [], None)
22: 
23:         rolling_mean = np.cumsum(series) / (np.arange(n) + 1)
24:         diffs = np.abs(series - rolling_mean)
25:         probs = diffs / (diffs.max() + 1e-8)
26:         peak_threshold = float(ctx.settings.get("peak_threshold", 0.6))
27:         changepoints = []
28:         for idx, prob in enumerate(probs):
29:             if prob >= peak_threshold and idx > 0:
30:                 changepoints.append(
31:                     {"kind": "changepoint", "index": int(idx), "prob": float(prob)}
32:                 )
33:                 break
34: 
35:         artifacts_dir = ctx.artifacts_dir("analysis_bocpd_gaussian")
36:         out_path = artifacts_dir / "changepoints.json"
37:         write_json(out_path, changepoints)
38:         artifacts = [
39:             PluginArtifact(
40:                 path=str(out_path.relative_to(ctx.run_dir)),
41:                 type="json",
42:                 description="Changepoints",
43:             )
44:         ]
45:         return PluginResult(
46:             "ok",
47:             "Computed BOCPD changepoints",
48:             {"count": len(changepoints)},
49:             changepoints,
50:             artifacts,
51:             None,
52:         )
````

## File: plugins/analysis_bocpd_gaussian/plugin.yaml
````yaml
 1: id: analysis_bocpd_gaussian
 2: name: BOCPD Gaussian
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Simple BOCPD
12:   defaults:
13:     value_column: null
14:     time_column: null
15:     hazard: 0.005
16:     peak_threshold: 0.6
17: config_schema: config.schema.json
18: output_schema: output.schema.json
19: sandbox:
20:   no_network: true
21:   fs_allowlist:
22:   - appdata
23:   - plugins
````

## File: plugins/analysis_capacity_scaling/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.column_inference import choose_timestamp_column
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: INVALID_STRINGS = {"", "nan", "none", "null"}
 13: 
 14: 
 15: def _normalize_text(value: Any) -> str:
 16:     if value is None:
 17:         return ""
 18:     if isinstance(value, float) and pd.isna(value):
 19:         return ""
 20:     return str(value).strip()
 21: 
 22: 
 23: def _pick_column(
 24:     preferred: str | None,
 25:     columns: list[str],
 26:     role_by_name: dict[str, str],
 27:     roles: set[str],
 28:     patterns: list[str],
 29:     lower_names: dict[str, str],
 30:     exclude: set[str],
 31: ) -> str | None:
 32:     if preferred and preferred in columns:
 33:         return preferred
 34:     for col in columns:
 35:         if col in exclude:
 36:             continue
 37:         if role_by_name.get(col) in roles:
 38:             return col
 39:     for col in columns:
 40:         if col in exclude:
 41:             continue
 42:         name = lower_names[col]
 43:         if any(pattern in name for pattern in patterns):
 44:             return col
 45:     return None
 46: 
 47: 
 48: def _pick_timestamp_column(
 49:     preferred: str | None,
 50:     columns: list[str],
 51:     role_by_name: dict[str, str],
 52:     roles: set[str],
 53:     patterns: list[str],
 54:     lower_names: dict[str, str],
 55:     exclude: set[str],
 56:     df: pd.DataFrame,
 57: ) -> str | None:
 58:     candidates: list[str] = []
 59:     if preferred and preferred in columns and preferred not in exclude:
 60:         candidates.append(preferred)
 61:     for col in columns:
 62:         if col in exclude:
 63:             continue
 64:         if role_by_name.get(col) in roles and col not in candidates:
 65:             candidates.append(col)
 66:     for col in columns:
 67:         if col in exclude:
 68:             continue
 69:         name = lower_names[col]
 70:         if any(pattern in name for pattern in patterns) and col not in candidates:
 71:             candidates.append(col)
 72:     if not candidates:
 73:         return None
 74:     return choose_timestamp_column(df, candidates)
 75: 
 76: 
 77: class Plugin:
 78:     def run(self, ctx) -> PluginResult:
 79:         df = ctx.dataset_loader()
 80:         if df.empty:
 81:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 82: 
 83:         columns_meta = []
 84:         role_by_name: dict[str, str] = {}
 85:         if ctx.dataset_version_id:
 86:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 87:             if dataset_template and dataset_template.get("status") == "ready":
 88:                 fields = ctx.storage.fetch_template_fields(
 89:                     int(dataset_template["template_id"])
 90:                 )
 91:                 columns_meta = fields
 92:                 role_by_name = {
 93:                     field["name"]: (field.get("role") or "") for field in fields
 94:                 }
 95:             else:
 96:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 97:                 role_by_name = {
 98:                     col["original_name"]: (col.get("role") or "")
 99:                     for col in columns_meta
100:                 }
101: 
102:         columns = list(df.columns)
103:         lower_names = {col: str(col).lower() for col in columns}
104:         used: set[str] = set()
105: 
106:         process_col = _pick_column(
107:             ctx.settings.get("process_column"),
108:             columns,
109:             role_by_name,
110:             {"process_name", "process_id"},
111:             ["process", "activity", "event", "step", "task", "action", "job"],
112:             lower_names,
113:             used,
114:         )
115:         if process_col:
116:             used.add(process_col)
117: 
118:         module_col = _pick_column(
119:             ctx.settings.get("module_column"),
120:             columns,
121:             role_by_name,
122:             {"module_code"},
123:             ["module", "mod"],
124:             lower_names,
125:             used,
126:         )
127:         if module_col:
128:             used.add(module_col)
129: 
130:         host_col = _pick_column(
131:             ctx.settings.get("host_column"),
132:             columns,
133:             role_by_name,
134:             {"server", "host", "node", "instance"},
135:             ["server", "host", "node", "instance", "machine"],
136:             lower_names,
137:             used,
138:         )
139:         if host_col:
140:             used.add(host_col)
141: 
142:         queue_col = _pick_timestamp_column(
143:             ctx.settings.get("queue_column"),
144:             columns,
145:             role_by_name,
146:             {"queue", "queued", "enqueue"},
147:             ["queue", "queued", "enqueue"],
148:             lower_names,
149:             used,
150:             df,
151:         )
152:         if queue_col:
153:             used.add(queue_col)
154: 
155:         eligible_col = _pick_timestamp_column(
156:             ctx.settings.get("eligible_column"),
157:             columns,
158:             role_by_name,
159:             {"eligible", "ready", "available"},
160:             ["eligible", "ready", "available"],
161:             lower_names,
162:             used,
163:             df,
164:         )
165:         if eligible_col:
166:             used.add(eligible_col)
167: 
168:         start_col = _pick_timestamp_column(
169:             ctx.settings.get("start_column"),
170:             columns,
171:             role_by_name,
172:             {"start_time", "start"},
173:             ["start", "begin"],
174:             lower_names,
175:             used,
176:             df,
177:         )
178:         if start_col:
179:             used.add(start_col)
180: 
181:         if not start_col or (not queue_col and not eligible_col):
182:             return PluginResult(
183:                 "ok",
184:                 "Capacity scaling not applicable",
185:                 {"rows": 0},
186:                 [
187:                     {
188:                         "kind": "capacity_scaling",
189:                         "measurement_type": "not_applicable",
190:                         "reason": "Missing queue/eligible and start columns.",
191:                         "columns": [
192:                             col for col in [queue_col, eligible_col, start_col] if col
193:                         ],
194:                     }
195:                 ],
196:                 [],
197:                 None,
198:             )
199: 
200:         selected = [
201:             col
202:             for col in [
203:                 process_col,
204:                 module_col,
205:                 host_col,
206:                 queue_col,
207:                 eligible_col,
208:                 start_col,
209:             ]
210:             if col
211:         ]
212:         work = df.loc[:, selected].copy()
213: 
214:         eligible_ts = (
215:             pd.to_datetime(work[eligible_col], errors="coerce", utc=False)
216:             if eligible_col
217:             else None
218:         )
219:         if eligible_ts is None:
220:             eligible_ts = pd.to_datetime(work[queue_col], errors="coerce", utc=False)
221:             eligible_basis = "queue"
222:         else:
223:             eligible_basis = "eligible"
224:         start_ts = pd.to_datetime(work[start_col], errors="coerce", utc=False)
225: 
226:         work["__eligible_ts"] = eligible_ts
227:         work["__start_ts"] = start_ts
228:         work = work.loc[work["__eligible_ts"].notna() & work["__start_ts"].notna()].copy()
229:         if work.empty:
230:             return PluginResult(
231:                 "ok",
232:                 "No valid timestamps",
233:                 {"rows": 0},
234:                 [],
235:                 [],
236:                 None,
237:             )
238: 
239:         work["__eligible_wait_sec"] = (
240:             work["__start_ts"] - work["__eligible_ts"]
241:         ).dt.total_seconds().clip(lower=0).fillna(0)
242: 
243:         host_count = 0
244:         if host_col and host_col in work.columns:
245:             work["__host"] = work[host_col].map(_normalize_text)
246:             work["__host_norm"] = work["__host"].str.lower()
247:             work = work.loc[~work["__host_norm"].isin(INVALID_STRINGS)].copy()
248:             host_count = int(work["__host_norm"].nunique())
249: 
250:         raw_scale_factor = ctx.settings.get("scale_factor")
251:         scale_factor = raw_scale_factor
252:         try:
253:             scale_factor = float(scale_factor) if scale_factor is not None else None
254:         except (TypeError, ValueError):
255:             scale_factor = None
256:         derived_scale = scale_factor is None
257:         if scale_factor is None:
258:             if host_count >= 1:
259:                 scale_factor = (host_count + 1) / float(host_count)
260:             else:
261:                 scale_factor = 1.0
262:         if scale_factor <= 0:
263:             scale_factor = 1.0
264:         host_count_modeled = None
265:         if derived_scale and host_count > 0:
266:             host_count_modeled = host_count + 1
267: 
268:         work["__scaled_wait_sec"] = work["__eligible_wait_sec"] / float(scale_factor)
269:         work["__reduction_sec"] = work["__eligible_wait_sec"] - work["__scaled_wait_sec"]
270: 
271:         assumptions = [
272:             "Eligible wait scales inversely with capacity.",
273:             "Prerequisite wait unchanged.",
274:         ]
275:         scope = {"metric": "eligible_wait_hours", "eligible_basis": eligible_basis}
276: 
277:         max_groups = int(ctx.settings.get("max_groups", 5))
278:         max_examples = int(ctx.settings.get("max_examples", 25))
279: 
280:         findings = []
281: 
282:         def _emit_dimension(name: str, col: str | None) -> None:
283:             if not col or col not in work.columns:
284:                 return
285:             temp = work.copy()
286:             temp["__key"] = temp[col].map(_normalize_text)
287:             temp = temp.loc[~temp["__key"].str.lower().isin(INVALID_STRINGS)].copy()
288:             if temp.empty:
289:                 return
290:             grouped = (
291:                 temp.groupby("__key")
292:                 .agg(
293:                     rows=("__key", "size"),
294:                     baseline_sec=("__eligible_wait_sec", "sum"),
295:                     modeled_sec=("__scaled_wait_sec", "sum"),
296:                     reduction_sec=("__reduction_sec", "sum"),
297:                 )
298:                 .reset_index()
299:             )
300:             grouped = grouped.sort_values(
301:                 ["baseline_sec", "__key"], ascending=[False, True]
302:             )
303:             for _, row in grouped.head(max_groups).iterrows():
304:                 key = row["__key"]
305:                 row_ids = temp.loc[temp["__key"] == key].index.tolist()[:max_examples]
306:                 findings.append(
307:                     {
308:                         "kind": "capacity_scaling",
309:                         "dimension": name,
310:                         "key": key,
311:                         "rows": int(row["rows"]),
312:                         "baseline_wait_hours": float(row["baseline_sec"]) / 3600.0,
313:                         "modeled_wait_hours": float(row["modeled_sec"]) / 3600.0,
314:                         "reduction_hours": float(row["reduction_sec"]) / 3600.0,
315:                         "scale_factor": float(scale_factor),
316:                         "host_count_baseline": host_count or None,
317:                         "host_count_modeled": host_count_modeled,
318:                         "eligible_basis": eligible_basis,
319:                         "assumptions": assumptions,
320:                         "scope": scope,
321:                         "measurement_type": "modeled",
322:                         "row_ids": [int(i) for i in row_ids],
323:                         "columns": [
324:                             col
325:                             for col in [process_col, module_col, host_col, queue_col, eligible_col, start_col]
326:                             if col
327:                         ],
328:                     }
329:                 )
330: 
331:         _emit_dimension("process", process_col)
332:         _emit_dimension("module", module_col)
333: 
334:         total_baseline = float(work["__eligible_wait_sec"].sum())
335:         total_modeled = float(work["__scaled_wait_sec"].sum())
336:         total_reduction = float(work["__reduction_sec"].sum())
337: 
338:         findings.insert(
339:             0,
340:             {
341:                 "kind": "capacity_scaling",
342:                 "dimension": "overall",
343:                 "key": "overall",
344:                 "rows": int(work.shape[0]),
345:                 "baseline_wait_hours": total_baseline / 3600.0,
346:                 "modeled_wait_hours": total_modeled / 3600.0,
347:                 "reduction_hours": total_reduction / 3600.0,
348:                 "scale_factor": float(scale_factor),
349:                 "host_count_baseline": host_count or None,
350:                 "host_count_modeled": host_count_modeled,
351:                 "eligible_basis": eligible_basis,
352:                 "assumptions": assumptions,
353:                 "scope": scope,
354:                 "measurement_type": "modeled",
355:                 "row_ids": [int(i) for i in work.index.tolist()[:max_examples]],
356:                 "columns": [
357:                     col
358:                     for col in [process_col, module_col, host_col, queue_col, eligible_col, start_col]
359:                     if col
360:                 ],
361:             },
362:         )
363: 
364:         metrics = {
365:             "rows": int(work.shape[0]),
366:             "baseline_wait_hours": total_baseline / 3600.0,
367:             "modeled_wait_hours": total_modeled / 3600.0,
368:             "reduction_hours": total_reduction / 3600.0,
369:             "scale_factor": float(scale_factor),
370:             "host_count": host_count,
371:         }
372: 
373:         artifacts_dir = ctx.artifacts_dir("analysis_capacity_scaling")
374:         out_path = artifacts_dir / "capacity_scaling.json"
375:         write_json(
376:             out_path,
377:             {
378:                 "summary": {
379:                     "process_column": process_col,
380:                     "module_column": module_col,
381:                     "host_column": host_col,
382:                     "queue_column": queue_col,
383:                     "eligible_column": eligible_col,
384:                     "start_column": start_col,
385:                 "eligible_basis": eligible_basis,
386:                 "scale_factor": float(scale_factor),
387:                 "assumptions": assumptions,
388:             },
389:                 "metrics": metrics,
390:             },
391:         )
392:         artifacts = [
393:             PluginArtifact(
394:                 path=str(out_path.relative_to(ctx.run_dir)),
395:                 type="json",
396:                 description="Capacity scaling summary",
397:             )
398:         ]
399: 
400:         return PluginResult(
401:             "ok",
402:             "Modeled capacity scaling impact",
403:             metrics,
404:             findings,
405:             artifacts,
406:             None,
407:         )
````

## File: plugins/analysis_chain_makespan/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.column_inference import choose_timestamp_column
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: INVALID_STRINGS = {"", "nan", "none", "null"}
 13: 
 14: 
 15: def _normalize_text(value: Any) -> str:
 16:     if value is None:
 17:         return ""
 18:     if isinstance(value, float) and pd.isna(value):
 19:         return ""
 20:     return str(value).strip()
 21: 
 22: 
 23: def _pick_column(
 24:     preferred: str | None,
 25:     columns: list[str],
 26:     role_by_name: dict[str, str],
 27:     roles: set[str],
 28:     patterns: list[str],
 29:     lower_names: dict[str, str],
 30:     exclude: set[str],
 31: ) -> str | None:
 32:     if preferred and preferred in columns:
 33:         return preferred
 34:     for col in columns:
 35:         if col in exclude:
 36:             continue
 37:         if role_by_name.get(col) in roles:
 38:             return col
 39:     for col in columns:
 40:         if col in exclude:
 41:             continue
 42:         name = lower_names[col]
 43:         if any(pattern in name for pattern in patterns):
 44:             return col
 45:     return None
 46: 
 47: 
 48: def _pick_timestamp_column(
 49:     preferred: str | None,
 50:     columns: list[str],
 51:     role_by_name: dict[str, str],
 52:     roles: set[str],
 53:     patterns: list[str],
 54:     lower_names: dict[str, str],
 55:     exclude: set[str],
 56:     df: pd.DataFrame,
 57: ) -> str | None:
 58:     candidates: list[str] = []
 59:     if preferred and preferred in columns and preferred not in exclude:
 60:         candidates.append(preferred)
 61:     for col in columns:
 62:         if col in exclude:
 63:             continue
 64:         if role_by_name.get(col) in roles and col not in candidates:
 65:             candidates.append(col)
 66:     for col in columns:
 67:         if col in exclude:
 68:             continue
 69:         name = lower_names[col]
 70:         if any(pattern in name for pattern in patterns) and col not in candidates:
 71:             candidates.append(col)
 72:     if not candidates:
 73:         return None
 74:     return choose_timestamp_column(df, candidates)
 75: 
 76: 
 77: class Plugin:
 78:     def run(self, ctx) -> PluginResult:
 79:         df = ctx.dataset_loader()
 80:         if df.empty:
 81:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 82: 
 83:         columns_meta = []
 84:         role_by_name: dict[str, str] = {}
 85:         if ctx.dataset_version_id:
 86:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 87:             if dataset_template and dataset_template.get("status") == "ready":
 88:                 fields = ctx.storage.fetch_template_fields(
 89:                     int(dataset_template["template_id"])
 90:                 )
 91:                 columns_meta = fields
 92:                 role_by_name = {
 93:                     field["name"]: (field.get("role") or "") for field in fields
 94:                 }
 95:             else:
 96:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 97:                 role_by_name = {
 98:                     col["original_name"]: (col.get("role") or "")
 99:                     for col in columns_meta
100:                 }
101: 
102:         columns = list(df.columns)
103:         lower_names = {col: str(col).lower() for col in columns}
104:         used: set[str] = set()
105: 
106:         sequence_col = _pick_column(
107:             ctx.settings.get("sequence_column"),
108:             columns,
109:             role_by_name,
110:             {"master_id", "sequence_id", "master", "chain", "batch", "case_id"},
111:             ["master", "sequence", "chain", "batch", "workflow", "case", "group"],
112:             lower_names,
113:             used,
114:         )
115:         if sequence_col:
116:             used.add(sequence_col)
117: 
118:         start_col = _pick_timestamp_column(
119:             ctx.settings.get("start_column"),
120:             columns,
121:             role_by_name,
122:             {"start_time", "start"},
123:             ["start", "begin"],
124:             lower_names,
125:             used,
126:             df,
127:         )
128:         if start_col:
129:             used.add(start_col)
130: 
131:         end_col = _pick_timestamp_column(
132:             ctx.settings.get("end_column"),
133:             columns,
134:             role_by_name,
135:             {"end_time", "end"},
136:             ["end", "finish", "complete", "stop"],
137:             lower_names,
138:             used,
139:             df,
140:         )
141:         if end_col:
142:             used.add(end_col)
143: 
144:         duration_col = _pick_column(
145:             ctx.settings.get("duration_column"),
146:             columns,
147:             role_by_name,
148:             {"duration", "runtime", "elapsed"},
149:             ["duration", "runtime", "elapsed", "secs", "seconds", "ms"],
150:             lower_names,
151:             used,
152:         )
153: 
154:         if not sequence_col or not start_col or (not end_col and not duration_col):
155:             return PluginResult(
156:                 "ok",
157:                 "Chain makespan not applicable",
158:                 {"chains": 0},
159:                 [
160:                     {
161:                         "kind": "chain_makespan",
162:                         "measurement_type": "not_applicable",
163:                         "reason": "Missing sequence and start/end columns.",
164:                         "columns": [
165:                             col
166:                             for col in [sequence_col, start_col, end_col, duration_col]
167:                             if col
168:                         ],
169:                     }
170:                 ],
171:                 [],
172:                 None,
173:             )
174: 
175:         selected: list[str] = []
176:         for col in [sequence_col, start_col, end_col, duration_col]:
177:             if col and col in columns and col not in selected:
178:                 selected.append(col)
179:         work = df.loc[:, selected].copy()
180: 
181:         work["__sequence"] = work[sequence_col].map(_normalize_text)
182:         work["__sequence_norm"] = work["__sequence"].str.lower()
183:         work = work.loc[~work["__sequence_norm"].isin(INVALID_STRINGS)].copy()
184:         if work.empty:
185:             return PluginResult(
186:                 "ok",
187:                 "No sequence ids found",
188:                 {"chains": 0},
189:                 [
190:                     {
191:                         "kind": "chain_makespan",
192:                         "measurement_type": "not_applicable",
193:                         "reason": "No sequence values detected.",
194:                         "columns": [sequence_col],
195:                     }
196:                 ],
197:                 [],
198:                 None,
199:             )
200: 
201:         work["__start_ts"] = pd.to_datetime(
202:             work[start_col], errors="coerce", utc=False
203:         )
204:         if end_col:
205:             work["__end_ts"] = pd.to_datetime(
206:                 work[end_col], errors="coerce", utc=False
207:             )
208:         else:
209:             work["__end_ts"] = pd.NaT
210: 
211:         if duration_col:
212:             work["__duration_sec"] = pd.to_numeric(
213:                 work[duration_col], errors="coerce"
214:             )
215:         else:
216:             work["__duration_sec"] = pd.NA
217: 
218:         missing_end = work["__end_ts"].isna() & work["__duration_sec"].notna()
219:         if missing_end.any():
220:             work.loc[missing_end, "__end_ts"] = work.loc[
221:                 missing_end, "__start_ts"
222:             ] + pd.to_timedelta(work.loc[missing_end, "__duration_sec"], unit="s")
223: 
224:         computed_duration = (work["__end_ts"] - work["__start_ts"]).dt.total_seconds()
225:         if duration_col:
226:             work["__duration_sec"] = computed_duration.where(
227:                 computed_duration.notna(), work["__duration_sec"]
228:             )
229:         else:
230:             work["__duration_sec"] = computed_duration
231: 
232:         work = work.loc[work["__start_ts"].notna() & work["__end_ts"].notna()].copy()
233:         if work.empty:
234:             return PluginResult(
235:                 "ok",
236:                 "No valid timestamps",
237:                 {"chains": 0},
238:                 [
239:                     {
240:                         "kind": "chain_makespan",
241:                         "measurement_type": "not_applicable",
242:                         "reason": "No valid start/end timestamps.",
243:                         "columns": [start_col, end_col or duration_col],
244:                     }
245:                 ],
246:                 [],
247:                 None,
248:             )
249: 
250:         grouped = work.groupby("__sequence_norm", sort=False)
251:         rows: list[tuple[str, str, float, float, float, list[int]]] = []
252:         for seq_norm, frame in grouped:
253:             seq_label = frame["__sequence"].iloc[0]
254:             start_min = frame["__start_ts"].min()
255:             end_max = frame["__end_ts"].max()
256:             makespan = (end_max - start_min).total_seconds()
257:             runtime = (
258:                 frame["__duration_sec"].clip(lower=0).fillna(0).sum()
259:                 if "__duration_sec" in frame
260:                 else 0.0
261:             )
262:             makespan = float(max(makespan, 0.0))
263:             runtime = float(runtime)
264:             idle_gap = max(makespan - runtime, 0.0)
265:             row_ids = [int(i) for i in frame.index.tolist()]
266:             rows.append((seq_norm, seq_label, makespan, runtime, idle_gap, row_ids))
267: 
268:         if not rows:
269:             return PluginResult(
270:                 "ok",
271:                 "No chain rows found",
272:                 {"chains": 0},
273:                 [],
274:                 [],
275:                 None,
276:             )
277: 
278:         rows.sort(key=lambda item: (-item[2], item[0]))
279:         max_sequences = int(ctx.settings.get("max_sequences", 10))
280:         max_examples = int(ctx.settings.get("max_examples", 25))
281: 
282:         findings = []
283:         for seq_norm, seq_label, makespan, runtime, idle_gap, row_ids in rows[:max_sequences]:
284:             runtime_ratio = runtime / makespan if makespan > 0 else 0.0
285:             idle_ratio = idle_gap / makespan if makespan > 0 else 0.0
286:             findings.append(
287:                 {
288:                     "kind": "chain_makespan",
289:                     "sequence_id": seq_label,
290:                     "sequence_norm": seq_norm,
291:                     "makespan_seconds": float(makespan),
292:                     "runtime_seconds": float(runtime),
293:                     "idle_gap_seconds": float(idle_gap),
294:                     "runtime_ratio": float(runtime_ratio),
295:                     "idle_ratio": float(idle_ratio),
296:                     "measurement_type": "measured",
297:                     "row_ids": row_ids[:max_examples],
298:                     "columns": [
299:                         col
300:                         for col in [sequence_col, start_col, end_col, duration_col]
301:                         if col
302:                     ],
303:                 }
304:             )
305: 
306:         metrics = {
307:             "chains": len(rows),
308:             "max_makespan_seconds": float(rows[0][2]),
309:         }
310: 
311:         artifacts_dir = ctx.artifacts_dir("analysis_chain_makespan")
312:         out_path = artifacts_dir / "chain_makespan.json"
313:         write_json(
314:             out_path,
315:             {
316:                 "summary": {
317:                     "sequence_column": sequence_col,
318:                     "start_column": start_col,
319:                     "end_column": end_col,
320:                     "duration_column": duration_col,
321:                 },
322:                 "chains": len(rows),
323:                 "max_makespan_seconds": float(rows[0][2]),
324:             },
325:         )
326:         artifacts = [
327:             PluginArtifact(
328:                 path=str(out_path.relative_to(ctx.run_dir)),
329:                 type="json",
330:                 description="Chain makespan summary",
331:             )
332:         ]
333: 
334:         return PluginResult(
335:             "ok",
336:             "Computed chain makespan statistics",
337:             metrics,
338:             findings,
339:             artifacts,
340:             None,
341:         )
````

## File: plugins/analysis_close_cycle_capacity_impact/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "title": "Close Cycle Capacity Impact Config",
 4:   "type": "object",
 5:   "additionalProperties": false,
 6:   "properties": {
 7:     "process_column": { "type": ["string", "null"] },
 8:     "host_column": { "type": ["string", "null"] },
 9:     "start_column": { "type": ["string", "null"] },
10:     "end_column": { "type": ["string", "null"] },
11:     "queue_column": { "type": ["string", "null"] },
12:     "eligible_column": { "type": ["string", "null"] },
13:     "close_window_mode": { "type": "string", "enum": ["infer", "override", "infer_or_default", "calendar"] },
14:     "close_cycle_start_day": { "type": "integer", "minimum": 1, "maximum": 31 },
15:     "close_cycle_end_day": { "type": "integer", "minimum": 1, "maximum": 31 },
16:     "min_close_days": { "type": "integer", "minimum": 1 },
17:     "max_close_days": { "type": "integer", "minimum": 1 },
18:     "lookahead_days": { "type": "integer", "minimum": 0 },
19:     "min_close_confidence": { "type": "number", "minimum": 0, "maximum": 1 },
20:     "min_close_data_ratio": { "type": "number", "minimum": 0, "maximum": 1 },
21:     "bucket_size": { "type": "string", "enum": ["day", "hour"] },
22:     "min_bucket_rows": { "type": "integer", "minimum": 0 },
23:     "min_buckets_per_group": { "type": "integer", "minimum": 1 },
24:     "min_buckets_per_month": { "type": "integer", "minimum": 1 },
25:     "min_months": { "type": "integer", "minimum": 1 },
26:     "target_reduction": { "type": "number", "minimum": 0, "maximum": 1 },
27:     "tolerance": { "type": "number", "minimum": 0, "maximum": 1 },
28:     "alpha": { "type": "number", "minimum": 0, "maximum": 1 },
29:     "bootstrap_samples": { "type": "integer", "minimum": 100 },
30:     "max_js_divergence": { "type": "number", "minimum": 0, "maximum": 1 },
31:     "min_volume_ratio": { "type": "number", "minimum": 0 },
32:     "max_volume_ratio": { "type": "number", "minimum": 0 },
33:     "max_examples": { "type": "integer", "minimum": 1 }
34:   }
35: }
````

## File: plugins/analysis_close_cycle_capacity_impact/plugin.yaml
````yaml
 1: id: analysis_close_cycle_capacity_impact
 2: name: Close Cycle Capacity Impact
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Detect close-cycle completion-time improvement when a 3rd server is added.
 9:   defaults:
10:     process_column: null
11:     host_column: null
12:     start_column: null
13:     end_column: null
14:     queue_column: null
15:     eligible_column: null
16:     close_window_mode: infer_or_default
17:     close_cycle_start_day: 20
18:     close_cycle_end_day: 5
19:     min_close_days: 5
20:     max_close_days: 20
21:     lookahead_days: 7
22:     min_close_confidence: 0.15
23:     min_close_data_ratio: 0.5
24:     bucket_size: day
25:     min_bucket_rows: 50
26:     min_buckets_per_group: 5
27:     min_buckets_per_month: 3
28:     min_months: 2
29:     target_reduction: 0.30
30:     tolerance: 0.05
31:     alpha: 0.01
32:     bootstrap_samples: 1000
33:     max_js_divergence: 0.2
34:     min_volume_ratio: 0.5
35:     max_volume_ratio: 2.0
36:     max_examples: 25
37: capabilities:
38:   - needs_eventlog
39:   - needs_timestamp
40: config_schema: config.schema.json
41: output_schema: output.schema.json
42: sandbox:
43:   no_network: true
44:   fs_allowlist:
45:   - appdata
46:   - plugins
47:   - run_dir
````

## File: plugins/analysis_close_cycle_capacity_model/plugin.yaml
````yaml
 1: id: analysis_close_cycle_capacity_model
 2: name: Close Cycle Capacity Model
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Model close-cycle completion-time impact from adding capacity (model-only findings).
 9:   defaults:
10:     process_column: null
11:     host_column: null
12:     start_column: null
13:     end_column: null
14:     queue_column: null
15:     eligible_column: null
16:     close_window_mode: infer_or_default
17:     close_cycle_start_day: 20
18:     close_cycle_end_day: 5
19:     min_close_days: 5
20:     max_close_days: 20
21:     lookahead_days: 7
22:     min_close_confidence: 0.1
23:     min_close_data_ratio: 0.5
24:     bucket_size: day
25:     min_bucket_rows: 50
26:     min_buckets_per_group: 5
27:     min_months: 1
28:     baseline_host_count: null
29:     added_hosts: 1
30:     baseline_match_mode: exact
31:     target_reduction: 0.30
32:     tolerance: 0.05
33:     max_examples: 25
34: capabilities:
35:   - needs_eventlog
36:   - needs_timestamp
37: config_schema: config.schema.json
38: output_schema: output.schema.json
39: sandbox:
40:   no_network: true
41:   fs_allowlist:
42:   - appdata
43:   - plugins
44:   - run_dir
````

## File: plugins/analysis_close_cycle_contention/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: import json
  4: import re
  5: from collections import Counter
  6: from typing import Any
  7: 
  8: import pandas as pd
  9: 
 10: from statistic_harness.core.types import PluginArtifact, PluginResult
 11: from statistic_harness.core.utils import write_json
 12: 
 13: 
 14: def _spearman_corr(left: pd.Series, right: pd.Series) -> float:
 15:     if left.empty or right.empty:
 16:         return float("nan")
 17:     left_rank = left.rank(method="average")
 18:     right_rank = right.rank(method="average")
 19:     return float(left_rank.corr(right_rank, method="pearson"))
 20: 
 21: 
 22: def _normalize_param(value: Any) -> str | None:
 23:     if value is None:
 24:         return None
 25:     if isinstance(value, float) and pd.isna(value):
 26:         return None
 27:     raw = str(value).strip()
 28:     if not raw:
 29:         return None
 30:     if raw.startswith("{") and raw.endswith("}"):
 31:         try:
 32:             parsed = json.loads(raw)
 33:         except json.JSONDecodeError:
 34:             parsed = None
 35:         if isinstance(parsed, dict):
 36:             items = [(str(k).strip().lower(), str(v).strip()) for k, v in parsed.items()]
 37:             items = sorted(set(items))
 38:             return ";".join(f"{k}={v}" for k, v in items)
 39:     tokens = re.split(r"[;|,\n]+", raw)
 40:     pairs = []
 41:     for token in tokens:
 42:         token = token.strip()
 43:         if not token:
 44:             continue
 45:         if "=" in token:
 46:             key, val = token.split("=", 1)
 47:         elif ":" in token:
 48:             key, val = token.split(":", 1)
 49:         else:
 50:             continue
 51:         key = key.strip().lower()
 52:         val = val.strip()
 53:         if key:
 54:             pairs.append((key, val))
 55:     if pairs:
 56:         pairs = sorted(set(pairs))
 57:         return ";".join(f"{k}={v}" for k, v in pairs)
 58:     return raw.lower()
 59: 
 60: 
 61: def _pick_column(
 62:     preferred: str | None,
 63:     columns: list[str],
 64:     role_by_name: dict[str, str],
 65:     roles: set[str],
 66:     patterns: list[str],
 67:     lower_names: dict[str, str],
 68:     exclude: set[str],
 69: ) -> str | None:
 70:     if preferred and preferred in columns:
 71:         return preferred
 72:     for col in columns:
 73:         if col in exclude:
 74:             continue
 75:         if role_by_name.get(col) in roles:
 76:             return col
 77:     for col in columns:
 78:         if col in exclude:
 79:             continue
 80:         name = lower_names[col]
 81:         if any(pattern in name for pattern in patterns):
 82:             return col
 83:     return None
 84: 
 85: 
 86: class Plugin:
 87:     def run(self, ctx) -> PluginResult:
 88:         df = ctx.dataset_loader()
 89:         if df.empty:
 90:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 91: 
 92:         columns_meta = []
 93:         role_by_name: dict[str, str] = {}
 94:         if ctx.dataset_version_id:
 95:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 96:             if dataset_template and dataset_template.get("status") == "ready":
 97:                 fields = ctx.storage.fetch_template_fields(
 98:                     int(dataset_template["template_id"])
 99:                 )
100:                 columns_meta = fields
101:                 role_by_name = {
102:                     field["name"]: (field.get("role") or "") for field in fields
103:                 }
104:             else:
105:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
106:                 role_by_name = {
107:                     col["original_name"]: (col.get("role") or "")
108:                     for col in columns_meta
109:                 }
110: 
111:         columns = list(df.columns)
112:         lower_names = {col: str(col).lower() for col in columns}
113:         used: set[str] = set()
114: 
115:         process_col = _pick_column(
116:             ctx.settings.get("process_column"),
117:             columns,
118:             role_by_name,
119:             {"process", "activity", "event", "step", "task", "action"},
120:             ["process", "activity", "event", "step", "task", "action", "job"],
121:             lower_names,
122:             used,
123:         )
124:         if process_col:
125:             used.add(process_col)
126: 
127:         timestamp_col = _pick_column(
128:             ctx.settings.get("timestamp_column"),
129:             columns,
130:             role_by_name,
131:             {"timestamp"},
132:             ["timestamp", "time", "date"],
133:             lower_names,
134:             used,
135:         )
136:         if timestamp_col:
137:             used.add(timestamp_col)
138: 
139:         start_col = _pick_column(
140:             ctx.settings.get("start_column"),
141:             columns,
142:             role_by_name,
143:             {"start"},
144:             ["start", "begin", "queued", "enqueue"],
145:             lower_names,
146:             used,
147:         )
148:         if start_col:
149:             used.add(start_col)
150: 
151:         end_col = _pick_column(
152:             ctx.settings.get("end_column"),
153:             columns,
154:             role_by_name,
155:             {"end", "finish", "complete"},
156:             ["end", "finish", "complete", "stop", "dequeue"],
157:             lower_names,
158:             used,
159:         )
160:         if end_col:
161:             used.add(end_col)
162: 
163:         duration_col = _pick_column(
164:             ctx.settings.get("duration_column"),
165:             columns,
166:             role_by_name,
167:             {"duration", "latency", "elapsed", "runtime"},
168:             ["duration", "elapsed", "latency", "runtime", "seconds", "secs", "ms"],
169:             lower_names,
170:             used,
171:         )
172:         if duration_col:
173:             used.add(duration_col)
174: 
175:         server_col = _pick_column(
176:             ctx.settings.get("server_column"),
177:             columns,
178:             role_by_name,
179:             {"server", "host", "node", "instance"},
180:             ["server", "host", "node", "instance", "machine"],
181:             lower_names,
182:             used,
183:         )
184:         if server_col:
185:             used.add(server_col)
186: 
187:         user_col = _pick_column(
188:             ctx.settings.get("user_column"),
189:             columns,
190:             role_by_name,
191:             {"user", "owner", "actor"},
192:             ["user", "owner", "actor"],
193:             lower_names,
194:             used,
195:         )
196:         if user_col:
197:             used.add(user_col)
198: 
199:         param_col = _pick_column(
200:             ctx.settings.get("param_column"),
201:             columns,
202:             role_by_name,
203:             {"parameter", "params", "config", "meta"},
204:             ["param", "parameter", "params", "config", "meta"],
205:             lower_names,
206:             used,
207:         )
208: 
209:         if not process_col:
210:             return PluginResult(
211:                 "skipped", "No process/activity column detected", {}, [], [], None
212:             )
213: 
214:         base_timestamp_col = timestamp_col or start_col or end_col
215:         if not base_timestamp_col:
216:             return PluginResult(
217:                 "skipped", "No timestamp column detected", {}, [], [], None
218:             )
219: 
220:         work = df.copy()
221:         selected_cols: list[str] = []
222:         for col in [
223:             process_col,
224:             base_timestamp_col,
225:             duration_col,
226:             start_col,
227:             end_col,
228:             server_col,
229:             user_col,
230:             param_col,
231:         ]:
232:             if col and col in work.columns and col not in selected_cols:
233:                 selected_cols.append(col)
234:         work = work.loc[:, selected_cols]
235: 
236:         work["__timestamp"] = pd.to_datetime(
237:             work[base_timestamp_col], errors="coerce", utc=False
238:         )
239:         work = work.loc[work["__timestamp"].notna()].copy()
240:         if work.empty:
241:             return PluginResult(
242:                 "skipped", "No valid timestamps found", {}, [], [], None
243:             )
244: 
245:         duration_label = duration_col
246:         if duration_col and duration_col in work.columns:
247:             duration = pd.to_numeric(work[duration_col], errors="coerce")
248:             if duration.isna().all():
249:                 duration = pd.to_timedelta(work[duration_col], errors="coerce").dt.total_seconds()
250:         elif start_col and end_col and start_col in work.columns and end_col in work.columns:
251:             start_ts = pd.to_datetime(work[start_col], errors="coerce", utc=False)
252:             end_ts = pd.to_datetime(work[end_col], errors="coerce", utc=False)
253:             duration = (end_ts - start_ts).dt.total_seconds()
254:             duration_label = f"{start_col}->{end_col}"
255:         else:
256:             return PluginResult(
257:                 "skipped",
258:                 "No duration data available",
259:                 {},
260:                 [],
261:                 [],
262:                 None,
263:             )
264: 
265:         work["__duration"] = duration
266:         work = work.loc[work["__duration"].notna() & (work["__duration"] > 0)].copy()
267:         if work.empty:
268:             return PluginResult(
269:                 "skipped", "No valid durations found", {}, [], [], None
270:             )
271: 
272:         work["__process"] = work[process_col].astype(str).str.strip()
273:         work["__process_norm"] = work["__process"].str.lower()
274:         invalid = {"", "nan", "none", "null"}
275:         work = work.loc[~work["__process_norm"].isin(invalid)].copy()
276:         if work.empty:
277:             return PluginResult(
278:                 "skipped", "No valid process values", {}, [], [], None
279:             )
280: 
281:         close_start = int(ctx.settings.get("close_cycle_start_day", 20))
282:         close_end = int(ctx.settings.get("close_cycle_end_day", 5))
283:         min_close_count = int(ctx.settings.get("min_close_count", 20))
284:         min_open_count = int(ctx.settings.get("min_open_count", 10))
285:         slowdown_ratio_threshold = float(ctx.settings.get("slowdown_ratio_threshold", 2.0))
286:         correlation_threshold = float(ctx.settings.get("correlation_threshold", 0.7))
287:         max_unique_ratio = float(ctx.settings.get("max_unique_ratio", 0.05))
288:         min_days = int(ctx.settings.get("min_days", 5))
289:         max_examples = int(ctx.settings.get("max_examples", 25))
290:         min_recommendation_pct = float(ctx.settings.get("min_recommendation_pct", 0.1))
291: 
292:         work["__day"] = work["__timestamp"].dt.day
293:         work["__date"] = work["__timestamp"].dt.date
294:         if close_start <= close_end:
295:             work["__close"] = (work["__day"] >= close_start) & (work["__day"] <= close_end)
296:         else:
297:             work["__close"] = (work["__day"] >= close_start) | (work["__day"] <= close_end)
298: 
299:         counts = (
300:             work.groupby(["__process_norm", "__close"]).size().unstack(fill_value=0)
301:         )
302: 
303:         findings = []
304:         candidate_stats = []
305: 
306:         process_labels = (
307:             work.groupby("__process_norm")["__process"]
308:             .agg(lambda series: series.value_counts().index[0])
309:             .to_dict()
310:         )
311: 
312:         for process_norm, row in counts.iterrows():
313:             close_count = int(row.get(True, 0))
314:             open_count = int(row.get(False, 0))
315:             if close_count < min_close_count or open_count < min_open_count:
316:                 continue
317: 
318:             close_days = work.loc[
319:                 (work["__process_norm"] == process_norm) & work["__close"], "__date"
320:             ].nunique()
321:             if close_days < min_days:
322:                 continue
323: 
324:             close_other = work.loc[
325:                 work["__close"] & (work["__process_norm"] != process_norm),
326:                 "__duration",
327:             ]
328:             open_other = work.loc[
329:                 (~work["__close"]) & (work["__process_norm"] != process_norm),
330:                 "__duration",
331:             ]
332:             if close_other.empty or open_other.empty:
333:                 continue
334: 
335:             median_close = float(close_other.median())
336:             median_open = float(open_other.median())
337:             if median_open <= 0 or median_close <= 0:
338:                 continue
339:             slowdown_ratio = median_close / median_open
340:             if slowdown_ratio < slowdown_ratio_threshold:
341:                 continue
342: 
343:             improvement_pct = max(0.0, (median_close - median_open) / median_close)
344:             if improvement_pct < min_recommendation_pct:
345:                 continue
346: 
347:             close_days_df = work.loc[work["__close"]]
348:             daily_counts = (
349:                 close_days_df.groupby("__date")["__process_norm"]
350:                 .apply(lambda series: int((series == process_norm).sum()))
351:             )
352:             daily_median = (
353:                 close_days_df.loc[close_days_df["__process_norm"] != process_norm]
354:                 .groupby("__date")["__duration"]
355:                 .median()
356:             )
357:             aligned = pd.concat(
358:                 [daily_counts.rename("count"), daily_median.rename("median")], axis=1
359:             ).dropna()
360:             if len(aligned) < min_days:
361:                 continue
362:             correlation = _spearman_corr(aligned["count"], aligned["median"])
363:             if pd.isna(correlation) or correlation < correlation_threshold:
364:                 continue
365: 
366:             param_unique_ratio = None
367:             if param_col and param_col in work.columns:
368:                 params = (
369:                     work.loc[work["__process_norm"] == process_norm, param_col]
370:                     .map(_normalize_param)
371:                     .dropna()
372:                 )
373:                 if not params.empty:
374:                     param_unique_ratio = float(params.nunique() / len(params))
375: 
376:             server_list: list[str] = []
377:             server_count = 0
378:             if server_col and server_col in work.columns:
379:                 servers = (
380:                     work.loc[work["__process_norm"] == process_norm, server_col]
381:                     .dropna()
382:                     .astype(str)
383:                     .str.strip()
384:                 )
385:                 if not servers.empty:
386:                     counter = Counter(servers)
387:                     server_list = [name for name, _ in counter.most_common(5)]
388:                     server_count = len(counter)
389: 
390:             row_ids = []
391:             for idx in work.loc[
392:                 (work["__process_norm"] == process_norm) & work["__close"]
393:             ].index.tolist():
394:                 try:
395:                     row_ids.append(int(idx))
396:                 except (TypeError, ValueError):
397:                     continue
398:             row_ids = row_ids[:max_examples]
399: 
400:             process_label = process_labels.get(process_norm, process_norm)
401:             columns = [process_col, base_timestamp_col]
402:             if duration_col and duration_col in work.columns:
403:                 columns.append(duration_col)
404:             elif start_col and end_col:
405:                 columns.extend([start_col, end_col])
406:             if server_col:
407:                 columns.append(server_col)
408:             if user_col:
409:                 columns.append(user_col)
410:             if param_col:
411:                 columns.append(param_col)
412: 
413:             finding = {
414:                 "kind": "close_cycle_contention",
415:                 "process": process_label,
416:                 "process_norm": process_norm,
417:                 "close_count": close_count,
418:                 "open_count": open_count,
419:                 "close_cycle_days": int(close_days),
420:                 "slowdown_ratio": float(slowdown_ratio),
421:                 "correlation": float(correlation),
422:                 "median_duration_close": float(median_close),
423:                 "median_duration_open": float(median_open),
424:                 "estimated_improvement_pct": float(improvement_pct),
425:                 "server_count": int(server_count),
426:                 "servers": server_list,
427:                 "param_unique_ratio": param_unique_ratio,
428:                 "columns": columns,
429:                 "row_ids": row_ids,
430:                 "query": f"process={process_label}",
431:             }
432:             findings.append(finding)
433:             candidate_stats.append(
434:                 {
435:                     "process": process_label,
436:                     "process_norm": process_norm,
437:                     "close_count": close_count,
438:                     "open_count": open_count,
439:                     "close_cycle_days": int(close_days),
440:                     "slowdown_ratio": float(slowdown_ratio),
441:                     "correlation": float(correlation),
442:                     "estimated_improvement_pct": float(improvement_pct),
443:                     "param_unique_ratio": param_unique_ratio,
444:                     "server_count": int(server_count),
445:                     "servers": server_list,
446:                 }
447:             )
448: 
449:         artifacts = []
450:         artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_contention")
451:         out_path = artifacts_dir / "results.json"
452:         write_json(
453:             out_path,
454:             {
455:                 "summary": {
456:                     "process_column": process_col,
457:                     "timestamp_column": base_timestamp_col,
458:                     "duration_column": duration_label,
459:                     "server_column": server_col,
460:                     "param_column": param_col,
461:                     "close_cycle_start_day": close_start,
462:                     "close_cycle_end_day": close_end,
463:                 },
464:                 "candidates": candidate_stats,
465:             },
466:         )
467:         artifacts.append(
468:             PluginArtifact(
469:                 path=str(out_path.relative_to(ctx.run_dir)),
470:                 type="json",
471:                 description="Close cycle contention candidates",
472:             )
473:         )
474: 
475:         if not findings:
476:             return PluginResult(
477:                 "ok",
478:                 "No close-cycle contention candidates found",
479:                 {
480:                     "candidates": 0,
481:                     "process_column": process_col,
482:                     "timestamp_column": base_timestamp_col,
483:                     "duration_column": duration_label,
484:                 },
485:                 [],
486:                 artifacts,
487:                 None,
488:             )
489: 
490:         return PluginResult(
491:             "ok",
492:             "Detected close-cycle contention candidates",
493:             {
494:                 "candidates": len(findings),
495:                 "process_column": process_col,
496:                 "timestamp_column": base_timestamp_col,
497:                 "duration_column": duration_label,
498:                 "server_column": server_col,
499:                 "param_column": param_col,
500:             },
501:             findings,
502:             artifacts,
503:             None,
504:         )
````

## File: plugins/analysis_close_cycle_revenue_compression/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "title": "Close Cycle Revenue Compression Config",
 4:   "type": "object",
 5:   "additionalProperties": false,
 6:   "properties": {
 7:     "process_column": { "type": ["string", "null"] },
 8:     "start_column": { "type": ["string", "null"] },
 9:     "end_column": { "type": ["string", "null"] },
10:     "duration_column": { "type": ["string", "null"] },
11:     "queue_column": { "type": ["string", "null"] },
12:     "eligible_column": { "type": ["string", "null"] },
13:     "host_column": { "type": ["string", "null"] },
14:     "revenue_selector_strategy": { "type": "string", "enum": ["auto", "value_hints", "composite"] },
15:     "revenue_process_names": { "type": ["array", "string", "null"] },
16:     "revenue_process_patterns": { "type": ["array", "string", "null"] },
17:     "selector_min_coverage": { "type": "number", "minimum": 0, "maximum": 1 },
18:     "selector_min_cardinality": { "type": "integer", "minimum": 1 },
19:     "selector_max_cardinality": { "type": "integer", "minimum": 1 },
20:     "selector_top_columns": { "type": "integer", "minimum": 1 },
21:     "selector_max_composite": { "type": "integer", "minimum": 1 },
22:     "selector_min_close_rows": { "type": "integer", "minimum": 1 },
23:     "selector_min_close_share": { "type": "number", "minimum": 0, "maximum": 1 },
24:     "selector_lift_threshold": { "type": "number", "minimum": 0 },
25:     "selector_late_threshold": { "type": "number", "minimum": 0, "maximum": 1 },
26:     "selector_min_late_fraction": { "type": "number", "minimum": 0, "maximum": 1 },
27:     "selector_top_keys": { "type": "integer", "minimum": 1 },
28:     "close_window_mode": { "type": "string", "enum": ["infer", "override", "infer_or_default", "calendar"] },
29:     "close_cycle_start_day": { "type": "integer", "minimum": 1, "maximum": 31 },
30:     "close_cycle_end_day": { "type": "integer", "minimum": 1, "maximum": 31 },
31:     "min_close_days": { "type": "integer", "minimum": 1 },
32:     "max_close_days": { "type": "integer", "minimum": 1 },
33:     "lookahead_days": { "type": "integer", "minimum": 0 },
34:     "min_close_confidence": { "type": "number", "minimum": 0, "maximum": 1 },
35:     "min_close_data_ratio": { "type": "number", "minimum": 0, "maximum": 1 },
36:     "target_days": { "type": "number", "minimum": 0 },
37:     "max_scale": { "type": "number", "minimum": 1 },
38:     "min_month_rows": { "type": "integer", "minimum": 1 },
39:     "max_months_output": { "type": "integer", "minimum": 1 },
40:     "max_wait_days": { "type": "number", "minimum": 0 }
41:   }
42: }
````

## File: plugins/analysis_close_cycle_revenue_compression/plugin.py
````python
   1: from __future__ import annotations
   2: 
   3: from datetime import date, timedelta
   4: from typing import Any, Iterable
   5: import math
   6: import re
   7: 
   8: import numpy as np
   9: import pandas as pd
  10: 
  11: from statistic_harness.core.types import PluginArtifact, PluginResult
  12: from statistic_harness.core.utils import write_json
  13: 
  14: 
  15: INVALID_STRINGS = {"", "nan", "none", "null"}
  16: 
  17: 
  18: def _normalize_text(value: Any) -> str:
  19:     if value is None:
  20:         return ""
  21:     if isinstance(value, float) and pd.isna(value):
  22:         return ""
  23:     return str(value).strip()
  24: 
  25: 
  26: def _normalize_list(value: Any) -> list[str]:
  27:     if value is None:
  28:         return []
  29:     if isinstance(value, str):
  30:         return [item.strip() for item in value.split(",") if item.strip()]
  31:     if isinstance(value, (list, tuple, set)):
  32:         return [str(item).strip() for item in value if str(item).strip()]
  33:     return [str(value).strip()]
  34: 
  35: 
  36: def _pick_column(
  37:     preferred: str | None,
  38:     columns: list[str],
  39:     role_by_name: dict[str, str],
  40:     roles: set[str],
  41:     patterns: list[str],
  42:     lower_names: dict[str, str],
  43:     exclude: set[str],
  44: ) -> str | None:
  45:     if preferred and preferred in columns:
  46:         return preferred
  47:     for col in columns:
  48:         if col in exclude:
  49:             continue
  50:         if role_by_name.get(col) in roles:
  51:             return col
  52:     for col in columns:
  53:         if col in exclude:
  54:             continue
  55:         name = lower_names[col]
  56:         if any(pattern in name for pattern in patterns):
  57:             return col
  58:     return None
  59: 
  60: 
  61: def _candidate_columns(
  62:     columns: list[str],
  63:     role_by_name: dict[str, str],
  64:     roles: set[str],
  65:     patterns: list[str],
  66:     lower_names: dict[str, str],
  67:     exclude: set[str],
  68: ) -> list[str]:
  69:     candidates: list[str] = []
  70:     for col in columns:
  71:         if col in exclude:
  72:             continue
  73:         if role_by_name.get(col) in roles:
  74:             candidates.append(col)
  75:     for col in columns:
  76:         if col in exclude or col in candidates:
  77:             continue
  78:         name = lower_names[col]
  79:         if any(pattern in name for pattern in patterns):
  80:             candidates.append(col)
  81:     return candidates
  82: 
  83: 
  84: def _score_process_column(name: str, series: pd.Series) -> float:
  85:     score = 0.0
  86:     lower_name = name.lower()
  87:     if lower_name in {"process", "process_id"}:
  88:         score += 3.0
  89:     if lower_name.endswith("_id") or lower_name.endswith("id"):
  90:         score += 1.5
  91:     for token in (
  92:         "queue",
  93:         "status",
  94:         "step",
  95:         "parent",
  96:         "child",
  97:         "hold",
  98:         "lock",
  99:         "schedule",
 100:         "master",
 101:         "dep",
 102:         "ext",
 103:         "attempt",
 104:         "priority",
 105:     ):
 106:         if token in lower_name:
 107:             score -= 2.0
 108: 
 109:     sample = series.dropna()
 110:     if sample.empty:
 111:         return score - 5.0
 112:     if sample.shape[0] > 5000:
 113:         sample = sample.sample(5000, random_state=0)
 114: 
 115:     if pd.api.types.is_numeric_dtype(sample):
 116:         score -= 1.5
 117:     else:
 118:         score += 1.5
 119: 
 120:     sample_str = sample.astype(str).str.strip()
 121:     if not pd.api.types.is_numeric_dtype(sample):
 122:         numeric_like = sample_str.str.match(r"^\d+(\.\d+)?$").mean()
 123:         if numeric_like > 0.8:
 124:             score -= 2.0
 125: 
 126:     unique_ratio = sample.nunique(dropna=True) / max(1, sample.shape[0])
 127:     score += (1.0 - unique_ratio) * 4.0
 128:     if unique_ratio > 0.9:
 129:         score -= 2.0
 130: 
 131:     lengths = sample_str.str.len()
 132:     median_len = float(lengths.median()) if not lengths.empty else 0.0
 133:     if 3 <= median_len <= 20:
 134:         score += 0.5
 135:     elif median_len > 40:
 136:         score -= 0.5
 137: 
 138:     return score
 139: 
 140: 
 141: def _choose_best_process_column(
 142:     candidates: Iterable[str], df: pd.DataFrame
 143: ) -> str | None:
 144:     candidates = list(candidates)
 145:     if not candidates:
 146:         return None
 147:     if len(candidates) == 1:
 148:         return candidates[0]
 149:     scored = []
 150:     for col in candidates:
 151:         scored.append((_score_process_column(str(col), df[col]), col))
 152:     scored.sort(reverse=True, key=lambda item: item[0])
 153:     return scored[0][1]
 154: 
 155: 
 156: def _score_datetime_column(name: str, series: pd.Series, tokens: Iterable[str]) -> float:
 157:     score = 0.0
 158:     lower_name = name.lower()
 159:     if any(token in lower_name for token in tokens):
 160:         score += 2.0
 161: 
 162:     sample = series.dropna()
 163:     if sample.empty:
 164:         return score - 5.0
 165:     if sample.shape[0] > 2000:
 166:         sample = sample.sample(2000, random_state=0)
 167: 
 168:     if pd.api.types.is_numeric_dtype(sample):
 169:         max_val = float(sample.max()) if not sample.empty else 0.0
 170:         if max_val < 1e8:
 171:             score -= 5.0
 172: 
 173:     parsed = pd.to_datetime(sample, errors="coerce", utc=False)
 174:     success = float(parsed.notna().mean())
 175:     score += success * 5.0
 176:     return score
 177: 
 178: 
 179: def _choose_best_datetime_column(
 180:     candidates: Iterable[str], df: pd.DataFrame, tokens: Iterable[str]
 181: ) -> str | None:
 182:     candidates = list(candidates)
 183:     if not candidates:
 184:         return None
 185:     if len(candidates) == 1:
 186:         return candidates[0]
 187:     scored = []
 188:     for col in candidates:
 189:         scored.append((_score_datetime_column(str(col), df[col], tokens), col))
 190:     scored.sort(reverse=True, key=lambda item: item[0])
 191:     best_score, best_col = scored[0]
 192:     if best_score <= 0:
 193:         return None
 194:     return best_col
 195: 
 196: 
 197: def _series_is_datetime(series: pd.Series, min_success: float = 0.8) -> bool:
 198:     sample = series.dropna()
 199:     if sample.empty:
 200:         return False
 201:     if sample.shape[0] > 2000:
 202:         sample = sample.sample(2000, random_state=0)
 203:     sample_str = sample.astype(str)
 204:     digit_ratio = float(sample_str.str.contains(r"\d").mean())
 205:     if digit_ratio < 0.5:
 206:         return False
 207:     parsed = pd.to_datetime(sample, errors="coerce", utc=False)
 208:     return float(parsed.notna().mean()) >= min_success
 209: 
 210: 
 211: def _score_selector_column(series: pd.Series) -> float:
 212:     sample = series.dropna()
 213:     if sample.empty:
 214:         return -10.0
 215:     if sample.shape[0] > 5000:
 216:         sample = sample.sample(5000, random_state=0)
 217:     non_null_ratio = float(series.notna().mean())
 218:     nunique = int(sample.nunique(dropna=True))
 219:     if nunique <= 0:
 220:         return -10.0
 221:     sample_str = sample.astype(str).str.strip()
 222:     code_like = float(sample_str.str.match(r"^[A-Za-z0-9_-]{2,12}$").mean())
 223:     cardinality_score = 1.0 / (1.0 + math.log1p(nunique))
 224:     return non_null_ratio * 0.5 + cardinality_score * 0.4 + code_like * 0.3
 225: 
 226: 
 227: def _build_close_position(close_dates: set[date]) -> dict[date, float]:
 228:     close_by_month: dict[str, list[date]] = {}
 229:     for day in close_dates:
 230:         close_by_month.setdefault(day.strftime("%Y-%m"), []).append(day)
 231:     position: dict[date, float] = {}
 232:     for month, days in close_by_month.items():
 233:         days_sorted = sorted(days)
 234:         if len(days_sorted) == 1:
 235:             position[days_sorted[0]] = 1.0
 236:             continue
 237:         denom = float(len(days_sorted) - 1)
 238:         for idx, day in enumerate(days_sorted):
 239:             position[day] = idx / denom
 240:     return position
 241: 
 242: 
 243: def _select_composite_revenue(
 244:     work: pd.DataFrame,
 245:     close_dates: set[date],
 246:     exclude: set[str],
 247:     settings: dict[str, Any],
 248: ) -> tuple[pd.Series, dict[str, Any], pd.Series | None]:
 249:     selector_min_coverage = float(settings.get("selector_min_coverage", 0.2))
 250:     selector_min_cardinality = int(settings.get("selector_min_cardinality", 2))
 251:     selector_max_cardinality = int(settings.get("selector_max_cardinality", 200))
 252:     selector_top_columns = int(settings.get("selector_top_columns", 3))
 253:     selector_max_composite = int(settings.get("selector_max_composite", 5000))
 254:     selector_min_close_rows = int(settings.get("selector_min_close_rows", 100))
 255:     selector_lift_threshold = float(settings.get("selector_lift_threshold", 1.2))
 256:     selector_late_threshold = float(settings.get("selector_late_threshold", 0.7))
 257:     selector_min_late_fraction = float(settings.get("selector_min_late_fraction", 0.2))
 258:     selector_min_close_share = float(settings.get("selector_min_close_share", 0.8))
 259:     selector_top_keys = int(settings.get("selector_top_keys", 10))
 260: 
 261:     candidates: list[tuple[str, float]] = []
 262:     for col in work.columns:
 263:         if col in exclude:
 264:             continue
 265:         series = work[col]
 266:         if series.isna().all():
 267:             continue
 268:         if _series_is_datetime(series):
 269:             continue
 270:         non_null_ratio = float(series.notna().mean())
 271:         if non_null_ratio < selector_min_coverage:
 272:             continue
 273:         nunique = int(series.nunique(dropna=True))
 274:         if nunique < selector_min_cardinality or nunique > selector_max_cardinality:
 275:             continue
 276:         if pd.api.types.is_numeric_dtype(series):
 277:             sample = series.dropna()
 278:             if sample.shape[0] > 2000:
 279:                 sample = sample.sample(2000, random_state=0)
 280:             if (sample % 1 != 0).any():
 281:                 continue
 282:         candidates.append((col, _score_selector_column(series)))
 283: 
 284:     relaxed = False
 285:     if not candidates:
 286:         relaxed = True
 287:         relaxed_max_cardinality = selector_max_cardinality * 5
 288:         for col in work.columns:
 289:             if col in exclude:
 290:                 continue
 291:             if str(col).startswith("__"):
 292:                 continue
 293:             series = work[col]
 294:             if series.isna().all():
 295:                 continue
 296:             non_null_ratio = float(series.notna().mean())
 297:             if non_null_ratio <= 0:
 298:                 continue
 299:             nunique = int(series.nunique(dropna=True))
 300:             if nunique < selector_min_cardinality or nunique > relaxed_max_cardinality:
 301:                 continue
 302:             candidates.append((col, _score_selector_column(series)))
 303: 
 304:     candidates.sort(key=lambda item: item[1], reverse=True)
 305:     selected_cols = [col for col, _score in candidates[:selector_top_columns]]
 306: 
 307:     selector_info: dict[str, Any] = {
 308:         "strategy": "composite",
 309:         "selector_columns": selected_cols,
 310:         "candidate_columns": [
 311:             {"column": col, "score": round(score, 4)} for col, score in candidates[:20]
 312:         ],
 313:         "relaxed": relaxed,
 314:     }
 315: 
 316:     if not selected_cols:
 317:         return work["__start_ts"].isna(), selector_info, None
 318: 
 319:     def _make_key(cols: list[str]) -> pd.Series:
 320:         normalized = work[cols].copy()
 321:         for col in cols:
 322:             normalized[col] = normalized[col].map(_normalize_text)
 323:         key = normalized.agg("|".join, axis=1)
 324:         empty_mask = key.str.replace("|", "", regex=False).str.strip().eq("")
 325:         key = key.mask(empty_mask)
 326:         return key
 327: 
 328:     key_series = _make_key(selected_cols)
 329:     while key_series.nunique(dropna=True) > selector_max_composite and len(selected_cols) > 1:
 330:         selected_cols = selected_cols[:-1]
 331:         key_series = _make_key(selected_cols)
 332: 
 333:     selector_info["selector_columns"] = selected_cols
 334:     selector_info["selector_key_count"] = int(key_series.nunique(dropna=True))
 335: 
 336:     close_mask = work["__start_ts"].dt.date.isin(close_dates)
 337:     if not close_mask.any():
 338:         return work["__start_ts"].isna(), selector_info, key_series
 339: 
 340:     overall_close_ratio = float(close_mask.mean())
 341:     position = _build_close_position(close_dates)
 342:     close_pos = work["__start_ts"].dt.date.map(position).fillna(0.0)
 343:     late_mask = close_mask & (close_pos >= selector_late_threshold)
 344: 
 345:     total_counts = key_series.value_counts()
 346:     close_counts = key_series[close_mask].value_counts()
 347:     late_counts = key_series[late_mask].value_counts()
 348: 
 349:     key_rows = []
 350:     for key, close_cnt in close_counts.items():
 351:         total_cnt = int(total_counts.get(key, 0))
 352:         if total_cnt == 0:
 353:             continue
 354:         close_share = close_cnt / total_cnt
 355:         lift = close_share / overall_close_ratio if overall_close_ratio > 0 else 0.0
 356:         late_frac = (
 357:             float(late_counts.get(key, 0)) / float(close_cnt)
 358:             if close_cnt > 0
 359:             else 0.0
 360:         )
 361:         score = math.log1p(close_cnt) * lift * (1.0 + late_frac)
 362:         key_rows.append(
 363:             {
 364:                 "key": key,
 365:                 "close_rows": int(close_cnt),
 366:                 "total_rows": total_cnt,
 367:                 "close_share": float(close_share),
 368:                 "close_lift": float(lift),
 369:                 "late_fraction": float(late_frac),
 370:                 "score": float(score),
 371:             }
 372:         )
 373: 
 374:     key_rows.sort(key=lambda item: item["score"], reverse=True)
 375:     selector_info["selector_keys"] = key_rows[:selector_top_keys]
 376: 
 377:     selected_keys = [
 378:         row["key"]
 379:         for row in key_rows
 380:         if row["close_rows"] >= selector_min_close_rows
 381:         and row["close_share"] >= selector_min_close_share
 382:         and row["close_lift"] >= selector_lift_threshold
 383:         and row["late_fraction"] >= selector_min_late_fraction
 384:     ]
 385: 
 386:     selector_info["selector_thresholds"] = {
 387:         "min_close_rows": selector_min_close_rows,
 388:         "min_close_share": selector_min_close_share,
 389:         "min_close_lift": selector_lift_threshold,
 390:         "min_late_fraction": selector_min_late_fraction,
 391:         "late_threshold": selector_late_threshold,
 392:         "max_composite": selector_max_composite,
 393:     }
 394: 
 395:     if not selected_keys:
 396:         return work["__start_ts"].isna(), selector_info, key_series
 397: 
 398:     mask = key_series.isin(selected_keys)
 399:     return mask, selector_info, key_series
 400: 
 401: 
 402: def _month_key(day: date) -> str:
 403:     return f"{day.year:04d}-{day.month:02d}"
 404: 
 405: 
 406: def _build_calendar_days(start_day: date, end_day: date) -> list[date]:
 407:     days: list[date] = []
 408:     cursor = start_day
 409:     while cursor <= end_day:
 410:         days.append(cursor)
 411:         cursor = cursor + timedelta(days=1)
 412:     return days
 413: 
 414: 
 415: def _calendar_close_window(
 416:     daily: pd.DataFrame, close_start_day: int, close_end_day: int, mode: str
 417: ) -> tuple[set[date], list[dict[str, Any]], set[str]]:
 418:     close_dates: set[date] = set()
 419:     for day in daily["date"].tolist():
 420:         if close_start_day <= close_end_day:
 421:             is_close = close_start_day <= day.day <= close_end_day
 422:         else:
 423:             is_close = day.day >= close_start_day or day.day <= close_end_day
 424:         if is_close:
 425:             close_dates.add(day)
 426:     close_windows = [
 427:         {
 428:             "mode": mode,
 429:             "start_day": close_start_day,
 430:             "end_day": close_end_day,
 431:         }
 432:     ]
 433:     confident_months = {day.strftime("%Y-%m") for day in close_dates}
 434:     return close_dates, close_windows, confident_months
 435: 
 436: 
 437: def _infer_close_windows(
 438:     daily: pd.DataFrame,
 439:     min_days: int,
 440:     max_days: int,
 441:     lookahead_days: int,
 442:     min_confidence: float,
 443:     min_data_ratio: float,
 444: ) -> tuple[set[date], list[dict[str, Any]], set[str]]:
 445:     if daily.empty:
 446:         return set(), [], set()
 447: 
 448:     daily = daily.copy()
 449:     daily["month"] = daily["date"].apply(_month_key)
 450:     daily_map = {
 451:         row["date"]: {
 452:             "count": int(row["count"]),
 453:             "median": float(row["median_ttc"]) if row["median_ttc"] is not None else None,
 454:         }
 455:         for row in daily.to_dict("records")
 456:     }
 457: 
 458:     months = sorted(daily["month"].unique())
 459:     close_windows: list[dict[str, Any]] = []
 460:     close_dates: dict[date, str] = {}
 461:     confident_months: set[str] = set()
 462: 
 463:     for month in months:
 464:         year, month_num = [int(part) for part in month.split("-")]
 465:         month_start = date(year, month_num, 1)
 466:         if month_num == 12:
 467:             next_month_start = date(year + 1, 1, 1)
 468:         else:
 469:             next_month_start = date(year, month_num + 1, 1)
 470:         month_end = next_month_start - timedelta(days=1)
 471: 
 472:         month_days = _build_calendar_days(month_start, month_end)
 473:         next_days = _build_calendar_days(
 474:             next_month_start, next_month_start + timedelta(days=max(0, lookahead_days - 1))
 475:         )
 476:         extended_days = month_days + next_days
 477: 
 478:         counts = [daily_map.get(day, {}).get("count", 0) for day in month_days]
 479:         medians = [daily_map.get(day, {}).get("median") for day in month_days]
 480:         medians_clean = [val for val in medians if val is not None]
 481:         count_mean = float(np.mean(counts)) if counts else 0.0
 482:         count_std = float(np.std(counts)) if counts else 0.0
 483:         median_mean = float(np.mean(medians_clean)) if medians_clean else 0.0
 484:         median_std = float(np.std(medians_clean)) if medians_clean else 0.0
 485: 
 486:         def _z(val: float, mean: float, std: float) -> float:
 487:             if std <= 0:
 488:                 return 0.0
 489:             return (val - mean) / std
 490: 
 491:         pressures = []
 492:         for day in extended_days:
 493:             record = daily_map.get(day)
 494:             count_val = record.get("count", 0) if record else 0
 495:             median_val = record.get("median") if record else None
 496:             z_count = _z(float(count_val), count_mean, count_std)
 497:             z_median = 0.0
 498:             if median_val is not None:
 499:                 z_median = _z(float(median_val), median_mean, median_std)
 500:             pressures.append(z_count + z_median)
 501: 
 502:         best_score = None
 503:         best_window = None
 504:         second_score = None
 505: 
 506:         for length in range(min_days, max_days + 1):
 507:             if length <= 0 or length > len(extended_days):
 508:                 continue
 509:             for start_idx in range(len(month_days)):
 510:                 end_idx = start_idx + length
 511:                 if end_idx > len(extended_days):
 512:                     continue
 513:                 score = float(sum(pressures[start_idx:end_idx]))
 514:                 if best_score is None or score > best_score:
 515:                     second_score = best_score
 516:                     best_score = score
 517:                     best_window = (start_idx, end_idx)
 518:                 elif second_score is None or score > second_score:
 519:                     second_score = score
 520: 
 521:         confidence = 0.0
 522:         if best_score is not None and best_score > 0:
 523:             if second_score is None:
 524:                 confidence = 1.0
 525:             else:
 526:                 confidence = (best_score - second_score) / abs(best_score)
 527: 
 528:         if best_window is None or confidence < min_confidence:
 529:             close_windows.append(
 530:                 {
 531:                     "month": month,
 532:                     "start": None,
 533:                     "end": None,
 534:                     "length": 0,
 535:                     "confidence": float(confidence),
 536:                     "mode": "infer",
 537:                 }
 538:             )
 539:             continue
 540: 
 541:         start_idx, end_idx = best_window
 542:         window_days = extended_days[start_idx:end_idx]
 543:         data_days = [day for day in window_days if daily_map.get(day, {}).get("count", 0) > 0]
 544:         data_ratio = len(data_days) / max(1, len(window_days))
 545:         if data_ratio < min_data_ratio:
 546:             close_windows.append(
 547:                 {
 548:                     "month": month,
 549:                     "start": None,
 550:                     "end": None,
 551:                     "length": len(window_days),
 552:                     "confidence": float(confidence),
 553:                     "mode": "infer",
 554:                     "reason": "insufficient_data_days",
 555:                 }
 556:             )
 557:             continue
 558: 
 559:         start_day = window_days[0]
 560:         end_day = window_days[-1]
 561:         close_windows.append(
 562:             {
 563:                 "month": month,
 564:                 "start": start_day.isoformat(),
 565:                 "end": end_day.isoformat(),
 566:                 "length": len(window_days),
 567:                 "confidence": float(confidence),
 568:                 "mode": "infer",
 569:             }
 570:         )
 571:         confident_months.add(month)
 572: 
 573:         for day in window_days:
 574:             close_dates.setdefault(day, month)
 575: 
 576:     close_date_set = set(close_dates.keys())
 577:     return close_date_set, close_windows, confident_months
 578: 
 579: 
 580: def _match_revenue(process_values: pd.Series, names: list[str], patterns: list[str]) -> pd.Series:
 581:     if process_values.empty:
 582:         return pd.Series([], dtype=bool)
 583:     series = process_values.fillna("").astype(str)
 584:     normalized = series.str.strip().str.lower()
 585: 
 586:     if names:
 587:         name_set = {name.lower() for name in names}
 588:         return normalized.isin(name_set)
 589: 
 590:     if not patterns:
 591:         return pd.Series([False] * len(series), index=series.index)
 592: 
 593:     mask = pd.Series([False] * len(series), index=series.index)
 594:     for raw in patterns:
 595:         if not raw:
 596:             continue
 597:         pattern = raw.strip()
 598:         if pattern.lower().startswith("re:"):
 599:             regex = pattern[3:]
 600:             try:
 601:                 mask = mask | normalized.str.contains(regex, regex=True, na=False)
 602:             except re.error:
 603:                 continue
 604:         else:
 605:             mask = mask | normalized.str.contains(re.escape(pattern.lower()), regex=True, na=False)
 606:     return mask
 607: 
 608: 
 609: def _modeled_span(end_sec: np.ndarray, wait_sec: np.ndarray, scale: float) -> float:
 610:     if scale <= 0:
 611:         return float("inf")
 612:     adj_end = end_sec - wait_sec * (1.0 - 1.0 / scale)
 613:     return float(adj_end.max())
 614: 
 615: 
 616: def _required_scale(
 617:     end_sec: np.ndarray,
 618:     wait_sec: np.ndarray,
 619:     target_sec: float,
 620:     max_scale: float,
 621: ) -> tuple[float | None, float, bool]:
 622:     baseline_span = _modeled_span(end_sec, wait_sec, 1.0)
 623:     if baseline_span <= target_sec:
 624:         return 1.0, baseline_span, True
 625: 
 626:     high = 1.0
 627:     while high < max_scale and _modeled_span(end_sec, wait_sec, high) > target_sec:
 628:         high *= 2.0
 629:     if high > max_scale:
 630:         high = max_scale
 631: 
 632:     if _modeled_span(end_sec, wait_sec, high) > target_sec:
 633:         return None, baseline_span, False
 634: 
 635:     low = 1.0
 636:     for _ in range(40):
 637:         mid = (low + high) / 2.0
 638:         if _modeled_span(end_sec, wait_sec, mid) <= target_sec:
 639:             high = mid
 640:         else:
 641:             low = mid
 642:     return high, baseline_span, True
 643: 
 644: 
 645: class Plugin:
 646:     def run(self, ctx) -> PluginResult:
 647:         df = ctx.dataset_loader()
 648:         if df.empty:
 649:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 650: 
 651:         columns_meta = []
 652:         role_by_name: dict[str, str] = {}
 653:         if ctx.dataset_version_id:
 654:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 655:             if dataset_template and dataset_template.get("status") == "ready":
 656:                 fields = ctx.storage.fetch_template_fields(
 657:                     int(dataset_template["template_id"])
 658:                 )
 659:                 columns_meta = fields
 660:                 role_by_name = {
 661:                     field["name"]: (field.get("role") or "") for field in fields
 662:                 }
 663:             else:
 664:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 665:                 role_by_name = {
 666:                     col["original_name"]: (col.get("role") or "")
 667:                     for col in columns_meta
 668:                 }
 669: 
 670:         columns = list(df.columns)
 671:         lower_names = {col: str(col).lower() for col in columns}
 672:         used: set[str] = set()
 673: 
 674:         process_candidates = _candidate_columns(
 675:             columns,
 676:             role_by_name,
 677:             {"process", "activity", "event", "step", "task", "action"},
 678:             ["process", "activity", "event", "step", "task", "action", "job"],
 679:             lower_names,
 680:             used,
 681:         )
 682:         process_col = ctx.settings.get("process_column")
 683:         if not process_col or process_col not in columns:
 684:             process_col = _choose_best_process_column(process_candidates, df)
 685:         if process_col:
 686:             used.add(process_col)
 687: 
 688:         start_candidates = _candidate_columns(
 689:             columns,
 690:             role_by_name,
 691:             {"start_time", "start"},
 692:             ["start", "begin"],
 693:             lower_names,
 694:             used,
 695:         )
 696:         start_col = ctx.settings.get("start_column")
 697:         if not start_col or start_col not in columns:
 698:             start_col = _choose_best_datetime_column(
 699:                 start_candidates, df, ("start", "begin")
 700:             )
 701:         if start_col:
 702:             used.add(start_col)
 703: 
 704:         end_candidates = _candidate_columns(
 705:             columns,
 706:             role_by_name,
 707:             {"end_time", "end", "finish", "complete"},
 708:             ["end", "finish", "complete", "stop"],
 709:             lower_names,
 710:             used,
 711:         )
 712:         end_col = ctx.settings.get("end_column")
 713:         if not end_col or end_col not in columns:
 714:             end_col = _choose_best_datetime_column(
 715:                 end_candidates, df, ("end", "finish", "complete", "stop")
 716:             )
 717:         if end_col:
 718:             used.add(end_col)
 719: 
 720:         duration_col = _pick_column(
 721:             ctx.settings.get("duration_column"),
 722:             columns,
 723:             role_by_name,
 724:             {"duration", "latency", "elapsed", "runtime"},
 725:             ["duration", "elapsed", "latency", "runtime", "seconds", "secs", "ms"],
 726:             lower_names,
 727:             used,
 728:         )
 729:         if duration_col:
 730:             used.add(duration_col)
 731: 
 732:         queue_col = _pick_column(
 733:             ctx.settings.get("queue_column"),
 734:             columns,
 735:             role_by_name,
 736:             {"queue", "queued", "enqueue"},
 737:             ["queue", "queued", "enqueue"],
 738:             lower_names,
 739:             used,
 740:         )
 741:         if queue_col:
 742:             used.add(queue_col)
 743: 
 744:         eligible_col = _pick_column(
 745:             ctx.settings.get("eligible_column"),
 746:             columns,
 747:             role_by_name,
 748:             {"eligible", "ready", "available"},
 749:             ["eligible", "ready", "available"],
 750:             lower_names,
 751:             used,
 752:         )
 753:         if eligible_col:
 754:             used.add(eligible_col)
 755: 
 756:         host_col = _pick_column(
 757:             ctx.settings.get("host_column"),
 758:             columns,
 759:             role_by_name,
 760:             {"server", "host", "node", "instance"},
 761:             ["server", "host", "node", "instance", "machine"],
 762:             lower_names,
 763:             used,
 764:         )
 765:         if host_col:
 766:             used.add(host_col)
 767: 
 768:         summary: dict[str, Any] = {
 769:             "process_column": process_col,
 770:             "start_column": start_col,
 771:             "end_column": end_col,
 772:             "duration_column": duration_col,
 773:             "queue_column": queue_col,
 774:             "eligible_column": eligible_col,
 775:             "host_column": host_col,
 776:         }
 777: 
 778:         def _emit_not_applicable(reason: str) -> PluginResult:
 779:             findings = [
 780:                 {
 781:                     "kind": "close_cycle_revenue_compression",
 782:                     "decision": "not_applicable",
 783:                     "measurement_type": "not_applicable",
 784:                     "reason": reason,
 785:                     "columns": [
 786:                         col
 787:                         for col in [
 788:                             process_col,
 789:                             start_col,
 790:                             end_col,
 791:                             duration_col,
 792:                             queue_col,
 793:                             eligible_col,
 794:                             host_col,
 795:                         ]
 796:                         if col
 797:                     ],
 798:                 }
 799:             ]
 800:             artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_revenue_compression")
 801:             out_path = artifacts_dir / "results.json"
 802:             write_json(out_path, {"summary": summary, "findings": findings})
 803:             artifacts = [
 804:                 PluginArtifact(
 805:                     path=str(out_path.relative_to(ctx.run_dir)),
 806:                     type="json",
 807:                     description="Revenue compression summary",
 808:                 )
 809:             ]
 810:             csv_path = artifacts_dir / "results.csv"
 811:             with csv_path.open("w", encoding="utf-8", newline="") as handle:
 812:                 header = [
 813:                     "decision",
 814:                     "reason",
 815:                 ]
 816:                 handle.write(",".join(header) + "\n")
 817:                 for item in findings:
 818:                     handle.write(",".join([str(item.get("decision")), str(item.get("reason"))]) + "\n")
 819:             artifacts.append(
 820:                 PluginArtifact(
 821:                     path=str(csv_path.relative_to(ctx.run_dir)),
 822:                     type="csv",
 823:                     description="Revenue compression detail table",
 824:                 )
 825:             )
 826: 
 827:             md_path = artifacts_dir / "results.md"
 828:             lines = [
 829:                 "# Close-cycle revenue compression",
 830:                 "",
 831:                 f"Not applicable: {reason}",
 832:             ]
 833:             selector = summary.get("revenue_selector")
 834:             if isinstance(selector, dict) and selector:
 835:                 lines.extend(["", "Selector:", f"- strategy: {summary.get('revenue_selector_strategy')}"])
 836:                 selector_cols = selector.get("selector_columns") or []
 837:                 if selector_cols:
 838:                     lines.append(f"- columns: {', '.join(selector_cols)}")
 839:                 key_count = selector.get("selector_key_count")
 840:                 if key_count is not None:
 841:                     lines.append(f"- key_count: {key_count}")
 842:             md_path.write_text("\n".join(lines), encoding="utf-8")
 843:             artifacts.append(
 844:                 PluginArtifact(
 845:                     path=str(md_path.relative_to(ctx.run_dir)),
 846:                     type="markdown",
 847:                     description="Revenue compression summary",
 848:                 )
 849:             )
 850: 
 851:             return PluginResult("ok", reason, {"findings": 1}, findings, artifacts, None)
 852: 
 853:         if not process_col:
 854:             return _emit_not_applicable("missing_process_column")
 855:         if not start_col:
 856:             return _emit_not_applicable("missing_start_column")
 857:         if not end_col and not duration_col:
 858:             return _emit_not_applicable("missing_end_or_duration")
 859: 
 860:         selected_cols = [
 861:             col
 862:             for col in [
 863:                 process_col,
 864:                 start_col,
 865:                 end_col,
 866:                 duration_col,
 867:                 queue_col,
 868:                 eligible_col,
 869:                 host_col,
 870:             ]
 871:             if col
 872:         ]
 873:         work = df.loc[:, selected_cols].copy()
 874: 
 875:         work["__start_ts"] = pd.to_datetime(work[start_col], errors="coerce", utc=False)
 876:         if end_col and end_col in work.columns:
 877:             work["__end_ts"] = pd.to_datetime(work[end_col], errors="coerce", utc=False)
 878:         else:
 879:             work["__end_ts"] = pd.NaT
 880: 
 881:         if duration_col and duration_col in work.columns:
 882:             duration = pd.to_numeric(work[duration_col], errors="coerce")
 883:             if duration.isna().all():
 884:                 duration = pd.to_timedelta(work[duration_col], errors="coerce").dt.total_seconds()
 885:             work["__duration_sec"] = duration
 886:         else:
 887:             work["__duration_sec"] = np.nan
 888: 
 889:         if end_col and work["__end_ts"].notna().any():
 890:             pass
 891:         elif work["__duration_sec"].notna().any():
 892:             work["__end_ts"] = work["__start_ts"] + pd.to_timedelta(
 893:                 work["__duration_sec"], unit="s"
 894:             )
 895:         else:
 896:             return _emit_not_applicable("missing_end_timestamp")
 897: 
 898:         work = work.loc[work["__start_ts"].notna() & work["__end_ts"].notna()].copy()
 899:         if work.empty:
 900:             return _emit_not_applicable("no_valid_timestamps")
 901: 
 902:         work = work.loc[work["__end_ts"] >= work["__start_ts"]].copy()
 903:         if work.empty:
 904:             return _emit_not_applicable("no_non_negative_durations")
 905: 
 906:         work["__service_sec"] = (
 907:             work["__end_ts"] - work["__start_ts"]
 908:         ).dt.total_seconds()
 909:         work = work.loc[work["__service_sec"] > 0].copy()
 910:         if work.empty:
 911:             return _emit_not_applicable("no_positive_ttc")
 912: 
 913:         close_mode = str(ctx.settings.get("close_window_mode", "infer_or_default") or "infer_or_default").lower()
 914:         if close_mode == "calendar":
 915:             close_mode = "override"
 916:         close_start_day = int(ctx.settings.get("close_cycle_start_day", 20))
 917:         close_end_day = int(ctx.settings.get("close_cycle_end_day", 5))
 918:         min_close_days = int(ctx.settings.get("min_close_days", 5))
 919:         max_close_days = int(ctx.settings.get("max_close_days", 20))
 920:         lookahead_days = int(ctx.settings.get("lookahead_days", 7))
 921:         min_close_confidence = float(ctx.settings.get("min_close_confidence", 0.1))
 922:         min_close_data_ratio = float(ctx.settings.get("min_close_data_ratio", 0.5))
 923: 
 924:         work["__date"] = work["__start_ts"].dt.date
 925:         daily = (
 926:             work.groupby("__date")
 927:             .agg(count=("__date", "size"), median_ttc=("__service_sec", "median"))
 928:             .reset_index()
 929:             .rename(columns={"__date": "date"})
 930:         )
 931: 
 932:         close_dates: set[date] = set()
 933:         close_windows: list[dict[str, Any]] = []
 934:         confident_months: set[str] = set()
 935:         fallback_used = False
 936:         fallback_reason: str | None = None
 937: 
 938:         if close_mode == "override":
 939:             close_dates, close_windows, confident_months = _calendar_close_window(
 940:                 daily, close_start_day, close_end_day, "override"
 941:             )
 942:         elif close_mode == "infer_or_default":
 943:             close_dates, close_windows, confident_months = _infer_close_windows(
 944:                 daily,
 945:                 min_close_days,
 946:                 max_close_days,
 947:                 lookahead_days,
 948:                 min_close_confidence,
 949:                 min_close_data_ratio,
 950:             )
 951:             if not close_dates:
 952:                 fallback_used = True
 953:                 fallback_reason = "calendar_default"
 954:                 close_dates, close_windows, confident_months = _calendar_close_window(
 955:                     daily, close_start_day, close_end_day, "fallback"
 956:                 )
 957:         else:
 958:             close_dates, close_windows, confident_months = _infer_close_windows(
 959:                 daily,
 960:                 min_close_days,
 961:                 max_close_days,
 962:                 lookahead_days,
 963:                 min_close_confidence,
 964:                 min_close_data_ratio,
 965:             )
 966: 
 967:         summary.update(
 968:             {
 969:                 "close_window_mode": close_mode,
 970:                 "close_cycle_start_day": close_start_day,
 971:                 "close_cycle_end_day": close_end_day,
 972:                 "min_close_days": min_close_days,
 973:                 "max_close_days": max_close_days,
 974:                 "lookahead_days": lookahead_days,
 975:                 "min_close_confidence": min_close_confidence,
 976:                 "min_close_data_ratio": min_close_data_ratio,
 977:                 "close_windows": close_windows,
 978:                 "close_window_fallback": fallback_used,
 979:                 "close_window_fallback_reason": fallback_reason,
 980:             }
 981:         )
 982: 
 983:         if not close_dates:
 984:             return _emit_not_applicable("close_window_not_inferred")
 985: 
 986:         close_window_source = (
 987:             "fallback" if fallback_used else ("override" if close_mode == "override" else "infer")
 988:         )
 989: 
 990:         selector_strategy = str(
 991:             ctx.settings.get("revenue_selector_strategy", "auto") or "auto"
 992:         ).lower()
 993:         if selector_strategy not in {"auto", "value_hints", "composite"}:
 994:             selector_strategy = "auto"
 995: 
 996:         revenue_names = _normalize_list(ctx.settings.get("revenue_process_names"))
 997:         revenue_patterns = _normalize_list(ctx.settings.get("revenue_process_patterns"))
 998:         selector_info: dict[str, Any] = {}
 999:         revenue_mask: pd.Series | None = None
1000:         strategy_used: str | None = None
1001: 
1002:         if selector_strategy in {"auto", "value_hints"} and (revenue_names or revenue_patterns):
1003:             if process_col and process_col in work.columns:
1004:                 work["__process"] = work[process_col].map(_normalize_text)
1005:                 work["__process_norm"] = work["__process"].str.lower()
1006:                 work = work.loc[~work["__process_norm"].isin(INVALID_STRINGS)].copy()
1007:                 if work.empty:
1008:                     return _emit_not_applicable("no_valid_process_values")
1009: 
1010:                 revenue_mask = _match_revenue(
1011:                     work["__process_norm"], revenue_names, revenue_patterns
1012:                 )
1013:                 selector_info = {
1014:                     "strategy": "value_hints",
1015:                     "process_column": process_col,
1016:                     "revenue_process_names": revenue_names,
1017:                     "revenue_process_patterns": revenue_patterns,
1018:                 }
1019:                 strategy_used = "value_hints"
1020:             elif selector_strategy == "value_hints":
1021:                 return _emit_not_applicable("missing_process_column_for_hints")
1022: 
1023:         if (revenue_mask is None or not revenue_mask.any()) and selector_strategy in {
1024:             "auto",
1025:             "composite",
1026:         }:
1027:             exclude_cols = {
1028:                 col
1029:                 for col in [
1030:                     start_col,
1031:                     end_col,
1032:                     duration_col,
1033:                     queue_col,
1034:                     eligible_col,
1035:                     host_col,
1036:                 ]
1037:                 if col
1038:             }
1039:             composite_mask, composite_info, key_series = _select_composite_revenue(
1040:                 work, close_dates, exclude_cols, ctx.settings
1041:             )
1042:             selector_info = composite_info
1043:             strategy_used = "composite"
1044:             if key_series is not None:
1045:                 work["__process"] = key_series
1046:                 work["__process_norm"] = work["__process"].astype(str).str.lower()
1047:             revenue_mask = composite_mask
1048: 
1049:         if revenue_mask is None or not revenue_mask.any():
1050:             summary.update(
1051:                 {
1052:                     "revenue_selector_strategy": strategy_used,
1053:                     "revenue_selector": selector_info,
1054:                 }
1055:             )
1056:             return _emit_not_applicable("no_revenue_process_match")
1057: 
1058:         revenue_rows = work.loc[revenue_mask].copy()
1059:         summary.update(
1060:             {
1061:                 "revenue_selector_strategy": strategy_used,
1062:                 "revenue_selector": selector_info,
1063:             }
1064:         )
1065: 
1066:         revenue_rows = revenue_rows.loc[
1067:             revenue_rows["__start_ts"].dt.date.isin(close_dates)
1068:         ].copy()
1069:         if revenue_rows.empty:
1070:             return _emit_not_applicable("no_revenue_rows_in_close_window")
1071: 
1072:         if queue_col and queue_col in revenue_rows.columns:
1073:             revenue_rows["__queue_ts"] = pd.to_datetime(
1074:                 revenue_rows[queue_col], errors="coerce", utc=False
1075:             )
1076:             revenue_rows["__queue_wait_sec"] = (
1077:                 revenue_rows["__start_ts"] - revenue_rows["__queue_ts"]
1078:             ).dt.total_seconds().clip(lower=0)
1079:         else:
1080:             revenue_rows["__queue_ts"] = pd.NaT
1081:             revenue_rows["__queue_wait_sec"] = np.nan
1082: 
1083:         if eligible_col and eligible_col in revenue_rows.columns:
1084:             revenue_rows["__eligible_ts"] = pd.to_datetime(
1085:                 revenue_rows[eligible_col], errors="coerce", utc=False
1086:             )
1087:             revenue_rows["__eligible_wait_sec"] = (
1088:                 revenue_rows["__start_ts"] - revenue_rows["__eligible_ts"]
1089:             ).dt.total_seconds().clip(lower=0)
1090:         else:
1091:             revenue_rows["__eligible_ts"] = pd.NaT
1092:             revenue_rows["__eligible_wait_sec"] = np.nan
1093: 
1094:         basis = "duration"
1095:         anchor_basis = "start"
1096:         wait_series = revenue_rows["__service_sec"]
1097:         anchor_series = revenue_rows["__start_ts"]
1098:         assumption = "Full end-to-end duration scales inversely with capacity."
1099: 
1100:         max_wait_days = float(ctx.settings.get("max_wait_days", 30.0))
1101:         max_wait_sec = max_wait_days * 86400.0
1102: 
1103:         eligible_wait = revenue_rows["__eligible_wait_sec"]
1104:         queue_wait = revenue_rows["__queue_wait_sec"]
1105:         eligible_median = float(eligible_wait.median()) if eligible_wait.notna().any() else None
1106:         queue_median = float(queue_wait.median()) if queue_wait.notna().any() else None
1107: 
1108:         if eligible_median is not None and eligible_median <= max_wait_sec:
1109:             basis = "eligible_wait"
1110:             anchor_basis = "eligible"
1111:             wait_series = eligible_wait
1112:             anchor_series = revenue_rows["__eligible_ts"]
1113:             assumption = "Eligible wait scales inversely with capacity; service time unchanged."
1114:         elif queue_median is not None and queue_median <= max_wait_sec:
1115:             basis = "queue_wait"
1116:             anchor_basis = "queue"
1117:             wait_series = queue_wait
1118:             anchor_series = revenue_rows["__queue_ts"]
1119:             assumption = "Queue wait scales inversely with capacity; service time unchanged."
1120: 
1121:         revenue_rows = revenue_rows.loc[anchor_series.notna()].copy()
1122:         if revenue_rows.empty:
1123:             return _emit_not_applicable("missing_anchor_timestamps")
1124: 
1125:         target_days = float(ctx.settings.get("target_days", 7.0))
1126:         target_sec = target_days * 86400.0
1127:         max_scale = float(ctx.settings.get("max_scale", 10.0))
1128:         min_month_rows = int(ctx.settings.get("min_month_rows", 10))
1129:         max_months_output = int(ctx.settings.get("max_months_output", 24))
1130: 
1131:         host_count = None
1132:         if host_col and host_col in revenue_rows.columns:
1133:             host_series = revenue_rows[host_col].map(_normalize_text).str.lower()
1134:             host_series = host_series.loc[~host_series.isin(INVALID_STRINGS)]
1135:             if not host_series.empty:
1136:                 host_count = int(host_series.nunique())
1137: 
1138:         if close_start_day > close_end_day:
1139:             def _close_cycle_month(ts: pd.Timestamp) -> str:
1140:                 if ts.day >= close_start_day:
1141:                     return f"{ts.year:04d}-{ts.month:02d}"
1142:                 prev = (ts.replace(day=1) - pd.Timedelta(days=1))
1143:                 return f"{prev.year:04d}-{prev.month:02d}"
1144: 
1145:             revenue_rows["__month"] = revenue_rows["__start_ts"].apply(_close_cycle_month)
1146:         else:
1147:             revenue_rows["__month"] = revenue_rows["__start_ts"].dt.to_period("M").astype(str)
1148:         month_stats: list[dict[str, Any]] = []
1149:         for month, frame in revenue_rows.groupby("__month"):
1150:             if frame.empty:
1151:                 continue
1152:             month_rows = int(frame.shape[0])
1153:             reason = "ok"
1154:             decision = "modeled"
1155:             required_scale = None
1156:             modeled_span = None
1157:             baseline_span = None
1158:             achievable = True
1159:             if month_rows < min_month_rows:
1160:                 decision = "not_applicable"
1161:                 reason = "insufficient_rows"
1162:             else:
1163:                 anchor_min = anchor_series.loc[frame.index].min()
1164:                 end_sec = (
1165:                     frame["__end_ts"] - anchor_min
1166:                 ).dt.total_seconds().to_numpy(dtype=float)
1167:                 wait_sec = wait_series.loc[frame.index].fillna(0.0).to_numpy(dtype=float)
1168:                 required_scale, baseline_span, achievable = _required_scale(
1169:                     end_sec, wait_sec, target_sec, max_scale
1170:                 )
1171:                 if required_scale is None and not achievable:
1172:                     decision = "not_applicable"
1173:                     reason = "exceeds_max_scale"
1174:                 elif required_scale == 1.0:
1175:                     reason = "already_within_target"
1176:                 if required_scale is not None:
1177:                     modeled_span = _modeled_span(end_sec, wait_sec, required_scale)
1178:             month_stats.append(
1179:                 {
1180:                     "month": month,
1181:                     "rows": month_rows,
1182:                     "decision": decision,
1183:                     "reason": reason,
1184:                     "baseline_span_days": baseline_span / 86400.0 if baseline_span is not None else None,
1185:                     "modeled_span_days": modeled_span / 86400.0 if modeled_span is not None else None,
1186:                     "required_scale_factor": required_scale,
1187:                 }
1188:             )
1189: 
1190:         valid_scales = [m for m in month_stats if m.get("required_scale_factor") is not None]
1191:         if valid_scales:
1192:             worst_month = max(valid_scales, key=lambda m: m.get("required_scale_factor") or 0.0)
1193:             scales = [m["required_scale_factor"] for m in valid_scales if m.get("required_scale_factor") is not None]
1194:             scales_sorted = sorted(scales)
1195:             scale_median = float(np.median(scales_sorted))
1196:             scale_p90 = float(np.quantile(scales_sorted, 0.9))
1197:             baseline_spans = [m["baseline_span_days"] for m in valid_scales if m.get("baseline_span_days") is not None]
1198:             baseline_span_median = float(np.median(baseline_spans)) if baseline_spans else None
1199:             decision = "modeled"
1200:             reason = "fallback_calendar_default" if fallback_used else "ok"
1201:         else:
1202:             worst_month = None
1203:             scale_median = None
1204:             scale_p90 = None
1205:             baseline_span_median = None
1206:             decision = "not_applicable"
1207:             reason = "no_eligible_months"
1208: 
1209:         selector_columns = []
1210:         selector_info = summary.get("revenue_selector")
1211:         if isinstance(selector_info, dict):
1212:             selector_columns = selector_info.get("selector_columns") or []
1213: 
1214:         columns_used: list[str] = []
1215:         for col in [
1216:             process_col,
1217:             start_col,
1218:             end_col,
1219:             duration_col,
1220:             queue_col,
1221:             eligible_col,
1222:             host_col,
1223:             *selector_columns,
1224:         ]:
1225:             if col and col not in columns_used:
1226:                 columns_used.append(col)
1227: 
1228:         finding = {
1229:             "kind": "close_cycle_revenue_compression",
1230:             "decision": decision,
1231:             "measurement_type": "modeled" if decision == "modeled" else "not_applicable",
1232:             "reason": reason,
1233:             "process_label": revenue_rows["__process"].value_counts().index[0],
1234:             "process_matches": sorted(revenue_rows["__process_norm"].unique().tolist())[:10],
1235:             "basis": basis,
1236:             "anchor_basis": anchor_basis,
1237:             "assumptions": [assumption],
1238:             "scope": {
1239:                 "basis": basis,
1240:                 "anchor_basis": anchor_basis,
1241:                 "close_window_mode": close_mode,
1242:                 "close_window_source": close_window_source,
1243:             },
1244:             "target_days": target_days,
1245:             "baseline_span_days_median": baseline_span_median,
1246:             "required_scale_factor_median": scale_median,
1247:             "required_scale_factor_p90": scale_p90,
1248:             "worst_month": worst_month.get("month") if worst_month else None,
1249:             "worst_month_required_scale": worst_month.get("required_scale_factor") if worst_month else None,
1250:             "worst_month_baseline_days": worst_month.get("baseline_span_days") if worst_month else None,
1251:             "host_count": host_count,
1252:             "scale_factor": scale_median,
1253:             "host_count_baseline": host_count,
1254:             "host_count_modeled": (
1255:                 int(round(host_count * scale_median))
1256:                 if host_count is not None and scale_median
1257:                 else None
1258:             ),
1259:             "close_window_mode": close_mode,
1260:             "close_window_source": close_window_source,
1261:             "close_window_fallback": fallback_used,
1262:             "close_window_reason": fallback_reason,
1263:             "revenue_selector_strategy": summary.get("revenue_selector_strategy"),
1264:             "revenue_selector": summary.get("revenue_selector"),
1265:             "columns": columns_used,
1266:             "row_ids": [int(i) for i in revenue_rows.index.tolist()[:50]],
1267:         }
1268: 
1269:         findings = [finding]
1270: 
1271:         artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_revenue_compression")
1272:         out_path = artifacts_dir / "results.json"
1273:         write_json(out_path, {"summary": summary, "findings": findings})
1274:         artifacts = [
1275:             PluginArtifact(
1276:                 path=str(out_path.relative_to(ctx.run_dir)),
1277:                 type="json",
1278:                 description="Revenue compression summary",
1279:             )
1280:         ]
1281: 
1282:         csv_path = artifacts_dir / "results.csv"
1283:         with csv_path.open("w", encoding="utf-8", newline="") as handle:
1284:             header = [
1285:                 "decision",
1286:                 "reason",
1287:                 "month",
1288:                 "rows",
1289:                 "baseline_span_days",
1290:                 "modeled_span_days",
1291:                 "required_scale_factor",
1292:             ]
1293:             handle.write(",".join(header) + "\n")
1294:             for entry in month_stats:
1295:                 row = [
1296:                     str(entry.get("decision")),
1297:                     str(entry.get("reason")),
1298:                     str(entry.get("month")),
1299:                     str(entry.get("rows")),
1300:                     str(entry.get("baseline_span_days")),
1301:                     str(entry.get("modeled_span_days")),
1302:                     str(entry.get("required_scale_factor")),
1303:                 ]
1304:                 handle.write(",".join(row) + "\n")
1305:         artifacts.append(
1306:             PluginArtifact(
1307:                 path=str(csv_path.relative_to(ctx.run_dir)),
1308:                 type="csv",
1309:                 description="Revenue compression detail table",
1310:             )
1311:         )
1312: 
1313:         md_path = artifacts_dir / "results.md"
1314:         lines = [
1315:             "# Close-cycle revenue compression",
1316:             "",
1317:             "Summary:",
1318:             f"- close_window_mode: {close_mode}",
1319:             f"- close_window_source: {close_window_source}",
1320:             f"- target_days: {target_days}",
1321:             f"- basis: {basis}",
1322:             f"- anchor_basis: {anchor_basis}",
1323:             f"- months_evaluated: {len(month_stats)}",
1324:         ]
1325:         selector = finding.get("revenue_selector") if isinstance(finding, dict) else None
1326:         if isinstance(selector, dict) and selector:
1327:             lines.extend(
1328:                 [
1329:                     "",
1330:                     "Selector:",
1331:                     f"- strategy: {finding.get('revenue_selector_strategy')}",
1332:                 ]
1333:             )
1334:             selector_cols = selector.get("selector_columns") or []
1335:             if selector_cols:
1336:                 lines.append(f"- columns: {', '.join(selector_cols)}")
1337:             key_count = selector.get("selector_key_count")
1338:             if key_count is not None:
1339:                 lines.append(f"- key_count: {key_count}")
1340:             key_rows = selector.get("selector_keys") or []
1341:             if key_rows:
1342:                 lines.extend(
1343:                     [
1344:                         "",
1345:                         "Selector keys (top by score):",
1346:                         "| key | close_rows | total_rows | close_share | close_lift | late_fraction |",
1347:                         "| --- | --- | --- | --- | --- | --- |",
1348:                     ]
1349:                 )
1350:                 for row in key_rows[:10]:
1351:                     lines.append(
1352:                         "| {key} | {close_rows} | {total_rows} | {close_share:.2f} | {close_lift:.2f} | {late_fraction:.2f} |".format(
1353:                             key=row.get("key"),
1354:                             close_rows=row.get("close_rows"),
1355:                             total_rows=row.get("total_rows"),
1356:                             close_share=float(row.get("close_share", 0.0)),
1357:                             close_lift=float(row.get("close_lift", 0.0)),
1358:                             late_fraction=float(row.get("late_fraction", 0.0)),
1359:                         )
1360:                     )
1361:         lines.extend(
1362:             [
1363:                 "",
1364:                 "Result:",
1365:                 "| process | scale_median | scale_p90 | worst_month | worst_month_scale |",
1366:                 "| --- | --- | --- | --- | --- |",
1367:             ]
1368:         )
1369:         lines.append(
1370:             "| {process_label} | {scale_median} | {scale_p90} | {worst_month} | {worst_month_scale} |".format(
1371:                 process_label=finding.get("process_label"),
1372:                 scale_median=(
1373:                     f"{finding.get('required_scale_factor_median'):.3f}"
1374:                     if finding.get("required_scale_factor_median") is not None
1375:                     else ""
1376:                 ),
1377:                 scale_p90=(
1378:                     f"{finding.get('required_scale_factor_p90'):.3f}"
1379:                     if finding.get("required_scale_factor_p90") is not None
1380:                     else ""
1381:                 ),
1382:                 worst_month=finding.get("worst_month") or "",
1383:                 worst_month_scale=(
1384:                     f"{finding.get('worst_month_required_scale'):.3f}"
1385:                     if finding.get("worst_month_required_scale") is not None
1386:                     else ""
1387:                 ),
1388:             )
1389:         )
1390:         if month_stats:
1391:             lines.extend(["", "Month details:"])
1392:             lines.append("| month | rows | baseline_days | modeled_days | required_scale | reason |")
1393:             lines.append("| --- | --- | --- | --- | --- | --- |")
1394:             for entry in month_stats[:max_months_output]:
1395:                 lines.append(
1396:                     "| {month} | {rows} | {baseline} | {modeled} | {scale} | {reason} |".format(
1397:                         month=entry.get("month"),
1398:                         rows=entry.get("rows"),
1399:                         baseline=(
1400:                             f"{entry.get('baseline_span_days'):.2f}"
1401:                             if entry.get("baseline_span_days") is not None
1402:                             else ""
1403:                         ),
1404:                         modeled=(
1405:                             f"{entry.get('modeled_span_days'):.2f}"
1406:                             if entry.get("modeled_span_days") is not None
1407:                             else ""
1408:                         ),
1409:                         scale=(
1410:                             f"{entry.get('required_scale_factor'):.3f}"
1411:                             if entry.get("required_scale_factor") is not None
1412:                             else ""
1413:                         ),
1414:                         reason=entry.get("reason"),
1415:                     )
1416:                 )
1417:         md_path.write_text("\n".join(lines), encoding="utf-8")
1418:         artifacts.append(
1419:             PluginArtifact(
1420:                 path=str(md_path.relative_to(ctx.run_dir)),
1421:                 type="markdown",
1422:                 description="Revenue compression summary",
1423:             )
1424:         )
1425: 
1426:         metrics = {
1427:             "findings": len(findings),
1428:             "decision": decision,
1429:             "required_scale_factor_median": scale_median,
1430:             "required_scale_factor_p90": scale_p90,
1431:             "baseline_span_days_median": baseline_span_median,
1432:             "months_evaluated": len(month_stats),
1433:         }
1434: 
1435:         return PluginResult(
1436:             "ok",
1437:             "Computed revenue close-cycle compression model",
1438:             metrics,
1439:             findings,
1440:             artifacts,
1441:             None,
1442:         )
````

## File: plugins/analysis_concurrency_reconstruction/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.column_inference import choose_timestamp_column
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: INVALID_STRINGS = {"", "nan", "none", "null"}
 13: 
 14: 
 15: def _normalize_text(value: Any) -> str:
 16:     if value is None:
 17:         return ""
 18:     if isinstance(value, float) and pd.isna(value):
 19:         return ""
 20:     return str(value).strip()
 21: 
 22: 
 23: def _pick_column(
 24:     preferred: str | None,
 25:     columns: list[str],
 26:     role_by_name: dict[str, str],
 27:     roles: set[str],
 28:     patterns: list[str],
 29:     lower_names: dict[str, str],
 30:     exclude: set[str],
 31: ) -> str | None:
 32:     if preferred and preferred in columns:
 33:         return preferred
 34:     for col in columns:
 35:         if col in exclude:
 36:             continue
 37:         if role_by_name.get(col) in roles:
 38:             return col
 39:     for col in columns:
 40:         if col in exclude:
 41:             continue
 42:         name = lower_names[col]
 43:         if any(pattern in name for pattern in patterns):
 44:             return col
 45:     return None
 46: 
 47: 
 48: def _pick_timestamp_column(
 49:     preferred: str | None,
 50:     columns: list[str],
 51:     role_by_name: dict[str, str],
 52:     roles: set[str],
 53:     patterns: list[str],
 54:     lower_names: dict[str, str],
 55:     exclude: set[str],
 56:     df: pd.DataFrame,
 57: ) -> str | None:
 58:     candidates: list[str] = []
 59:     if preferred and preferred in columns and preferred not in exclude:
 60:         candidates.append(preferred)
 61:     for col in columns:
 62:         if col in exclude:
 63:             continue
 64:         if role_by_name.get(col) in roles and col not in candidates:
 65:             candidates.append(col)
 66:     for col in columns:
 67:         if col in exclude:
 68:             continue
 69:         name = lower_names[col]
 70:         if any(pattern in name for pattern in patterns) and col not in candidates:
 71:             candidates.append(col)
 72:     if not candidates:
 73:         return None
 74:     return choose_timestamp_column(df, candidates)
 75: 
 76: 
 77: def _sweep(events: list[tuple[pd.Timestamp, int]]) -> tuple[int, float]:
 78:     if not events:
 79:         return 0, 0.0
 80:     events = sorted(events, key=lambda item: (item[0], item[1]))
 81:     current = 0
 82:     max_concurrency = 0
 83:     total_weighted = 0.0
 84:     total_duration = 0.0
 85:     prev_time = None
 86:     for ts, delta in events:
 87:         if prev_time is not None and ts > prev_time:
 88:             duration = (ts - prev_time).total_seconds()
 89:             if duration > 0:
 90:                 total_weighted += current * duration
 91:                 total_duration += duration
 92:         current += delta
 93:         if current > max_concurrency:
 94:             max_concurrency = current
 95:         prev_time = ts
 96:     avg_concurrency = total_weighted / total_duration if total_duration > 0 else 0.0
 97:     return max_concurrency, avg_concurrency
 98: 
 99: 
100: class Plugin:
101:     def run(self, ctx) -> PluginResult:
102:         df = ctx.dataset_loader()
103:         if df.empty:
104:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
105: 
106:         columns_meta = []
107:         role_by_name: dict[str, str] = {}
108:         if ctx.dataset_version_id:
109:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
110:             if dataset_template and dataset_template.get("status") == "ready":
111:                 fields = ctx.storage.fetch_template_fields(
112:                     int(dataset_template["template_id"])
113:                 )
114:                 columns_meta = fields
115:                 role_by_name = {
116:                     field["name"]: (field.get("role") or "") for field in fields
117:                 }
118:             else:
119:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
120:                 role_by_name = {
121:                     col["original_name"]: (col.get("role") or "")
122:                     for col in columns_meta
123:                 }
124: 
125:         columns = list(df.columns)
126:         lower_names = {col: str(col).lower() for col in columns}
127:         used: set[str] = set()
128: 
129:         start_col = _pick_timestamp_column(
130:             ctx.settings.get("start_column"),
131:             columns,
132:             role_by_name,
133:             {"start_time", "start"},
134:             ["start", "begin"],
135:             lower_names,
136:             used,
137:             df,
138:         )
139:         if start_col:
140:             used.add(start_col)
141: 
142:         end_col = _pick_timestamp_column(
143:             ctx.settings.get("end_column"),
144:             columns,
145:             role_by_name,
146:             {"end_time", "end"},
147:             ["end", "finish", "complete", "stop"],
148:             lower_names,
149:             used,
150:             df,
151:         )
152:         if end_col:
153:             used.add(end_col)
154: 
155:         host_col = _pick_column(
156:             ctx.settings.get("host_column"),
157:             columns,
158:             role_by_name,
159:             {"server", "host", "node", "instance"},
160:             ["server", "host", "node", "instance", "machine"],
161:             lower_names,
162:             used,
163:         )
164:         if host_col:
165:             used.add(host_col)
166: 
167:         cap_col = _pick_column(
168:             ctx.settings.get("capacity_column"),
169:             columns,
170:             role_by_name,
171:             {"capacity", "cap", "limit"},
172:             ["capacity", "cap", "limit", "slots"],
173:             lower_names,
174:             used,
175:         )
176: 
177:         if not start_col or not end_col:
178:             return PluginResult(
179:                 "ok",
180:                 "Concurrency reconstruction not applicable",
181:                 {"hosts": 0},
182:                 [
183:                     {
184:                         "kind": "concurrency_summary",
185:                         "measurement_type": "not_applicable",
186:                         "reason": "Missing start/end columns.",
187:                         "columns": [col for col in [start_col, end_col, host_col] if col],
188:                     }
189:                 ],
190:                 [],
191:                 None,
192:             )
193: 
194:         selected = [col for col in [start_col, end_col, host_col, cap_col] if col]
195:         work = df.loc[:, selected].copy()
196:         work["__start_ts"] = pd.to_datetime(work[start_col], errors="coerce", utc=False)
197:         work["__end_ts"] = pd.to_datetime(work[end_col], errors="coerce", utc=False)
198:         work = work.loc[work["__start_ts"].notna() & work["__end_ts"].notna()].copy()
199:         if work.empty:
200:             return PluginResult(
201:                 "ok",
202:                 "No valid timestamps",
203:                 {"hosts": 0},
204:                 [],
205:                 [],
206:                 None,
207:             )
208: 
209:         work = work.loc[work["__end_ts"] >= work["__start_ts"]].copy()
210:         if work.empty:
211:             return PluginResult(
212:                 "ok",
213:                 "No non-negative durations",
214:                 {"hosts": 0},
215:                 [],
216:                 [],
217:                 None,
218:             )
219: 
220:         if host_col:
221:             work["__host"] = work[host_col].map(_normalize_text)
222:             work["__host_norm"] = work["__host"].str.lower()
223:             work = work.loc[~work["__host_norm"].isin(INVALID_STRINGS)].copy()
224:         else:
225:             work["__host"] = "overall"
226:             work["__host_norm"] = "overall"
227: 
228:         if work.empty:
229:             return PluginResult(
230:                 "ok",
231:                 "No valid host values",
232:                 {"hosts": 0},
233:                 [],
234:                 [],
235:                 None,
236:             )
237: 
238:         capacity_limit = ctx.settings.get("capacity_limit")
239:         try:
240:             capacity_limit = float(capacity_limit) if capacity_limit is not None else None
241:         except (TypeError, ValueError):
242:             capacity_limit = None
243: 
244:         findings = []
245:         host_stats = []
246:         max_hosts = int(ctx.settings.get("max_hosts", 10))
247:         max_examples = int(ctx.settings.get("max_examples", 25))
248: 
249:         grouped = work.groupby("__host_norm", sort=False)
250:         for host_norm, frame in grouped:
251:             host_label = frame["__host"].iloc[0]
252:             events: list[tuple[pd.Timestamp, int]] = []
253:             for _, row in frame.iterrows():
254:                 events.append((row["__start_ts"], 1))
255:                 events.append((row["__end_ts"], -1))
256:             max_conc, avg_conc = _sweep(events)
257: 
258:             host_cap = capacity_limit
259:             if host_cap is None and cap_col and cap_col in frame.columns:
260:                 values = pd.to_numeric(frame[cap_col], errors="coerce")
261:                 values = values.dropna()
262:                 if not values.empty:
263:                     host_cap = float(values.median())
264: 
265:             cap_util = None
266:             cap_exceeded = None
267:             if host_cap is not None and host_cap > 0:
268:                 cap_util = float(max_conc) / float(host_cap)
269:                 cap_exceeded = bool(max_conc > host_cap)
270: 
271:             row_ids = [int(i) for i in frame.index.tolist()][:max_examples]
272:             findings.append(
273:                 {
274:                     "kind": "concurrency_summary",
275:                     "host": host_label,
276:                     "peak_concurrency": int(max_conc),
277:                     "avg_concurrency": float(avg_conc),
278:                     "cap_limit": host_cap,
279:                     "cap_utilization": cap_util,
280:                     "cap_exceeded": cap_exceeded,
281:                     "measurement_type": "measured",
282:                     "row_ids": row_ids,
283:                     "columns": [col for col in [start_col, end_col, host_col, cap_col] if col],
284:                 }
285:             )
286:             host_stats.append(
287:                 {
288:                     "host": host_label,
289:                     "peak_concurrency": int(max_conc),
290:                     "avg_concurrency": float(avg_conc),
291:                     "cap_limit": host_cap,
292:                 }
293:             )
294: 
295:         findings.sort(key=lambda item: (-item.get("peak_concurrency", 0), str(item.get("host", ""))))
296:         findings = findings[:max_hosts]
297: 
298:         overall_events: list[tuple[pd.Timestamp, int]] = []
299:         for _, row in work.iterrows():
300:             overall_events.append((row["__start_ts"], 1))
301:             overall_events.append((row["__end_ts"], -1))
302:         overall_max, overall_avg = _sweep(overall_events)
303: 
304:         metrics = {
305:             "hosts": int(len(grouped)),
306:             "peak_concurrency": int(overall_max),
307:             "avg_concurrency": float(overall_avg),
308:         }
309: 
310:         artifacts_dir = ctx.artifacts_dir("analysis_concurrency_reconstruction")
311:         out_path = artifacts_dir / "concurrency_summary.json"
312:         write_json(
313:             out_path,
314:             {
315:                 "summary": {
316:                     "start_column": start_col,
317:                     "end_column": end_col,
318:                     "host_column": host_col,
319:                     "capacity_column": cap_col,
320:                 },
321:                 "metrics": metrics,
322:                 "hosts": host_stats,
323:             },
324:         )
325:         artifacts = [
326:             PluginArtifact(
327:                 path=str(out_path.relative_to(ctx.run_dir)),
328:                 type="json",
329:                 description="Concurrency reconstruction summary",
330:             )
331:         ]
332: 
333:         return PluginResult(
334:             "ok",
335:             "Reconstructed concurrency timeline",
336:             metrics,
337:             findings,
338:             artifacts,
339:             None,
340:         )
````

## File: plugins/analysis_conformal_feature_prediction/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_conformal_feature_prediction/plugin.yaml
````yaml
 1: id: analysis_conformal_feature_prediction
 2: name: Conformal Feature Prediction
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Conformal anomaly detection
12:   defaults:
13:     alpha: 0.1
14:     max_target_cols: 5
15:     model: ridge
16: config_schema: config.schema.json
17: output_schema: output.schema.json
18: sandbox:
19:   no_network: true
20:   fs_allowlist:
21:   - appdata
22:   - plugins
````

## File: plugins/analysis_dp_gmm/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_dp_gmm/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import pandas as pd
 4: 
 5: from statistic_harness.core.types import PluginArtifact, PluginResult
 6: from statistic_harness.core.utils import write_json
 7: 
 8: 
 9: class Plugin:
10:     def run(self, ctx) -> PluginResult:
11:         df = ctx.dataset_loader()
12:         numeric = df.select_dtypes(include="number")
13:         if numeric.empty:
14:             return PluginResult("skipped", "No numeric columns", {}, [], [], None)
15:         values = numeric.to_numpy()
16:         mean = values.mean(axis=0)
17:         labels = (values[:, 0] > mean[0]).astype(int)
18:         assignments = pd.DataFrame({"cluster_id": labels})
19: 
20:         clusters = []
21:         findings = []
22:         for cluster_id in sorted(set(labels)):
23:             cluster_points = values[labels == cluster_id]
24:             clusters.append(
25:                 {
26:                     "cluster_id": int(cluster_id),
27:                     "size": int(cluster_points.shape[0]),
28:                     "mean": cluster_points.mean(axis=0).tolist(),
29:                 }
30:             )
31:             findings.append(
32:                 {
33:                     "kind": "cluster",
34:                     "cluster_id": int(cluster_id),
35:                     "size": int(cluster_points.shape[0]),
36:                 }
37:             )
38: 
39:         artifacts_dir = ctx.artifacts_dir("analysis_dp_gmm")
40:         assignments_path = artifacts_dir / "assignments.csv"
41:         assignments.to_csv(assignments_path, index=False)
42:         summary_path = artifacts_dir / "summary.json"
43:         write_json(summary_path, {"clusters": clusters})
44:         artifacts = [
45:             PluginArtifact(
46:                 path=str(assignments_path.relative_to(ctx.run_dir)),
47:                 type="csv",
48:                 description="Assignments",
49:             ),
50:             PluginArtifact(
51:                 path=str(summary_path.relative_to(ctx.run_dir)),
52:                 type="json",
53:                 description="Summary",
54:             ),
55:         ]
56:         return PluginResult(
57:             "ok",
58:             "Computed DP GMM clusters",
59:             {"clusters": len(clusters)},
60:             findings,
61:             artifacts,
62:             None,
63:         )
````

## File: plugins/analysis_dp_gmm/plugin.yaml
````yaml
 1: id: analysis_dp_gmm
 2: name: DP GMM
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Simplified DP GMM
12:   defaults:
13:     alpha: 1.0
14:     sigma2: null
15:     n_iter: 50
16:     burn_in: 10
17: config_schema: config.schema.json
18: output_schema: output.schema.json
19: sandbox:
20:   no_network: true
21:   fs_allowlist:
22:   - appdata
23:   - plugins
````

## File: plugins/analysis_gaussian_copula_shift/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_gaussian_copula_shift/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "budget": {
 6:       "additionalProperties": false,
 7:       "properties": {
 8:         "cpu_limit_ms": {
 9:           "type": ["integer", "null"]
10:         },
11:         "row_limit": {
12:           "type": ["integer", "null"]
13:         },
14:         "sampled": {
15:           "type": "boolean"
16:         },
17:         "time_limit_ms": {
18:           "type": ["integer", "null"]
19:         }
20:       },
21:       "type": "object"
22:     },
23:     "max_pairs": {
24:       "minimum": 1,
25:       "type": "integer"
26:     },
27:     "max_rows": {
28:       "minimum": 1,
29:       "type": "integer"
30:     },
31:     "n_permutations": {
32:       "minimum": 1,
33:       "type": "integer"
34:     },
35:     "segment": {
36:       "type": "string"
37:     }
38:   },
39:   "title": "Gaussian Copula Shift Config",
40:   "type": "object"
41: }
````

## File: plugins/analysis_gaussian_copula_shift/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "delta": {
187:                     "type": "number"
188:                   },
189:                   "evidence": {
190:                     "additionalProperties": true,
191:                     "properties": {
192:                       "column_ids": {
193:                         "items": {
194:                           "type": "integer"
195:                         },
196:                         "type": "array"
197:                       },
198:                       "dataset_id": {
199:                         "type": "string"
200:                       },
201:                       "dataset_version_id": {
202:                         "type": "string"
203:                       },
204:                       "query": {
205:                         "type": [
206:                           "string",
207:                           "null"
208:                         ]
209:                       },
210:                       "row_ids": {
211:                         "items": {
212:                           "type": "integer"
213:                         },
214:                         "type": "array"
215:                       },
216:                       "row_ranges": {
217:                         "items": {
218:                           "additionalProperties": false,
219:                           "properties": {
220:                             "end": {
221:                               "type": "integer"
222:                             },
223:                             "start": {
224:                               "type": "integer"
225:                             }
226:                           },
227:                           "required": [
228:                             "start",
229:                             "end"
230:                           ],
231:                           "type": "object"
232:                         },
233:                         "type": "array"
234:                       }
235:                     },
236:                     "required": [
237:                       "dataset_id",
238:                       "dataset_version_id",
239:                       "row_ids",
240:                       "column_ids"
241:                     ],
242:                     "type": "object"
243:                   },
244:                   "kind": {
245:                     "const": "dependence_shift"
246:                   },
247:                   "p_value": {
248:                     "type": "number"
249:                   },
250:                   "pair": {
251:                     "items": {
252:                       "type": "string"
253:                     },
254:                     "minItems": 2,
255:                     "type": "array"
256:                   }
257:                 },
258:                 "required": [
259:                   "kind",
260:                   "pair",
261:                   "delta",
262:                   "p_value"
263:                 ],
264:                 "type": "object"
265:               }
266:             ]
267:           }
268:         ],
269:         "properties": {
270:           "confidence": {
271:             "type": "number"
272:           },
273:           "q_value": {
274:             "type": "number"
275:           }
276:         }
277:       },
278:       "type": "array"
279:     },
280:     "metrics": {
281:       "additionalProperties": false,
282:       "properties": {
283:         "pairs": {
284:           "type": "integer"
285:         },
286:         "permutations_run": {
287:           "type": "integer"
288:         },
289:         "rows_used": {
290:           "type": "integer"
291:         },
292:         "sampled": {
293:           "type": "boolean"
294:         },
295:         "time_limit_hit": {
296:           "type": "boolean"
297:         }
298:       },
299:       "type": "object"
300:     },
301:     "status": {
302:       "enum": [
303:         "ok",
304:         "skipped",
305:         "error"
306:       ],
307:       "type": "string"
308:     },
309:     "summary": {
310:       "type": "string"
311:     }
312:   },
313:   "required": [
314:     "status",
315:     "summary",
316:     "metrics",
317:     "findings",
318:     "artifacts",
319:     "budget"
320:   ],
321:   "title": "Plugin Result",
322:   "type": "object"
323: }
````

## File: plugins/analysis_gaussian_knockoffs/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_gaussian_knockoffs/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import numpy as np
 4: 
 5: from statistic_harness.core.types import PluginArtifact, PluginResult
 6: from statistic_harness.core.utils import write_json
 7: 
 8: 
 9: class Plugin:
10:     def run(self, ctx) -> PluginResult:
11:         df = ctx.dataset_loader()
12:         numeric = df.select_dtypes(include="number")
13:         if numeric.shape[1] < 2:
14:             return PluginResult(
15:                 "skipped", "Not enough numeric columns", {}, [], [], None
16:             )
17:         target_column = ctx.settings.get("target_column") or numeric.columns[-1]
18:         y = numeric[target_column]
19:         X = numeric.drop(columns=[target_column])
20: 
21:         rng = np.random.default_rng(ctx.run_seed)
22:         scores = {}
23:         for col in X.columns:
24:             corr = np.corrcoef(X[col], y)[0, 1]
25:             knockoff = rng.permutation(X[col])
26:             knock_corr = np.corrcoef(knockoff, y)[0, 1]
27:             scores[col] = abs(corr) - abs(knock_corr)
28: 
29:         score_values = np.array(list(scores.values()))
30:         threshold = np.quantile(score_values, 1 - float(ctx.settings.get("fdr_q", 0.1)))
31:         findings = []
32:         for feature, score in scores.items():
33:             selected = bool(score >= threshold)
34:             findings.append(
35:                 {
36:                     "kind": "feature_discovery",
37:                     "feature": feature,
38:                     "score": float(score),
39:                     "selected": selected,
40:                 }
41:             )
42: 
43:         artifacts_dir = ctx.artifacts_dir("analysis_gaussian_knockoffs")
44:         selection_path = artifacts_dir / "selection.json"
45:         write_json(selection_path, findings)
46:         artifacts = [
47:             PluginArtifact(
48:                 path=str(selection_path.relative_to(ctx.run_dir)),
49:                 type="json",
50:                 description="Selection",
51:             )
52:         ]
53:         selected_count = sum(1 for f in findings if f["selected"])
54:         return PluginResult(
55:             "ok",
56:             "Computed knockoff selection",
57:             {"selected": selected_count},
58:             findings,
59:             artifacts,
60:             None,
61:         )
````

## File: plugins/analysis_gaussian_knockoffs/plugin.yaml
````yaml
 1: id: analysis_gaussian_knockoffs
 2: name: Gaussian Knockoffs
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_multi_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Simple knockoff-style feature selection
12:   defaults:
13:     target_column: null
14:     fdr_q: 0.1
15:     lasso_alpha: 0.01
16: config_schema: config.schema.json
17: output_schema: output.schema.json
18: sandbox:
19:   no_network: true
20:   fs_allowlist:
21:   - appdata
22:   - plugins
````

## File: plugins/analysis_graph_topology_curves/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_graph_topology_curves/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import numpy as np
 4: 
 5: from statistic_harness.core.types import PluginArtifact, PluginResult
 6: from statistic_harness.core.utils import write_json
 7: 
 8: 
 9: class UnionFind:
10:     def __init__(self, n: int) -> None:
11:         self.parent = list(range(n))
12:         self.rank = [0] * n
13: 
14:     def find(self, x: int) -> int:
15:         if self.parent[x] != x:
16:             self.parent[x] = self.find(self.parent[x])
17:         return self.parent[x]
18: 
19:     def union(self, a: int, b: int) -> None:
20:         ra = self.find(a)
21:         rb = self.find(b)
22:         if ra == rb:
23:             return
24:         if self.rank[ra] < self.rank[rb]:
25:             self.parent[ra] = rb
26:         elif self.rank[ra] > self.rank[rb]:
27:             self.parent[rb] = ra
28:         else:
29:             self.parent[rb] = ra
30:             self.rank[ra] += 1
31: 
32:     def count_components(self) -> int:
33:         return len({self.find(i) for i in range(len(self.parent))})
34: 
35: 
36: class Plugin:
37:     def run(self, ctx) -> PluginResult:
38:         df = ctx.dataset_loader()
39:         numeric = df.select_dtypes(include="number")
40:         if numeric.empty:
41:             return PluginResult("skipped", "No numeric columns", {}, [], [], None)
42:         max_points = int(ctx.settings.get("max_points", 100))
43:         n_thresholds = int(ctx.settings.get("n_thresholds", 10))
44:         sample = numeric.head(max_points).to_numpy()
45:         n = sample.shape[0]
46:         if n < 2:
47:             return PluginResult("skipped", "Not enough points", {}, [], [], None)
48: 
49:         dists = np.linalg.norm(sample[:, None, :] - sample[None, :, :], axis=-1)
50:         max_dist = float(dists.max())
51:         eps_values = np.linspace(0.0, max_dist, n_thresholds)
52:         beta0 = []
53:         beta1 = []
54: 
55:         for eps in eps_values:
56:             uf = UnionFind(n)
57:             edges = 0
58:             for i in range(n):
59:                 for j in range(i + 1, n):
60:                     if dists[i, j] <= eps:
61:                         uf.union(i, j)
62:                         edges += 1
63:             components = uf.count_components()
64:             beta0.append(int(components))
65:             beta1.append(int(edges - n + components))
66: 
67:         curves = {"eps": [float(e) for e in eps_values], "beta0": beta0, "beta1": beta1}
68:         artifacts_dir = ctx.artifacts_dir("analysis_graph_topology_curves")
69:         out_path = artifacts_dir / "curves.json"
70:         write_json(out_path, curves)
71:         findings = []
72:         if beta1:
73:             findings.append(
74:                 {"kind": "topology", "metric": "beta1_peak", "value": float(max(beta1))}
75:             )
76:         artifacts = [
77:             PluginArtifact(
78:                 path=str(out_path.relative_to(ctx.run_dir)),
79:                 type="json",
80:                 description="Topology curves",
81:             )
82:         ]
83:         return PluginResult(
84:             "ok",
85:             "Computed topology curves",
86:             {"beta1_peak": max(beta1)},
87:             findings,
88:             artifacts,
89:             None,
90:         )
````

## File: plugins/analysis_graph_topology_curves/plugin.yaml
````yaml
 1: id: analysis_graph_topology_curves
 2: name: Graph Topology Curves
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Approximate topology curves
12:   defaults:
13:     max_points: 100
14:     n_thresholds: 10
15: config_schema: config.schema.json
16: output_schema: output.schema.json
17: sandbox:
18:   no_network: true
19:   fs_allowlist:
20:   - appdata
21:   - plugins
````

## File: plugins/analysis_knockoff_wrapper_rf/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_knockoff_wrapper_rf/plugin.yaml
````yaml
 1: id: analysis_knockoff_wrapper_rf
 2: name: Knockoff Wrapper RF
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_multi_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Knockoff wrapper with RandomForestRegressor
12:   defaults:
13:     target_column: null
14:     fdr_q: 0.1
15:     n_estimators: 50
16: config_schema: config.schema.json
17: output_schema: output.schema.json
18: sandbox:
19:   no_network: true
20:   fs_allowlist:
21:   - appdata
22:   - plugins
````

## File: plugins/analysis_notears_linear/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_notears_linear/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: 
 4: from statistic_harness.core.types import PluginArtifact, PluginResult
 5: from statistic_harness.core.utils import write_json
 6: 
 7: 
 8: class Plugin:
 9:     def run(self, ctx) -> PluginResult:
10:         df = ctx.dataset_loader()
11:         numeric = df.select_dtypes(include="number")
12:         if numeric.shape[1] < 2:
13:             return PluginResult(
14:                 "skipped", "Not enough numeric columns", {}, [], [], None
15:             )
16:         max_cols = int(ctx.settings.get("max_cols", 20))
17:         numeric = numeric.iloc[:, :max_cols]
18:         corr = numeric.corr().fillna(0.0).to_numpy()
19:         nodes = list(numeric.columns)
20:         edges = []
21:         edges_compact = []
22:         threshold = float(ctx.settings.get("weight_threshold", 0.2))
23:         for i in range(len(nodes)):
24:             for j in range(len(nodes)):
25:                 if i == j:
26:                     continue
27:                 weight = corr[i, j]
28:                 if abs(weight) >= threshold:
29:                     edges.append(
30:                         {
31:                             "source": nodes[i],
32:                             "target": nodes[j],
33:                             "weight": float(weight),
34:                         }
35:                     )
36:                     edges_compact.append([i, j, float(weight)])
37:         graph = {"nodes": nodes, "edges": edges, "edges_compact": edges_compact}
38:         artifacts_dir = ctx.artifacts_dir("analysis_notears_linear")
39:         graph_path = artifacts_dir / "graph.json"
40:         write_json(graph_path, graph)
41:         findings = [
42:             {
43:                 "kind": "graph_edge",
44:                 "source": e["source"],
45:                 "target": e["target"],
46:                 "weight": e["weight"],
47:             }
48:             for e in edges
49:         ]
50:         artifacts = [
51:             PluginArtifact(
52:                 path=str(graph_path.relative_to(ctx.run_dir)),
53:                 type="json",
54:                 description="Graph",
55:             )
56:         ]
57:         return PluginResult(
58:             "ok", "Estimated graph", {"edges": len(edges)}, findings, artifacts, None
59:         )
````

## File: plugins/analysis_notears_linear/plugin.yaml
````yaml
 1: id: analysis_notears_linear
 2: name: NOTEARS Linear
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_multi_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Simplified NOTEARS
12:   defaults:
13:     max_cols: 20
14:     lambda_l1: 0.1
15:     max_iter: 100
16:     lr: 0.01
17:     weight_threshold: 0.2
18: config_schema: config.schema.json
19: output_schema: output.schema.json
20: sandbox:
21:   no_network: true
22:   fs_allowlist:
23:   - appdata
24:   - plugins
````

## File: plugins/analysis_online_conformal_changepoint/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_online_conformal_changepoint/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import numpy as np
 4: 
 5: from statistic_harness.core.types import PluginArtifact, PluginResult
 6: from statistic_harness.core.utils import write_json
 7: 
 8: 
 9: class Plugin:
10:     def run(self, ctx) -> PluginResult:
11:         df = ctx.dataset_loader()
12:         value_col = ctx.settings.get("value_column")
13:         if not value_col:
14:             numeric = df.select_dtypes(include="number")
15:             if numeric.empty:
16:                 return PluginResult("skipped", "No numeric columns", {}, [], [], None)
17:             value_col = numeric.columns[0]
18:         series = df[value_col].to_numpy()
19:         n = len(series)
20:         forecast_window = int(ctx.settings.get("forecast_window", 5))
21:         calib_window = int(ctx.settings.get("calib_window", 10))
22:         alpha = float(ctx.settings.get("alpha", 0.1))
23:         alarm_rate_window = int(ctx.settings.get("alarm_rate_window", 10))
24:         alarm_rate_threshold = float(ctx.settings.get("alarm_rate_threshold", 0.4))
25: 
26:         scores = []
27:         anomalies = []
28:         for i in range(forecast_window, n):
29:             forecast = series[i - forecast_window : i].mean()
30:             score = abs(series[i] - forecast)
31:             scores.append(score)
32:             if i >= calib_window:
33:                 calib_scores = np.array(scores[-calib_window:])
34:                 thresh = np.quantile(calib_scores, 1 - alpha)
35:                 anomalies.append(score > thresh)
36:             else:
37:                 anomalies.append(False)
38: 
39:         changepoints = []
40:         for i in range(alarm_rate_window, len(anomalies)):
41:             window = anomalies[i - alarm_rate_window : i]
42:             rate = sum(window) / alarm_rate_window
43:             if rate > alarm_rate_threshold:
44:                 changepoints.append(
45:                     {
46:                         "kind": "changepoint",
47:                         "index": int(i),
48:                         "time": int(i),
49:                         "score": float(rate),
50:                     }
51:                 )
52:                 break
53: 
54:         artifacts_dir = ctx.artifacts_dir("analysis_online_conformal_changepoint")
55:         alerts_path = artifacts_dir / "alerts.json"
56:         write_json(alerts_path, changepoints)
57:         artifacts = [
58:             PluginArtifact(
59:                 path=str(alerts_path.relative_to(ctx.run_dir)),
60:                 type="json",
61:                 description="Changepoints",
62:             )
63:         ]
64:         return PluginResult(
65:             "ok",
66:             "Detected changepoints",
67:             {"count": len(changepoints)},
68:             changepoints,
69:             artifacts,
70:             None,
71:         )
````

## File: plugins/analysis_online_conformal_changepoint/plugin.yaml
````yaml
 1: id: analysis_online_conformal_changepoint
 2: name: Online Conformal Changepoint
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Rolling mean conformal changepoint detection
12:   defaults:
13:     time_column: null
14:     value_column: null
15:     alpha: 0.1
16:     forecast_window: 5
17:     calib_window: 10
18:     alarm_rate_window: 10
19:     alarm_rate_threshold: 0.4
20: config_schema: config.schema.json
21: output_schema: output.schema.json
22: sandbox:
23:   no_network: true
24:   fs_allowlist:
25:   - appdata
26:   - plugins
````

## File: plugins/analysis_scan_statistics/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/analysis_scan_statistics/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "additionalProperties": false,
 4:   "properties": {
 5:     "max_window": {
 6:       "minimum": 1,
 7:       "type": "integer"
 8:     },
 9:     "min_window": {
10:       "minimum": 1,
11:       "type": "integer"
12:     },
13:     "n_permutations": {
14:       "minimum": 1,
15:       "type": "integer"
16:     },
17:     "max_rows": {
18:       "minimum": 1,
19:       "type": "integer"
20:     },
21:     "value_column": {
22:       "type": [
23:         "string",
24:         "null"
25:       ]
26:     }
27:   },
28:   "title": "Scan Statistics Config",
29:   "type": "object"
30: }
````

## File: plugins/analysis_scan_statistics/output.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "additionalProperties": false,
  4:   "properties": {
  5:     "artifacts": {
  6:       "items": {
  7:         "additionalProperties": false,
  8:         "properties": {
  9:           "description": {
 10:             "type": "string"
 11:           },
 12:           "path": {
 13:             "type": "string"
 14:           },
 15:           "type": {
 16:             "type": "string"
 17:           }
 18:         },
 19:         "required": [
 20:           "path",
 21:           "type",
 22:           "description"
 23:         ],
 24:         "type": "object"
 25:       },
 26:       "type": "array"
 27:     },
 28:     "budget": {
 29:       "additionalProperties": true,
 30:       "properties": {
 31:         "cpu_limit_ms": {
 32:           "anyOf": [
 33:             {
 34:               "type": "integer"
 35:             },
 36:             {
 37:               "type": "null"
 38:             }
 39:           ]
 40:         },
 41:         "notes": {
 42:           "anyOf": [
 43:             {
 44:               "type": "string"
 45:             },
 46:             {
 47:               "type": "null"
 48:             }
 49:           ]
 50:         },
 51:         "row_limit": {
 52:           "anyOf": [
 53:             {
 54:               "type": "integer"
 55:             },
 56:             {
 57:               "type": "null"
 58:             }
 59:           ]
 60:         },
 61:         "sampled": {
 62:           "type": "boolean"
 63:         },
 64:         "time_limit_ms": {
 65:           "anyOf": [
 66:             {
 67:               "type": "integer"
 68:             },
 69:             {
 70:               "type": "null"
 71:             }
 72:           ]
 73:         }
 74:       },
 75:       "required": [
 76:         "row_limit",
 77:         "sampled",
 78:         "time_limit_ms",
 79:         "cpu_limit_ms"
 80:       ],
 81:       "type": "object"
 82:     },
 83:     "error": {
 84:       "anyOf": [
 85:         {
 86:           "type": "null"
 87:         },
 88:         {
 89:           "additionalProperties": false,
 90:           "properties": {
 91:             "message": {
 92:               "type": "string"
 93:             },
 94:             "traceback": {
 95:               "type": "string"
 96:             },
 97:             "type": {
 98:               "type": "string"
 99:             }
100:           },
101:           "required": [
102:             "type",
103:             "message",
104:             "traceback"
105:           ],
106:           "type": "object"
107:         }
108:       ]
109:     },
110:     "findings": {
111:       "items": {
112:         "oneOf": [
113:           {
114:             "allOf": [
115:               {
116:                 "additionalProperties": true,
117:                 "properties": {
118:                   "evidence": {
119:                     "additionalProperties": true,
120:                     "properties": {
121:                       "column_ids": {
122:                         "items": {
123:                           "type": "integer"
124:                         },
125:                         "type": "array"
126:                       },
127:                       "dataset_id": {
128:                         "type": "string"
129:                       },
130:                       "dataset_version_id": {
131:                         "type": "string"
132:                       },
133:                       "query": {
134:                         "type": [
135:                           "string",
136:                           "null"
137:                         ]
138:                       },
139:                       "row_ids": {
140:                         "items": {
141:                           "type": "integer"
142:                         },
143:                         "type": "array"
144:                       },
145:                       "row_ranges": {
146:                         "items": {
147:                           "additionalProperties": false,
148:                           "properties": {
149:                             "end": {
150:                               "type": "integer"
151:                             },
152:                             "start": {
153:                               "type": "integer"
154:                             }
155:                           },
156:                           "required": [
157:                             "start",
158:                             "end"
159:                           ],
160:                           "type": "object"
161:                         },
162:                         "type": "array"
163:                       }
164:                     },
165:                     "required": [
166:                       "dataset_id",
167:                       "dataset_version_id",
168:                       "row_ids",
169:                       "column_ids"
170:                     ],
171:                     "type": "object"
172:                   },
173:                   "kind": {
174:                     "type": "string"
175:                   }
176:                 },
177:                 "required": [
178:                   "kind",
179:                   "evidence"
180:                 ],
181:                 "type": "object"
182:               },
183:               {
184:                 "additionalProperties": true,
185:                 "properties": {
186:                   "end": {
187:                     "type": "integer"
188:                   },
189:                   "evidence": {
190:                     "additionalProperties": true,
191:                     "properties": {
192:                       "column_ids": {
193:                         "items": {
194:                           "type": "integer"
195:                         },
196:                         "type": "array"
197:                       },
198:                       "dataset_id": {
199:                         "type": "string"
200:                       },
201:                       "dataset_version_id": {
202:                         "type": "string"
203:                       },
204:                       "query": {
205:                         "type": [
206:                           "string",
207:                           "null"
208:                         ]
209:                       },
210:                       "row_ids": {
211:                         "items": {
212:                           "type": "integer"
213:                         },
214:                         "type": "array"
215:                       },
216:                       "row_ranges": {
217:                         "items": {
218:                           "additionalProperties": false,
219:                           "properties": {
220:                             "end": {
221:                               "type": "integer"
222:                             },
223:                             "start": {
224:                               "type": "integer"
225:                             }
226:                           },
227:                           "required": [
228:                             "start",
229:                             "end"
230:                           ],
231:                           "type": "object"
232:                         },
233:                         "type": "array"
234:                       }
235:                     },
236:                     "required": [
237:                       "dataset_id",
238:                       "dataset_version_id",
239:                       "row_ids",
240:                       "column_ids"
241:                     ],
242:                     "type": "object"
243:                   },
244:                   "kind": {
245:                     "const": "cluster"
246:                   },
247:                   "p_value": {
248:                     "type": "number"
249:                   },
250:                   "score": {
251:                     "type": "number"
252:                   },
253:                   "start": {
254:                     "type": "integer"
255:                   }
256:                 },
257:                 "required": [
258:                   "kind",
259:                   "start",
260:                   "end",
261:                   "score",
262:                   "p_value"
263:                 ],
264:                 "type": "object"
265:               }
266:             ]
267:           }
268:         ],
269:         "properties": {
270:           "confidence": {
271:             "type": "number"
272:           },
273:           "q_value": {
274:             "type": "number"
275:           }
276:         }
277:       },
278:       "type": "array"
279:     },
280:     "metrics": {
281:       "additionalProperties": false,
282:       "properties": {
283:         "count": {
284:           "type": "integer"
285:         },
286:         "rows_used": {
287:           "type": "integer"
288:         },
289:         "sampled": {
290:           "type": "boolean"
291:         }
292:       },
293:       "type": "object"
294:     },
295:     "status": {
296:       "enum": [
297:         "ok",
298:         "skipped",
299:         "error"
300:       ],
301:       "type": "string"
302:     },
303:     "summary": {
304:       "type": "string"
305:     }
306:   },
307:   "required": [
308:     "status",
309:     "summary",
310:     "metrics",
311:     "findings",
312:     "artifacts",
313:     "budget"
314:   ],
315:   "title": "Plugin Result",
316:   "type": "object"
317: }
````

## File: plugins/analysis_sequence_classification/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.types import PluginArtifact, PluginResult
  8: from statistic_harness.core.utils import write_json
  9: 
 10: 
 11: INVALID_STRINGS = {"", "nan", "none", "null"}
 12: MAX_SAMPLE_ROWS = 50000
 13: MIN_PROCESS_ALPHA_RATIO = 0.15
 14: MIN_DEP_OVERLAP_RATIO = 0.02
 15: MIN_DEP_OVERLAP_COUNT = 10
 16: 
 17: 
 18: def _candidate_columns(
 19:     preferred: str | None,
 20:     columns: list[str],
 21:     role_by_name: dict[str, str],
 22:     roles: set[str],
 23:     patterns: list[str],
 24:     lower_names: dict[str, str],
 25: ) -> list[str]:
 26:     seen: set[str] = set()
 27:     candidates: list[str] = []
 28:     if preferred and preferred in columns and preferred not in seen:
 29:         candidates.append(preferred)
 30:         seen.add(preferred)
 31:     for col in columns:
 32:         if role_by_name.get(col) in roles and col not in seen:
 33:             candidates.append(col)
 34:             seen.add(col)
 35:     for col in columns:
 36:         if col in seen:
 37:             continue
 38:         name = lower_names[col]
 39:         if any(pattern in name for pattern in patterns):
 40:             candidates.append(col)
 41:             seen.add(col)
 42:     return candidates
 43: 
 44: 
 45: def _parse_list(value: Any) -> list[str]:
 46:     if value is None:
 47:         return []
 48:     if isinstance(value, str):
 49:         return [item.strip() for item in value.split(",") if item.strip()]
 50:     if isinstance(value, (list, tuple, set)):
 51:         return [str(item).strip() for item in value if str(item).strip()]
 52:     return []
 53: 
 54: 
 55: def _sample_series(series: pd.Series, max_rows: int = MAX_SAMPLE_ROWS) -> pd.Series:
 56:     if len(series) > max_rows:
 57:         return series.head(max_rows)
 58:     return series
 59: 
 60: 
 61: def _normalize_values(series: pd.Series) -> pd.Series:
 62:     sample = _sample_series(series)
 63:     values = sample.dropna().astype(str).str.strip()
 64:     if values.empty:
 65:         return values
 66:     values = values[~values.str.lower().isin(INVALID_STRINGS)]
 67:     return values
 68: 
 69: 
 70: def _score_process_column(series: pd.Series) -> tuple[float, float]:
 71:     values = _normalize_values(series)
 72:     if values.empty:
 73:         return -1.0, 0.0
 74:     alpha_ratio = float(values.str.contains(r"[A-Za-z]", regex=True).mean())
 75:     digit_ratio = float(values.str.match(r"^\\d+(\\.0+)?$").mean())
 76:     unique_ratio = float(values.nunique() / max(len(values), 1))
 77:     score = alpha_ratio * 3.0 - digit_ratio * 2.0 + min(unique_ratio, 1.0) * 0.2
 78:     return score, alpha_ratio
 79: 
 80: 
 81: def _best_process_column(candidates: list[str], df: pd.DataFrame) -> str | None:
 82:     best_col = None
 83:     best_score = -1.0
 84:     best_alpha = 0.0
 85:     for col in candidates:
 86:         score, alpha = _score_process_column(df[col])
 87:         if score > best_score:
 88:             best_col = col
 89:             best_score = score
 90:             best_alpha = alpha
 91:     if best_col is None or best_alpha < MIN_PROCESS_ALPHA_RATIO:
 92:         return None
 93:     return best_col
 94: 
 95: 
 96: def _select_dependency_columns(
 97:     candidates: list[str], id_values: set[str], df: pd.DataFrame
 98: ) -> list[str]:
 99:     row_count = len(df)
100:     min_overlap_count = 1 if row_count < 500 else MIN_DEP_OVERLAP_COUNT
101:     if not id_values:
102:         if row_count > 1000:
103:             return []
104:         selected: list[str] = []
105:         for col in candidates:
106:             values = _normalize_values(df[col])
107:             if values.empty:
108:                 continue
109:             if values.shape[0] < 1:
110:                 continue
111:             selected.append(col)
112:         return selected
113:     selected: list[str] = []
114:     for col in candidates:
115:         values = _normalize_values(df[col])
116:         if values.empty:
117:             continue
118:         dep_set = set(values.tolist())
119:         if len(dep_set) < min_overlap_count:
120:             continue
121:         overlap_count = len(dep_set & id_values)
122:         if overlap_count < min_overlap_count:
123:             continue
124:         ratio = overlap_count / max(len(dep_set), 1)
125:         if ratio >= MIN_DEP_OVERLAP_RATIO:
126:             selected.append(col)
127:     return selected
128: 
129: 
130: class Plugin:
131:     def run(self, ctx) -> PluginResult:
132:         df = ctx.dataset_loader()
133:         if df.empty:
134:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
135: 
136:         columns_meta = []
137:         role_by_name: dict[str, str] = {}
138:         if ctx.dataset_version_id:
139:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
140:             if dataset_template and dataset_template.get("status") == "ready":
141:                 fields = ctx.storage.fetch_template_fields(
142:                     int(dataset_template["template_id"])
143:                 )
144:                 columns_meta = fields
145:                 role_by_name = {
146:                     field["name"]: (field.get("role") or "") for field in fields
147:                 }
148:             else:
149:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
150:                 role_by_name = {
151:                     col["original_name"]: (col.get("role") or "")
152:                     for col in columns_meta
153:                 }
154: 
155:         columns = list(df.columns)
156:         lower_names = {col: str(col).lower() for col in columns}
157: 
158:         process_candidates = _candidate_columns(
159:             ctx.settings.get("process_column"),
160:             columns,
161:             role_by_name,
162:             {"process", "activity", "event", "step", "task", "action"},
163:             ["process", "activity", "event", "step", "task", "action", "job"],
164:             lower_names,
165:         )
166:         process_col = _best_process_column(process_candidates, df)
167: 
168:         dep_cols = _parse_list(ctx.settings.get("dependency_columns"))
169:         dep_candidates: list[str]
170:         if dep_cols:
171:             dep_candidates = [col for col in dep_cols if col in columns]
172:         else:
173:             dep_tokens = ("dep", "dependency", "parent", "master", "preced")
174:             dep_candidates = [
175:                 col
176:                 for col in columns
177:                 if any(tok in lower_names[col] for tok in dep_tokens)
178:             ]
179: 
180:         id_candidates = _candidate_columns(
181:             None,
182:             columns,
183:             role_by_name,
184:             {"process_id", "id"},
185:             ["process_id", "proc_id", "queue_id", "job_id", "run_id", "id"],
186:             lower_names,
187:         )
188:         id_values: set[str] = set()
189:         for col in id_candidates:
190:             id_values |= set(_normalize_values(df[col]).tolist())
191: 
192:         dep_cols = _select_dependency_columns(dep_candidates, id_values, df)
193: 
194:         if not dep_cols:
195:             return PluginResult(
196:                 "ok",
197:                 "No dependency columns detected",
198:                 {
199:                     "standalone_runs": int(df.shape[0]),
200:                     "sequence_runs": 0,
201:                 },
202:                 [
203:                     {
204:                         "kind": "sequence_classification",
205:                         "measurement_type": "not_applicable",
206:                         "reason": "No dependency pointer columns were detected.",
207:                         "columns": [col for col in [process_col] if col],
208:                     }
209:                 ],
210:                 [],
211:                 None,
212:             )
213: 
214:         work = df.loc[:, [col for col in [process_col, *dep_cols] if col]].copy()
215:         if process_col and process_col in work.columns:
216:             work["__process"] = work[process_col].astype(str).str.strip()
217:             work["__process_norm"] = work["__process"].str.lower()
218:         else:
219:             work["__process"] = "unknown"
220:             work["__process_norm"] = "unknown"
221: 
222:         sequence_mask = pd.Series(False, index=work.index)
223:         for col in dep_cols:
224:             series = work[col]
225:             mask = series.notna()
226:             if mask.any():
227:                 text = series.astype(str).str.strip().str.lower()
228:                 mask = mask & (~text.isin(INVALID_STRINGS))
229:             sequence_mask |= mask
230: 
231:         work["__sequence"] = sequence_mask
232:         work["__standalone"] = ~sequence_mask
233: 
234:         total_runs = int(work.shape[0])
235:         sequence_runs = int(sequence_mask.sum())
236:         standalone_runs = total_runs - sequence_runs
237: 
238:         summaries = (
239:             work.groupby("__process_norm")
240:             .agg(
241:                 runs=("__process_norm", "size"),
242:                 sequence_runs=("__sequence", "sum"),
243:             )
244:             .reset_index()
245:         )
246:         summaries["sequence_ratio"] = summaries["sequence_runs"] / summaries["runs"].replace(0, 1)
247:         summaries = summaries.sort_values(
248:             ["sequence_ratio", "__process_norm"], ascending=[False, True]
249:         )
250: 
251:         max_processes = int(ctx.settings.get("max_processes", 10))
252:         min_ratio = float(ctx.settings.get("min_sequence_ratio", 0.0))
253:         max_examples = int(ctx.settings.get("max_examples", 25))
254:         columns_used = [col for col in [process_col, *dep_cols] if col]
255: 
256:         findings = []
257:         for _, row in summaries.iterrows():
258:             ratio = float(row["sequence_ratio"])
259:             if ratio < min_ratio:
260:                 continue
261:             proc = row["__process_norm"]
262:             label_series = work.loc[work["__process_norm"] == proc, "__process"]
263:             label = (
264:                 label_series.value_counts().index[0]
265:                 if not label_series.empty
266:                 else proc
267:             )
268:             row_ids = work.loc[
269:                 (work["__process_norm"] == proc) & work["__sequence"]
270:             ].index.tolist()[:max_examples]
271:             findings.append(
272:                 {
273:                     "kind": "sequence_classification",
274:                     "process": label,
275:                     "process_norm": proc,
276:                     "runs_total": int(row["runs"]),
277:                     "sequence_runs": int(row["sequence_runs"]),
278:                     "standalone_runs": int(row["runs"] - row["sequence_runs"]),
279:                     "sequence_ratio": ratio,
280:                     "measurement_type": "measured",
281:                     "row_ids": [int(i) for i in row_ids],
282:                     "dependency_columns": dep_cols,
283:                     "columns": columns_used,
284:                 }
285:             )
286:             if len(findings) >= max_processes:
287:                 break
288: 
289:         artifacts_dir = ctx.artifacts_dir("analysis_sequence_classification")
290:         out_path = artifacts_dir / "classification.json"
291:         write_json(
292:             out_path,
293:             {
294:                 "summary": {
295:                     "process_column": process_col,
296:                     "dependency_columns": dep_cols,
297:                 },
298:                 "totals": {
299:                     "total_runs": total_runs,
300:                     "sequence_runs": sequence_runs,
301:                     "standalone_runs": standalone_runs,
302:                 },
303:                 "by_process": [
304:                     {
305:                         "process_norm": row["__process_norm"],
306:                         "runs": int(row["runs"]),
307:                         "sequence_runs": int(row["sequence_runs"]),
308:                         "sequence_ratio": float(row["sequence_ratio"]),
309:                     }
310:                     for _, row in summaries.iterrows()
311:                 ],
312:             },
313:         )
314:         artifacts = [
315:             PluginArtifact(
316:                 path=str(out_path.relative_to(ctx.run_dir)),
317:                 type="json",
318:                 description="Standalone vs sequence classification",
319:             )
320:         ]
321: 
322:         return PluginResult(
323:             "ok",
324:             "Classified standalone vs sequence-linked runs",
325:             {
326:                 "standalone_runs": standalone_runs,
327:                 "sequence_runs": sequence_runs,
328:             },
329:             findings,
330:             artifacts,
331:             None,
332:         )
````

## File: plugins/ingest_tabular/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/ingest_tabular/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: import hashlib
  4: import json
  5: from pathlib import Path
  6: from typing import Any, Iterable
  7: 
  8: import pandas as pd
  9: 
 10: from statistic_harness.core.types import PluginArtifact, PluginResult
 11: from statistic_harness.core.utils import json_dumps, now_iso, write_json
 12: 
 13: 
 14: class Plugin:
 15:     def run(self, ctx) -> PluginResult:
 16:         input_file = ctx.settings.get("input_file")
 17:         if not input_file:
 18:             raise ValueError("Input file not found")
 19:         path = Path(input_file)
 20:         if not path.exists():
 21:             raise ValueError("Input file not found")
 22: 
 23:         encoding = ctx.settings.get("encoding", "utf-8")
 24:         delimiter = ctx.settings.get("delimiter")
 25:         sheet_name = ctx.settings.get("sheet_name")
 26:         chunk_size = int(ctx.settings.get("chunk_size", 1000))
 27: 
 28:         dataset_version_id = ctx.dataset_version_id
 29:         if not dataset_version_id:
 30:             raise ValueError("Dataset version not provided")
 31:         existing = ctx.storage.get_dataset_version(dataset_version_id)
 32:         if existing and int(existing.get("row_count") or 0) > 0:
 33:             columns = ctx.storage.fetch_dataset_columns(dataset_version_id)
 34:             schema = {
 35:                 "columns": [
 36:                     {
 37:                         "name": col["original_name"],
 38:                         "dtype": col.get("dtype"),
 39:                     }
 40:                     for col in columns
 41:                 ]
 42:             }
 43:             schema_path = ctx.run_dir / "dataset" / "schema.json"
 44:             write_json(schema_path, schema)
 45:             return PluginResult(
 46:                 status="ok",
 47:                 summary=f"Dataset already ingested ({existing['row_count']} rows)",
 48:                 metrics={
 49:                     "rows": int(existing.get("row_count") or 0),
 50:                     "cols": int(existing.get("column_count") or 0),
 51:                 },
 52:                 findings=[],
 53:                 artifacts=[
 54:                     PluginArtifact(
 55:                         path=str(schema_path.relative_to(ctx.run_dir)),
 56:                         type="json",
 57:                         description="Schema",
 58:                     )
 59:                 ],
 60:                 error=None,
 61:             )
 62: 
 63:         def dtype_to_sqlite(dtype: Any) -> str:
 64:             if pd.api.types.is_integer_dtype(dtype):
 65:                 return "INTEGER"
 66:             if pd.api.types.is_float_dtype(dtype):
 67:                 return "REAL"
 68:             if pd.api.types.is_bool_dtype(dtype):
 69:                 return "INTEGER"
 70:             return "TEXT"
 71: 
 72:         def normalize_value(value: Any) -> Any:
 73:             if pd.isna(value):
 74:                 return None
 75:             if isinstance(value, pd.Timestamp):
 76:                 return value.isoformat()
 77:             return value
 78: 
 79:         def build_columns(columns: list[str], dtypes: list[Any]) -> list[dict[str, Any]]:
 80:             safe_names = [f"c{idx+1}" for idx in range(len(columns))]
 81:             out = []
 82:             for idx, (orig, safe, dtype) in enumerate(zip(columns, safe_names, dtypes), start=1):
 83:                 out.append(
 84:                     {
 85:                         "column_id": idx,
 86:                         "safe_name": safe,
 87:                         "original_name": str(orig),
 88:                         "dtype": str(dtype),
 89:                         "sqlite_type": dtype_to_sqlite(dtype),
 90:                     }
 91:                 )
 92:             return out
 93: 
 94:         def iter_csv_chunks() -> Iterable[pd.DataFrame]:
 95:             return pd.read_csv(
 96:                 path, delimiter=delimiter, encoding=encoding, chunksize=chunk_size
 97:             )
 98: 
 99:         def iter_json_chunks() -> Iterable[pd.DataFrame]:
100:             try:
101:                 for chunk in pd.read_json(path, lines=True, chunksize=chunk_size):
102:                     yield chunk
103:                 return
104:             except ValueError:
105:                 df = pd.read_json(path)
106:                 yield df
107: 
108:         def iter_excel_rows() -> Iterable[list[Any]]:
109:             import mimetypes
110: 
111:             # Prevent mimetypes from probing system files blocked by the sandbox.
112:             mimetypes.knownfiles = []
113:             from openpyxl import load_workbook
114: 
115:             wb = load_workbook(path, read_only=True, data_only=True)
116:             ws = wb[sheet_name] if sheet_name else wb.active
117:             rows = ws.iter_rows(values_only=True)
118:             try:
119:                 headers = next(rows)
120:             except StopIteration:
121:                 return
122:             headers = [
123:                 (str(h) if h is not None else f"column_{idx+1}")
124:                 for idx, h in enumerate(headers)
125:             ]
126:             yield headers
127:             for row in rows:
128:                 yield list(row)
129: 
130:         columns_meta: list[dict[str, Any]] = []
131:         row_index = 0
132:         table_name = f"dataset_{dataset_version_id}"
133: 
134:         with ctx.storage.connection() as conn:
135:             if path.suffix.lower() in {".xlsx"}:
136:                 row_iter = iter_excel_rows()
137:                 headers = next(row_iter, None)
138:                 if headers is None:
139:                     raise ValueError("No rows found")
140:                 sample = []
141:                 for _ in range(chunk_size):
142:                     row = next(row_iter, None)
143:                     if row is None:
144:                         break
145:                     sample.append(row)
146:                 sample_df = pd.DataFrame(sample, columns=headers)
147:                 columns_meta = build_columns(
148:                     list(sample_df.columns), list(sample_df.dtypes)
149:                 )
150:                 ctx.storage.create_dataset_table(table_name, columns_meta, conn)
151:                 ctx.storage.add_append_only_triggers(table_name, conn)
152:                 ctx.storage.replace_dataset_columns(dataset_version_id, columns_meta, conn)
153: 
154:                 def emit_rows(rows: list[list[Any]]) -> None:
155:                     nonlocal row_index
156:                     safe_columns = [col["safe_name"] for col in columns_meta]
157:                     batch = []
158:                     for row in rows:
159:                         values = [normalize_value(v) for v in row]
160:                         row_dict = dict(zip(headers, values))
161:                         row_json = json.dumps(row_dict, ensure_ascii=False)
162:                         batch.append((row_index, row_json, *values))
163:                         row_index += 1
164:                     ctx.storage.insert_dataset_rows(table_name, safe_columns, batch, conn)
165: 
166:                 emit_rows(sample)
167:                 buffer = []
168:                 for row in row_iter:
169:                     buffer.append(row)
170:                     if len(buffer) >= chunk_size:
171:                         emit_rows(buffer)
172:                         buffer = []
173:                 if buffer:
174:                     emit_rows(buffer)
175:                 ctx.storage.update_dataset_version_stats(
176:                     dataset_version_id,
177:                     row_index,
178:                     len(columns_meta),
179:                     conn,
180:                 )
181:             elif path.suffix.lower() == ".json":
182:                 for chunk in iter_json_chunks():
183:                     if not columns_meta:
184:                         columns_meta = build_columns(
185:                             list(chunk.columns), list(chunk.dtypes)
186:                         )
187:                         ctx.storage.create_dataset_table(table_name, columns_meta, conn)
188:                         ctx.storage.add_append_only_triggers(table_name, conn)
189:                         ctx.storage.replace_dataset_columns(
190:                             dataset_version_id, columns_meta, conn
191:                         )
192:                     safe_columns = [col["safe_name"] for col in columns_meta]
193:                     batch = []
194:                     for row in chunk.itertuples(index=False, name=None):
195:                         values = [normalize_value(v) for v in row]
196:                         row_dict = dict(zip(chunk.columns, values))
197:                         row_json = json.dumps(row_dict, ensure_ascii=False)
198:                         batch.append((row_index, row_json, *values))
199:                         row_index += 1
200:                     ctx.storage.insert_dataset_rows(table_name, safe_columns, batch, conn)
201:                 ctx.storage.update_dataset_version_stats(
202:                     dataset_version_id, row_index, len(columns_meta), conn
203:                 )
204:             else:
205:                 for chunk in iter_csv_chunks():
206:                     if not columns_meta:
207:                         columns_meta = build_columns(
208:                             list(chunk.columns), list(chunk.dtypes)
209:                         )
210:                         ctx.storage.create_dataset_table(table_name, columns_meta, conn)
211:                         ctx.storage.add_append_only_triggers(table_name, conn)
212:                         ctx.storage.replace_dataset_columns(
213:                             dataset_version_id, columns_meta, conn
214:                         )
215:                     safe_columns = [col["safe_name"] for col in columns_meta]
216:                     batch = []
217:                     for row in chunk.itertuples(index=False, name=None):
218:                         values = [normalize_value(v) for v in row]
219:                         row_dict = dict(zip(chunk.columns, values))
220:                         row_json = json.dumps(row_dict, ensure_ascii=False)
221:                         batch.append((row_index, row_json, *values))
222:                         row_index += 1
223:                     ctx.storage.insert_dataset_rows(table_name, safe_columns, batch, conn)
224:                 ctx.storage.update_dataset_version_stats(
225:                     dataset_version_id, row_index, len(columns_meta), conn
226:                 )
227: 
228:         schema = {
229:             "columns": [
230:                 {
231:                     "name": col["original_name"],
232:                     "dtype": col.get("dtype"),
233:                 }
234:                 for col in columns_meta
235:             ]
236:         }
237:         schema_path = ctx.run_dir / "dataset" / "schema.json"
238:         write_json(schema_path, schema)
239: 
240:         if ctx.dataset_version_id:
241:             fingerprint_payload = [
242:                 {
243:                     "name": col["original_name"].lower().strip(),
244:                     "dtype": col.get("dtype"),
245:                 }
246:                 for col in columns_meta
247:             ]
248:             fingerprint_payload = sorted(
249:                 fingerprint_payload, key=lambda item: item["name"]
250:             )
251:             fingerprint = hashlib.sha256(
252:                 json_dumps(fingerprint_payload).encode("utf-8")
253:             ).hexdigest()
254:             format_id = ctx.storage.ensure_raw_format(
255:                 fingerprint, name=None, created_at=now_iso()
256:             )
257:             ctx.storage.set_dataset_raw_format(ctx.dataset_version_id, format_id)
258: 
259:         return PluginResult(
260:             status="ok",
261:             summary=f"Ingested {row_index} rows",
262:             metrics={"rows": int(row_index), "cols": int(len(columns_meta))},
263:             findings=[],
264:             artifacts=[
265:                 PluginArtifact(
266:                     path=str(schema_path.relative_to(ctx.run_dir)),
267:                     type="json",
268:                     description="Schema",
269:                 )
270:             ],
271:             error=None,
272:         )
````

## File: plugins/ingest_tabular/plugin.yaml
````yaml
 1: id: ingest_tabular
 2: name: Ingest Tabular
 3: version: 0.1.0
 4: type: ingest
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Read tabular files into canonical CSV
 9:   defaults:
10:     delimiter: null
11:     sheet_name: null
12:     encoding: utf-8
13:     chunk_size: 1000
14: capabilities: []
15: config_schema: config.schema.json
16: output_schema: output.schema.json
17: sandbox:
18:   no_network: true
19:   fs_allowlist:
20:   - appdata
21:   - plugins
````

## File: plugins/ingest_tabular/README.md
````markdown
1: # Ingest Tabular
2: 
3: Reads CSV/TSV/XLSX/JSON files into canonical CSV.
````

## File: plugins/llm_prompt_builder/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/llm_prompt_builder/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import json
 4: 
 5: from statistic_harness.core.types import PluginArtifact, PluginResult
 6: from statistic_harness.core.utils import DEFAULT_TENANT_ID
 7: 
 8: 
 9: class Plugin:
10:     def run(self, ctx) -> PluginResult:
11:         report_path = ctx.run_dir / "report.json"
12:         report = json.loads(report_path.read_text(encoding="utf-8"))
13:         tenant_id = ctx.tenant_id or DEFAULT_TENANT_ID
14:         pii_entities = ctx.storage.fetch_pii_entities(tenant_id)
15:         pii_map: dict[str, list[tuple[str, str]]] = {}
16:         for entry in pii_entities:
17:             raw = str(entry.get("raw_value", ""))
18:             pii_type = str(entry.get("pii_type", "pii"))
19:             value_hash = str(entry.get("value_hash", ""))
20:             if raw:
21:                 pii_map.setdefault(raw, []).append((pii_type, value_hash))
22: 
23:         def _sanitize_value(value):
24:             if isinstance(value, str) and value in pii_map:
25:                 types = sorted({t for t, _ in pii_map[value] if t})
26:                 value_hash = pii_map[value][0][1] if pii_map[value] else ""
27:                 label = ",".join(types) if types else "pii"
28:                 return f"pii:{label}:{value_hash}"
29:             if isinstance(value, dict):
30:                 sanitized = {}
31:                 for key, val in value.items():
32:                     new_key = (
33:                         _sanitize_value(key)
34:                         if isinstance(key, str) and key in pii_map
35:                         else key
36:                     )
37:                     sanitized[new_key] = _sanitize_value(val)
38:                 return sanitized
39:             if isinstance(value, list):
40:                 return [_sanitize_value(item) for item in value]
41:             return value
42: 
43:         sanitized_report = _sanitize_value(report)
44:         artifacts_dir = ctx.artifacts_dir("llm_prompt_builder")
45:         prompt = "# Analysis Summary\n\n" + json.dumps(sanitized_report, indent=2)
46:         brief = "# Brief\n\nSummary of findings."
47:         prompt_path = artifacts_dir / "prompt.md"
48:         brief_path = artifacts_dir / "brief.md"
49:         prompt_path.write_text(prompt, encoding="utf-8")
50:         brief_path.write_text(brief, encoding="utf-8")
51:         artifacts = [
52:             PluginArtifact(
53:                 path=str(prompt_path.relative_to(ctx.run_dir)),
54:                 type="markdown",
55:                 description="Prompt",
56:             ),
57:             PluginArtifact(
58:                 path=str(brief_path.relative_to(ctx.run_dir)),
59:                 type="markdown",
60:                 description="Brief",
61:             ),
62:         ]
63:         return PluginResult("ok", "Built LLM prompt", {}, [], artifacts, None)
````

## File: plugins/llm_prompt_builder/plugin.yaml
````yaml
 1: id: llm_prompt_builder
 2: name: LLM Prompt Builder
 3: version: 0.1.0
 4: type: llm
 5: entrypoint: plugin.py:Plugin
 6: depends_on:
 7: - report_bundle
 8: settings:
 9:   description: Offline prompt builder
10:   defaults: {}
11: capabilities: []
12: config_schema: config.schema.json
13: output_schema: output.schema.json
14: sandbox:
15:   no_network: true
16:   fs_allowlist:
17:   - appdata
18:   - plugins
````

## File: plugins/planner_basic/plugin.yaml
````yaml
 1: id: planner_basic
 2: name: Planner Basic
 3: version: 0.1.0
 4: type: planner
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Deterministic plugin selection
 9:   defaults:
10:     allow: []
11:     deny: []
12: capabilities: []
13: config_schema: config.schema.json
14: output_schema: output.schema.json
15: sandbox:
16:   no_network: true
17:   fs_allowlist:
18:   - appdata
19:   - plugins
20:   - docs
````

## File: plugins/profile_basic/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/profile_basic/plugin.yaml
````yaml
 1: id: profile_basic
 2: name: Profile Basic
 3: version: 0.1.0
 4: type: profile
 5: entrypoint: plugin.py:Plugin
 6: depends_on:
 7: - ingest_tabular
 8: settings:
 9:   description: Basic profiling
10:   defaults:
11:     max_corr_cols: 10
12: capabilities: []
13: config_schema: config.schema.json
14: output_schema: output.schema.json
15: sandbox:
16:   no_network: true
17:   fs_allowlist:
18:   - appdata
19:   - plugins
````

## File: plugins/profile_basic/README.md
````markdown
1: # Profile Basic
2: 
3: Computes column statistics and correlations.
````

## File: plugins/profile_eventlog/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: import re
  4: from typing import Any
  5: 
  6: import pandas as pd
  7: 
  8: from statistic_harness.core.column_inference import infer_timestamp_series
  9: from statistic_harness.core.types import PluginResult
 10: from statistic_harness.core.utils import quote_identifier
 11: 
 12: ROLE_KEYS = [
 13:     "queue_time",
 14:     "start_time",
 15:     "end_time",
 16:     "process_id",
 17:     "process_name",
 18:     "module_code",
 19:     "user_id",
 20:     "dependency_id",
 21:     "master_id",
 22:     "host_id",
 23:     "status",
 24: ]
 25: 
 26: ROLE_TOKENS = {
 27:     "queue_time": ["queue", "queued", "enqueue"],
 28:     "start_time": ["start", "begin"],
 29:     "end_time": ["end", "finish", "complete", "stop"],
 30:     "process_id": ["process_id", "proc_id", "processqueue", "queue_id", "processqueueid"],
 31:     "process_name": ["process", "job", "task", "activity", "step", "action", "proc"],
 32:     "module_code": ["module", "module_cd", "module_code", "mod"],
 33:     "user_id": ["user", "userid", "user_id", "operator", "owner"],
 34:     "dependency_id": ["dep", "dependency", "parent", "prereq", "precede"],
 35:     "master_id": ["master", "sequence", "chain", "workflow", "batch", "group", "case"],
 36:     "host_id": ["host", "server", "node", "qpec", "worker", "machine"],
 37:     "status": ["status", "state", "result", "outcome"],
 38: }
 39: 
 40: TOKEN_SPLIT = re.compile(r"[^a-zA-Z0-9]+")
 41: 
 42: 
 43: def _tokenize(name: str) -> list[str]:
 44:     raw = TOKEN_SPLIT.split(name.lower())
 45:     return [token for token in raw if token]
 46: 
 47: 
 48: def _numeric_ratio(values: pd.Series) -> float:
 49:     if values.empty:
 50:         return 0.0
 51:     numeric = pd.to_numeric(values, errors="coerce")
 52:     return float(numeric.notna().mean())
 53: 
 54: 
 55: def _unique_ratio(values: pd.Series) -> float:
 56:     total = len(values)
 57:     if total == 0:
 58:         return 0.0
 59:     return float(values.nunique(dropna=True) / total)
 60: 
 61: 
 62: def _score_role(role: str, name: str, tokens: list[str], stats: dict[str, float]) -> tuple[float, list[str]]:
 63:     score = 0.0
 64:     reasons: list[str] = []
 65:     lower_name = name.lower()
 66:     token_set = set(tokens)
 67:     role_tokens = ROLE_TOKENS.get(role, [])
 68:     if any(token in token_set for token in role_tokens) or any(t in lower_name for t in role_tokens):
 69:         score += 2.0
 70:         reasons.append("name_match")
 71: 
 72:     if role in {"queue_time", "start_time", "end_time"}:
 73:         if any(token in token_set for token in ["time", "date", "dt"]):
 74:             score += 1.0
 75:             reasons.append("time_token")
 76:         if stats.get("timestamp_valid"):
 77:             if stats["timestamp_ratio"] >= 0.8:
 78:                 score += 2.0
 79:                 reasons.append("timestamp_high")
 80:             elif stats["timestamp_ratio"] >= 0.5:
 81:                 score += 1.0
 82:                 reasons.append("timestamp_medium")
 83:         else:
 84:             score -= 0.8
 85:     else:
 86:         if stats.get("timestamp_valid") and stats["timestamp_ratio"] >= 0.8:
 87:             score -= 0.5
 88: 
 89:     if role in {"process_id", "dependency_id", "master_id", "user_id"}:
 90:         if "id" in token_set or lower_name.endswith("id"):
 91:             score += 1.0
 92:             reasons.append("id_token")
 93:         if stats["unique_ratio"] >= 0.8:
 94:             score += 1.5
 95:             reasons.append("unique_high")
 96:         elif stats["unique_ratio"] >= 0.5:
 97:             score += 1.0
 98:             reasons.append("unique_medium")
 99:         if stats["numeric_ratio"] >= 0.8:
100:             score += 0.5
101:             reasons.append("numeric_high")
102: 
103:     if role in {"process_name", "module_code", "status", "host_id"}:
104:         if stats["unique_ratio"] <= 0.2:
105:             score += 0.5
106:             reasons.append("unique_low")
107:         if stats["numeric_ratio"] >= 0.7:
108:             score -= 0.5
109: 
110:     if role == "process_name" and stats["unique_ratio"] >= 0.6:
111:         score -= 0.5
112:     if role == "status" and stats["unique_ratio"] <= 0.1:
113:         score += 0.5
114: 
115:     return max(score, 0.0), reasons
116: 
117: 
118: class Plugin:
119:     def run(self, ctx) -> PluginResult:
120:         if not ctx.dataset_version_id:
121:             return PluginResult("error", "Missing dataset version", {}, [], [], None)
122: 
123:         sample_rows = int(ctx.settings.get("sample_rows", 500))
124:         min_confidence = float(ctx.settings.get("min_confidence", 2.0))
125: 
126:         columns = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
127:         if not columns:
128:             return PluginResult("skipped", "No columns available", {}, [], [], None)
129: 
130:         with ctx.storage.connection() as conn:
131:             version = ctx.storage.get_dataset_version(ctx.dataset_version_id, conn)
132:             if not version:
133:                 return PluginResult("error", "Dataset version not found", {}, [], [], None)
134:             safe_cols = [col["safe_name"] for col in columns]
135:             if not safe_cols:
136:                 return PluginResult("skipped", "No dataset columns", {}, [], [], None)
137:             quoted = ", ".join(quote_identifier(col) for col in safe_cols)
138:             sql = (
139:                 f"SELECT {quoted} FROM {quote_identifier(version['table_name'])} "
140:                 "ORDER BY row_index LIMIT ?"
141:             )
142:             df = pd.read_sql_query(sql, conn, params=(sample_rows,))
143: 
144:         df = df.rename(columns={col["safe_name"]: col["original_name"] for col in columns})
145:         candidates: list[dict[str, Any]] = []
146:         best_by_column: dict[int, tuple[str, float]] = {}
147:         low_confidence_roles: set[str] = set()
148: 
149:         for col in columns:
150:             name = col["original_name"]
151:             if name not in df.columns:
152:                 continue
153:             series = df[name].dropna()
154:             ts_info = infer_timestamp_series(series, name_hint=name, sample_size=sample_rows)
155:             stats = {
156:                 "timestamp_ratio": ts_info.parse_ratio if ts_info.valid else 0.0,
157:                 "numeric_ratio": _numeric_ratio(series),
158:                 "unique_ratio": _unique_ratio(series),
159:                 "timestamp_valid": ts_info.valid,
160:             }
161:             tokens = _tokenize(name)
162:             best_role = ""
163:             best_score = 0.0
164:             for role in ROLE_KEYS:
165:                 score, reasons = _score_role(role, name, tokens, stats)
166:                 if score <= 0:
167:                     continue
168:                 candidates.append(
169:                     {
170:                         "column_id": col["column_id"],
171:                         "role": role,
172:                         "score": score,
173:                         "reasons": reasons,
174:                     }
175:                 )
176:                 if score > best_score or (score == best_score and role < best_role):
177:                     best_role = role
178:                     best_score = score
179:             if best_role:
180:                 best_by_column[col["column_id"]] = (best_role, best_score)
181: 
182:         ctx.storage.replace_dataset_role_candidates(ctx.dataset_version_id, candidates)
183: 
184:         role_by_name: dict[str, str] = {}
185:         for col in columns:
186:             entry = best_by_column.get(col["column_id"])
187:             if not entry:
188:                 continue
189:             role, score = entry
190:             if score >= min_confidence:
191:                 role_by_name[col["original_name"]] = role
192:             else:
193:                 low_confidence_roles.add(role)
194: 
195:         if role_by_name:
196:             ctx.storage.update_dataset_column_roles(ctx.dataset_version_id, role_by_name)
197: 
198:         findings = []
199:         for col in columns:
200:             entry = best_by_column.get(col["column_id"])
201:             if not entry:
202:                 continue
203:             role, score = entry
204:             measurement_type = "measured" if score >= min_confidence else "not_applicable"
205:             findings.append(
206:                 {
207:                     "kind": "role_inference",
208:                     "role": role,
209:                     "column": col["original_name"],
210:                     "score": score,
211:                     "measurement_type": measurement_type,
212:                 }
213:             )
214: 
215:         for role in sorted(low_confidence_roles):
216:             findings.append(
217:                 {
218:                     "kind": "role_confidence_low",
219:                     "role": role,
220:                     "measurement_type": "not_applicable",
221:                     "detail": "Best candidate below confidence threshold",
222:                 }
223:             )
224: 
225:         metrics = {
226:             "columns_scanned": len(columns),
227:             "candidates": len(candidates),
228:             "roles_assigned": len(role_by_name),
229:             "low_confidence_roles": len(low_confidence_roles),
230:         }
231:         summary = f"Inferred roles for {len(role_by_name)} columns"
232:         return PluginResult("ok", summary, metrics, findings, [], None)
````

## File: plugins/report_bundle/__init__.py
````python
1: """Plugin package."""
````

## File: plugins/report_bundle/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: from pathlib import Path
 4: import json
 5: import traceback
 6: 
 7: from statistic_harness.core.report import build_report, write_report
 8: from statistic_harness.core.types import PluginArtifact, PluginError, PluginResult
 9: from statistic_harness.core.utils import DEFAULT_TENANT_ID, vector_store_enabled
10: from statistic_harness.core.vector_store import VectorStore, hash_embedding
11: 
12: 
13: _VECTOR_DIMENSIONS = 128
14: 
15: 
16: def _index_report(ctx, report: dict) -> int:
17:     tenant_id = ctx.tenant_id or DEFAULT_TENANT_ID
18:     store = VectorStore(ctx.storage.db_path, tenant_id=tenant_id)
19:     run_id = report.get("run_id") or ctx.run_id
20:     collection = f"report_{run_id}"
21:     vectors: list[list[float]] = []
22:     item_ids: list[str] = []
23:     payloads: list[dict] = []
24: 
25:     plugins = report.get("plugins", {}) or {}
26:     for plugin_id in sorted(plugins.keys()):
27:         payload = plugins.get(plugin_id) or {}
28:         summary = payload.get("summary")
29:         if summary:
30:             text = f"{plugin_id} summary: {summary}"
31:             vectors.append(hash_embedding(text, _VECTOR_DIMENSIONS))
32:             item_ids.append(f"{run_id}:{plugin_id}:summary")
33:             payloads.append(
34:                 {
35:                     "run_id": run_id,
36:                     "plugin_id": plugin_id,
37:                     "type": "summary",
38:                     "text": text,
39:                 }
40:             )
41:         findings = payload.get("findings", []) or []
42:         for idx, finding in enumerate(findings):
43:             kind = finding.get("kind", "")
44:             title = finding.get("title", "")
45:             description = finding.get("description", "")
46:             text_parts = [plugin_id, str(kind), str(title), str(description)]
47:             text = " ".join(part for part in text_parts if part).strip()
48:             if not text:
49:                 text = json.dumps(finding, sort_keys=True)
50:             vectors.append(hash_embedding(text, _VECTOR_DIMENSIONS))
51:             item_ids.append(f"{run_id}:{plugin_id}:finding:{idx}")
52:             payloads.append(
53:                 {
54:                     "run_id": run_id,
55:                     "plugin_id": plugin_id,
56:                     "type": "finding",
57:                     "index": idx,
58:                     "kind": kind,
59:                     "text": text,
60:                 }
61:             )
62: 
63:     if vectors:
64:         store.add(collection, vectors, item_ids=item_ids, payloads=payloads)
65:     return len(vectors)
66: 
67: 
68: class Plugin:
69:     def run(self, ctx) -> PluginResult:
70:         schema_path = Path("docs/report.schema.json")
71:         report = build_report(ctx.storage, ctx.run_id, ctx.run_dir, schema_path)
72:         write_report(report, ctx.run_dir)
73:         artifacts = [
74:             PluginArtifact(path="report.json", type="json", description="Report JSON"),
75:             PluginArtifact(
76:                 path="report.md", type="markdown", description="Report Markdown"
77:             ),
78:         ]
79:         summary = "Report generated"
80:         status = "ok"
81:         error = None
82:         if vector_store_enabled():
83:             try:
84:                 count = _index_report(ctx, report)
85:                 summary = f"Report generated; indexed {count} vectors"
86:             except Exception as exc:  # pragma: no cover - optional path
87:                 status = "error"
88:                 summary = "Report generated; vector index failed"
89:                 error = PluginError(
90:                     type=type(exc).__name__,
91:                     message=str(exc),
92:                     traceback=traceback.format_exc(),
93:                 )
94:         return PluginResult(status, summary, {}, [], artifacts, error)
````

## File: plugins/report_bundle/plugin.yaml
````yaml
 1: id: report_bundle
 2: name: Report Bundle
 3: version: 0.1.0
 4: type: report
 5: entrypoint: plugin.py:Plugin
 6: depends_on: []
 7: settings:
 8:   description: Build report
 9:   defaults: {}
10: capabilities: []
11: config_schema: config.schema.json
12: output_schema: output.schema.json
13: sandbox:
14:   no_network: true
15:   fs_allowlist:
16:   - appdata
17:   - plugins
18:   - docs
````

## File: plugins/__init__.py
````python
1: """Plugins package."""
````

## File: scripts/bootstrap.ps1
````powershell
 1: $python = Get-Command python3 -ErrorAction SilentlyContinue
 2: if (-not $python) {
 3:     $python = Get-Command python -ErrorAction SilentlyContinue
 4: }
 5: if (-not $python) {
 6:     throw "Python not found. Install Python 3.11+."
 7: }
 8: & $python.Path -m venv .venv
 9: .\.venv\Scripts\Activate.ps1
10: .\scripts\install_dev.ps1
````

## File: scripts/bootstrap.sh
````bash
1: #!/usr/bin/env bash
2: set -euo pipefail
3: python3 -m venv .venv
4: . .venv/bin/activate
5: ./scripts/install_dev.sh
````

## File: scripts/install_dev.ps1
````powershell
 1: $ErrorActionPreference = "Stop"
 2: 
 3: $RootDir = Resolve-Path (Join-Path $PSScriptRoot "..")
 4: $env:STAT_HARNESS_SAFE_RENAME = "1"
 5: $env:PYTHONPATH = "$RootDir\\tools" + ($(if ($env:PYTHONPATH) { ";" + $env:PYTHONPATH } else { "" }))
 6: 
 7: $VenvDir = Join-Path $RootDir ".venv"
 8: if (-not (Test-Path $VenvDir)) {
 9:     $PythonCmd = Get-Command python -ErrorAction SilentlyContinue
10:     if (-not $PythonCmd) {
11:         $PythonCmd = Get-Command py -ErrorAction SilentlyContinue
12:     }
13:     if (-not $PythonCmd) {
14:         throw "Python not found. Install Python 3 to continue."
15:     }
16:     & $PythonCmd.Source -m venv $VenvDir
17: }
18: 
19: $PipTmp = Join-Path $RootDir ".pip-tmp"
20: if (-not (Test-Path $PipTmp)) {
21:     New-Item -ItemType Directory -Path $PipTmp | Out-Null
22: }
23: $env:TEMP = $PipTmp
24: $env:TMP = $PipTmp
25: 
26: & (Join-Path $VenvDir "Scripts\\Activate.ps1")
27: python -m pip install -e ".[dev]" --no-build-isolation
````

## File: scripts/install_dev.sh
````bash
 1: #!/usr/bin/env bash
 2: set -euo pipefail
 3: 
 4: ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
 5: export STAT_HARNESS_SAFE_RENAME=1
 6: export PYTHONPATH="$ROOT_DIR/tools${PYTHONPATH:+:$PYTHONPATH}"
 7: 
 8: VENV_DIR="$ROOT_DIR/.venv"
 9: if [[ ! -d "$VENV_DIR" ]]; then
10:   PYTHON_BIN="python3"
11:   if ! command -v "$PYTHON_BIN" >/dev/null 2>&1; then
12:     PYTHON_BIN="python"
13:   fi
14:   "$PYTHON_BIN" -m venv "$VENV_DIR"
15: fi
16: 
17: mkdir -p "$ROOT_DIR/.pip-tmp"
18: export TMPDIR="$ROOT_DIR/.pip-tmp"
19: 
20: # shellcheck source=/dev/null
21: . "$VENV_DIR/bin/activate"
22: python -m pip install -e ".[dev]" --no-build-isolation
````

## File: scripts/run_cli_example.ps1
````powershell
 1: .\.venv\Scripts\Activate.ps1
 2: 
 3: $PythonCmd = Get-Command python -ErrorAction SilentlyContinue
 4: if (-not $PythonCmd) {
 5:     $PythonCmd = Get-Command py -ErrorAction SilentlyContinue
 6: }
 7: if ($PythonCmd) {
 8:     $VecScript = @'
 9: import importlib.util
10: import pathlib
11: 
12: spec = importlib.util.find_spec("sqlite_vec")
13: path = ""
14: if spec and spec.origin:
15:     base = pathlib.Path(spec.origin).resolve().parent
16:     matches = [p for p in base.rglob("vec0.*") if p.suffix in (".so", ".dylib", ".dll")]
17:     if matches:
18:         path = str(matches[0])
19: print(path)
20: '@
21:     $VecPath = & $PythonCmd -c $VecScript
22:     if ($VecPath) {
23:         $env:STAT_HARNESS_ENABLE_VECTOR_STORE = "1"
24:         $env:STAT_HARNESS_SQLITE_VEC_PATH = $VecPath
25:     }
26: }
27: 
28: stat-harness run --file tests/fixtures/synth_linear.csv --plugins ingest_tabular,profile_basic,report_bundle --run-seed 42
````

## File: scripts/run_cli_example.sh
````bash
 1: #!/usr/bin/env bash
 2: set -euo pipefail
 3: . .venv/bin/activate
 4: VEC_PATH="$(python - <<'PY'
 5: import importlib.util
 6: import pathlib
 7: 
 8: spec = importlib.util.find_spec("sqlite_vec")
 9: if spec and spec.origin:
10:     base = pathlib.Path(spec.origin).resolve().parent
11:     for path in base.rglob("vec0.*"):
12:         if path.suffix in {".so", ".dylib", ".dll"}:
13:             print(path)
14:             break
15: PY
16: )"
17: if [[ -n "${VEC_PATH}" ]]; then
18:   export STAT_HARNESS_ENABLE_VECTOR_STORE=1
19:   export STAT_HARNESS_SQLITE_VEC_PATH="${VEC_PATH}"
20: fi
21: stat-harness run --file tests/fixtures/synth_linear.csv --plugins ingest_tabular,profile_basic,report_bundle --run-seed 42
````

## File: scripts/run_ui.ps1
````powershell
 1: $ErrorActionPreference = "Stop"
 2: 
 3: $RootDir = Resolve-Path (Join-Path $PSScriptRoot "..")
 4: Set-Location $RootDir
 5: 
 6: function Resolve-Python {
 7:     $py = Get-Command py -ErrorAction SilentlyContinue
 8:     if ($py) {
 9:         return @("py", "-3")
10:     }
11:     $python = Get-Command python -ErrorAction SilentlyContinue
12:     if ($python) {
13:         return @("python")
14:     }
15:     throw "Python not found. Install Python 3.11+ and re-run."
16: }
17: 
18: $PythonCmd = Resolve-Python
19: $PythonExe = $PythonCmd[0]
20: $PythonArgs = @()
21: if ($PythonCmd.Length -gt 1) {
22:     $PythonArgs = $PythonCmd[1..($PythonCmd.Length - 1)]
23: }
24: $VenvActivate = Join-Path $RootDir ".venv\Scripts\Activate.ps1"
25: 
26: if (-not (Test-Path $VenvActivate)) {
27:     & $PythonExe @PythonArgs -m venv .venv
28: }
29: 
30: . $VenvActivate
31: 
32: $env:STAT_HARNESS_SAFE_RENAME = "1"
33: $env:PYTHONPATH = "$RootDir\\tools" + ($(if ($env:PYTHONPATH) { ";" + $env:PYTHONPATH } else { "" }))
34: 
35: $VecScript = @'
36: import importlib.util
37: import pathlib
38: 
39: spec = importlib.util.find_spec("sqlite_vec")
40: path = ""
41: if spec and spec.origin:
42:     base = pathlib.Path(spec.origin).resolve().parent
43:     matches = [p for p in base.rglob("vec0.*") if p.suffix in (".so", ".dylib", ".dll")]
44:     if matches:
45:         path = str(matches[0])
46: print(path)
47: '@
48: $VecPath = & $PythonExe @PythonArgs -c $VecScript
49: if ($VecPath) {
50:     $env:STAT_HARNESS_ENABLE_VECTOR_STORE = "1"
51:     $env:STAT_HARNESS_SQLITE_VEC_PATH = $VecPath
52: }
53: 
54: python -m pip install -U pip setuptools wheel
55: python -m pip install -e ".[dev]" --no-build-isolation
56: python -m pip install sqlite-vec
57: 
58: python -m statistic_harness.cli serve
````

## File: scripts/run_ui.sh
````bash
 1: #!/usr/bin/env bash
 2: set -euo pipefail
 3: . .venv/bin/activate
 4: VEC_PATH="$(python - <<'PY'
 5: import importlib.util
 6: import pathlib
 7: 
 8: spec = importlib.util.find_spec("sqlite_vec")
 9: if spec and spec.origin:
10:     base = pathlib.Path(spec.origin).resolve().parent
11:     for path in base.rglob("vec0.*"):
12:         if path.suffix in {".so", ".dylib", ".dll"}:
13:             print(path)
14:             break
15: PY
16: )"
17: if [[ -n "${VEC_PATH}" ]]; then
18:   export STAT_HARNESS_ENABLE_VECTOR_STORE=1
19:   export STAT_HARNESS_SQLITE_VEC_PATH="${VEC_PATH}"
20: fi
21: stat-harness serve --host 127.0.0.1 --port 8000
````

## File: src/statistic_harness/core/__init__.py
````python
1: """Core functionality for statistic harness."""
````

## File: src/statistic_harness/core/dataset_io.py
````python
  1: from __future__ import annotations
  2: 
  3: import json
  4: from typing import Any
  5: 
  6: import pandas as pd
  7: 
  8: from .storage import Storage
  9: from .utils import quote_identifier
 10: 
 11: 
 12: class DatasetAccessor:
 13:     def __init__(self, storage: Storage, dataset_version_id: str) -> None:
 14:         self.storage = storage
 15:         self.dataset_version_id = dataset_version_id
 16:         self._df: pd.DataFrame | None = None
 17: 
 18:     def _load_df(
 19:         self,
 20:         columns: list[str] | None = None,
 21:         row_limit: int | None = None,
 22:     ) -> pd.DataFrame:
 23:         limit = None
 24:         if row_limit is not None:
 25:             limit = int(row_limit)
 26:             if limit <= 0:
 27:                 raise ValueError("row_limit must be positive")
 28:         requested = columns
 29:         with self.storage.connection() as conn:
 30:             version = self.storage.get_dataset_version(self.dataset_version_id, conn)
 31:             if not version:
 32:                 raise ValueError("Dataset version not found")
 33:             columns = self.storage.fetch_dataset_columns(
 34:                 self.dataset_version_id, conn
 35:             )
 36:             safe_cols = [col["safe_name"] for col in columns]
 37:             original_cols = [col["original_name"] for col in columns]
 38:             selected_safe = safe_cols
 39:             selected_orig = original_cols
 40:             if requested is not None:
 41:                 mapping = dict(zip(original_cols, safe_cols))
 42:                 missing = [col for col in requested if col not in mapping]
 43:                 if missing:
 44:                     raise ValueError(f"Unknown columns: {missing}")
 45:                 selected_orig = list(requested)
 46:                 selected_safe = [mapping[col] for col in selected_orig]
 47:             if safe_cols:
 48:                 quoted_cols = ", ".join(
 49:                     quote_identifier(col) for col in selected_safe
 50:                 )
 51:                 sql = (
 52:                     f"SELECT row_index, {quoted_cols} FROM "
 53:                     f"{quote_identifier(version['table_name'])} ORDER BY row_index"
 54:                 )
 55:                 if limit is not None:
 56:                     sql = f"{sql} LIMIT {limit}"
 57:                 df = pd.read_sql_query(sql, conn)
 58:             else:
 59:                 df = pd.DataFrame()
 60:         df = df.rename(columns=dict(zip(selected_safe, selected_orig)))
 61:         if "row_index" in df.columns:
 62:             df = df.set_index("row_index")
 63:             df.index.name = None
 64:         return df
 65: 
 66:     def load(
 67:         self, columns: list[str] | None = None, row_limit: int | None = None
 68:     ) -> pd.DataFrame:
 69:         if columns is None and row_limit is None:
 70:             if self._df is None:
 71:                 self._df = self._load_df()
 72:             return self._df.copy()
 73:         return self._load_df(columns=columns, row_limit=row_limit)
 74: 
 75:     def info(self) -> dict[str, Any]:
 76:         with self.storage.connection() as conn:
 77:             version = self.storage.get_dataset_version(self.dataset_version_id, conn)
 78:             if not version:
 79:                 raise ValueError("Dataset version not found")
 80:             columns = self.storage.fetch_dataset_columns(
 81:                 self.dataset_version_id, conn
 82:             )
 83:         inferred = {col["original_name"]: col.get("dtype") for col in columns}
 84:         return {
 85:             "rows": int(version.get("row_count") or 0),
 86:             "cols": int(version.get("column_count") or 0),
 87:             "inferred_types": inferred,
 88:         }
 89: 
 90: 
 91: class TemplateAccessor:
 92:     def __init__(
 93:         self,
 94:         storage: Storage,
 95:         dataset_version_id: str,
 96:         template_id: int,
 97:         table_name: str,
 98:         scope: str = "dataset",
 99:         filters: dict[str, Any] | None = None,
100:     ) -> None:
101:         self.storage = storage
102:         self.dataset_version_id = dataset_version_id
103:         self.template_id = template_id
104:         self.table_name = table_name
105:         self.scope = scope
106:         self.filters = filters or {}
107:         self._df: pd.DataFrame | None = None
108: 
109:     def _filtered_dataset_versions(self, conn) -> list[str] | None:
110:         filters = self.filters or {}
111:         project_ids = filters.get("project_ids") or []
112:         dataset_ids = filters.get("dataset_ids") or []
113:         dataset_version_ids = filters.get("dataset_version_ids") or []
114:         raw_format_ids = filters.get("raw_format_ids") or []
115:         created_after = filters.get("created_after")
116:         created_before = filters.get("created_before")
117:         conditions = []
118:         params: list[Any] = []
119:         join = ""
120: 
121:         if project_ids:
122:             join = " JOIN datasets d ON d.dataset_id = dv.dataset_id"
123:             placeholders = ", ".join(["?"] * len(project_ids))
124:             conditions.append(f"d.project_id IN ({placeholders})")
125:             params.extend(project_ids)
126:         if dataset_ids:
127:             placeholders = ", ".join(["?"] * len(dataset_ids))
128:             conditions.append(f"dv.dataset_id IN ({placeholders})")
129:             params.extend(dataset_ids)
130:         if dataset_version_ids:
131:             placeholders = ", ".join(["?"] * len(dataset_version_ids))
132:             conditions.append(f"dv.dataset_version_id IN ({placeholders})")
133:             params.extend(dataset_version_ids)
134:         if raw_format_ids:
135:             placeholders = ", ".join(["?"] * len(raw_format_ids))
136:             conditions.append(f"dv.raw_format_id IN ({placeholders})")
137:             params.extend(raw_format_ids)
138:         if created_after:
139:             conditions.append("dv.created_at >= ?")
140:             params.append(created_after)
141:         if created_before:
142:             conditions.append("dv.created_at <= ?")
143:             params.append(created_before)
144: 
145:         if not conditions:
146:             return None
147:         query = (
148:             "SELECT dv.dataset_version_id FROM dataset_versions dv"
149:             + join
150:             + " WHERE "
151:             + " AND ".join(conditions)
152:             + " ORDER BY dv.created_at, dv.dataset_version_id"
153:         )
154:         cur = conn.execute(query, params)
155:         return [row[0] for row in cur.fetchall()]
156: 
157:     def _load_df(
158:         self,
159:         columns: list[str] | None = None,
160:         row_limit: int | None = None,
161:     ) -> pd.DataFrame:
162:         limit = None
163:         if row_limit is not None:
164:             limit = int(row_limit)
165:             if limit <= 0:
166:                 raise ValueError("row_limit must be positive")
167:         fields = self.storage.fetch_template_fields(self.template_id)
168:         safe_cols = [field["safe_name"] for field in fields]
169:         names = [field["name"] for field in fields]
170:         selected_safe = safe_cols
171:         selected_names = names
172:         if columns is not None:
173:             mapping = dict(zip(names, safe_cols))
174:             missing = [col for col in columns if col not in mapping]
175:             if missing:
176:                 raise ValueError(f"Unknown columns: {missing}")
177:             selected_names = list(columns)
178:             selected_safe = [mapping[col] for col in selected_names]
179:         with self.storage.connection() as conn:
180:             if safe_cols:
181:                 quoted_cols = ", ".join(
182:                     quote_identifier(col) for col in selected_safe
183:                 )
184:                 if self.scope == "all":
185:                     ids = self._filtered_dataset_versions(conn)
186:                     if ids is None:
187:                         sql = (
188:                             f"SELECT dataset_version_id, row_index, {quoted_cols} FROM "
189:                             f"{quote_identifier(self.table_name)} ORDER BY dataset_version_id, row_index"
190:                         )
191:                         if limit is not None:
192:                             sql = f"{sql} LIMIT {limit}"
193:                         df = pd.read_sql_query(sql, conn)
194:                     elif ids:
195:                         placeholders = ", ".join(["?"] * len(ids))
196:                         sql = (
197:                             f"SELECT dataset_version_id, row_index, {quoted_cols} FROM "
198:                             f"{quote_identifier(self.table_name)} WHERE dataset_version_id IN ({placeholders}) "
199:                             f"ORDER BY dataset_version_id, row_index"
200:                         )
201:                         if limit is not None:
202:                             sql = f"{sql} LIMIT {limit}"
203:                         df = pd.read_sql_query(sql, conn, params=ids)
204:                     else:
205:                         df = pd.DataFrame(
206:                             columns=["dataset_version_id", "row_index", *selected_safe]
207:                         )
208:                 else:
209:                     sql = (
210:                         f"SELECT row_index, {quoted_cols} FROM "
211:                         f"{quote_identifier(self.table_name)} WHERE dataset_version_id = ? "
212:                         f"ORDER BY row_index"
213:                     )
214:                     if limit is not None:
215:                         sql = f"{sql} LIMIT {limit}"
216:                     df = pd.read_sql_query(sql, conn, params=(self.dataset_version_id,))
217:             else:
218:                 df = pd.DataFrame()
219:         df = df.rename(columns=dict(zip(selected_safe, selected_names)))
220:         if "row_index" in df.columns:
221:             if self.scope == "all":
222:                 df = df.reset_index(drop=True)
223:             else:
224:                 df = df.set_index("row_index")
225:                 df.index.name = None
226:         return df
227: 
228:     def load(
229:         self, columns: list[str] | None = None, row_limit: int | None = None
230:     ) -> pd.DataFrame:
231:         if columns is None and row_limit is None:
232:             if self._df is None:
233:                 self._df = self._load_df()
234:             return self._df.copy()
235:         return self._load_df(columns=columns, row_limit=row_limit)
236: 
237:     def info(self) -> dict[str, Any]:
238:         fields = self.storage.fetch_template_fields(self.template_id)
239:         with self.storage.connection() as conn:
240:             if self.scope == "all":
241:                 ids = self._filtered_dataset_versions(conn)
242:                 if ids is None:
243:                     cur = conn.execute(
244:                         f"SELECT COUNT(*) FROM {quote_identifier(self.table_name)}"
245:                     )
246:                 elif ids:
247:                     placeholders = ", ".join(["?"] * len(ids))
248:                     cur = conn.execute(
249:                         f"SELECT COUNT(*) FROM {quote_identifier(self.table_name)} WHERE dataset_version_id IN ({placeholders})",
250:                         ids,
251:                     )
252:                 else:
253:                     rows = 0
254:                     inferred = {field["name"]: field.get("dtype") for field in fields}
255:                     return {"rows": rows, "cols": len(fields), "inferred_types": inferred}
256:             else:
257:                 cur = conn.execute(
258:                     f"SELECT COUNT(*) FROM {quote_identifier(self.table_name)} WHERE dataset_version_id = ?",
259:                     (self.dataset_version_id,),
260:                 )
261:             rows = int(cur.fetchone()[0])
262:         inferred = {field["name"]: field.get("dtype") for field in fields}
263:         return {"rows": rows, "cols": len(fields), "inferred_types": inferred}
264: 
265: 
266: def resolve_dataset_accessor(
267:     storage: Storage, dataset_version_id: str
268: ) -> tuple[Any, dict[str, Any] | None]:
269:     dataset_template = storage.fetch_dataset_template(dataset_version_id)
270:     if dataset_template and dataset_template.get("status") == "ready":
271:         mapping = {}
272:         try:
273:             mapping = json.loads(dataset_template.get("mapping_json") or "{}")
274:         except json.JSONDecodeError:
275:             mapping = {}
276:         scope = "all" if mapping.get("scope") == "all" else "dataset"
277:         filters = mapping.get("filters") if scope == "all" else None
278:         if isinstance(filters, dict):
279:             filters = {k: v for k, v in filters.items() if v not in (None, [], "")}
280:             if not filters:
281:                 filters = None
282:         else:
283:             filters = None
284:         accessor = TemplateAccessor(
285:             storage,
286:             dataset_version_id,
287:             int(dataset_template["template_id"]),
288:             dataset_template["table_name"],
289:             scope=scope,
290:             filters=filters,
291:         )
292:         return accessor, dataset_template
293:     return DatasetAccessor(storage, dataset_version_id), None
````

## File: src/statistic_harness/core/evaluation.py
````python
  1: from __future__ import annotations
  2: 
  3: import json
  4: from pathlib import Path
  5: from typing import Any
  6: 
  7: import yaml
  8: 
  9: 
 10: def _parse_tolerance(value: Any, default_abs: float = 0.0) -> dict[str, float]:
 11:     if value is None:
 12:         return {"absolute": float(default_abs), "relative": 0.0}
 13:     if isinstance(value, (int, float)):
 14:         return {"absolute": float(value), "relative": 0.0}
 15:     if isinstance(value, dict):
 16:         abs_tol = value.get("absolute", value.get("abs", default_abs))
 17:         rel_tol = value.get("relative", value.get("rel", 0.0))
 18:         try:
 19:             abs_val = float(abs_tol)
 20:         except (TypeError, ValueError):
 21:             abs_val = float(default_abs)
 22:         try:
 23:             rel_val = float(rel_tol)
 24:         except (TypeError, ValueError):
 25:             rel_val = 0.0
 26:         return {"absolute": abs_val, "relative": rel_val}
 27:     return {"absolute": float(default_abs), "relative": 0.0}
 28: 
 29: 
 30: def _within_tolerance(expected: float, actual: float, tol: dict[str, float]) -> bool:
 31:     diff = abs(float(expected) - float(actual))
 32:     abs_tol = tol.get("absolute", 0.0)
 33:     rel_tol = tol.get("relative", 0.0)
 34:     if diff <= abs_tol:
 35:         return True
 36:     if rel_tol <= 0:
 37:         return False
 38:     return diff <= rel_tol * max(1.0, abs(float(expected)))
 39: 
 40: 
 41: def _collect_findings(report: dict[str, Any], kind: str) -> list[dict[str, Any]]:
 42:     findings: list[dict[str, Any]] = []
 43:     for plugin in report.get("plugins", {}).values():
 44:         for item in plugin.get("findings", []):
 45:             if item.get("kind") == kind:
 46:                 findings.append(item)
 47:     return findings
 48: 
 49: 
 50: def _collect_findings_for_plugin(
 51:     report: dict[str, Any], plugin_id: str | None, kind: str | None = None
 52: ) -> list[dict[str, Any]]:
 53:     findings: list[dict[str, Any]] = []
 54:     plugins = report.get("plugins", {})
 55:     for pid, plugin in plugins.items():
 56:         if plugin_id and pid != plugin_id:
 57:             continue
 58:         for item in plugin.get("findings", []):
 59:             if kind and item.get("kind") != kind:
 60:                 continue
 61:             findings.append(item)
 62:     return findings
 63: 
 64: 
 65: def _matches_expected(
 66:     item: dict[str, Any],
 67:     where: dict[str, Any] | None,
 68:     contains: dict[str, Any] | None,
 69: ) -> bool:
 70:     if where:
 71:         for key, expected in where.items():
 72:             actual = item.get(key)
 73:             if actual != expected:
 74:                 return False
 75:     if contains:
 76:         for key, expected in contains.items():
 77:             actual = item.get(key)
 78:             if isinstance(actual, str):
 79:                 if str(expected) not in actual:
 80:                     return False
 81:             elif isinstance(actual, (list, tuple, set)):
 82:                 if isinstance(expected, (list, tuple, set)):
 83:                     if not set(expected).issubset(set(actual)):
 84:                         return False
 85:                 else:
 86:                     if expected not in actual:
 87:                         return False
 88:             else:
 89:                 return False
 90:     return True
 91: 
 92: 
 93: def evaluate_report(
 94:     report_path: Path, ground_truth_path: Path
 95: ) -> tuple[bool, list[str]]:
 96:     report = json.loads(report_path.read_text(encoding="utf-8"))
 97:     truth = yaml.safe_load(ground_truth_path.read_text(encoding="utf-8"))
 98:     if not isinstance(truth, dict):
 99:         truth = {}
100: 
101:     messages: list[str] = []
102:     ok = True
103: 
104:     if "strict" in truth:
105:         strict = bool(truth.get("strict"))
106:     else:
107:         strict = True
108: 
109:     expected_features = set(truth.get("features", []))
110:     found_features = {
111:         f.get("feature") for f in _collect_findings(report, "feature_discovery")
112:     }
113:     if strict:
114:         unexpected_features = found_features - expected_features
115:         if unexpected_features:
116:             ok = False
117:             messages.append(f"Unexpected features: {sorted(unexpected_features)}")
118:     missing_features = expected_features - found_features
119:     if missing_features:
120:         ok = False
121:         messages.append(f"Missing features: {sorted(missing_features)}")
122: 
123:     expected_changepoints = truth.get("changepoints", [])
124:     found_cp = [f.get("index") for f in _collect_findings(report, "changepoint")]
125:     tolerance = _parse_tolerance(truth.get("changepoint_tolerance", 3), default_abs=3)
126:     for expected in expected_changepoints:
127:         if isinstance(expected, dict):
128:             expected_value = expected.get("index")
129:             tolerance = _parse_tolerance(
130:                 expected.get("tolerance", tolerance), default_abs=tolerance["absolute"]
131:             )
132:         else:
133:             expected_value = expected
134:         if expected_value is None:
135:             continue
136:         if not any(
137:             _within_tolerance(float(expected_value), float(found), tolerance)
138:             for found in found_cp
139:             if found is not None
140:         ):
141:             ok = False
142:             messages.append(
143:                 f"Changepoint {expected_value} not found within tolerance"
144:             )
145:     if strict and expected_changepoints is not None:
146:         expected_values = []
147:         for expected in expected_changepoints:
148:             if isinstance(expected, dict):
149:                 value = expected.get("index")
150:                 tol = _parse_tolerance(
151:                     expected.get("tolerance", tolerance), default_abs=tolerance["absolute"]
152:                 )
153:             else:
154:                 value = expected
155:                 tol = tolerance
156:             if value is None:
157:                 continue
158:             expected_values.append((float(value), tol))
159:         for found in found_cp:
160:             if found is None:
161:                 continue
162:             if not any(
163:                 _within_tolerance(float(found), expected, tol)
164:                 for expected, tol in expected_values
165:             ):
166:                 ok = False
167:                 messages.append(f"Unexpected changepoint: {found}")
168: 
169:     expected_pairs = {tuple(pair) for pair in truth.get("dependence_shift_pairs", [])}
170:     found_pairs = {
171:         tuple(f.get("pair", [])) for f in _collect_findings(report, "dependence_shift")
172:     }
173:     if strict:
174:         unexpected_pairs = found_pairs - expected_pairs
175:         if unexpected_pairs:
176:             ok = False
177:             messages.append(
178:                 f"Unexpected dependence shift pairs: {sorted(unexpected_pairs)}"
179:             )
180:     missing_pairs = expected_pairs - found_pairs
181:     if missing_pairs:
182:         ok = False
183:         messages.append(f"Missing dependence shift pairs: {sorted(missing_pairs)}")
184: 
185:     expected_anomalies = truth.get("anomalies", [])
186:     found_anomalies = [f.get("row_index") for f in _collect_findings(report, "anomaly")]
187:     min_hits = truth.get("min_anomaly_hits", len(expected_anomalies))
188:     anomaly_tol = _parse_tolerance(
189:         truth.get("anomaly_tolerance", 0), default_abs=0
190:     )
191:     matches = 0
192:     for expected in expected_anomalies:
193:         if isinstance(expected, dict):
194:             expected_value = expected.get("row_index")
195:             tol = _parse_tolerance(
196:                 expected.get("tolerance", anomaly_tol),
197:                 default_abs=anomaly_tol["absolute"],
198:             )
199:         else:
200:             expected_value = expected
201:             tol = anomaly_tol
202:         if expected_value is None:
203:             continue
204:         if any(
205:             _within_tolerance(float(expected_value), float(found), tol)
206:             for found in found_anomalies
207:             if found is not None
208:         ):
209:             matches += 1
210:     if matches < min_hits:
211:         ok = False
212:         messages.append("Not enough anomalies detected")
213:     if strict and expected_anomalies is not None:
214:         expected_rows = []
215:         for expected in expected_anomalies:
216:             if isinstance(expected, dict):
217:                 value = expected.get("row_index")
218:                 tol = _parse_tolerance(
219:                     expected.get("tolerance", anomaly_tol),
220:                     default_abs=anomaly_tol["absolute"],
221:                 )
222:             else:
223:                 value = expected
224:                 tol = anomaly_tol
225:             if value is None:
226:                 continue
227:             expected_rows.append((float(value), tol))
228:         for found in found_anomalies:
229:             if found is None:
230:                 continue
231:             if not any(
232:                 _within_tolerance(float(found), expected, tol)
233:                 for expected, tol in expected_rows
234:             ):
235:                 ok = False
236:                 messages.append(f"Unexpected anomaly: {found}")
237: 
238:     expected_findings = truth.get("expected_findings", []) or []
239:     expected_matchers: list[dict[str, Any]] = []
240:     for entry in expected_findings:
241:         if not isinstance(entry, dict):
242:             continue
243:         plugin_id = entry.get("plugin_id")
244:         kind = entry.get("kind")
245:         if not kind:
246:             continue
247:         where = entry.get("where") or {}
248:         contains = entry.get("contains") or {}
249:         min_count = entry.get("min_count", 1)
250:         max_count = entry.get("max_count")
251:         candidates = _collect_findings_for_plugin(report, plugin_id, kind)
252:         matches = [
253:             item
254:             for item in candidates
255:             if _matches_expected(item, where, contains)
256:         ]
257:         if len(matches) < int(min_count):
258:             ok = False
259:             messages.append(
260:                 f"Expected finding missing: kind={kind} plugin={plugin_id or '*'}"
261:             )
262:         if max_count is not None and len(matches) > int(max_count):
263:             ok = False
264:             messages.append(
265:                 f"Too many matches for kind={kind} plugin={plugin_id or '*'}"
266:             )
267:         expected_matchers.append(
268:             {
269:                 "plugin_id": plugin_id,
270:                 "kind": kind,
271:                 "where": where,
272:                 "contains": contains,
273:             }
274:         )
275: 
276:     if strict and expected_matchers:
277:         for plugin_id, plugin in report.get("plugins", {}).items():
278:             for item in plugin.get("findings", []):
279:                 kind = item.get("kind")
280:                 if not kind:
281:                     continue
282:                 if not any(matcher["kind"] == kind for matcher in expected_matchers):
283:                     continue
284:                 matched = False
285:                 for matcher in expected_matchers:
286:                     if matcher["kind"] != kind:
287:                         continue
288:                     if matcher["plugin_id"] and matcher["plugin_id"] != plugin_id:
289:                         continue
290:                     if _matches_expected(item, matcher["where"], matcher["contains"]):
291:                         matched = True
292:                         break
293:                 if not matched:
294:                     ok = False
295:                     messages.append(
296:                         f"Unexpected finding: kind={kind} plugin={plugin_id}"
297:                     )
298: 
299:     expected_metrics = truth.get("expected_metrics", []) or []
300:     for entry in expected_metrics:
301:         if not isinstance(entry, dict):
302:             continue
303:         plugin_id = entry.get("plugin_id")
304:         metric_path = entry.get("metric")
305:         if not plugin_id or not metric_path:
306:             continue
307:         plugin = report.get("plugins", {}).get(plugin_id, {})
308:         metrics = plugin.get("metrics", {})
309:         if not isinstance(metrics, dict):
310:             ok = False
311:             messages.append(f"Metrics missing for plugin={plugin_id}")
312:             continue
313:         value = metrics
314:         for part in str(metric_path).split("."):
315:             if isinstance(value, dict) and part in value:
316:                 value = value[part]
317:             else:
318:                 value = None
319:                 break
320:         if isinstance(value, dict) and "value" in value:
321:             value = value.get("value")
322:         if value is None:
323:             ok = False
324:             messages.append(f"Expected metric missing: {plugin_id}.{metric_path}")
325:             continue
326:         try:
327:             actual_val = float(value)
328:         except (TypeError, ValueError):
329:             ok = False
330:             messages.append(f"Expected metric not numeric: {plugin_id}.{metric_path}")
331:             continue
332:         expected_val = entry.get("value")
333:         try:
334:             expected_val_f = float(expected_val)
335:         except (TypeError, ValueError):
336:             ok = False
337:             messages.append(f"Expected metric invalid: {plugin_id}.{metric_path}")
338:             continue
339:         tol = _parse_tolerance(entry.get("tolerance", 0), default_abs=0.0)
340:         if not _within_tolerance(expected_val_f, actual_val, tol):
341:             ok = False
342:             messages.append(
343:                 f"Metric out of tolerance: {plugin_id}.{metric_path} "
344:                 f"expected={expected_val_f} actual={actual_val}"
345:             )
346: 
347:     return ok, messages
````

## File: src/statistic_harness/core/known_issue_compiler.py
````python
  1: from __future__ import annotations
  2: 
  3: import re
  4: from typing import Any
  5: 
  6: 
  7: _PROCESS_HINT_RE = re.compile(
  8:     r"(?:process|job|task|step|called)\s+([A-Za-z0-9_\-]+)", re.IGNORECASE
  9: )
 10: _UPPER_TOKEN_RE = re.compile(r"\b[A-Z][A-Z0-9_]{2,}\b")
 11: _STOPWORDS = {
 12:     "CLOSE",
 13:     "CYCLE",
 14:     "SERVER",
 15:     "SERVERS",
 16:     "QRA",
 17:     "ERP",
 18:     "HOST",
 19:     "HOSTS",
 20:     "QUEUE",
 21:     "WAIT",
 22:     "ELIGIBLE",
 23:     "CAPACITY",
 24: }
 25: 
 26: 
 27: def _extract_process(text: str, hint: str | None = None) -> str | None:
 28:     if hint:
 29:         cleaned = str(hint).strip()
 30:         if cleaned:
 31:             return cleaned
 32:     match = _PROCESS_HINT_RE.search(text)
 33:     if match:
 34:         return match.group(1)
 35:     tokens = [tok for tok in _UPPER_TOKEN_RE.findall(text) if tok not in _STOPWORDS]
 36:     if tokens:
 37:         return tokens[0]
 38:     return None
 39: 
 40: 
 41: def _flag(text: str, tokens: list[str]) -> bool:
 42:     return any(token in text for token in tokens)
 43: 
 44: 
 45: def _compact_title(text: str) -> str:
 46:     cleaned = re.sub(r"\s+", " ", text.strip())
 47:     if not cleaned:
 48:         return "Known issue"
 49:     if len(cleaned) <= 80:
 50:         return cleaned
 51:     return cleaned[:77] + "..."
 52: 
 53: 
 54: def compile_known_issues(
 55:     natural_language: list[dict[str, Any]] | None,
 56: ) -> tuple[list[dict[str, Any]], list[str]]:
 57:     if not natural_language:
 58:         return [], []
 59:     compiled: list[dict[str, Any]] = []
 60:     warnings: list[str] = []
 61: 
 62:     for entry in natural_language:
 63:         if not isinstance(entry, dict):
 64:             continue
 65:         raw_text = str(entry.get("text") or "").strip()
 66:         if not raw_text:
 67:             continue
 68:         text = raw_text.lower()
 69:         process = _extract_process(raw_text, entry.get("process_hint"))
 70:         mentions_wait = _flag(text, ["wait", "queue", "delay", "latency", "eligible"])
 71:         mentions_close = _flag(text, ["close cycle", "close window", "close period"])
 72:         mentions_disable = _flag(text, ["remove", "disable", "turn off", "eliminate"])
 73:         mentions_third = _flag(
 74:             text,
 75:             [
 76:                 "3rd",
 77:                 "third",
 78:                 "third server",
 79:                 "3rd server",
 80:                 "add server",
 81:                 "add host",
 82:                 "third qpec",
 83:                 "3rd qpec",
 84:             ],
 85:         )
 86:         mentions_contention = _flag(text, ["contention", "bottleneck", "congest"])
 87: 
 88:         issues: list[dict[str, Any]] = []
 89:         title = _compact_title(raw_text)
 90: 
 91:         if process:
 92:             process_norm = process.lower()
 93:             if mentions_wait or mentions_close or mentions_contention:
 94:                 issues.append(
 95:                     {
 96:                         "title": title,
 97:                         "description": raw_text,
 98:                         "plugin_id": "analysis_queue_delay_decomposition",
 99:                         "kind": "eligible_wait_process_stats",
100:                         "where": {"process_norm": process_norm},
101:                     }
102:                 )
103:             if mentions_disable:
104:                 issues.append(
105:                     {
106:                         "title": title,
107:                         "description": raw_text,
108:                         "plugin_id": "analysis_queue_delay_decomposition",
109:                         "kind": "eligible_wait_impact",
110:                         "where": {"process_norm": process_norm},
111:                     }
112:                 )
113:             if mentions_third:
114:                 issues.append(
115:                     {
116:                         "title": title,
117:                         "description": raw_text,
118:                         "plugin_id": "analysis_queue_delay_decomposition",
119:                         "kind": "capacity_scale_model",
120:                         "where": {"process_norm": process_norm},
121:                     }
122:                 )
123: 
124:         if mentions_third:
125:             issues.append(
126:                 {
127:                     "title": title,
128:                     "description": raw_text,
129:                     "plugin_id": "analysis_close_cycle_capacity_model",
130:                     "kind": "close_cycle_capacity_model",
131:                 }
132:             )
133:             issues.append(
134:                 {
135:                     "title": title,
136:                     "description": raw_text,
137:                     "plugin_id": "analysis_close_cycle_capacity_impact",
138:                     "kind": "close_cycle_capacity_impact",
139:                 }
140:             )
141: 
142:         if not issues:
143:             warnings.append(
144:                 f"Could not compile known issue: '{_compact_title(raw_text)}'"
145:             )
146:             continue
147: 
148:         for issue in issues:
149:             issue["source_text"] = raw_text
150:         compiled.extend(issues)
151: 
152:     return compiled, warnings
````

## File: src/statistic_harness/core/migrations.py
````python
  1: from __future__ import annotations
  2: 
  3: import sqlite3
  4: from typing import Callable
  5: 
  6: from .utils import DEFAULT_TENANT_ID, now_iso
  7: 
  8: Migration = Callable[[sqlite3.Connection], None]
  9: 
 10: 
 11: def _ensure_schema_migrations(conn: sqlite3.Connection) -> None:
 12:     conn.execute(
 13:         """
 14:         CREATE TABLE IF NOT EXISTS schema_migrations (
 15:             version INTEGER PRIMARY KEY,
 16:             applied_at TEXT NOT NULL
 17:         )
 18:         """
 19:     )
 20: 
 21: 
 22: def _column_exists(conn: sqlite3.Connection, table: str, column: str) -> bool:
 23:     cur = conn.execute(f"PRAGMA table_info({table})")
 24:     return any(row[1] == column for row in cur.fetchall())
 25: 
 26: 
 27: def migration_1(conn: sqlite3.Connection) -> None:
 28:     _ensure_schema_migrations(conn)
 29:     conn.execute(
 30:         """
 31:         CREATE TABLE IF NOT EXISTS runs (
 32:             run_id TEXT PRIMARY KEY,
 33:             created_at TEXT,
 34:             status TEXT,
 35:             upload_id TEXT,
 36:             input_filename TEXT,
 37:             canonical_path TEXT,
 38:             settings_json TEXT,
 39:             error_json TEXT,
 40:             run_seed INTEGER DEFAULT 0
 41:         )
 42:         """
 43:     )
 44:     conn.execute(
 45:         """
 46:         CREATE TABLE IF NOT EXISTS plugin_results (
 47:             run_id TEXT,
 48:             plugin_id TEXT,
 49:             status TEXT,
 50:             summary TEXT,
 51:             metrics_json TEXT,
 52:             findings_json TEXT,
 53:             artifacts_json TEXT,
 54:             error_json TEXT,
 55:             PRIMARY KEY (run_id, plugin_id)
 56:         )
 57:         """
 58:     )
 59: 
 60: 
 61: def migration_2(conn: sqlite3.Connection) -> None:
 62:     conn.execute(
 63:         """
 64:         CREATE TABLE IF NOT EXISTS projects (
 65:             project_id TEXT PRIMARY KEY,
 66:             fingerprint TEXT UNIQUE NOT NULL,
 67:             name TEXT,
 68:             created_at TEXT
 69:         )
 70:         """
 71:     )
 72:     conn.execute(
 73:         """
 74:         CREATE TABLE IF NOT EXISTS datasets (
 75:             dataset_id TEXT PRIMARY KEY,
 76:             project_id TEXT NOT NULL,
 77:             fingerprint TEXT UNIQUE NOT NULL,
 78:             created_at TEXT,
 79:             FOREIGN KEY (project_id) REFERENCES projects(project_id)
 80:         )
 81:         """
 82:     )
 83:     conn.execute(
 84:         """
 85:         CREATE TABLE IF NOT EXISTS dataset_versions (
 86:             dataset_version_id TEXT PRIMARY KEY,
 87:             dataset_id TEXT NOT NULL,
 88:             created_at TEXT,
 89:             table_name TEXT NOT NULL,
 90:             row_count INTEGER DEFAULT 0,
 91:             column_count INTEGER DEFAULT 0,
 92:             data_hash TEXT,
 93:             FOREIGN KEY (dataset_id) REFERENCES datasets(dataset_id)
 94:         )
 95:         """
 96:     )
 97:     conn.execute(
 98:         """
 99:         CREATE TABLE IF NOT EXISTS dataset_columns (
100:             dataset_version_id TEXT NOT NULL,
101:             column_id INTEGER NOT NULL,
102:             safe_name TEXT NOT NULL,
103:             original_name TEXT NOT NULL,
104:             dtype TEXT,
105:             role TEXT,
106:             PRIMARY KEY (dataset_version_id, column_id),
107:             FOREIGN KEY (dataset_version_id) REFERENCES dataset_versions(dataset_version_id)
108:         )
109:         """
110:     )
111:     conn.execute(
112:         """
113:         CREATE TABLE IF NOT EXISTS uploads (
114:             upload_id TEXT PRIMARY KEY,
115:             filename TEXT NOT NULL,
116:             size_bytes INTEGER NOT NULL,
117:             sha256 TEXT NOT NULL,
118:             created_at TEXT
119:         )
120:         """
121:     )
122: 
123:     if not _column_exists(conn, "runs", "project_id"):
124:         conn.execute("ALTER TABLE runs ADD COLUMN project_id TEXT")
125:     if not _column_exists(conn, "runs", "dataset_id"):
126:         conn.execute("ALTER TABLE runs ADD COLUMN dataset_id TEXT")
127:     if not _column_exists(conn, "runs", "dataset_version_id"):
128:         conn.execute("ALTER TABLE runs ADD COLUMN dataset_version_id TEXT")
129:     if not _column_exists(conn, "runs", "input_hash"):
130:         conn.execute("ALTER TABLE runs ADD COLUMN input_hash TEXT")
131: 
132:     conn.execute(
133:         "CREATE INDEX IF NOT EXISTS idx_dataset_columns_version ON dataset_columns(dataset_version_id)"
134:     )
135:     conn.execute(
136:         "CREATE INDEX IF NOT EXISTS idx_datasets_project ON datasets(project_id)"
137:     )
138:     conn.execute(
139:         "CREATE INDEX IF NOT EXISTS idx_dataset_versions_dataset ON dataset_versions(dataset_id)"
140:     )
141: 
142: 
143: def migration_3(conn: sqlite3.Connection) -> None:
144:     conn.execute(
145:         """
146:         CREATE TABLE IF NOT EXISTS plugin_results_v2 (
147:             result_id INTEGER PRIMARY KEY AUTOINCREMENT,
148:             run_id TEXT NOT NULL,
149:             plugin_id TEXT NOT NULL,
150:             plugin_version TEXT,
151:             executed_at TEXT,
152:             code_hash TEXT,
153:             settings_hash TEXT,
154:             dataset_hash TEXT,
155:             status TEXT,
156:             summary TEXT,
157:             metrics_json TEXT,
158:             findings_json TEXT,
159:             artifacts_json TEXT,
160:             error_json TEXT,
161:             FOREIGN KEY (run_id) REFERENCES runs(run_id)
162:         )
163:         """
164:     )
165:     conn.execute(
166:         "CREATE INDEX IF NOT EXISTS idx_plugin_results_v2_run ON plugin_results_v2(run_id, plugin_id, result_id)"
167:     )
168: 
169:     cur = conn.execute("SELECT COUNT(*) FROM plugin_results_v2")
170:     if int(cur.fetchone()[0]) == 0:
171:         try:
172:             legacy_count = conn.execute("SELECT COUNT(*) FROM plugin_results").fetchone()
173:             if legacy_count and int(legacy_count[0]) > 0:
174:                 conn.execute(
175:                     """
176:                     INSERT INTO plugin_results_v2
177:                     (run_id, plugin_id, status, summary, metrics_json, findings_json, artifacts_json, error_json, executed_at)
178:                     SELECT run_id, plugin_id, status, summary, metrics_json, findings_json, artifacts_json, error_json, ?
179:                     FROM plugin_results
180:                     """,
181:                     (now_iso(),),
182:                 )
183:         except sqlite3.OperationalError:
184:             pass
185: 
186: 
187: def migration_4(conn: sqlite3.Connection) -> None:
188:     conn.execute(
189:         """
190:         CREATE TABLE IF NOT EXISTS parameter_entities (
191:             entity_id INTEGER PRIMARY KEY AUTOINCREMENT,
192:             canonical_text TEXT UNIQUE NOT NULL
193:         )
194:         """
195:     )
196:     conn.execute(
197:         """
198:         CREATE TABLE IF NOT EXISTS parameter_kv (
199:             entity_id INTEGER NOT NULL,
200:             key TEXT NOT NULL,
201:             value TEXT,
202:             PRIMARY KEY (entity_id, key, value),
203:             FOREIGN KEY (entity_id) REFERENCES parameter_entities(entity_id)
204:         )
205:         """
206:     )
207:     conn.execute(
208:         """
209:         CREATE TABLE IF NOT EXISTS row_parameter_link (
210:             dataset_version_id TEXT NOT NULL,
211:             row_index INTEGER NOT NULL,
212:             entity_id INTEGER NOT NULL,
213:             PRIMARY KEY (dataset_version_id, row_index, entity_id),
214:             FOREIGN KEY (entity_id) REFERENCES parameter_entities(entity_id)
215:         )
216:         """
217:     )
218:     conn.execute(
219:         "CREATE INDEX IF NOT EXISTS idx_parameter_kv ON parameter_kv(key, value)"
220:     )
221:     conn.execute(
222:         "CREATE INDEX IF NOT EXISTS idx_row_parameter_link ON row_parameter_link(dataset_version_id, entity_id)"
223:     )
224:     conn.execute(
225:         """
226:         CREATE TABLE IF NOT EXISTS entities (
227:             entity_id INTEGER PRIMARY KEY AUTOINCREMENT,
228:             type TEXT NOT NULL,
229:             key TEXT NOT NULL,
230:             UNIQUE (type, key)
231:         )
232:         """
233:     )
234:     conn.execute(
235:         """
236:         CREATE TABLE IF NOT EXISTS edges (
237:             edge_id INTEGER PRIMARY KEY AUTOINCREMENT,
238:             src_entity_id INTEGER NOT NULL,
239:             dst_entity_id INTEGER NOT NULL,
240:             kind TEXT NOT NULL,
241:             evidence_json TEXT,
242:             score REAL,
243:             FOREIGN KEY (src_entity_id) REFERENCES entities(entity_id),
244:             FOREIGN KEY (dst_entity_id) REFERENCES entities(entity_id)
245:         )
246:         """
247:     )
248:     conn.execute(
249:         "CREATE INDEX IF NOT EXISTS idx_entities_type_key ON entities(type, key)"
250:     )
251:     conn.execute(
252:         "CREATE INDEX IF NOT EXISTS idx_edges_src ON edges(src_entity_id)"
253:     )
254:     conn.execute(
255:         "CREATE INDEX IF NOT EXISTS idx_edges_dst ON edges(dst_entity_id)"
256:     )
257: 
258: 
259: def migration_5(conn: sqlite3.Connection) -> None:
260:     conn.execute(
261:         """
262:         CREATE TABLE IF NOT EXISTS analysis_jobs (
263:             job_id INTEGER PRIMARY KEY AUTOINCREMENT,
264:             dataset_version_id TEXT NOT NULL,
265:             plugin_id TEXT NOT NULL,
266:             plugin_version TEXT,
267:             code_hash TEXT,
268:             settings_hash TEXT,
269:             run_seed INTEGER DEFAULT 0,
270:             status TEXT NOT NULL,
271:             created_at TEXT,
272:             started_at TEXT,
273:             completed_at TEXT,
274:             error_json TEXT,
275:             UNIQUE (dataset_version_id, plugin_id, plugin_version, code_hash, settings_hash)
276:         )
277:         """
278:     )
279:     conn.execute(
280:         "CREATE INDEX IF NOT EXISTS idx_analysis_jobs_status ON analysis_jobs(status)"
281:     )
282:     conn.execute(
283:         """
284:         CREATE TABLE IF NOT EXISTS deliveries (
285:             delivery_id INTEGER PRIMARY KEY AUTOINCREMENT,
286:             project_id TEXT NOT NULL,
287:             dataset_version_id TEXT NOT NULL,
288:             plugin_id TEXT NOT NULL,
289:             plugin_version TEXT,
290:             code_hash TEXT,
291:             dataset_hash TEXT,
292:             delivered_at TEXT,
293:             notes TEXT,
294:             UNIQUE (dataset_version_id, plugin_id, plugin_version, code_hash, dataset_hash)
295:         )
296:         """
297:     )
298:     conn.execute(
299:         "CREATE INDEX IF NOT EXISTS idx_deliveries_dataset ON deliveries(dataset_version_id, plugin_id)"
300:     )
301: 
302: 
303: def migration_6(conn: sqlite3.Connection) -> None:
304:     conn.execute(
305:         """
306:         CREATE TABLE IF NOT EXISTS raw_formats (
307:             format_id INTEGER PRIMARY KEY AUTOINCREMENT,
308:             fingerprint TEXT UNIQUE NOT NULL,
309:             name TEXT,
310:             created_at TEXT
311:         )
312:         """
313:     )
314:     conn.execute(
315:         """
316:         CREATE TABLE IF NOT EXISTS raw_format_notes (
317:             note_id INTEGER PRIMARY KEY AUTOINCREMENT,
318:             format_id INTEGER NOT NULL,
319:             note TEXT NOT NULL,
320:             created_at TEXT,
321:             FOREIGN KEY (format_id) REFERENCES raw_formats(format_id)
322:         )
323:         """
324:     )
325:     conn.execute(
326:         """
327:         CREATE TABLE IF NOT EXISTS templates (
328:             template_id INTEGER PRIMARY KEY AUTOINCREMENT,
329:             name TEXT UNIQUE NOT NULL,
330:             description TEXT,
331:             version TEXT,
332:             created_at TEXT,
333:             table_name TEXT NOT NULL
334:         )
335:         """
336:     )
337:     conn.execute(
338:         """
339:         CREATE TABLE IF NOT EXISTS template_fields (
340:             template_id INTEGER NOT NULL,
341:             field_id INTEGER NOT NULL,
342:             safe_name TEXT NOT NULL,
343:             name TEXT NOT NULL,
344:             dtype TEXT,
345:             role TEXT,
346:             required INTEGER DEFAULT 0,
347:             PRIMARY KEY (template_id, field_id),
348:             UNIQUE (template_id, name),
349:             FOREIGN KEY (template_id) REFERENCES templates(template_id)
350:         )
351:         """
352:     )
353:     conn.execute(
354:         """
355:         CREATE TABLE IF NOT EXISTS dataset_templates (
356:             dataset_version_id TEXT NOT NULL,
357:             template_id INTEGER NOT NULL,
358:             mapping_json TEXT NOT NULL,
359:             mapping_hash TEXT NOT NULL,
360:             status TEXT NOT NULL,
361:             created_at TEXT,
362:             updated_at TEXT,
363:             PRIMARY KEY (dataset_version_id, template_id),
364:             FOREIGN KEY (template_id) REFERENCES templates(template_id)
365:         )
366:         """
367:     )
368:     conn.execute(
369:         """
370:         CREATE TABLE IF NOT EXISTS template_conversions (
371:             conversion_id INTEGER PRIMARY KEY AUTOINCREMENT,
372:             dataset_version_id TEXT NOT NULL,
373:             template_id INTEGER NOT NULL,
374:             status TEXT NOT NULL,
375:             started_at TEXT,
376:             completed_at TEXT,
377:             error_json TEXT,
378:             mapping_hash TEXT NOT NULL,
379:             row_count INTEGER DEFAULT 0,
380:             FOREIGN KEY (template_id) REFERENCES templates(template_id)
381:         )
382:         """
383:     )
384:     if not _column_exists(conn, "dataset_versions", "raw_format_id"):
385:         conn.execute("ALTER TABLE dataset_versions ADD COLUMN raw_format_id INTEGER")
386:     conn.execute(
387:         "CREATE INDEX IF NOT EXISTS idx_dataset_templates_status ON dataset_templates(status)"
388:     )
389:     conn.execute(
390:         "CREATE INDEX IF NOT EXISTS idx_template_fields_template ON template_fields(template_id)"
391:     )
392: 
393: 
394: def migration_7(conn: sqlite3.Connection) -> None:
395:     conn.execute(
396:         """
397:         CREATE TABLE IF NOT EXISTS raw_format_mappings (
398:             mapping_id INTEGER PRIMARY KEY AUTOINCREMENT,
399:             format_id INTEGER NOT NULL,
400:             template_id INTEGER NOT NULL,
401:             mapping_json TEXT NOT NULL,
402:             mapping_hash TEXT NOT NULL,
403:             notes TEXT,
404:             created_at TEXT,
405:             UNIQUE (format_id, template_id, mapping_hash),
406:             FOREIGN KEY (format_id) REFERENCES raw_formats(format_id),
407:             FOREIGN KEY (template_id) REFERENCES templates(template_id)
408:         )
409:         """
410:     )
411:     conn.execute(
412:         "CREATE INDEX IF NOT EXISTS idx_raw_format_mappings_format ON raw_format_mappings(format_id, template_id)"
413:     )
414: 
415: 
416: def migration_8(conn: sqlite3.Connection) -> None:
417:     conn.execute(
418:         """
419:         CREATE TABLE IF NOT EXISTS plugin_executions (
420:             execution_id INTEGER PRIMARY KEY AUTOINCREMENT,
421:             run_id TEXT NOT NULL,
422:             plugin_id TEXT NOT NULL,
423:             plugin_version TEXT,
424:             started_at TEXT,
425:             completed_at TEXT,
426:             duration_ms INTEGER,
427:             status TEXT,
428:             exit_code INTEGER,
429:             cpu_user REAL,
430:             cpu_system REAL,
431:             max_rss INTEGER,
432:             warnings_count INTEGER,
433:             stdout TEXT,
434:             stderr TEXT,
435:             FOREIGN KEY (run_id) REFERENCES runs(run_id)
436:         )
437:         """
438:     )
439:     conn.execute(
440:         "CREATE INDEX IF NOT EXISTS idx_plugin_executions_run ON plugin_executions(run_id, plugin_id)"
441:     )
442: 
443: 
444: def migration_9(conn: sqlite3.Connection) -> None:
445:     if not _column_exists(conn, "runs", "run_seed"):
446:         conn.execute("ALTER TABLE runs ADD COLUMN run_seed INTEGER DEFAULT 0")
447: 
448: 
449: def migration_10(conn: sqlite3.Connection) -> None:
450:     conn.execute(
451:         """
452:         CREATE TABLE IF NOT EXISTS known_issue_sets (
453:             set_id INTEGER PRIMARY KEY AUTOINCREMENT,
454:             sha256 TEXT NOT NULL UNIQUE,
455:             upload_id TEXT,
456:             strict INTEGER DEFAULT 1,
457:             notes TEXT,
458:             created_at TEXT,
459:             updated_at TEXT
460:         )
461:         """
462:     )
463:     conn.execute(
464:         """
465:         CREATE TABLE IF NOT EXISTS known_issues (
466:             issue_id INTEGER PRIMARY KEY AUTOINCREMENT,
467:             set_id INTEGER NOT NULL,
468:             title TEXT,
469:             plugin_id TEXT,
470:             kind TEXT NOT NULL,
471:             where_json TEXT,
472:             contains_json TEXT,
473:             min_count INTEGER,
474:             max_count INTEGER,
475:             created_at TEXT,
476:             updated_at TEXT,
477:             FOREIGN KEY (set_id) REFERENCES known_issue_sets(set_id)
478:         )
479:         """
480:     )
481:     conn.execute(
482:         "CREATE INDEX IF NOT EXISTS idx_known_issues_set ON known_issues(set_id)"
483:     )
484:     conn.execute(
485:         "CREATE INDEX IF NOT EXISTS idx_known_issues_kind ON known_issues(kind)"
486:     )
487: 
488: 
489: def migration_11(conn: sqlite3.Connection) -> None:
490:     if not _column_exists(conn, "projects", "erp_type"):
491:         conn.execute("ALTER TABLE projects ADD COLUMN erp_type TEXT")
492:     conn.execute(
493:         "UPDATE projects SET erp_type = 'unknown' WHERE erp_type IS NULL OR erp_type = ''"
494:     )
495: 
496:     if not _column_exists(conn, "known_issue_sets", "scope_type"):
497:         conn.execute("ALTER TABLE known_issue_sets ADD COLUMN scope_type TEXT")
498:     if not _column_exists(conn, "known_issue_sets", "scope_value"):
499:         conn.execute("ALTER TABLE known_issue_sets ADD COLUMN scope_value TEXT")
500:     conn.execute(
501:         "UPDATE known_issue_sets SET scope_type = 'sha256', scope_value = sha256 "
502:         "WHERE scope_type IS NULL OR scope_value IS NULL"
503:     )
504: 
505:     conn.execute(
506:         """
507:         CREATE TABLE IF NOT EXISTS dataset_role_candidates (
508:             dataset_version_id TEXT NOT NULL,
509:             column_id INTEGER NOT NULL,
510:             role TEXT NOT NULL,
511:             score REAL NOT NULL,
512:             reasons_json TEXT,
513:             created_at TEXT,
514:             PRIMARY KEY (dataset_version_id, column_id, role)
515:         )
516:         """
517:     )
518:     conn.execute(
519:         "CREATE INDEX IF NOT EXISTS idx_dataset_role_candidates "
520:         "ON dataset_role_candidates(dataset_version_id, role, score)"
521:     )
522: 
523:     conn.execute(
524:         """
525:         CREATE TABLE IF NOT EXISTS project_role_overrides (
526:             project_id TEXT NOT NULL,
527:             role TEXT NOT NULL,
528:             column_name TEXT NOT NULL,
529:             created_at TEXT,
530:             updated_at TEXT,
531:             PRIMARY KEY (project_id, role),
532:             FOREIGN KEY (project_id) REFERENCES projects(project_id)
533:         )
534:         """
535:     )
536:     conn.execute(
537:         "CREATE INDEX IF NOT EXISTS idx_project_role_overrides_project "
538:         "ON project_role_overrides(project_id)"
539:     )
540: 
541: 
542: def migration_12(conn: sqlite3.Connection) -> None:
543:     if not _column_exists(conn, "known_issues", "description"):
544:         conn.execute("ALTER TABLE known_issues ADD COLUMN description TEXT")
545: 
546: 
547: def migration_13(conn: sqlite3.Connection) -> None:
548:     conn.execute(
549:         """
550:         CREATE TABLE IF NOT EXISTS project_plugin_settings (
551:             project_id TEXT NOT NULL,
552:             plugin_id TEXT NOT NULL,
553:             settings_json TEXT,
554:             created_at TEXT,
555:             updated_at TEXT,
556:             PRIMARY KEY (project_id, plugin_id),
557:             FOREIGN KEY (project_id) REFERENCES projects(project_id)
558:         )
559:         """
560:     )
561:     conn.execute(
562:         "CREATE INDEX IF NOT EXISTS idx_project_plugin_settings_project "
563:         "ON project_plugin_settings(project_id)"
564:     )
565: 
566: 
567: def migration_14(conn: sqlite3.Connection) -> None:
568:     if not _column_exists(conn, "plugin_results_v2", "budget_json"):
569:         conn.execute("ALTER TABLE plugin_results_v2 ADD COLUMN budget_json TEXT")
570: 
571: 
572: def migration_15(conn: sqlite3.Connection) -> None:
573:     if not _column_exists(conn, "dataset_columns", "pii_tags_json"):
574:         conn.execute("ALTER TABLE dataset_columns ADD COLUMN pii_tags_json TEXT")
575:     conn.execute(
576:         """
577:         CREATE TABLE IF NOT EXISTS pii_salts (
578:             tenant_id TEXT PRIMARY KEY,
579:             salt TEXT NOT NULL,
580:             created_at TEXT
581:         )
582:         """
583:     )
584:     conn.execute(
585:         """
586:         CREATE TABLE IF NOT EXISTS pii_entities (
587:             entity_id INTEGER PRIMARY KEY AUTOINCREMENT,
588:             tenant_id TEXT NOT NULL,
589:             pii_type TEXT NOT NULL,
590:             raw_value TEXT NOT NULL,
591:             value_hash TEXT NOT NULL,
592:             created_at TEXT,
593:             UNIQUE (tenant_id, pii_type, raw_value)
594:         )
595:         """
596:     )
597:     conn.execute(
598:         "CREATE INDEX IF NOT EXISTS idx_pii_entities_hash ON pii_entities(tenant_id, value_hash)"
599:     )
600: 
601: 
602: def migration_16(conn: sqlite3.Connection) -> None:
603:     conn.execute(
604:         """
605:         CREATE TABLE IF NOT EXISTS tenants (
606:             tenant_id TEXT PRIMARY KEY,
607:             name TEXT,
608:             created_at TEXT,
609:             is_default INTEGER DEFAULT 0
610:         )
611:         """
612:     )
613:     conn.execute(
614:         """
615:         INSERT OR IGNORE INTO tenants (tenant_id, name, created_at, is_default)
616:         VALUES (?, ?, ?, 1)
617:         """,
618:         (DEFAULT_TENANT_ID, "Default", now_iso()),
619:     )
620: 
621:     tenant_tables = [
622:         "projects",
623:         "datasets",
624:         "dataset_versions",
625:         "dataset_columns",
626:         "uploads",
627:         "runs",
628:         "plugin_results_v2",
629:         "plugin_executions",
630:         "known_issue_sets",
631:         "dataset_role_candidates",
632:         "project_role_overrides",
633:         "project_plugin_settings",
634:         "analysis_jobs",
635:         "deliveries",
636:         "raw_formats",
637:         "raw_format_notes",
638:         "raw_format_mappings",
639:         "templates",
640:         "template_fields",
641:         "dataset_templates",
642:         "template_conversions",
643:     ]
644:     for table in tenant_tables:
645:         if not _column_exists(conn, table, "tenant_id"):
646:             conn.execute(f"ALTER TABLE {table} ADD COLUMN tenant_id TEXT")
647:         conn.execute(
648:             f"UPDATE {table} SET tenant_id = ? WHERE tenant_id IS NULL OR tenant_id = ''",
649:             (DEFAULT_TENANT_ID,),
650:         )
651: 
652:     conn.execute(
653:         "CREATE INDEX IF NOT EXISTS idx_projects_tenant ON projects(tenant_id)"
654:     )
655:     conn.execute(
656:         "CREATE INDEX IF NOT EXISTS idx_datasets_tenant ON datasets(tenant_id)"
657:     )
658:     conn.execute(
659:         "CREATE INDEX IF NOT EXISTS idx_dataset_versions_tenant ON dataset_versions(tenant_id)"
660:     )
661:     conn.execute(
662:         "CREATE INDEX IF NOT EXISTS idx_runs_tenant ON runs(tenant_id)"
663:     )
664:     conn.execute(
665:         "CREATE INDEX IF NOT EXISTS idx_uploads_tenant ON uploads(tenant_id)"
666:     )
667:     conn.execute(
668:         "CREATE INDEX IF NOT EXISTS idx_plugin_results_tenant ON plugin_results_v2(tenant_id)"
669:     )
670:     conn.execute(
671:         "CREATE INDEX IF NOT EXISTS idx_known_issue_sets_tenant ON known_issue_sets(tenant_id)"
672:     )
673: 
674: 
675: def migration_17(conn: sqlite3.Connection) -> None:
676:     conn.execute(
677:         """
678:         CREATE TABLE IF NOT EXISTS users (
679:             user_id INTEGER PRIMARY KEY AUTOINCREMENT,
680:             email TEXT UNIQUE NOT NULL,
681:             name TEXT,
682:             password_hash TEXT NOT NULL,
683:             is_admin INTEGER DEFAULT 0,
684:             created_at TEXT,
685:             disabled_at TEXT
686:         )
687:         """
688:     )
689:     conn.execute(
690:         """
691:         CREATE TABLE IF NOT EXISTS tenant_memberships (
692:             membership_id INTEGER PRIMARY KEY AUTOINCREMENT,
693:             tenant_id TEXT NOT NULL,
694:             user_id INTEGER NOT NULL,
695:             role TEXT NOT NULL,
696:             created_at TEXT,
697:             UNIQUE (tenant_id, user_id),
698:             FOREIGN KEY (tenant_id) REFERENCES tenants(tenant_id),
699:             FOREIGN KEY (user_id) REFERENCES users(user_id)
700:         )
701:         """
702:     )
703:     conn.execute(
704:         """
705:         CREATE TABLE IF NOT EXISTS user_sessions (
706:             session_id INTEGER PRIMARY KEY AUTOINCREMENT,
707:             user_id INTEGER NOT NULL,
708:             tenant_id TEXT NOT NULL,
709:             token_hash TEXT UNIQUE NOT NULL,
710:             created_at TEXT,
711:             last_seen_at TEXT,
712:             expires_at TEXT,
713:             revoked_at TEXT,
714:             FOREIGN KEY (user_id) REFERENCES users(user_id),
715:             FOREIGN KEY (tenant_id) REFERENCES tenants(tenant_id)
716:         )
717:         """
718:     )
719:     conn.execute(
720:         """
721:         CREATE TABLE IF NOT EXISTS api_keys (
722:             key_id INTEGER PRIMARY KEY AUTOINCREMENT,
723:             user_id INTEGER NOT NULL,
724:             tenant_id TEXT NOT NULL,
725:             name TEXT,
726:             key_hash TEXT UNIQUE NOT NULL,
727:             created_at TEXT,
728:             last_used_at TEXT,
729:             revoked_at TEXT,
730:             FOREIGN KEY (user_id) REFERENCES users(user_id),
731:             FOREIGN KEY (tenant_id) REFERENCES tenants(tenant_id)
732:         )
733:         """
734:     )
735:     conn.execute(
736:         "CREATE INDEX IF NOT EXISTS idx_users_email ON users(email)"
737:     )
738:     conn.execute(
739:         "CREATE INDEX IF NOT EXISTS idx_memberships_tenant ON tenant_memberships(tenant_id)"
740:     )
741:     conn.execute(
742:         "CREATE INDEX IF NOT EXISTS idx_memberships_user ON tenant_memberships(user_id)"
743:     )
744:     conn.execute(
745:         "CREATE INDEX IF NOT EXISTS idx_sessions_token ON user_sessions(token_hash)"
746:     )
747:     conn.execute(
748:         "CREATE INDEX IF NOT EXISTS idx_sessions_user ON user_sessions(user_id)"
749:     )
750:     conn.execute(
751:         "CREATE INDEX IF NOT EXISTS idx_api_keys_hash ON api_keys(key_hash)"
752:     )
753: 
754: 
755: def migration_18(conn: sqlite3.Connection) -> None:
756:     conn.execute(
757:         """
758:         CREATE TABLE IF NOT EXISTS vector_collections (
759:             collection_id INTEGER PRIMARY KEY AUTOINCREMENT,
760:             tenant_id TEXT NOT NULL,
761:             name TEXT NOT NULL,
762:             dimensions INTEGER NOT NULL,
763:             table_name TEXT NOT NULL,
764:             created_at TEXT,
765:             UNIQUE (tenant_id, name, dimensions)
766:         )
767:         """
768:     )
769:     conn.execute(
770:         "CREATE INDEX IF NOT EXISTS idx_vector_collections_tenant ON vector_collections(tenant_id)"
771:     )
772: 
773: 
774: def migration_19(conn: sqlite3.Connection) -> None:
775:     if not _column_exists(conn, "known_issue_sets", "nl_json"):
776:         conn.execute("ALTER TABLE known_issue_sets ADD COLUMN nl_json TEXT")
777:     if not _column_exists(conn, "known_issues", "source_text"):
778:         conn.execute("ALTER TABLE known_issues ADD COLUMN source_text TEXT")
779: 
780: 
781: MIGRATIONS: list[Migration] = [
782:     migration_1,
783:     migration_2,
784:     migration_3,
785:     migration_4,
786:     migration_5,
787:     migration_6,
788:     migration_7,
789:     migration_8,
790:     migration_9,
791:     migration_10,
792:     migration_11,
793:     migration_12,
794:     migration_13,
795:     migration_14,
796:     migration_15,
797:     migration_16,
798:     migration_17,
799:     migration_18,
800:     migration_19,
801: ]
802: 
803: 
804: def run_migrations(conn: sqlite3.Connection) -> None:
805:     _ensure_schema_migrations(conn)
806:     cur = conn.execute("PRAGMA user_version")
807:     current = int(cur.fetchone()[0])
808:     for version, migration in enumerate(MIGRATIONS, start=1):
809:         if version <= current:
810:             continue
811:         migration(conn)
812:         conn.execute(
813:             "INSERT INTO schema_migrations (version, applied_at) VALUES (?, ?)",
814:             (version, now_iso()),
815:         )
816:         conn.execute(f"PRAGMA user_version = {version}")
````

## File: src/statistic_harness/core/pipeline.py
````python
  1: from __future__ import annotations
  2: 
  3: import hashlib
  4: import traceback
  5: from pathlib import Path
  6: from concurrent.futures import ThreadPoolExecutor
  7: import os
  8: from typing import Any
  9: 
 10: from .dataset_io import resolve_dataset_accessor
 11: from .plugin_manager import PluginManager, PluginSpec
 12: from .plugin_runner import run_plugin_subprocess
 13: from .storage import Storage
 14: from .tenancy import get_tenant_context, scope_identifier, tenancy_enabled
 15: from .types import PluginContext, PluginError, PluginResult
 16: from .utils import (
 17:     dataset_key,
 18:     ensure_dir,
 19:     file_sha256,
 20:     json_dumps,
 21:     make_run_id,
 22:     now_iso,
 23:     DEFAULT_TENANT_ID,
 24: )
 25: from .report import build_report, write_report
 26: 
 27: 
 28: class Pipeline:
 29:     def __init__(
 30:         self, base_dir: Path, plugins_dir: Path, tenant_id: str | None = None
 31:     ) -> None:
 32:         tenant_ctx = get_tenant_context(tenant_id, base_dir)
 33:         self.base_dir = tenant_ctx.tenant_root
 34:         self.appdata_root = tenant_ctx.appdata_root
 35:         self.tenant_id = tenant_ctx.tenant_id
 36:         self.plugins_dir = plugins_dir
 37:         self.storage = Storage(tenant_ctx.db_path, tenant_ctx.tenant_id)
 38:         self.manager = PluginManager(plugins_dir)
 39: 
 40:     def _toposort_layers(
 41:         self, specs: list[PluginSpec], selected: set[str]
 42:     ) -> list[list[PluginSpec]]:
 43:         spec_map = {spec.plugin_id: spec for spec in specs}
 44:         deps: dict[str, set[str]] = {}
 45:         indegree: dict[str, int] = {}
 46:         for pid in selected:
 47:             spec = spec_map.get(pid)
 48:             if not spec:
 49:                 continue
 50:             dep_set = {dep for dep in spec.depends_on if dep in selected}
 51:             deps[pid] = dep_set
 52:             indegree[pid] = len(dep_set)
 53: 
 54:         layers: list[list[PluginSpec]] = []
 55:         remaining = set(indegree.keys())
 56:         while remaining:
 57:             ready = sorted(pid for pid in remaining if indegree.get(pid, 0) == 0)
 58:             if not ready:
 59:                 raise ValueError("Cycle detected in plugin dependencies")
 60:             layer_specs = [spec_map[pid] for pid in ready]
 61:             layers.append(layer_specs)
 62:             for pid in ready:
 63:                 remaining.remove(pid)
 64:             for pid in remaining:
 65:                 if deps[pid].intersection(ready):
 66:                     indegree[pid] -= len(deps[pid].intersection(ready))
 67:         return layers
 68: 
 69:     def run(
 70:         self,
 71:         input_file: Path | None,
 72:         plugin_ids: list[str],
 73:         settings: dict[str, Any],
 74:         run_seed: int,
 75:         upload_id: str = "local",
 76:         run_id: str | None = None,
 77:         dataset_version_id: str | None = None,
 78:         project_id: str | None = None,
 79:     ) -> str:
 80:         run_id = run_id or make_run_id()
 81:         tenant_id = self.tenant_id
 82: 
 83:         def _scope(value: str | None) -> str | None:
 84:             if not value:
 85:                 return value
 86:             if tenancy_enabled() and tenant_id != DEFAULT_TENANT_ID:
 87:                 return scope_identifier(tenant_id, value)
 88:             return value
 89: 
 90:         dataset_version_id = _scope(dataset_version_id)
 91:         project_id = _scope(project_id)
 92: 
 93:         run_dir = self.base_dir / "runs" / run_id
 94:         ensure_dir(run_dir / "dataset")
 95:         ensure_dir(run_dir / "logs")
 96: 
 97:         canonical_path = run_dir / "dataset" / "canonical.csv"
 98:         if input_file is None:
 99:             if not dataset_version_id:
100:                 raise ValueError("Dataset version is required for DB-only runs")
101:             ctx_row = self.storage.get_dataset_version_context(dataset_version_id)
102:             if not ctx_row:
103:                 raise ValueError("Dataset version not found")
104:             project_id = ctx_row["project_id"]
105:             dataset_id = ctx_row["dataset_id"]
106:             input_hash = ctx_row.get("data_hash") or dataset_version_id
107:             input_filename = f"db://{dataset_version_id}"
108:         else:
109:             input_hash = file_sha256(input_file)
110:             project_id = project_id or input_hash
111:             dataset_id = dataset_version_id or (
112:                 input_hash
113:                 if project_id == input_hash
114:                 else dataset_key(project_id, input_hash)
115:             )
116:             dataset_version_id = dataset_version_id or dataset_id
117:             project_id = _scope(project_id)
118:             dataset_id = _scope(dataset_id)
119:             dataset_version_id = _scope(dataset_version_id)
120:             table_name = f"dataset_{dataset_version_id}"
121:             self.storage.ensure_project(project_id, project_id, now_iso())
122:             self.storage.ensure_dataset(dataset_id, project_id, dataset_id, now_iso())
123:             self.storage.ensure_dataset_version(
124:                 dataset_version_id, dataset_id, now_iso(), table_name, input_hash
125:             )
126:             input_filename = input_file.name
127: 
128:         self.storage.create_run(
129:             run_id=run_id,
130:             created_at=now_iso(),
131:             status="running",
132:             upload_id=upload_id,
133:             input_filename=input_filename,
134:             canonical_path=str(canonical_path),
135:             settings=settings,
136:             error=None,
137:             run_seed=run_seed,
138:             project_id=project_id,
139:             dataset_id=dataset_id,
140:             dataset_version_id=dataset_version_id,
141:             input_hash=input_hash,
142:         )
143: 
144:         specs = self.manager.discover()
145:         spec_map = {spec.plugin_id: spec for spec in specs}
146:         for err in self.manager.discovery_errors:
147:             plugin_id = err.plugin_id
148:             if plugin_id in spec_map:
149:                 plugin_id = f"{plugin_id}__discovery_error"
150:             record_missing(
151:                 plugin_id,
152:                 f"Plugin discovery error in {err.path}: {err.message}",
153:             )
154: 
155:         def record_missing(plugin_id: str, message: str) -> None:
156:             result = PluginResult(
157:                 status="error",
158:                 summary=message,
159:                 metrics={},
160:                 findings=[],
161:                 artifacts=[],
162:                 error=PluginError(
163:                     type="MissingPlugin",
164:                     message=message,
165:                     traceback="",
166:                 ),
167:             )
168:             self.storage.save_plugin_result(
169:                 run_id,
170:                 plugin_id,
171:                 None,
172:                 now_iso(),
173:                 None,
174:                 None,
175:                 input_hash,
176:                 result,
177:             )
178:             self.storage.insert_plugin_execution(
179:                 run_id=run_id,
180:                 plugin_id=plugin_id,
181:                 plugin_version=None,
182:                 started_at=now_iso(),
183:                 completed_at=now_iso(),
184:                 duration_ms=0,
185:                 status="error",
186:                 exit_code=None,
187:                 cpu_user=None,
188:                 cpu_system=None,
189:                 max_rss=None,
190:                 warnings_count=None,
191:                 stdout=None,
192:                 stderr=None,
193:             )
194: 
195:         selected = set(plugin_ids)
196:         auto_plan = not selected or "auto" in selected
197:         selected.discard("auto")
198:         if "all" in selected:
199:             selected = {spec.plugin_id for spec in specs if spec.type == "analysis"}
200:             auto_plan = False
201:         llm_selected = "llm_prompt_builder" in selected
202: 
203:         dataset_accessor, dataset_template = resolve_dataset_accessor(
204:             self.storage, dataset_version_id
205:         )
206: 
207:         column_lookup: dict[str, int] | None = None
208: 
209:         def get_column_lookup() -> dict[str, int]:
210:             nonlocal column_lookup
211:             if column_lookup is None and dataset_version_id:
212:                 if dataset_template:
213:                     fields = self.storage.fetch_template_fields(
214:                         int(dataset_template["template_id"])
215:                     )
216:                     column_lookup = {field["name"]: int(field["field_id"]) for field in fields}
217:                 else:
218:                     columns = self.storage.fetch_dataset_columns(dataset_version_id)
219:                     column_lookup = {
220:                         col["original_name"]: int(col["column_id"]) for col in columns
221:                     }
222:             return column_lookup or {}
223: 
224:         def infer_column_ids(item: dict[str, Any]) -> list[int]:
225:             lookup = get_column_lookup()
226:             ids: list[int] = []
227:             if "feature" in item and isinstance(item["feature"], str):
228:                 if item["feature"] in lookup:
229:                     ids.append(lookup[item["feature"]])
230:             if "pair" in item and isinstance(item["pair"], (list, tuple)):
231:                 for name in item["pair"]:
232:                     if isinstance(name, str) and name in lookup:
233:                         ids.append(lookup[name])
234:             if "columns" in item and isinstance(item["columns"], (list, tuple)):
235:                 for name in item["columns"]:
236:                     if isinstance(name, str) and name in lookup:
237:                         ids.append(lookup[name])
238:             return sorted(set(ids))
239: 
240:         def attach_evidence(findings: list[Any]) -> list[dict[str, Any]]:
241:             enriched: list[dict[str, Any]] = []
242:             for item in findings:
243:                 if isinstance(item, dict):
244:                     entry = dict(item)
245:                 else:
246:                     entry = {"value": item}
247:                 evidence = dict(entry.get("evidence") or {})
248:                 if isinstance(entry.get("dataset_id"), str):
249:                     evidence.setdefault("dataset_id", entry["dataset_id"])
250:                 evidence.setdefault("dataset_id", dataset_id or "unknown")
251:                 if isinstance(entry.get("dataset_version_id"), str):
252:                     evidence.setdefault("dataset_version_id", entry["dataset_version_id"])
253:                 evidence.setdefault("dataset_version_id", dataset_version_id or "unknown")
254:                 row_ids: list[int] = []
255:                 if "row_ids" in entry and isinstance(entry["row_ids"], list):
256:                     try:
257:                         row_ids = [int(v) for v in entry["row_ids"]]
258:                     except (TypeError, ValueError):
259:                         row_ids = []
260:                 if "row_index" in entry:
261:                     try:
262:                         row_ids.append(int(entry["row_index"]))
263:                     except (TypeError, ValueError):
264:                         pass
265:                 if row_ids:
266:                     row_ids = sorted(set(row_ids))
267:                 evidence.setdefault("row_ids", row_ids)
268:                 evidence.setdefault("column_ids", infer_column_ids(entry))
269:                 evidence.setdefault("query", entry.get("query"))
270:                 if "start" in entry and "end" in entry:
271:                     try:
272:                         evidence.setdefault(
273:                             "row_ranges",
274:                             [
275:                                 {
276:                                     "start": int(entry["start"]),
277:                                     "end": int(entry["end"]),
278:                                 }
279:                             ],
280:                         )
281:                     except (TypeError, ValueError):
282:                         pass
283:                 if "measurement_type" not in entry:
284:                     entry["measurement_type"] = "measured"
285:                 entry["evidence"] = evidence
286:                 enriched.append(entry)
287:             return enriched
288: 
289:         def validate_modeled_findings(findings: list[dict[str, Any]]) -> list[str]:
290:             errors: list[str] = []
291:             for item in findings:
292:                 if item.get("measurement_type") != "modeled":
293:                     continue
294:                 scope = item.get("scope")
295:                 assumptions = item.get("assumptions")
296:                 if not isinstance(scope, dict) or not scope:
297:                     errors.append("modeled finding missing scope")
298:                 if (
299:                     not isinstance(assumptions, list)
300:                     or not assumptions
301:                     or not all(isinstance(a, str) and a.strip() for a in assumptions)
302:                 ):
303:                     errors.append("modeled finding missing assumptions")
304:             return errors
305: 
306:         def logger(msg: str) -> None:
307:             log_path = run_dir / "logs" / "run.log"
308:             log_path.parent.mkdir(parents=True, exist_ok=True)
309:             with log_path.open("a", encoding="utf-8") as handle:
310:                 handle.write(msg + "\n")
311: 
312:         def run_spec(spec: PluginSpec, include_input: bool = False) -> PluginResult:
313:             plugin_settings = dict(spec.settings.get("defaults", {}))
314:             plugin_settings.update(settings.get(spec.plugin_id, {}))
315:             if include_input and input_file is not None:
316:                 plugin_settings["input_file"] = str(input_file)
317:             budget = plugin_settings.get("budget")
318:             if not isinstance(budget, dict):
319:                 budget = {}
320:             budget = {
321:                 "row_limit": budget.get("row_limit"),
322:                 "sampled": bool(budget.get("sampled", False)),
323:                 "time_limit_ms": budget.get("time_limit_ms"),
324:                 "cpu_limit_ms": budget.get("cpu_limit_ms"),
325:             }
326:             execution_id = self.storage.start_plugin_execution(
327:                 run_id,
328:                 spec.plugin_id,
329:                 spec.version,
330:                 now_iso(),
331:                 status="running",
332:             )
333:             def dataset_loader(
334:                 columns: list[str] | None = None, row_limit: int | None = None
335:             ):
336:                 limit = row_limit
337:                 if limit is None:
338:                     limit = budget.get("row_limit")
339:                 return dataset_accessor.load(columns=columns, row_limit=limit)
340: 
341:             ctx = PluginContext(
342:                 run_id=run_id,
343:                 run_dir=run_dir,
344:                 settings=plugin_settings,
345:                 run_seed=run_seed,
346:                 logger=logger,
347:                 storage=self.storage,
348:                 dataset_loader=dataset_loader,
349:                 budget=budget,
350:                 tenant_id=self.tenant_id,
351:                 project_id=project_id,
352:                 dataset_id=dataset_id,
353:                 dataset_version_id=dataset_version_id,
354:                 input_hash=input_hash,
355:             )
356:             try:
357:                 self.manager.validate_config(spec, plugin_settings)
358:                 allow_paths: list[str] = []
359:                 for token in spec.sandbox.get("fs_allowlist", []):
360:                     if token == "appdata":
361:                         allow_paths.append(str(self.base_dir))
362:                     elif token == "plugins":
363:                         allow_paths.append(str(self.plugins_dir))
364:                     elif token == "run_dir":
365:                         allow_paths.append(str(run_dir))
366:                     else:
367:                         allow_paths.append(
368:                             str((self.plugins_dir.parent / token).resolve())
369:                         )
370:                 allow_paths.append(str(self.plugins_dir.parent / "src"))
371:                 allow_paths.append(str(run_dir))
372:                 allow_paths.append(str(self.appdata_root / "state.sqlite"))
373:                 if include_input and input_file is not None:
374:                     allow_paths.append(str(input_file))
375:                 request = {
376:                     "plugin_id": spec.plugin_id,
377:                     "entrypoint": spec.entrypoint,
378:                     "settings": plugin_settings,
379:                     "run_id": run_id,
380:                     "run_dir": str(run_dir),
381:                     "run_seed": run_seed,
382:                     "dataset_version_id": dataset_version_id,
383:                     "project_id": project_id,
384:                     "dataset_id": dataset_id,
385:                     "input_hash": input_hash,
386:                     "budget": budget,
387:                     "tenant_id": self.tenant_id,
388:                     "appdata_dir": str(self.appdata_root),
389:                     "root_dir": str(self.plugins_dir.parent.resolve()),
390:                     "sandbox": spec.sandbox,
391:                     "allow_paths": allow_paths,
392:                 }
393:                 runner = run_plugin_subprocess(
394:                     spec, request, run_dir, self.plugins_dir.parent
395:                 )
396:                 result = runner.result
397:             except Exception as exc:  # pragma: no cover - error flow
398:                 tb = traceback.format_exc()
399:                 result = PluginResult(
400:                     status="error",
401:                     summary=f"{spec.plugin_id} failed",
402:                     metrics={},
403:                     findings=[],
404:                     artifacts=[],
405:                     error=PluginError(
406:                         type=type(exc).__name__, message=str(exc), traceback=tb
407:                     ),
408:                 )
409:             result.findings = attach_evidence(result.findings)
410:             try:
411:                 payload = self.manager.result_payload(result)
412:                 self.manager.validate_output(spec, payload)
413:                 modeled_errors = validate_modeled_findings(result.findings)
414:                 if modeled_errors:
415:                     raise ValueError("; ".join(sorted(set(modeled_errors))))
416:             except Exception as exc:  # pragma: no cover - error flow
417:                 tb = traceback.format_exc()
418:                 result = PluginResult(
419:                     status="error",
420:                     summary=f"{spec.plugin_id} output validation failed: {exc}",
421:                     metrics={},
422:                     findings=[],
423:                     artifacts=[],
424:                     error=PluginError(
425:                         type=type(exc).__name__, message=str(exc), traceback=tb
426:                     ),
427:                 )
428:             if result.error:
429:                 logger(f"[ERROR] {spec.plugin_id}: {result.error.message}")
430:             if "runner" in locals():
431:                 exec_info = runner.execution
432:                 self.storage.update_plugin_execution(
433:                     execution_id=execution_id,
434:                     completed_at=exec_info.get("completed_at"),
435:                     duration_ms=exec_info.get("duration_ms"),
436:                     status=result.status,
437:                     exit_code=runner.exit_code,
438:                     cpu_user=exec_info.get("cpu_user"),
439:                     cpu_system=exec_info.get("cpu_system"),
440:                     max_rss=exec_info.get("max_rss"),
441:                     warnings_count=exec_info.get("warnings_count"),
442:                     stdout=runner.stdout,
443:                     stderr=runner.stderr,
444:                 )
445:             else:
446:                 self.storage.update_plugin_execution(
447:                     execution_id=execution_id,
448:                     completed_at=now_iso(),
449:                     duration_ms=0,
450:                     status=result.status,
451:                     exit_code=None,
452:                     cpu_user=None,
453:                     cpu_system=None,
454:                     max_rss=None,
455:                     warnings_count=None,
456:                     stdout=None,
457:                     stderr=None,
458:                 )
459:             module_path, _ = spec.entrypoint.split(":", 1)
460:             if module_path.endswith(".py"):
461:                 module_file = spec.path / module_path
462:             else:
463:                 module_file = spec.path / f"{module_path}.py"
464:             code_hash = file_sha256(module_file) if module_file.exists() else None
465:             settings_hash = hashlib.sha256(
466:                 json_dumps(plugin_settings).encode("utf-8")
467:             ).hexdigest()
468:             self.storage.save_plugin_result(
469:                 run_id,
470:                 spec.plugin_id,
471:                 spec.version,
472:                 now_iso(),
473:                 code_hash,
474:                 settings_hash,
475:                 input_hash,
476:                 result,
477:             )
478:             return result
479: 
480:         if input_file is not None:
481:             ingest_spec = spec_map.get("ingest_tabular")
482:             if ingest_spec:
483:                 run_spec(ingest_spec, include_input=True)
484:             else:
485:                 record_missing("ingest_tabular", "Missing ingest plugin")
486: 
487:         if auto_plan:
488:             profile_specs = [spec for spec in specs if spec.type == "profile"]
489:             if profile_specs:
490:                 for spec in sorted(profile_specs, key=lambda item: item.plugin_id):
491:                     run_spec(spec)
492:             else:
493:                 record_missing("profile_basic", "Missing profile plugin")
494:             planner_spec = spec_map.get("planner_basic")
495:             if planner_spec:
496:                 plan_result = run_spec(planner_spec)
497:                 planned = plan_result.metrics.get("selected_plugins") or []
498:                 selected.update(planned)
499:             else:
500:                 record_missing("planner_basic", "Missing planner plugin")
501: 
502:         missing_manual = sorted(pid for pid in selected if pid not in spec_map)
503:         for pid in missing_manual:
504:             record_missing(pid, f"Unknown plugin id: {pid}")
505: 
506:         transform_ids = {
507:             pid
508:             for pid in selected
509:             if pid in spec_map and spec_map[pid].type == "transform"
510:         }
511:         if transform_ids:
512:             layers = self._toposort_layers(specs, transform_ids)
513:             for layer in layers:
514:                 for spec in layer:
515:                     run_spec(spec)
516:             dataset_accessor, dataset_template = resolve_dataset_accessor(
517:                 self.storage, dataset_version_id
518:             )
519:             column_lookup = None
520: 
521:         analysis_ids = {
522:             pid
523:             for pid in selected
524:             if pid in spec_map and spec_map[pid].type == "analysis"
525:         }
526:         layers = self._toposort_layers(specs, analysis_ids)
527:         for layer in layers:
528:             if len(layer) == 1:
529:                 run_spec(layer[0])
530:                 continue
531:             max_workers = min(len(layer), (os.cpu_count() or 1))
532:             with ThreadPoolExecutor(max_workers=max_workers) as executor:
533:                 futures = [executor.submit(run_spec, spec) for spec in layer]
534:                 for future in futures:
535:                     future.result()
536: 
537:         report_spec = spec_map.get("report_bundle")
538:         if report_spec:
539:             run_spec(report_spec)
540:             report_json = run_dir / "report.json"
541:             report_md = run_dir / "report.md"
542:             if not report_json.exists() or not report_md.exists():
543:                 report = build_report(
544:                     self.storage, run_id, run_dir, Path("docs/report.schema.json")
545:                 )
546:                 write_report(report, run_dir)
547:         else:
548:             record_missing("report_bundle", "Missing report plugin")
549:             report = build_report(
550:                 self.storage, run_id, run_dir, Path("docs/report.schema.json")
551:             )
552:             write_report(report, run_dir)
553: 
554:         if llm_selected:
555:             llm_spec = spec_map.get("llm_prompt_builder")
556:             if llm_spec:
557:                 run_spec(llm_spec)
558:             else:
559:                 record_missing("llm_prompt_builder", "Missing llm plugin")
560: 
561:         self.storage.update_run_status(run_id, "completed")
562:         return run_id
````

## File: src/statistic_harness/core/plugin_manager.py
````python
  1: from __future__ import annotations
  2: 
  3: import importlib
  4: from dataclasses import asdict, dataclass
  5: from pathlib import Path
  6: from typing import Any
  7: 
  8: import yaml
  9: from jsonschema import ValidationError, validate
 10: 
 11: from .utils import read_json
 12: 
 13: 
 14: @dataclass
 15: class PluginSpec:
 16:     plugin_id: str
 17:     name: str
 18:     version: str
 19:     type: str
 20:     entrypoint: str
 21:     depends_on: list[str]
 22:     settings: dict[str, Any]
 23:     path: Path
 24:     capabilities: list[str]
 25:     config_schema: Path
 26:     output_schema: Path
 27:     sandbox: dict[str, Any]
 28: 
 29: 
 30: @dataclass(frozen=True)
 31: class PluginDiscoveryError:
 32:     plugin_id: str
 33:     path: Path
 34:     message: str
 35: 
 36: 
 37: class PluginManager:
 38:     def __init__(self, plugins_dir: Path) -> None:
 39:         self.plugins_dir = plugins_dir
 40:         self._manifest_schema: dict[str, Any] | None = None
 41:         self._schema_cache: dict[Path, dict[str, Any]] = {}
 42:         self.discovery_errors: list[PluginDiscoveryError] = []
 43: 
 44:     def _record_discovery_error(
 45:         self, plugin_id: str, manifest: Path, message: str
 46:     ) -> None:
 47:         self.discovery_errors.append(
 48:             PluginDiscoveryError(
 49:                 plugin_id=plugin_id or manifest.parent.name,
 50:                 path=manifest,
 51:                 message=message,
 52:             )
 53:         )
 54: 
 55:     def discover(self) -> list[PluginSpec]:
 56:         specs: list[PluginSpec] = []
 57:         self.discovery_errors = []
 58:         manifest_schema = self._load_manifest_schema()
 59:         seen: set[str] = set()
 60:         for manifest in sorted(self.plugins_dir.glob("*/plugin.yaml")):
 61:             try:
 62:                 data = yaml.safe_load(manifest.read_text(encoding="utf-8"))
 63:             except Exception as exc:  # pragma: no cover - malformed YAML
 64:                 self._record_discovery_error(
 65:                     manifest.parent.name,
 66:                     manifest,
 67:                     f"Invalid YAML: {exc}",
 68:                 )
 69:                 continue
 70:             if not isinstance(data, dict):
 71:                 self._record_discovery_error(
 72:                     manifest.parent.name, manifest, "Invalid manifest payload"
 73:                 )
 74:                 continue
 75:             plugin_id = str(data.get("id") or manifest.parent.name)
 76:             try:
 77:                 validate(instance=data, schema=manifest_schema)
 78:             except ValidationError as exc:
 79:                 self._record_discovery_error(
 80:                     plugin_id, manifest, f"Invalid manifest: {exc.message}"
 81:                 )
 82:                 continue
 83:             if plugin_id in seen:
 84:                 self._record_discovery_error(
 85:                     plugin_id, manifest, "Duplicate plugin id"
 86:                 )
 87:                 continue
 88:             config_schema_path = manifest.parent / data["config_schema"]
 89:             output_schema_path = manifest.parent / data["output_schema"]
 90:             if not config_schema_path.exists():
 91:                 self._record_discovery_error(
 92:                     plugin_id,
 93:                     manifest,
 94:                     f"Missing config schema: {config_schema_path}",
 95:                 )
 96:                 continue
 97:             if not output_schema_path.exists():
 98:                 self._record_discovery_error(
 99:                     plugin_id,
100:                     manifest,
101:                     f"Missing output schema: {output_schema_path}",
102:                 )
103:                 continue
104:             defaults = data.get("settings", {}).get("defaults", {})
105:             if defaults is not None:
106:                 try:
107:                     self.validate_config_schema(config_schema_path, defaults)
108:                 except ValidationError as exc:
109:                     self._record_discovery_error(
110:                         plugin_id,
111:                         manifest,
112:                         f"Invalid config defaults: {exc.message}",
113:                     )
114:                     continue
115:             seen.add(plugin_id)
116:             specs.append(
117:                 PluginSpec(
118:                     plugin_id=plugin_id,
119:                     name=data["name"],
120:                     version=data["version"],
121:                     type=data["type"],
122:                     entrypoint=data["entrypoint"],
123:                     depends_on=data.get("depends_on", []),
124:                     settings=data.get("settings", {}),
125:                     path=manifest.parent,
126:                     capabilities=list(data.get("capabilities", [])),
127:                     config_schema=config_schema_path,
128:                     output_schema=output_schema_path,
129:                     sandbox=dict(data.get("sandbox", {})),
130:                 )
131:             )
132:         return specs
133: 
134:     def load_plugin(self, spec: PluginSpec) -> Any:
135:         module_path, class_name = spec.entrypoint.split(":", 1)
136:         if module_path.endswith(".py"):
137:             module_path = module_path[:-3]
138:         module_name = f"plugins.{spec.plugin_id}.{module_path}"
139:         module = importlib.import_module(module_name)
140:         return getattr(module, class_name)()
141: 
142:     def validate_config(self, spec: PluginSpec, config: dict[str, Any]) -> None:
143:         schema = self._load_schema(spec.config_schema)
144:         validate(instance=config, schema=schema)
145: 
146:     def validate_output(self, spec: PluginSpec, payload: dict[str, Any]) -> None:
147:         schema = self._load_schema(spec.output_schema)
148:         validate(instance=payload, schema=schema)
149: 
150:     @staticmethod
151:     def result_payload(result: Any) -> dict[str, Any]:
152:         return {
153:             "status": getattr(result, "status", None),
154:             "summary": getattr(result, "summary", ""),
155:             "metrics": getattr(result, "metrics", {}),
156:             "findings": getattr(result, "findings", []),
157:             "artifacts": [asdict(a) for a in getattr(result, "artifacts", [])],
158:             "budget": getattr(result, "budget", None),
159:             "error": asdict(result.error) if getattr(result, "error", None) else None,
160:         }
161: 
162:     def _load_schema(self, path: Path) -> dict[str, Any]:
163:         if path not in self._schema_cache:
164:             self._schema_cache[path] = read_json(path)
165:         return self._schema_cache[path]
166: 
167:     def _load_manifest_schema(self) -> dict[str, Any]:
168:         if self._manifest_schema is None:
169:             schema_path = self.plugins_dir.parent / "docs" / "plugin_manifest.schema.json"
170:             self._manifest_schema = read_json(schema_path)
171:         return self._manifest_schema
172: 
173:     def validate_config_schema(self, schema_path: Path, defaults: dict[str, Any]) -> None:
174:         schema = self._load_schema(schema_path)
175:         validate(instance=defaults, schema=schema)
````

## File: src/statistic_harness/core/tenancy.py
````python
 1: """Tenant context helpers for phase 2."""
 2: 
 3: from __future__ import annotations
 4: 
 5: from dataclasses import dataclass
 6: from pathlib import Path
 7: import os
 8: import re
 9: 
10: from .utils import DEFAULT_TENANT_ID, get_appdata_dir
11: 
12: 
13: _TENANT_RE = re.compile(r"^[A-Za-z0-9][A-Za-z0-9_]{0,63}$")
14: 
15: 
16: def tenancy_enabled() -> bool:
17:     raw = os.environ.get("STAT_HARNESS_ENABLE_TENANCY", "").strip().lower()
18:     return raw in {"1", "true", "yes", "on"}
19: 
20: 
21: def resolve_tenant_id(requested: str | None = None) -> str:
22:     env = os.environ.get("STAT_HARNESS_TENANT_ID", "").strip()
23:     tenant_id = (requested or env or DEFAULT_TENANT_ID).strip() or DEFAULT_TENANT_ID
24:     if not _TENANT_RE.match(tenant_id):
25:         raise ValueError("Invalid tenant_id")
26:     return tenant_id
27: 
28: 
29: @dataclass(frozen=True)
30: class TenantContext:
31:     tenant_id: str
32:     appdata_root: Path
33:     tenant_root: Path
34:     db_path: Path
35: 
36: 
37: def get_tenant_context(
38:     tenant_id: str | None = None, appdata_root: Path | None = None
39: ) -> TenantContext:
40:     root = appdata_root or get_appdata_dir()
41:     resolved = resolve_tenant_id(tenant_id)
42:     if tenancy_enabled():
43:         tenant_root = root / "tenants" / resolved
44:     else:
45:         tenant_root = root
46:     db_path = root / "state.sqlite"
47:     return TenantContext(
48:         tenant_id=resolved,
49:         appdata_root=root,
50:         tenant_root=tenant_root,
51:         db_path=db_path,
52:     )
53: 
54: 
55: def scope_identifier(tenant_id: str, raw_id: str) -> str:
56:     prefix = f"{tenant_id}__"
57:     if raw_id.startswith(prefix):
58:         return raw_id
59:     return f"{prefix}{raw_id}"
````

## File: src/statistic_harness/core/types.py
````python
 1: from __future__ import annotations
 2: 
 3: from dataclasses import dataclass, field
 4: from pathlib import Path
 5: from typing import Any, Callable, Protocol
 6: 
 7: 
 8: @dataclass
 9: class PluginArtifact:
10:     path: str
11:     type: str
12:     description: str
13: 
14: 
15: @dataclass
16: class PluginError:
17:     type: str
18:     message: str
19:     traceback: str
20: 
21: 
22: @dataclass
23: class PluginResult:
24:     status: str
25:     summary: str
26:     metrics: dict[str, Any]
27:     findings: list[dict[str, Any]]
28:     artifacts: list[PluginArtifact]
29:     error: PluginError | None = None
30:     budget: dict[str, Any] = field(
31:         default_factory=lambda: {
32:             "row_limit": None,
33:             "sampled": False,
34:             "time_limit_ms": None,
35:             "cpu_limit_ms": None,
36:         }
37:     )
38: 
39: 
40: @dataclass
41: class PluginContext:
42:     run_id: str
43:     run_dir: Path
44:     settings: dict[str, Any]
45:     run_seed: int
46:     logger: Callable[[str], None]
47:     storage: Any
48:     dataset_loader: Callable[..., Any]
49:     budget: dict[str, Any] = field(
50:         default_factory=lambda: {
51:             "row_limit": None,
52:             "sampled": False,
53:             "time_limit_ms": None,
54:             "cpu_limit_ms": None,
55:         }
56:     )
57:     tenant_id: str | None = None
58:     project_id: str | None = None
59:     dataset_id: str | None = None
60:     dataset_version_id: str | None = None
61:     input_hash: str | None = None
62: 
63:     def artifacts_dir(self, plugin_id: str) -> Path:
64:         path = self.run_dir / "artifacts" / plugin_id
65:         path.mkdir(parents=True, exist_ok=True)
66:         return path
67: 
68:     def write_text(self, path: Path, content: str) -> None:
69:         path.parent.mkdir(parents=True, exist_ok=True)
70:         path.write_text(content, encoding="utf-8")
71: 
72: 
73: class Plugin(Protocol):
74:     def run(self, ctx: PluginContext) -> PluginResult:  # pragma: no cover - protocol
75:         ...
````

## File: src/statistic_harness/core/utils.py
````python
  1: from __future__ import annotations
  2: 
  3: import hashlib
  4: import json
  5: import os
  6: import re
  7: import uuid
  8: from datetime import datetime, timezone
  9: from pathlib import Path
 10: from typing import Any
 11: 
 12: 
 13: def now_iso() -> str:
 14:     return datetime.now(timezone.utc).isoformat()
 15: 
 16: 
 17: def make_run_id() -> str:
 18:     return uuid.uuid4().hex
 19: 
 20: 
 21: def ensure_dir(path: Path) -> None:
 22:     path.mkdir(parents=True, exist_ok=True)
 23: 
 24: 
 25: _FLOAT_PRECISION = 10
 26: DEFAULT_TENANT_ID = "default"
 27: 
 28: 
 29: def _canonicalize(value: Any) -> Any:
 30:     if isinstance(value, float):
 31:         return round(value, _FLOAT_PRECISION)
 32:     if isinstance(value, dict):
 33:         return {key: _canonicalize(item) for key, item in value.items()}
 34:     if isinstance(value, list):
 35:         return [_canonicalize(item) for item in value]
 36:     if isinstance(value, tuple):
 37:         return [_canonicalize(item) for item in value]
 38:     return value
 39: 
 40: 
 41: def json_dumps(data: Any) -> str:
 42:     return json.dumps(
 43:         _canonicalize(data), ensure_ascii=False, indent=2, sort_keys=True
 44:     )
 45: 
 46: 
 47: def safe_join(base: Path, *paths: str) -> Path:
 48:     base_resolved = base.resolve()
 49:     joined = base_resolved.joinpath(*paths).resolve()
 50:     try:
 51:         joined.relative_to(base_resolved)
 52:     except ValueError as exc:
 53:         raise ValueError("Path traversal detected") from exc
 54:     return joined
 55: 
 56: 
 57: def stable_hash(text: str) -> int:
 58:     digest = hashlib.sha256(text.encode("utf-8")).hexdigest()
 59:     return int(digest[:8], 16)
 60: 
 61: 
 62: def scope_key(scope_type: str, scope_value: str) -> str:
 63:     if scope_type == "sha256":
 64:         return scope_value
 65:     digest = hashlib.sha256(f"{scope_type}:{scope_value}".encode("utf-8")).hexdigest()
 66:     return digest
 67: 
 68: 
 69: def file_sha256(path: Path, chunk_size: int = 1024 * 1024) -> str:
 70:     hasher = hashlib.sha256()
 71:     with path.open("rb") as handle:
 72:         while True:
 73:             chunk = handle.read(chunk_size)
 74:             if not chunk:
 75:                 break
 76:             hasher.update(chunk)
 77:     return hasher.hexdigest()
 78: 
 79: 
 80: def dataset_key(project_id: str, input_hash: str) -> str:
 81:     payload = f"{project_id}:{input_hash}".encode("utf-8")
 82:     return hashlib.sha256(payload).hexdigest()
 83: 
 84: 
 85: _IDENT_RE = re.compile(r"^[A-Za-z_][A-Za-z0-9_]*$")
 86: 
 87: 
 88: def quote_identifier(name: str) -> str:
 89:     if not _IDENT_RE.match(name):
 90:         raise ValueError(f"Unsafe identifier: {name}")
 91:     return f"\"{name}\""
 92: 
 93: 
 94: def read_json(path: Path) -> Any:
 95:     return json.loads(path.read_text(encoding="utf-8"))
 96: 
 97: 
 98: def write_json(path: Path, data: Any) -> None:
 99:     path.parent.mkdir(parents=True, exist_ok=True)
100:     path.write_text(json_dumps(data), encoding="utf-8")
101: 
102: 
103: def file_size_limit(path: Path, max_bytes: int) -> None:
104:     size = path.stat().st_size
105:     if size > max_bytes:
106:         raise ValueError(f"File too large: {size} bytes")
107: 
108: 
109: def max_upload_bytes() -> int | None:
110:     raw = os.environ.get("STAT_HARNESS_MAX_UPLOAD_BYTES", "").strip()
111:     if not raw:
112:         return None
113:     try:
114:         limit = int(raw)
115:     except ValueError:
116:         return None
117:     if limit <= 0:
118:         return None
119:     return limit
120: 
121: 
122: def auth_enabled() -> bool:
123:     raw = os.environ.get("STAT_HARNESS_ENABLE_AUTH", "").strip().lower()
124:     return raw in {"1", "true", "yes", "on"}
125: 
126: 
127: def vector_store_enabled() -> bool:
128:     raw = os.environ.get("STAT_HARNESS_ENABLE_VECTOR_STORE", "").strip().lower()
129:     return raw in {"1", "true", "yes", "on"}
130: 
131: 
132: def get_appdata_dir() -> Path:
133:     return Path(os.environ.get("STAT_HARNESS_APPDATA", "appdata"))
````

## File: src/statistic_harness/core/vector_store.py
````python
  1: """Local sqlite-vec vector store backend."""
  2: 
  3: from __future__ import annotations
  4: 
  5: import hashlib
  6: import json
  7: import math
  8: import os
  9: import re
 10: import sqlite3
 11: from contextlib import contextmanager
 12: from pathlib import Path
 13: from typing import Any, Iterable
 14: 
 15: from .utils import DEFAULT_TENANT_ID, ensure_dir, json_dumps, now_iso, quote_identifier, scope_key, vector_store_enabled
 16: 
 17: 
 18: _TOKEN_RE = re.compile(r"[A-Za-z0-9_]+")
 19: 
 20: 
 21: def hash_embedding(text: str, dimensions: int = 128) -> list[float]:
 22:     tokens = _TOKEN_RE.findall(text.lower())
 23:     vector = [0.0] * dimensions
 24:     for token in tokens:
 25:         digest = hashlib.sha256(token.encode("utf-8")).digest()
 26:         idx = int.from_bytes(digest[:4], "big") % dimensions
 27:         sign = 1.0 if digest[4] % 2 == 0 else -1.0
 28:         vector[idx] += sign
 29:     norm = math.sqrt(sum(val * val for val in vector))
 30:     if norm > 0:
 31:         vector = [val / norm for val in vector]
 32:     return vector
 33: 
 34: 
 35: class VectorStore:
 36:     def __init__(self, db_path: Path, tenant_id: str | None = None) -> None:
 37:         if not vector_store_enabled():
 38:             raise RuntimeError(
 39:                 "Vector store disabled (set STAT_HARNESS_ENABLE_VECTOR_STORE=1)"
 40:             )
 41:         ensure_dir(db_path.parent)
 42:         self.db_path = db_path
 43:         self.tenant_id = tenant_id or DEFAULT_TENANT_ID
 44:         with self.connection() as conn:
 45:             self._ensure_meta(conn)
 46: 
 47:     def _connect(self) -> sqlite3.Connection:
 48:         conn = sqlite3.connect(self.db_path, check_same_thread=False)
 49:         conn.row_factory = sqlite3.Row
 50:         conn.execute("PRAGMA foreign_keys = ON")
 51:         conn.execute("PRAGMA journal_mode = WAL")
 52:         conn.execute("PRAGMA synchronous = NORMAL")
 53:         self._ensure_extension(conn)
 54:         return conn
 55: 
 56:     @contextmanager
 57:     def connection(self) -> Iterable[sqlite3.Connection]:
 58:         conn = self._connect()
 59:         try:
 60:             yield conn
 61:             conn.commit()
 62:         except Exception:
 63:             conn.rollback()
 64:             raise
 65:         finally:
 66:             conn.close()
 67: 
 68:     def _ensure_extension(self, conn: sqlite3.Connection) -> None:
 69:         try:
 70:             conn.execute("SELECT vec_version()").fetchone()
 71:             return
 72:         except sqlite3.OperationalError:
 73:             pass
 74: 
 75:         path = os.environ.get("STAT_HARNESS_SQLITE_VEC_PATH", "").strip()
 76:         if not path:
 77:             raise RuntimeError(
 78:                 "sqlite-vec extension unavailable; set STAT_HARNESS_SQLITE_VEC_PATH"
 79:             )
 80:         try:
 81:             conn.enable_load_extension(True)
 82:             conn.load_extension(path)
 83:         except (AttributeError, sqlite3.OperationalError) as exc:
 84:             raise RuntimeError(
 85:                 f"Failed to load sqlite-vec extension from {path}: {exc}"
 86:             ) from exc
 87:         finally:
 88:             try:
 89:                 conn.enable_load_extension(False)
 90:             except Exception:
 91:                 pass
 92:         try:
 93:             conn.execute("SELECT vec_version()").fetchone()
 94:         except sqlite3.OperationalError as exc:
 95:             raise RuntimeError("sqlite-vec extension not available") from exc
 96: 
 97:     def _ensure_meta(self, conn: sqlite3.Connection) -> None:
 98:         conn.execute(
 99:             """
100:             CREATE TABLE IF NOT EXISTS vector_collections (
101:                 collection_id INTEGER PRIMARY KEY AUTOINCREMENT,
102:                 tenant_id TEXT NOT NULL,
103:                 name TEXT NOT NULL,
104:                 dimensions INTEGER NOT NULL,
105:                 table_name TEXT NOT NULL,
106:                 created_at TEXT,
107:                 UNIQUE (tenant_id, name, dimensions)
108:             )
109:             """
110:         )
111:         conn.execute(
112:             "CREATE INDEX IF NOT EXISTS idx_vector_collections_tenant ON vector_collections(tenant_id)"
113:         )
114: 
115:     def _collection_table_name(self, name: str, dimensions: int) -> str:
116:         digest = scope_key("vector_collection", f"{self.tenant_id}:{name}:{dimensions}")
117:         return f"vec_{digest}"
118: 
119:     def _ensure_collection(self, conn: sqlite3.Connection, name: str, dimensions: int) -> str:
120:         row = conn.execute(
121:             """
122:             SELECT table_name
123:             FROM vector_collections
124:             WHERE tenant_id = ? AND name = ? AND dimensions = ?
125:             """,
126:             (self.tenant_id, name, int(dimensions)),
127:         ).fetchone()
128:         if row:
129:             return str(row["table_name"])
130:         table_name = self._collection_table_name(name, dimensions)
131:         conn.execute(
132:             """
133:             INSERT INTO vector_collections (tenant_id, name, dimensions, table_name, created_at)
134:             VALUES (?, ?, ?, ?, ?)
135:             """,
136:             (self.tenant_id, name, int(dimensions), table_name, now_iso()),
137:         )
138:         conn.execute(
139:             f"""
140:             CREATE VIRTUAL TABLE IF NOT EXISTS {quote_identifier(table_name)} USING vec0(
141:                 embedding float[{int(dimensions)}],
142:                 tenant_id TEXT,
143:                 collection TEXT,
144:                 item_id TEXT,
145:                 payload TEXT,
146:                 created_at TEXT
147:             )
148:             """
149:         )
150:         return table_name
151: 
152:     def _has_column(self, conn: sqlite3.Connection, table_name: str, column: str) -> bool:
153:         cur = conn.execute(
154:             f"PRAGMA table_info({quote_identifier(table_name)})"
155:         )
156:         return any(row["name"] == column for row in cur.fetchall())
157: 
158:     def _lookup_collection(
159:         self, conn: sqlite3.Connection, name: str, dimensions: int
160:     ) -> str | None:
161:         row = conn.execute(
162:             """
163:             SELECT table_name
164:             FROM vector_collections
165:             WHERE tenant_id = ? AND name = ? AND dimensions = ?
166:             """,
167:             (self.tenant_id, name, int(dimensions)),
168:         ).fetchone()
169:         return str(row["table_name"]) if row else None
170: 
171:     def list_collections(self) -> list[dict[str, Any]]:
172:         with self.connection() as conn:
173:             cur = conn.execute(
174:                 """
175:                 SELECT name, dimensions, created_at
176:                 FROM vector_collections
177:                 WHERE tenant_id = ?
178:                 ORDER BY created_at
179:                 """,
180:                 (self.tenant_id,),
181:             )
182:             return [dict(row) for row in cur.fetchall()]
183: 
184:     def collection_dimensions(self, name: str) -> list[int]:
185:         with self.connection() as conn:
186:             cur = conn.execute(
187:                 """
188:                 SELECT dimensions
189:                 FROM vector_collections
190:                 WHERE tenant_id = ? AND name = ?
191:                 ORDER BY dimensions
192:                 """,
193:                 (self.tenant_id, name),
194:             )
195:             return [int(row["dimensions"]) for row in cur.fetchall()]
196: 
197:     def add(
198:         self,
199:         collection: str,
200:         vectors: list[list[float]],
201:         item_ids: list[str] | None = None,
202:         payloads: list[dict[str, Any] | None] | None = None,
203:     ) -> list[str]:
204:         if not vectors:
205:             return []
206:         dimensions = len(vectors[0])
207:         for vec in vectors:
208:             if len(vec) != dimensions:
209:                 raise ValueError("Vector dimension mismatch")
210:         if item_ids is None:
211:             item_ids = [
212:                 self._default_item_id(collection, vec, payloads[idx] if payloads else None)
213:                 for idx, vec in enumerate(vectors)
214:             ]
215:         if len(item_ids) != len(vectors):
216:             raise ValueError("item_ids length mismatch")
217:         if payloads is None:
218:             payloads = [None] * len(vectors)
219:         if len(payloads) != len(vectors):
220:             raise ValueError("payloads length mismatch")
221: 
222:         with self.connection() as conn:
223:             table_name = self._ensure_collection(conn, collection, dimensions)
224:             has_created_at = self._has_column(conn, table_name, "created_at")
225:             rows = []
226:             for vec, item_id, payload in zip(vectors, item_ids, payloads):
227:                 vec_json = json.dumps(vec, separators=(",", ":"))
228:                 payload_json = json_dumps(payload) if payload is not None else ""
229:                 if has_created_at:
230:                     rows.append(
231:                         (
232:                             vec_json,
233:                             self.tenant_id,
234:                             collection,
235:                             item_id,
236:                             payload_json,
237:                             now_iso(),
238:                         )
239:                     )
240:                 else:
241:                     rows.append(
242:                         (vec_json, self.tenant_id, collection, item_id, payload_json)
243:                     )
244:             if has_created_at:
245:                 columns = "(embedding, tenant_id, collection, item_id, payload, created_at)"
246:                 placeholders = ", ".join(["?"] * 6)
247:             else:
248:                 columns = "(embedding, tenant_id, collection, item_id, payload)"
249:                 placeholders = ", ".join(["?"] * 5)
250:             conn.executemany(
251:                 f"""
252:                 INSERT INTO {quote_identifier(table_name)}
253:                 {columns}
254:                 VALUES ({placeholders})
255:                 """,
256:                 rows,
257:             )
258:         return item_ids
259: 
260:     def query(
261:         self,
262:         collection: str,
263:         vector: list[float],
264:         k: int = 10,
265:         as_of: str | None = None,
266:     ) -> list[dict[str, Any]]:
267:         if k <= 0:
268:             return []
269:         dimensions = len(vector)
270:         with self.connection() as conn:
271:             table_name = self._lookup_collection(conn, collection, dimensions)
272:             if not table_name:
273:                 return []
274:             vec_json = json.dumps(vector, separators=(",", ":"))
275:             params: list[Any] = [vec_json, int(k)]
276:             where = "WHERE embedding MATCH ? AND k = ?"
277:             if as_of and self._has_column(conn, table_name, "created_at"):
278:                 where += " AND (created_at IS NULL OR created_at <= ?)"
279:                 params.append(as_of)
280:             where += " AND tenant_id = ? AND collection = ?"
281:             params.extend([self.tenant_id, collection])
282:             cur = conn.execute(
283:                 f"""
284:                 SELECT item_id, payload, distance
285:                 FROM {quote_identifier(table_name)}
286:                 {where}
287:                 ORDER BY distance ASC
288:                 """,
289:                 params,
290:             )
291:             results = []
292:             for row in cur.fetchall():
293:                 payload = row["payload"]
294:                 if payload is None or payload == "":
295:                     payload = None
296:                 elif payload:
297:                     try:
298:                         payload = json.loads(payload)
299:                     except json.JSONDecodeError:
300:                         pass
301:                 results.append(
302:                     {
303:                         "item_id": row["item_id"],
304:                         "distance": row["distance"],
305:                         "payload": payload,
306:                     }
307:                 )
308:             results.sort(key=lambda item: (item["distance"], item["item_id"]))
309:             return results
310: 
311:     def delete(self, collection: str, item_ids: list[str], dimensions: int) -> int:
312:         if not item_ids:
313:             return 0
314:         with self.connection() as conn:
315:             table_name = self._lookup_collection(conn, collection, dimensions)
316:             if not table_name:
317:                 return 0
318:             placeholders = ", ".join(["?"] * len(item_ids))
319:             cur = conn.execute(
320:                 f"""
321:                 DELETE FROM {quote_identifier(table_name)}
322:                 WHERE tenant_id = ? AND collection = ? AND item_id IN ({placeholders})
323:                 """,
324:                 [self.tenant_id, collection, *item_ids],
325:             )
326:             return cur.rowcount if cur.rowcount is not None else 0
327: 
328:     def _default_item_id(
329:         self, collection: str, vector: list[float], payload: dict[str, Any] | None
330:     ) -> str:
331:         vec_json = json.dumps(vector, separators=(",", ":"))
332:         payload_json = json_dumps(payload) if payload is not None else ""
333:         digest = hashlib.sha256(
334:             f"{collection}:{vec_json}:{payload_json}".encode("utf-8")
335:         ).hexdigest()
336:         return digest
````

## File: src/statistic_harness/ui/static/app.css
````css
  1: :root {
  2:   color-scheme: dark;
  3:   --bg-0: #0b0d12;
  4:   --bg-1: #10131a;
  5:   --panel: #141822;
  6:   --panel-2: #1b2130;
  7:   --border: #2a3142;
  8:   --text: #e7e9ee;
  9:   --muted: #9aa3b2;
 10:   --accent: #5ce1b2;
 11:   --accent-2: #f7b267;
 12:   --danger: #ff6b6b;
 13:   --ok: #62d18c;
 14:   --shadow: 0 20px 50px rgba(0, 0, 0, 0.35);
 15: }
 16: 
 17: * {
 18:   box-sizing: border-box;
 19: }
 20: 
 21: html, body {
 22:   height: 100%;
 23: }
 24: 
 25: body {
 26:   margin: 0;
 27:   font-family: "Avenir Next", "Avenir", "Trebuchet MS", sans-serif;
 28:   background: radial-gradient(circle at top left, #1a2130 0%, #0b0d12 55%);
 29:   color: var(--text);
 30: }
 31: 
 32: a {
 33:   color: var(--accent);
 34:   text-decoration: none;
 35: }
 36: 
 37: a:hover {
 38:   text-decoration: underline;
 39: }
 40: 
 41: label {
 42:   display: block;
 43:   margin-top: 0.6rem;
 44:   color: var(--muted);
 45:   font-size: 0.9rem;
 46: }
 47: 
 48: input,
 49: textarea,
 50: select {
 51:   background: var(--panel-2);
 52:   color: var(--text);
 53:   border: 1px solid var(--border);
 54:   border-radius: 10px;
 55:   padding: 0.6rem 0.75rem;
 56:   width: 100%;
 57:   max-width: 760px;
 58: }
 59: 
 60: input[type="checkbox"] {
 61:   width: auto;
 62:   margin-right: 0.4rem;
 63: }
 64: 
 65: button,
 66: .button {
 67:   display: inline-flex;
 68:   align-items: center;
 69:   gap: 0.35rem;
 70:   background: var(--accent);
 71:   color: #0b0d12;
 72:   border: none;
 73:   border-radius: 999px;
 74:   padding: 0.6rem 1.1rem;
 75:   cursor: pointer;
 76:   font-weight: 600;
 77:   transition: transform 0.15s ease, box-shadow 0.15s ease;
 78: }
 79: 
 80: button:hover,
 81: .button:hover {
 82:   transform: translateY(-1px);
 83:   box-shadow: var(--shadow);
 84: }
 85: 
 86: button.ghost,
 87: .button.ghost {
 88:   background: transparent;
 89:   border: 1px solid var(--border);
 90:   color: var(--text);
 91: }
 92: 
 93: pre {
 94:   background: var(--panel-2);
 95:   border: 1px solid var(--border);
 96:   border-radius: 10px;
 97:   padding: 0.75rem;
 98:   overflow: auto;
 99: }
100: 
101: .container {
102:   max-width: 1100px;
103:   margin: 0 auto;
104:   padding: 2.5rem 1.5rem 4rem;
105: }
106: 
107: .hero {
108:   display: grid;
109:   gap: 1.5rem;
110:   grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
111:   background: linear-gradient(120deg, rgba(92, 225, 178, 0.08), rgba(247, 178, 103, 0.08));
112:   border: 1px solid var(--border);
113:   border-radius: 18px;
114:   padding: 2rem;
115:   margin-bottom: 2rem;
116: }
117: 
118: .top-nav {
119:   display: flex;
120:   gap: 1rem;
121:   align-items: center;
122:   margin-bottom: 1.5rem;
123:   padding: 0.75rem 1rem;
124:   border: 1px solid var(--border);
125:   border-radius: 999px;
126:   background: rgba(20, 24, 34, 0.7);
127:   backdrop-filter: blur(6px);
128: }
129: 
130: .top-nav a {
131:   color: var(--text);
132:   font-weight: 600;
133:   font-size: 0.85rem;
134:   letter-spacing: 0.08em;
135:   text-transform: uppercase;
136: }
137: 
138: .hero.compact {
139:   padding: 1.5rem;
140: }
141: 
142: .hero-panel {
143:   background: var(--panel);
144:   border: 1px solid var(--border);
145:   border-radius: 14px;
146:   padding: 1.2rem;
147: }
148: 
149: .hero-actions {
150:   display: flex;
151:   flex-wrap: wrap;
152:   gap: 0.8rem;
153:   margin-top: 1.2rem;
154: }
155: 
156: .eyebrow {
157:   text-transform: uppercase;
158:   letter-spacing: 0.2em;
159:   font-size: 0.7rem;
160:   color: var(--muted);
161:   margin: 0 0 0.5rem;
162: }
163: 
164: .lead {
165:   font-size: 1.05rem;
166:   color: var(--muted);
167: }
168: 
169: .card {
170:   background: var(--panel);
171:   border: 1px solid var(--border);
172:   border-radius: 16px;
173:   padding: 1.5rem;
174:   margin-bottom: 1.5rem;
175: }
176: 
177: .card.subtle {
178:   background: var(--panel-2);
179: }
180: 
181: .card-header {
182:   display: flex;
183:   align-items: center;
184:   justify-content: space-between;
185:   gap: 1rem;
186: }
187: 
188: .grid {
189:   display: grid;
190:   gap: 1.5rem;
191:   grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
192: }
193: 
194: .clean-list {
195:   list-style: none;
196:   padding: 0;
197:   margin: 0;
198:   display: grid;
199:   gap: 0.6rem;
200: }
201: 
202: .pill {
203:   display: inline-flex;
204:   padding: 0.25rem 0.7rem;
205:   border-radius: 999px;
206:   border: 1px solid var(--border);
207:   font-size: 0.75rem;
208:   text-transform: uppercase;
209:   letter-spacing: 0.08em;
210: }
211: 
212: .pill.completed,
213: .pill.ok {
214:   border-color: var(--ok);
215:   color: var(--ok);
216: }
217: 
218: .pill.failed {
219:   border-color: var(--danger);
220:   color: var(--danger);
221: }
222: 
223: .pill.running {
224:   border-color: var(--accent-2);
225:   color: var(--accent-2);
226: }
227: 
228: .pill.pending {
229:   border-color: var(--border);
230:   color: var(--muted);
231: }
232: 
233: .pill.not_run {
234:   border-color: var(--border);
235:   color: var(--muted);
236: }
237: 
238: .pill.error {
239:   border-color: var(--danger);
240:   color: var(--danger);
241: }
242: 
243: .pill-row {
244:   display: flex;
245:   flex-wrap: wrap;
246:   gap: 0.5rem;
247: }
248: 
249: .pill-row .pill {
250:   text-decoration: none;
251: }
252: 
253: .progress-row {
254:   display: flex;
255:   align-items: center;
256:   gap: 1rem;
257: }
258: 
259: .progress-bar {
260:   flex: 1;
261:   height: 12px;
262:   border-radius: 999px;
263:   background: var(--panel-2);
264:   border: 1px solid var(--border);
265:   overflow: hidden;
266: }
267: 
268: .progress-fill {
269:   height: 100%;
270:   width: 0%;
271:   background: linear-gradient(90deg, var(--accent), var(--accent-2));
272:   transition: width 0.2s ease;
273: }
274: 
275: .badge {
276:   background: rgba(92, 225, 178, 0.15);
277:   color: var(--accent);
278:   border-radius: 999px;
279:   padding: 0.15rem 0.6rem;
280:   font-size: 0.75rem;
281:   text-transform: uppercase;
282:   letter-spacing: 0.08em;
283: }
284: 
285: .muted {
286:   color: var(--muted);
287: }
288: 
289: .status {
290:   margin-top: 0.75rem;
291: }
292: 
293: .stack {
294:   display: grid;
295:   gap: 0.6rem;
296: }
297: 
298: .button-row {
299:   display: flex;
300:   flex-wrap: wrap;
301:   gap: 0.6rem;
302:   align-items: center;
303: }
304: 
305: .insight {
306:   border-top: 1px solid var(--border);
307:   margin-top: 1rem;
308:   padding-top: 1rem;
309: }
310: 
311: .insight-header {
312:   display: flex;
313:   align-items: center;
314:   justify-content: space-between;
315:   gap: 0.8rem;
316: }
317: 
318: .evidence {
319:   display: flex;
320:   flex-wrap: wrap;
321:   gap: 0.6rem;
322:   margin-top: 0.6rem;
323: }
324: 
325: .issue-row {
326:   display: grid;
327:   gap: 0.6rem;
328:   padding: 1rem;
329:   border: 1px solid var(--border);
330:   border-radius: 14px;
331:   margin-bottom: 1rem;
332:   background: var(--panel-2);
333: }
````

## File: src/statistic_harness/ui/templates/index.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Statistic Harness</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark4" />
 6: </head>
 7: <body>
 8:   <div class="container">
 9:     <nav class="top-nav">
10:       <a href="/">Home</a>
11:       <a href="/projects">Projects</a>
12:       <a href="/templates">Templates</a>
13:       <a href="/trace">Trace</a>
14:       <a href="/vectors">Vectors</a>
15:     </nav>
16:     <header class="hero">
17:       <div>
18:         <p class="eyebrow">Statistic Harness</p>
19:         <h1>Deterministic Insight Engine</h1>
20:         <p class="lead">Create a project, upload a dataset, and run every analysis plugin automatically.</p>
21:         <div class="hero-actions">
22:           <a class="button" href="/projects">Browse Projects</a>
23:           <a class="button ghost" href="/templates">Templates</a>
24:           <a class="button ghost" href="/trace">Trace</a>
25:         </div>
26:       </div>
27:       <div class="hero-panel">
28:         <h3>Recent Projects</h3>
29:         {% if projects %}
30:           <ul class="clean-list">
31:             {% for project in projects[:5] %}
32:               <li>
33:                 <a href="/projects/{{ project.project_id }}">{{ project.name or project.project_id }}</a>
34:                 <span class="muted">({{ project.erp_type or "unknown" }})</span>
35:               </li>
36:             {% endfor %}
37:           </ul>
38:         {% else %}
39:           <p class="muted">No projects yet.</p>
40:         {% endif %}
41:       </div>
42:     </header>
43: 
44:     <section class="card">
45:       <h2>Create Project</h2>
46:       <form action="/projects" method="post" class="stack">
47:         <label>Project name</label>
48:         <input type="text" name="name" placeholder="e.g., Quorum Close Cycle Analysis" required />
49:         <label>ERP type</label>
50:         <input type="text" name="erp_type" value="unknown" />
51:         <button type="submit">Create Project</button>
52:       </form>
53:     </section>
54:   </div>
55: </body>
56: </html>
````

## File: src/statistic_harness/ui/templates/known_issues.html
````html
  1: <!DOCTYPE html>
  2: <html>
  3: <head>
  4:   <title>Known Issues</title>
  5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
  6: </head>
  7: <body>
  8:   <h1>Known Issues</h1>
  9:   <p>
 10:     <a href="/">Home</a> |
 11:     <a href="/projects">Projects</a> |
 12:     <a href="/trace">Trace</a>
 13:   </p>
 14: 
 15:   <label>Upload:</label>
 16:   <select id="upload-select">
 17:     <option value="">Select an upload</option>
 18:     {% for upload in uploads %}
 19:       <option value="{{ upload.upload_id }}" {% if upload.upload_id == selected_upload_id %}selected{% endif %}>
 20:         {{ upload.filename }} ({{ upload.size_bytes }} bytes)
 21:       </option>
 22:     {% endfor %}
 23:   </select>
 24:   <label>ERP type:</label>
 25:   <input type="text" id="erp-type" value="{{ erp_type or 'unknown' }}" />
 26: 
 27:   {% if selected_upload %}
 28:     <p>
 29:       Selected: <strong>{{ selected_upload.filename }}</strong>
 30:       {% if sha256 %} | SHA256: {{ sha256[:10] }}...{% endif %}
 31:     </p>
 32:   {% else %}
 33:     <p>No upload selected.</p>
 34:   {% endif %}
 35: 
 36:   <section>
 37:     <h2>Known Issues Editor</h2>
 38:     <label><input type="checkbox" id="strict-flag" /> Strict (no false positives)</label>
 39:     <label>Notes:</label>
 40:     <textarea id="notes" rows="3"></textarea>
 41:     <label><input type="checkbox" id="autosave-flag" checked /> Auto-save</label>
 42:     <p>Where/Contains format: one <code>key=value</code> per line. Use commas for lists.</p>
 43: 
 44:     <h3>Natural Language Issues</h3>
 45:     <p>Describe issues in plain English. The compiler will map them to structured checks.</p>
 46:     <div id="nl-issues"></div>
 47:     <button type="button" id="add-nl-issue">Add NL Issue</button>
 48: 
 49:     <div id="issues"></div>
 50: 
 51:     <button type="button" id="add-issue">Add Issue</button>
 52:     <button type="button" id="quick-qemail">Quick add: qemail close-cycle contention</button>
 53:     <button type="button" id="save-issues">Save Known Issues</button>
 54:     {% if selected_upload_id %}
 55:       <form action="/runs/auto-evaluate" method="post" style="margin-top: 0.5rem;">
 56:         <input type="hidden" name="upload_id" value="{{ selected_upload_id }}" />
 57:         <button type="submit">Run Auto + Evaluate</button>
 58:       </form>
 59:     {% endif %}
 60:   </section>
 61: 
 62:   <section>
 63:     <h2>Generated Ground Truth (read-only)</h2>
 64:     <textarea id="ground-truth" rows="16" readonly></textarea>
 65:   </section>
 66: 
 67:   <pre id="save-result"></pre>
 68: 
 69:   <script id="known-issues-json" type="application/json">{{ known_issues_json | safe }}</script>
 70:   <script id="plugin-ids-json" type="application/json">{{ plugin_ids | tojson }}</script>
 71:   <script id="known-issues-yaml" type="text/plain">{{ known_issues_yaml }}</script>
 72:   <script>
 73:     const uploadSelect = document.getElementById("upload-select");
 74:     const issuesContainer = document.getElementById("issues");
 75:     const strictFlag = document.getElementById("strict-flag");
 76:     const notesEl = document.getElementById("notes");
 77:     const groundTruthEl = document.getElementById("ground-truth");
 78:     const saveResult = document.getElementById("save-result");
 79:     const autosaveFlag = document.getElementById("autosave-flag");
 80:     const erpTypeEl = document.getElementById("erp-type");
 81:     const nlContainer = document.getElementById("nl-issues");
 82: 
 83:     const knownIssues = JSON.parse(document.getElementById("known-issues-json").textContent || "{}");
 84:     const pluginIds = JSON.parse(document.getElementById("plugin-ids-json").textContent || "[]");
 85: 
 86:     function kvToText(obj) {
 87:       if (!obj) return "";
 88:       const lines = [];
 89:       for (const key of Object.keys(obj)) {
 90:         const value = obj[key];
 91:         if (Array.isArray(value)) {
 92:           lines.push(`${key}=${value.join(",")}`);
 93:         } else {
 94:           lines.push(`${key}=${value}`);
 95:         }
 96:       }
 97:       return lines.join("\n");
 98:     }
 99: 
100:     function parseKV(text) {
101:       const out = {};
102:       if (!text) return out;
103:       const lines = text.split("\n");
104:       for (const line of lines) {
105:         const trimmed = line.trim();
106:         if (!trimmed) continue;
107:         const idx = trimmed.indexOf("=");
108:         if (idx === -1) continue;
109:         const key = trimmed.slice(0, idx).trim();
110:         let raw = trimmed.slice(idx + 1).trim();
111:         if (!key) continue;
112:         let value = raw;
113:         if (raw.includes(",")) {
114:           value = raw.split(",").map((v) => v.trim()).filter((v) => v.length);
115:         } else if (raw.toLowerCase() === "true" || raw.toLowerCase() === "false") {
116:           value = raw.toLowerCase() === "true";
117:         } else if (!Number.isNaN(Number(raw)) && raw !== "") {
118:           value = Number(raw);
119:         }
120:         out[key] = value;
121:       }
122:       return out;
123:     }
124: 
125:     function addIssueRow(issue) {
126:       const row = document.createElement("div");
127:       row.className = "issue-row";
128:       row.style.border = "1px solid #2a3140";
129:       row.style.padding = "0.5rem";
130:       row.style.marginBottom = "0.5rem";
131:       const pluginSelect = document.createElement("select");
132:       const emptyOption = document.createElement("option");
133:       emptyOption.value = "";
134:       emptyOption.textContent = "(any plugin)";
135:       pluginSelect.appendChild(emptyOption);
136:       for (const pid of pluginIds) {
137:         const opt = document.createElement("option");
138:         opt.value = pid;
139:         opt.textContent = pid;
140:         pluginSelect.appendChild(opt);
141:       }
142:       pluginSelect.value = issue.plugin_id || "";
143: 
144:       const kindInput = document.createElement("input");
145:       kindInput.type = "text";
146:       kindInput.placeholder = "kind (required)";
147:       kindInput.value = issue.kind || "";
148: 
149:       const titleInput = document.createElement("input");
150:       titleInput.type = "text";
151:       titleInput.placeholder = "title (optional)";
152:       titleInput.value = issue.title || "";
153: 
154:       const whereInput = document.createElement("textarea");
155:       whereInput.rows = 3;
156:       whereInput.placeholder = "where: key=value per line";
157:       whereInput.value = kvToText(issue.where || {});
158: 
159:       const containsInput = document.createElement("textarea");
160:       containsInput.rows = 3;
161:       containsInput.placeholder = "contains: key=value per line";
162:       containsInput.value = kvToText(issue.contains || {});
163: 
164:       const minCount = document.createElement("input");
165:       minCount.type = "number";
166:       minCount.placeholder = "min";
167:       if (issue.min_count !== undefined && issue.min_count !== null) {
168:         minCount.value = issue.min_count;
169:       }
170: 
171:       const maxCount = document.createElement("input");
172:       maxCount.type = "number";
173:       maxCount.placeholder = "max";
174:       if (issue.max_count !== undefined && issue.max_count !== null) {
175:         maxCount.value = issue.max_count;
176:       }
177: 
178:       const removeBtn = document.createElement("button");
179:       removeBtn.type = "button";
180:       removeBtn.textContent = "Remove";
181:       removeBtn.addEventListener("click", () => row.remove());
182: 
183:       row.appendChild(pluginSelect);
184:       row.appendChild(kindInput);
185:       row.appendChild(titleInput);
186:       row.appendChild(whereInput);
187:       row.appendChild(containsInput);
188:       row.appendChild(minCount);
189:       row.appendChild(maxCount);
190:       row.appendChild(removeBtn);
191: 
192:       issuesContainer.appendChild(row);
193:     }
194: 
195:     function addNlRow(entry = {}) {
196:       const row = document.createElement("div");
197:       row.className = "nl-row";
198:       row.style.border = "1px solid #2a3140";
199:       row.style.padding = "0.5rem";
200:       row.style.marginBottom = "0.5rem";
201: 
202:       const text = document.createElement("textarea");
203:       text.rows = 2;
204:       text.placeholder = "Describe the known issue...";
205:       text.value = entry.text || "";
206: 
207:       const process = document.createElement("input");
208:       process.type = "text";
209:       process.placeholder = "Process hint (optional)";
210:       process.value = entry.process_hint || "";
211: 
212:       const removeBtn = document.createElement("button");
213:       removeBtn.type = "button";
214:       removeBtn.textContent = "Remove";
215:       removeBtn.addEventListener("click", () => row.remove());
216: 
217:       row.appendChild(text);
218:       row.appendChild(process);
219:       row.appendChild(removeBtn);
220:       nlContainer.appendChild(row);
221:     }
222: 
223:     function collectNlIssues() {
224:       const issues = [];
225:       const rows = document.querySelectorAll(".nl-row");
226:       rows.forEach((row) => {
227:         const areas = row.getElementsByTagName("textarea");
228:         const inputs = row.getElementsByTagName("input");
229:         const text = areas[0]?.value || "";
230:         const process = inputs[0]?.value || "";
231:         if (!text.trim()) return;
232:         const entry = { text: text.trim() };
233:         if (process.trim()) entry.process_hint = process.trim();
234:         issues.push(entry);
235:       });
236:       return issues;
237:     }
238: 
239:     function collectIssues() {
240:       const issues = [];
241:       const rows = document.querySelectorAll(".issue-row");
242:       rows.forEach((row) => {
243:         const selects = row.getElementsByTagName("select");
244:         const inputs = row.getElementsByTagName("input");
245:         const areas = row.getElementsByTagName("textarea");
246:         const plugin_id = selects[0]?.value || "";
247:         const kind = inputs[0]?.value || "";
248:         const title = inputs[1]?.value || "";
249:         const min_count = inputs[2]?.value;
250:         const max_count = inputs[3]?.value;
251:         const where = parseKV(areas[0]?.value || "");
252:         const contains = parseKV(areas[1]?.value || "");
253:         const issue = { kind };
254:         if (plugin_id) issue.plugin_id = plugin_id;
255:         if (title) issue.title = title;
256:         if (Object.keys(where).length) issue.where = where;
257:         if (Object.keys(contains).length) issue.contains = contains;
258:         if (min_count !== undefined && min_count !== "") issue.min_count = Number(min_count);
259:         if (max_count !== undefined && max_count !== "") issue.max_count = Number(max_count);
260:         if (kind) issues.push(issue);
261:       });
262:       return issues;
263:     }
264: 
265:     function refreshGroundTruth(yamlText) {
266:       groundTruthEl.value = yamlText || "";
267:     }
268: 
269:     let autosaveTimer = null;
270:     function scheduleAutosave() {
271:       if (!autosaveFlag || !autosaveFlag.checked) return;
272:       if (autosaveTimer) clearTimeout(autosaveTimer);
273:       autosaveTimer = setTimeout(() => {
274:         saveKnownIssues(true);
275:       }, 800);
276:     }
277: 
278:     async function saveKnownIssues(isAuto = false) {
279:       const uploadId = uploadSelect.value;
280:       if (!uploadId) {
281:         saveResult.textContent = "Select an upload first.";
282:         return null;
283:       }
284:       const payload = {
285:         strict: strictFlag.checked,
286:         notes: notesEl.value || "",
287:         expected_findings: collectIssues(),
288:         natural_language: collectNlIssues()
289:       };
290:       saveResult.textContent = isAuto ? "Auto-saving..." : "Saving...";
291:       const resp = await fetch("/api/known-issues", {
292:         method: "POST",
293:         headers: { "Content-Type": "application/json" },
294:         body: JSON.stringify({
295:           upload_id: uploadId,
296:           erp_type: erpTypeEl ? erpTypeEl.value : "",
297:           known_issues: payload
298:         })
299:       });
300:       const data = await resp.json();
301:       let summary = data.compile_warnings && data.compile_warnings.length
302:         ? `Saved with ${data.compile_warnings.length} compiler warning(s).`
303:         : "Saved.";
304:       if (data.compiled_count !== undefined) {
305:         summary += ` Compiled ${data.compiled_count} NL rule(s).`;
306:       }
307:       saveResult.textContent = summary;
308:       if (data.ground_truth_yaml) {
309:         refreshGroundTruth(data.ground_truth_yaml);
310:       }
311:       return data;
312:     }
313: 
314:     function reloadWithScope() {
315:       const id = uploadSelect ? uploadSelect.value : "";
316:       const erp = erpTypeEl ? erpTypeEl.value : "";
317:       if (id) {
318:         const params = new URLSearchParams({ upload_id: id, erp_type: erp || "unknown" });
319:         window.location = `/known-issues?${params.toString()}`;
320:       }
321:     }
322: 
323:     if (uploadSelect) {
324:       uploadSelect.addEventListener("change", () => {
325:         reloadWithScope();
326:       });
327:     }
328:     if (erpTypeEl) {
329:       erpTypeEl.addEventListener("change", () => {
330:         reloadWithScope();
331:       });
332:     }
333: 
334:     document.getElementById("add-issue").addEventListener("click", () => addIssueRow({}));
335:     document.getElementById("add-nl-issue").addEventListener("click", () => addNlRow({}));
336:     document.getElementById("quick-qemail").addEventListener("click", () => {
337:       addIssueRow({
338:         plugin_id: "analysis_close_cycle_contention",
339:         kind: "close_cycle_contention",
340:         where: { process: "qemail" },
341:         min_count: 1,
342:         max_count: 1
343:       });
344:     });
345: 
346:     document.getElementById("save-issues").addEventListener("click", async () => {
347:       await saveKnownIssues(false);
348:     });
349: 
350:     function init() {
351:       strictFlag.checked = !!knownIssues.strict;
352:       notesEl.value = knownIssues.notes || "";
353:       const issues = knownIssues.expected_findings || [];
354:       if (issues.length) {
355:         issues.forEach(addIssueRow);
356:       } else {
357:         addIssueRow({});
358:       }
359:       const nlIssues = knownIssues.natural_language || [];
360:       if (nlIssues.length) {
361:         nlIssues.forEach(addNlRow);
362:       } else {
363:         addNlRow({});
364:       }
365:       const yamlText = document.getElementById("known-issues-yaml").textContent || "";
366:       if (yamlText.trim()) {
367:         refreshGroundTruth(yamlText);
368:       }
369:     }
370: 
371:     init();
372: 
373:     document.addEventListener("input", (event) => {
374:       if (!event.target) return;
375:       if (event.target.closest(".issue-row") || event.target === notesEl || event.target === strictFlag) {
376:         scheduleAutosave();
377:       }
378:     });
379: 
380:     if (autosaveFlag) {
381:       autosaveFlag.addEventListener("change", () => {
382:         if (autosaveFlag.checked) {
383:           scheduleAutosave();
384:         }
385:       });
386:     }
387:   </script>
388: </body>
389: </html>
````

## File: src/statistic_harness/ui/templates/plugins.html
````html
 1: <!DOCTYPE html>
 2: <html>
 3: <head>
 4:   <title>Plugins</title>
 5:   <link rel="stylesheet" href="/static/app.css?v=dark3" />
 6: </head>
 7: <body>
 8:   <h1>Plugins</h1>
 9:   <ul>
10:     {% for plugin in plugins %}
11:       <li>{{ plugin.plugin_id }} - {{ plugin.name }} ({{ plugin.type }})</li>
12:     {% endfor %}
13:   </ul>
14: </body>
15: </html>
````

## File: src/statistic_harness/ui/templates/project_known_issues.html
````html
  1: <!DOCTYPE html>
  2: <html>
  3: <head>
  4:   <title>Known Issues — {{ project.name or project_id }}</title>
  5:   <link rel="stylesheet" href="/static/app.css?v=dark4" />
  6: </head>
  7: <body>
  8:   <div class="container">
  9:     <nav class="top-nav">
 10:       <a href="/">Home</a>
 11:       <a href="/projects">Projects</a>
 12:       <a href="/templates">Templates</a>
 13:       <a href="/trace">Trace</a>
 14:       <a href="/vectors">Vectors</a>
 15:     </nav>
 16:     <header class="hero compact">
 17:       <div>
 18:         <p class="eyebrow">Known Issues</p>
 19:         <h1>{{ project.name or project_id }}</h1>
 20:         <p class="lead">ERP: {{ project.erp_type or "unknown" }}</p>
 21:       </div>
 22:       <div class="hero-panel">
 23:         <p class="muted">Natural language notes are stored alongside detection rules.</p>
 24:       </div>
 25:     </header>
 26: 
 27:     <section class="card">
 28:       <h2>Settings</h2>
 29:       <label><input type="checkbox" id="strict-flag" checked /> Strict (no false positives)</label>
 30:       <label>Notes</label>
 31:       <textarea id="notes" rows="3"></textarea>
 32:     </section>
 33: 
 34:     <section class="card">
 35:       <div class="card-header">
 36:         <h2>Natural Language Issues</h2>
 37:         <div class="button-row">
 38:           <button type="button" class="ghost" id="add-nl-issue">Add NL Issue</button>
 39:         </div>
 40:       </div>
 41:       <div id="nl-issues"></div>
 42:     </section>
 43: 
 44:     <section class="card">
 45:       <div class="card-header">
 46:         <h2>Issues</h2>
 47:         <div class="button-row">
 48:           <button type="button" class="ghost" id="quick-qemail">Quick add: QEMAIL close-cycle</button>
 49:           <button type="button" class="ghost" id="add-issue">Add Issue</button>
 50:         </div>
 51:       </div>
 52:       <div id="issues"></div>
 53:       <div class="button-row">
 54:         <button type="button" id="save-issues">Save Known Issues</button>
 55:         <a class="button ghost" href="/projects/{{ project_id }}">Back to Project</a>
 56:       </div>
 57:       <div id="save-result" class="status"></div>
 58:     </section>
 59:   </div>
 60: 
 61:   <script id="plugin-ids-json" type="application/json">{{ plugin_ids | tojson }}</script>
 62:   <script id="known-issues-json" type="application/json">{{ known_issues | tojson }}</script>
 63:   <script>
 64:     const pluginIds = JSON.parse(document.getElementById("plugin-ids-json").textContent || "[]");
 65:     const known = JSON.parse(document.getElementById("known-issues-json").textContent || "{}");
 66:     const issuesContainer = document.getElementById("issues");
 67:     const strictFlag = document.getElementById("strict-flag");
 68:     const notesEl = document.getElementById("notes");
 69:     const saveResult = document.getElementById("save-result");
 70:     const nlContainer = document.getElementById("nl-issues");
 71: 
 72:     function kvToText(obj) {
 73:       if (!obj) return "";
 74:       const lines = [];
 75:       for (const key of Object.keys(obj)) {
 76:         const value = obj[key];
 77:         if (Array.isArray(value)) {
 78:           lines.push(`${key}=${value.join(",")}`);
 79:         } else {
 80:           lines.push(`${key}=${value}`);
 81:         }
 82:       }
 83:       return lines.join("\\n");
 84:     }
 85: 
 86:     function parseKV(text) {
 87:       const out = {};
 88:       if (!text) return out;
 89:       const lines = text.split("\\n");
 90:       for (const line of lines) {
 91:         const trimmed = line.trim();
 92:         if (!trimmed) continue;
 93:         const idx = trimmed.indexOf("=");
 94:         if (idx === -1) continue;
 95:         const key = trimmed.slice(0, idx).trim();
 96:         let raw = trimmed.slice(idx + 1).trim();
 97:         if (!key) continue;
 98:         let value = raw;
 99:         if (raw.includes(",")) {
100:           value = raw.split(",").map((v) => v.trim()).filter((v) => v.length);
101:         } else if (raw.toLowerCase() === "true" || raw.toLowerCase() === "false") {
102:           value = raw.toLowerCase() === "true";
103:         } else if (!Number.isNaN(Number(raw)) && raw !== "") {
104:           value = Number(raw);
105:         }
106:         out[key] = value;
107:       }
108:       return out;
109:     }
110: 
111:     function addIssueRow(issue = {}) {
112:       const row = document.createElement("div");
113:       row.className = "issue-row";
114: 
115:       const titleInput = document.createElement("input");
116:       titleInput.type = "text";
117:       titleInput.placeholder = "Title";
118:       titleInput.value = issue.title || "";
119: 
120:       const descInput = document.createElement("textarea");
121:       descInput.rows = 2;
122:       descInput.placeholder = "Describe the known issue in plain English.";
123:       descInput.value = issue.description || "";
124: 
125:       const pluginSelect = document.createElement("select");
126:       const emptyOption = document.createElement("option");
127:       emptyOption.value = "";
128:       emptyOption.textContent = "(any plugin)";
129:       pluginSelect.appendChild(emptyOption);
130:       for (const pid of pluginIds) {
131:         const opt = document.createElement("option");
132:         opt.value = pid;
133:         opt.textContent = pid;
134:         pluginSelect.appendChild(opt);
135:       }
136:       pluginSelect.value = issue.plugin_id || "";
137: 
138:       const kindInput = document.createElement("input");
139:       kindInput.type = "text";
140:       kindInput.placeholder = "kind (required)";
141:       kindInput.value = issue.kind || "";
142: 
143:       const whereInput = document.createElement("textarea");
144:       whereInput.rows = 2;
145:       whereInput.placeholder = "where: key=value per line";
146:       whereInput.value = kvToText(issue.where || {});
147: 
148:       const containsInput = document.createElement("textarea");
149:       containsInput.rows = 2;
150:       containsInput.placeholder = "contains: key=value per line";
151:       containsInput.value = kvToText(issue.contains || {});
152: 
153:       const minCount = document.createElement("input");
154:       minCount.type = "number";
155:       minCount.placeholder = "min";
156:       if (issue.min_count !== undefined && issue.min_count !== null) {
157:         minCount.value = issue.min_count;
158:       }
159: 
160:       const maxCount = document.createElement("input");
161:       maxCount.type = "number";
162:       maxCount.placeholder = "max";
163:       if (issue.max_count !== undefined && issue.max_count !== null) {
164:         maxCount.value = issue.max_count;
165:       }
166: 
167:       const removeBtn = document.createElement("button");
168:       removeBtn.type = "button";
169:       removeBtn.className = "ghost";
170:       removeBtn.textContent = "Remove";
171:       removeBtn.addEventListener("click", () => row.remove());
172: 
173:       row.appendChild(titleInput);
174:       row.appendChild(descInput);
175:       row.appendChild(pluginSelect);
176:       row.appendChild(kindInput);
177:       row.appendChild(whereInput);
178:       row.appendChild(containsInput);
179:       row.appendChild(minCount);
180:       row.appendChild(maxCount);
181:       row.appendChild(removeBtn);
182:       issuesContainer.appendChild(row);
183:     }
184: 
185:     function addNlRow(entry = {}) {
186:       const row = document.createElement("div");
187:       row.className = "issue-row";
188: 
189:       const text = document.createElement("textarea");
190:       text.rows = 2;
191:       text.placeholder = "Describe the known issue in plain English.";
192:       text.value = entry.text || "";
193: 
194:       const process = document.createElement("input");
195:       process.type = "text";
196:       process.placeholder = "Process hint (optional)";
197:       process.value = entry.process_hint || "";
198: 
199:       const removeBtn = document.createElement("button");
200:       removeBtn.type = "button";
201:       removeBtn.className = "ghost";
202:       removeBtn.textContent = "Remove";
203:       removeBtn.addEventListener("click", () => row.remove());
204: 
205:       row.appendChild(text);
206:       row.appendChild(process);
207:       row.appendChild(removeBtn);
208:       nlContainer.appendChild(row);
209:     }
210: 
211:     function collectNlIssues() {
212:       const issues = [];
213:       const rows = document.querySelectorAll("#nl-issues .issue-row");
214:       rows.forEach((row) => {
215:         const areas = row.getElementsByTagName("textarea");
216:         const inputs = row.getElementsByTagName("input");
217:         const text = areas[0]?.value || "";
218:         if (!text.trim()) return;
219:         const entry = { text: text.trim() };
220:         const process = inputs[0]?.value || "";
221:         if (process.trim()) entry.process_hint = process.trim();
222:         issues.push(entry);
223:       });
224:       return issues;
225:     }
226: 
227:     function collectIssues() {
228:       const issues = [];
229:       const rows = document.querySelectorAll(".issue-row");
230:       rows.forEach((row) => {
231:         const inputs = row.getElementsByTagName("input");
232:         const areas = row.getElementsByTagName("textarea");
233:         const selects = row.getElementsByTagName("select");
234:         const issue = { kind: inputs[1]?.value || "" };
235:         if (!issue.kind) return;
236:         if (inputs[0]?.value) issue.title = inputs[0].value;
237:         if (areas[0]?.value) issue.description = areas[0].value;
238:         const pluginId = selects[0]?.value || "";
239:         if (pluginId) issue.plugin_id = pluginId;
240:         const where = parseKV(areas[1]?.value || "");
241:         if (Object.keys(where).length) issue.where = where;
242:         const contains = parseKV(areas[2]?.value || "");
243:         if (Object.keys(contains).length) issue.contains = contains;
244:         if (inputs[2]?.value) issue.min_count = Number(inputs[2].value);
245:         if (inputs[3]?.value) issue.max_count = Number(inputs[3].value);
246:         issues.push(issue);
247:       });
248:       return issues;
249:     }
250: 
251:     async function saveIssues() {
252:       const payload = {
253:         strict: strictFlag.checked,
254:         notes: notesEl.value || "",
255:         expected_findings: collectIssues(),
256:         natural_language: collectNlIssues()
257:       };
258:       saveResult.textContent = "Saving...";
259:       const resp = await fetch("/api/projects/{{ project_id }}/known-issues", {
260:         method: "POST",
261:         headers: { "Content-Type": "application/json" },
262:         body: JSON.stringify({ known_issues: payload })
263:       });
264:       const data = await resp.json();
265:       if (!resp.ok) {
266:         saveResult.textContent = data.detail || "Save failed.";
267:         return;
268:       }
269:       let summary = "Saved.";
270:       if (data.compile_warnings && data.compile_warnings.length) {
271:         summary = `Saved with ${data.compile_warnings.length} compiler warning(s).`;
272:       }
273:       if (data.compiled_count !== undefined) {
274:         summary += ` Compiled ${data.compiled_count} NL rule(s).`;
275:       }
276:       saveResult.textContent = summary;
277:     }
278: 
279:     document.getElementById("add-issue").addEventListener("click", () => addIssueRow({}));
280:     document.getElementById("add-nl-issue").addEventListener("click", () => addNlRow({}));
281:     document.getElementById("save-issues").addEventListener("click", saveIssues);
282:     document.getElementById("quick-qemail").addEventListener("click", () => {
283:       addIssueRow({
284:         title: "QEMAIL close-cycle contention",
285:         description: "QEMAIL slows the close cycle; quantify eligible-wait and model capacity impact.",
286:         plugin_id: "analysis_queue_delay_decomposition",
287:         kind: "eligible_wait_process_stats",
288:         where: { process: "qemail" },
289:         min_count: 1
290:       });
291:     });
292: 
293:     if (known) {
294:       strictFlag.checked = !!known.strict;
295:       notesEl.value = known.notes || "";
296:       const existing = known.expected_findings || [];
297:       existing.forEach((issue) => addIssueRow(issue));
298:       const nlIssues = known.natural_language || [];
299:       if (nlIssues.length) {
300:         nlIssues.forEach((entry) => addNlRow(entry));
301:       } else {
302:         addNlRow({});
303:       }
304:     }
305:   </script>
306: </body>
307: </html>
````

## File: src/statistic_harness/ui/templates/report.html
````html
  1: <!DOCTYPE html>
  2: <html>
  3: <head>
  4:   <title>Run {{ run_id }} Report</title>
  5:   <link rel="stylesheet" href="/static/app.css?v=dark4" />
  6: </head>
  7: <body>
  8:   <div class="container">
  9:     <nav class="top-nav">
 10:       <a href="/">Home</a>
 11:       <a href="/projects">Projects</a>
 12:       <a href="/templates">Templates</a>
 13:       <a href="/trace">Trace</a>
 14:       <a href="/vectors">Vectors</a>
 15:     </nav>
 16:     <header class="hero compact">
 17:       <div>
 18:         <p class="eyebrow">Run Report</p>
 19:         <h1>{{ run_label or run_id }}</h1>
 20:         {% if project %}
 21:           <p class="lead">Project: <strong>{{ project.name or project.project_id }}</strong></p>
 22:         {% endif %}
 23:         <p class="muted">Status: {{ report.status }}</p>
 24:         {% if evaluation %}
 25:           <p class="muted">Known issues: <strong>{{ evaluation.result }}</strong></p>
 26:           {% if evaluation.messages %}
 27:             <ul>
 28:               {% for message in evaluation.messages %}
 29:                 <li>{{ message }}</li>
 30:               {% endfor %}
 31:             </ul>
 32:           {% endif %}
 33:         {% else %}
 34:           <p class="muted">Known issues: not evaluated</p>
 35:         {% endif %}
 36:       </div>
 37:       <div class="hero-panel">
 38:         <p class="muted">Run seed: {{ report.lineage.run.run_seed }}</p>
 39:         <p class="muted">Dataset: {{ report.lineage.dataset.dataset_version_id }}</p>
 40:         {% if report.lineage.raw_format %}
 41:           <p class="muted">Raw format: {{ report.lineage.raw_format.fingerprint }}</p>
 42:         {% endif %}
 43:       </div>
 44:     </header>
 45: 
 46:     <section class="card">
 47:       <h2>Known Issues (highest priority)</h2>
 48:       {% if known_results and known_results.results %}
 49:         <p class="muted">Strict mode: {{ "on" if known_results.strict else "off" }}</p>
 50:         <ul class="clean-list">
 51:           {% for issue in known_results.results %}
 52:             <li>
 53:               <span class="pill {{ 'ok' if issue.status == 'pass' else 'error' }}">{{ issue.status }}</span>
 54:               <strong>{{ issue.title }}</strong>
 55:               {% if issue.plugin_id %}
 56:                 <span class="muted">({{ issue.plugin_id }})</span>
 57:               {% endif %}
 58:               {% if issue.description %}
 59:                 <div class="muted">{{ issue.description }}</div>
 60:               {% endif %}
 61:               <div class="muted">
 62:                 Matched: {{ issue.matched }} |
 63:                 Expected: ≥ {{ issue.min_count }}
 64:                 {% if issue.max_count is not none %}
 65:                   and ≤ {{ issue.max_count }}
 66:                 {% endif %}
 67:               </div>
 68:               {% if issue.baseline_match_fallbacks %}
 69:                 <div class="muted">Baseline fallback: {{ issue.baseline_match_fallbacks | join(", ") }}</div>
 70:               {% endif %}
 71:               {% if issue.baseline_match_modes %}
 72:                 <div class="muted">Baseline mode: {{ issue.baseline_match_modes | join(", ") }}</div>
 73:               {% endif %}
 74:               {% if issue.baseline_host_sources %}
 75:                 <div class="muted">Baseline host source: {{ issue.baseline_host_sources | join(", ") }}</div>
 76:               {% endif %}
 77:               {% if issue.baseline_host_counts %}
 78:                 <div class="muted">Baseline host count: {{ issue.baseline_host_counts | join(", ") }}</div>
 79:               {% endif %}
 80:             </li>
 81:           {% endfor %}
 82:         </ul>
 83:         {% if known_results.unexpected %}
 84:           <div class="card subtle">
 85:             <p class="muted">Unexpected findings detected (strict mode):</p>
 86:             <ul class="clean-list">
 87:               {% for item in known_results.unexpected %}
 88:                 <li>{{ item.plugin_id }}: {{ item.kind }}</li>
 89:               {% endfor %}
 90:             </ul>
 91:           </div>
 92:         {% endif %}
 93:       {% else %}
 94:         <p class="muted">No known issues saved for this ERP/upload.</p>
 95:       {% endif %}
 96:     </section>
 97: 
 98:     {% if report_errors %}
 99:       <section class="card">
100:         <h2>Errors</h2>
101:         <ul class="clean-list">
102:           {% for err in report_errors %}
103:             <li>
104:               <span class="pill error">error</span>
105:               <strong>{{ err.plugin_id }}</strong>
106:               {% if err.message %}
107:                 <div class="muted">{{ err.message }}</div>
108:               {% endif %}
109:             </li>
110:           {% endfor %}
111:         </ul>
112:       </section>
113:     {% endif %}
114: 
115:     <section class="card">
116:       <h2>Insights</h2>
117:       {% for plugin_id, plugin in report.plugins.items() %}
118:         <div class="card subtle">
119:           <div class="card-header">
120:             <div>
121:               <h3>{{ plugin_labels.get(plugin_id, plugin_id) }}</h3>
122:               {% if plugin_descriptions.get(plugin_id) %}
123:                 <p class="muted">{{ plugin_descriptions.get(plugin_id) }}</p>
124:               {% endif %}
125:               <p class="muted">ID: {{ plugin_id }}</p>
126:             </div>
127:             <span class="pill {{ plugin.status }}">{{ plugin.status }}</span>
128:           </div>
129:           <p class="muted">{{ plugin.summary }}</p>
130:           {% if plugin.error %}
131:             <pre>{{ plugin.error }}</pre>
132:           {% endif %}
133:           {% set plugin_insights = insights.get(plugin_id) %}
134:           {% if plugin_insights %}
135:             {% for item in plugin_insights %}
136:               <div class="insight">
137:                 <div class="insight-header">
138:                   <h4>{{ item.title }}</h4>
139:                   <span class="badge">{{ item.measurement_type }}</span>
140:                 </div>
141:                 {% if item.bullets %}
142:                   <ul>
143:                     {% for bullet in item.bullets %}
144:                       <li>{{ bullet }}</li>
145:                     {% endfor %}
146:                   </ul>
147:                 {% endif %}
148:                 {% if item.evidence and item.evidence.dataset_version_id %}
149:                   <div class="evidence">
150:                     <span class="muted">Evidence:</span>
151:                     <a href="/trace?entity_type=dataset_version&key={{ item.evidence.dataset_version_id }}">Trace dataset</a>
152:                     <a href="/vectors?collection=report_{{ report.run_id }}">Vectors</a>
153:                     {% if item.evidence.row_ids %}
154:                       {% for row_id in item.evidence.row_ids[:3] %}
155:                         <a href="/trace/row?dataset_version_id={{ item.evidence.dataset_version_id }}&row_index={{ row_id }}">Row {{ row_id }}</a>
156:                       {% endfor %}
157:                       {% if item.evidence.row_ids|length > 3 %}
158:                         <span class="muted">…</span>
159:                       {% endif %}
160:                     {% endif %}
161:                   </div>
162:                 {% endif %}
163:               </div>
164:             {% endfor %}
165:           {% else %}
166:             <p class="muted">No findings.</p>
167:           {% endif %}
168:         </div>
169:       {% endfor %}
170:     </section>
171: 
172:     <p>
173:       <a href="/runs/{{ run_id }}">Run Status</a>
174:       {% if project %}
175:         | <a href="/projects/{{ project.project_id }}">Back to Project</a>
176:       {% endif %}
177:     </p>
178:   </div>
179: </body>
180: </html>
````

## File: src/statistic_harness/ui/templates/run.html
````html
  1: <!DOCTYPE html>
  2: <html>
  3: <head>
  4:   <title>Run {{ run_id }}</title>
  5:   <link rel="stylesheet" href="/static/app.css?v=dark4" />
  6: </head>
  7: <body>
  8:   <div class="container">
  9:     <nav class="top-nav">
 10:       <a href="/">Home</a>
 11:       <a href="/projects">Projects</a>
 12:       <a href="/templates">Templates</a>
 13:       <a href="/trace">Trace</a>
 14:       <a href="/vectors">Vectors</a>
 15:     </nav>
 16: 
 17:     <header class="hero compact">
 18:       <div>
 19:         <p class="eyebrow">Run</p>
 20:         <h1>{{ run_label or run_id }}</h1>
 21:         {% if project %}
 22:           <p class="lead">Project: <a href="/projects/{{ project.project_id }}">{{ project.name or project.project_id }}</a></p>
 23:         {% endif %}
 24:         <p class="muted">Status: <span id="run-status">loading</span></p>
 25:       </div>
 26:       <div class="hero-panel">
 27:         {% if evaluation %}
 28:           <p class="muted">Known issues: <strong>{{ evaluation.result }}</strong></p>
 29:           {% if evaluation.messages %}
 30:             <p class="muted">{{ evaluation.messages|length }} issue(s) flagged</p>
 31:           {% endif %}
 32:         {% else %}
 33:           <p class="muted">Known issues: not evaluated</p>
 34:         {% endif %}
 35:       </div>
 36:     </header>
 37: 
 38:     <section class="card">
 39:       <h2>Run Links</h2>
 40:       <ul class="clean-list">
 41:         <li><a href="/runs/{{ run_id }}/report">View report</a></li>
 42:         <li><a href="/runs/{{ run_id }}/evaluate">Evaluate</a></li>
 43:         <li><a href="/vectors?collection=report_{{ run_id }}">Vector search</a></li>
 44:         <li><a href="/api/runs/{{ run_id }}/report.json">report.json</a></li>
 45:         <li><a href="/api/runs/{{ run_id }}/report.md">report.md</a></li>
 46:       </ul>
 47:       {% if project %}
 48:         <p><a href="/projects/{{ project.project_id }}">Back to Project</a></p>
 49:       {% endif %}
 50:     </section>
 51: 
 52:     <section class="card">
 53:       <h2>Progress</h2>
 54:       <div class="progress-row">
 55:         <div class="progress-bar">
 56:           <div class="progress-fill" id="progress-fill"></div>
 57:         </div>
 58:         <span id="progress-label">0%</span>
 59:       </div>
 60:       <ul class="clean-list" id="plugin-progress"></ul>
 61:     </section>
 62: 
 63:     <section class="card">
 64:       <h2>Errors</h2>
 65:       <ul class="clean-list" id="error-list">
 66:         <li class="muted">No errors reported.</li>
 67:       </ul>
 68:     </section>
 69:   </div>
 70: 
 71:   <script>
 72:     async function pollProgress() {
 73:       const resp = await fetch(`/api/runs/{{ run_id }}/progress`);
 74:       if (!resp.ok) return;
 75:       const payload = await resp.json();
 76:       const status = payload.run_status || "unknown";
 77:       document.getElementById("run-status").textContent = status;
 78:       const percent = payload.percent_complete || 0;
 79:       const fill = document.getElementById("progress-fill");
 80:       const label = document.getElementById("progress-label");
 81:       if (fill) fill.style.width = `${percent}%`;
 82:       if (label) label.textContent = `${percent}%`;
 83:       const list = document.getElementById("plugin-progress");
 84:       if (list) {
 85:         list.innerHTML = "";
 86:         (payload.plugins || []).forEach((item) => {
 87:           const li = document.createElement("li");
 88:           const statusSpan = document.createElement("span");
 89:           statusSpan.className = `pill ${item.status}`;
 90:           statusSpan.textContent = item.status;
 91:           li.textContent = item.plugin_id + " ";
 92:           li.appendChild(statusSpan);
 93:           list.appendChild(li);
 94:         });
 95:       }
 96:       const errorList = document.getElementById("error-list");
 97:       if (errorList) {
 98:         errorList.innerHTML = "";
 99:         const errors = payload.errors || [];
100:         if (errors.length === 0) {
101:           const li = document.createElement("li");
102:           li.className = "muted";
103:           li.textContent = "No errors reported.";
104:           errorList.appendChild(li);
105:         } else {
106:           errors.forEach((err) => {
107:             const li = document.createElement("li");
108:             const badge = document.createElement("span");
109:             badge.className = "pill error";
110:             badge.textContent = "error";
111:             const label = document.createElement("strong");
112:             label.textContent = err.plugin_id || "plugin";
113:             li.appendChild(label);
114:             li.appendChild(document.createTextNode(" "));
115:             li.appendChild(badge);
116:             if (err.message) {
117:               const msg = document.createElement("div");
118:               msg.className = "muted";
119:               msg.textContent = err.message;
120:               li.appendChild(msg);
121:             }
122:             errorList.appendChild(li);
123:           });
124:         }
125:       }
126:       if (status !== "completed" && status !== "failed") {
127:         setTimeout(pollProgress, 1500);
128:       }
129:     }
130:     pollProgress();
131:   </script>
132: </body>
133: </html>
````

## File: src/statistic_harness/ui/__init__.py
````python
1: """UI package."""
````

## File: src/statistic_harness/__init__.py
````python
1: """Statistic harness package."""
````

## File: src/statistic_harness/cli.py
````python
  1: from __future__ import annotations
  2: 
  3: import argparse
  4: import os
  5: import hashlib
  6: import json
  7: from pathlib import Path
  8: from typing import Any
  9: 
 10: import uvicorn
 11: import yaml
 12: 
 13: from statistic_harness.core.auth import (
 14:     generate_api_key,
 15:     hash_password,
 16:     hash_token,
 17:     normalize_email,
 18:     verify_password,
 19: )
 20: from statistic_harness.core.evaluation import evaluate_report
 21: from statistic_harness.core.pipeline import Pipeline
 22: from statistic_harness.core.plugin_manager import PluginManager
 23: from statistic_harness.core.report import build_report, write_report
 24: from statistic_harness.core.storage import Storage
 25: from statistic_harness.core.tenancy import get_tenant_context
 26: from statistic_harness.core.utils import (
 27:     auth_enabled,
 28:     json_dumps,
 29:     now_iso,
 30:     file_sha256,
 31:     vector_store_enabled,
 32: )
 33: from statistic_harness.core.vector_store import VectorStore, hash_embedding
 34: from statistic_harness.ui.server import app
 35: 
 36: 
 37: def load_settings(path: str | None) -> dict[str, Any]:
 38:     if not path:
 39:         return {}
 40:     content = Path(path).read_text(encoding="utf-8")
 41:     if path.endswith(".json"):
 42:         return json.loads(content)
 43:     return yaml.safe_load(content)
 44: 
 45: 
 46: def cmd_list_plugins() -> None:
 47:     manager = PluginManager(Path("plugins"))
 48:     for spec in manager.discover():
 49:         print(f"{spec.plugin_id}: {spec.name} ({spec.type})")
 50: 
 51: 
 52: def cmd_serve(host: str, port: int) -> None:
 53:     allow_network = os.environ.get("STAT_HARNESS_ALLOW_NETWORK", "").lower() in {
 54:         "1",
 55:         "true",
 56:         "yes",
 57:     }
 58:     if not allow_network and host not in {"127.0.0.1", "localhost", "::1"}:
 59:         raise SystemExit("Network disabled: use localhost or set STAT_HARNESS_ALLOW_NETWORK=1")
 60:     uvicorn.run(app, host=host, port=port)
 61: 
 62: 
 63: def _require_cli_api_key(admin_required: bool = False) -> None:
 64:     if not auth_enabled():
 65:         return
 66:     token = os.environ.get("STAT_HARNESS_API_KEY", "").strip()
 67:     if not token:
 68:         raise SystemExit("Missing STAT_HARNESS_API_KEY")
 69:     tenant_ctx = get_tenant_context()
 70:     storage = Storage(tenant_ctx.db_path, tenant_ctx.tenant_id)
 71:     row = storage.fetch_api_key_by_hash(hash_token(token), tenant_id=tenant_ctx.tenant_id)
 72:     if not row or row.get("revoked_at") or row.get("disabled_at"):
 73:         raise SystemExit("Invalid API key")
 74:     membership = storage.fetch_membership(int(row["user_id"]), tenant_ctx.tenant_id)
 75:     if not membership:
 76:         raise SystemExit("API key not authorized for tenant")
 77:     if admin_required and not bool(row.get("is_admin")):
 78:         raise SystemExit("Admin API key required")
 79:     storage.touch_api_key(int(row["key_id"]), now_iso())
 80: 
 81: 
 82: def cmd_run(
 83:     file_path: str, plugins: str, settings_path: str | None, run_seed: int
 84: ) -> None:
 85:     _require_cli_api_key()
 86:     tenant_ctx = get_tenant_context()
 87:     pipeline = Pipeline(
 88:         tenant_ctx.appdata_root, Path("plugins"), tenant_id=tenant_ctx.tenant_id
 89:     )
 90:     plugin_ids = [p for p in plugins.split(",") if p]
 91:     settings = load_settings(settings_path)
 92:     run_id = pipeline.run(Path(file_path), plugin_ids, settings, run_seed)
 93:     run_dir = tenant_ctx.tenant_root / "runs" / run_id
 94:     report = build_report(
 95:         pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
 96:     )
 97:     write_report(report, run_dir)
 98:     print(run_id)
 99: 
100: 
101: def cmd_eval(report_path: str, ground_truth: str) -> None:
102:     ok, messages = evaluate_report(Path(report_path), Path(ground_truth))
103:     if not ok:
104:         for msg in messages:
105:             print(msg)
106:         raise SystemExit(1)
107:     print("Evaluation passed")
108: 
109: 
110: def cmd_make_ground_truth(report_path: str, output_path: str) -> None:
111:     report = json.loads(Path(report_path).read_text(encoding="utf-8"))
112:     features = [
113:         f.get("feature")
114:         for f in report.get("plugins", {})
115:         .get("analysis_gaussian_knockoffs", {})
116:         .get("findings", [])
117:     ]
118:     template = {
119:         "strict": False,
120:         "features": [f for f in features if f],
121:         "changepoints": [],
122:         "dependence_shift_pairs": [],
123:         "anomalies": [],
124:         "min_anomaly_hits": 0,
125:         "changepoint_tolerance": 3,
126:     }
127:     Path(output_path).write_text(yaml.safe_dump(template), encoding="utf-8")
128: 
129: 
130: def cmd_backfill(plugin_id: str, run_seed: int) -> None:
131:     _require_cli_api_key()
132:     tenant_ctx = get_tenant_context()
133:     pipeline = Pipeline(
134:         tenant_ctx.appdata_root, Path("plugins"), tenant_id=tenant_ctx.tenant_id
135:     )
136:     specs = {spec.plugin_id: spec for spec in pipeline.manager.discover()}
137:     if plugin_id not in specs:
138:         raise SystemExit(f"Unknown plugin: {plugin_id}")
139:     spec = specs[plugin_id]
140:     module_path, _ = spec.entrypoint.split(":", 1)
141:     if module_path.endswith(".py"):
142:         module_file = spec.path / module_path
143:     else:
144:         module_file = spec.path / f"{module_path}.py"
145:     code_hash = file_sha256(module_file) if module_file.exists() else None
146:     settings_hash = hashlib.sha256(json_dumps({}).encode("utf-8")).hexdigest()
147: 
148:     for dataset in pipeline.storage.list_dataset_versions():
149:         dataset_version_id = dataset["dataset_version_id"]
150:         pipeline.storage.enqueue_analysis_job(
151:             dataset_version_id,
152:             plugin_id,
153:             spec.version,
154:             code_hash,
155:             settings_hash,
156:             run_seed,
157:             now_iso(),
158:         )
159: 
160:     jobs = pipeline.storage.list_analysis_jobs(status="queued")
161:     for job in jobs:
162:         job_id = int(job["job_id"])
163:         pipeline.storage.update_analysis_job_status(
164:             job_id, "running", started_at=now_iso()
165:         )
166:         try:
167:             pipeline.run(
168:                 None,
169:                 [plugin_id],
170:                 {},
171:                 int(job.get("run_seed") or 0),
172:                 dataset_version_id=job["dataset_version_id"],
173:             )
174:             pipeline.storage.update_analysis_job_status(
175:                 job_id, "completed", completed_at=now_iso()
176:             )
177:         except Exception as exc:  # pragma: no cover - failure path
178:             pipeline.storage.update_analysis_job_status(
179:                 job_id, "error", completed_at=now_iso(), error={"message": str(exc)}
180:             )
181: 
182: 
183: def cmd_create_user(email: str, password: str, name: str | None, admin: bool) -> None:
184:     tenant_ctx = get_tenant_context()
185:     storage = Storage(tenant_ctx.db_path, tenant_ctx.tenant_id)
186:     if auth_enabled() and storage.count_users() > 0:
187:         _require_cli_api_key(admin_required=True)
188:     normalized = normalize_email(email)
189:     if storage.fetch_user_by_email(normalized):
190:         raise SystemExit("User already exists")
191:     user_id = storage.create_user(
192:         normalized, hash_password(password), name, admin, now_iso()
193:     )
194:     role = "admin" if admin else "member"
195:     storage.ensure_membership(user_id, role, now_iso())
196:     print(f"Created user {normalized}")
197: 
198: 
199: def cmd_create_api_key(email: str, password: str, name: str | None) -> None:
200:     tenant_ctx = get_tenant_context()
201:     storage = Storage(tenant_ctx.db_path, tenant_ctx.tenant_id)
202:     user = storage.fetch_user_by_email(normalize_email(email))
203:     if not user or user.get("disabled_at"):
204:         raise SystemExit("Invalid credentials")
205:     if not verify_password(password, user.get("password_hash") or ""):
206:         raise SystemExit("Invalid credentials")
207:     membership = storage.fetch_membership(int(user["user_id"]), tenant_ctx.tenant_id)
208:     if not membership:
209:         raise SystemExit("User not authorized for tenant")
210:     token = generate_api_key()
211:     storage.create_api_key(
212:         int(user["user_id"]), hash_token(token), name, now_iso()
213:     )
214:     print(token)
215: 
216: 
217: def cmd_create_tenant(tenant_id: str, name: str | None) -> None:
218:     tenant_ctx = get_tenant_context()
219:     storage = Storage(tenant_ctx.db_path, tenant_ctx.tenant_id)
220:     if auth_enabled():
221:         _require_cli_api_key(admin_required=True)
222:     storage.create_tenant(tenant_id.strip(), name, now_iso())
223:     print(f"Created tenant {tenant_id}")
224: 
225: 
226: def cmd_revoke_api_key(key_id: int) -> None:
227:     tenant_ctx = get_tenant_context()
228:     storage = Storage(tenant_ctx.db_path, tenant_ctx.tenant_id)
229:     if auth_enabled():
230:         _require_cli_api_key(admin_required=True)
231:     storage.revoke_api_key(int(key_id))
232:     print("API key revoked")
233: 
234: 
235: def cmd_disable_user(email: str) -> None:
236:     tenant_ctx = get_tenant_context()
237:     storage = Storage(tenant_ctx.db_path, tenant_ctx.tenant_id)
238:     if auth_enabled():
239:         _require_cli_api_key(admin_required=True)
240:     user = storage.fetch_user_by_email(normalize_email(email))
241:     if not user:
242:         raise SystemExit("User not found")
243:     storage.disable_user(int(user["user_id"]))
244:     storage.revoke_user_sessions(int(user["user_id"]))
245:     storage.revoke_user_api_keys(int(user["user_id"]))
246:     print(f"Disabled user {email}")
247: 
248: 
249: def _require_vector_store() -> None:
250:     if not vector_store_enabled():
251:         raise SystemExit("Vector store disabled (STAT_HARNESS_ENABLE_VECTOR_STORE=1)")
252: 
253: 
254: def cmd_vector_list() -> None:
255:     _require_vector_store()
256:     if auth_enabled():
257:         _require_cli_api_key()
258:     tenant_ctx = get_tenant_context()
259:     store = VectorStore(tenant_ctx.db_path, tenant_ctx.tenant_id)
260:     for row in store.list_collections():
261:         name = row.get("name")
262:         dims = row.get("dimensions")
263:         created = row.get("created_at") or ""
264:         print(f"{name}\t{dims}\t{created}")
265: 
266: 
267: def _resolve_dimensions(
268:     store: VectorStore, collection: str, dimensions: int | None
269: ) -> int:
270:     if dimensions is not None:
271:         return int(dimensions)
272:     dims = store.collection_dimensions(collection)
273:     if not dims:
274:         raise SystemExit("Unknown collection or dimensions required")
275:     if len(dims) > 1:
276:         raise SystemExit("Multiple dimensions found; use --dimensions")
277:     return int(dims[0])
278: 
279: 
280: def cmd_vector_query(
281:     collection: str,
282:     text: str | None,
283:     vector: str | None,
284:     k: int,
285:     dimensions: int | None,
286: ) -> None:
287:     _require_vector_store()
288:     if auth_enabled():
289:         _require_cli_api_key()
290:     tenant_ctx = get_tenant_context()
291:     store = VectorStore(tenant_ctx.db_path, tenant_ctx.tenant_id)
292:     if text and vector:
293:         raise SystemExit("Use either --text or --vector")
294:     if not text and not vector:
295:         raise SystemExit("Provide --text or --vector")
296:     if vector:
297:         parts = [part for part in vector.replace("\n", ",").split(",") if part.strip()]
298:         values = [float(part.strip()) for part in parts]
299:         query_vector = values
300:     else:
301:         dims = _resolve_dimensions(store, collection, dimensions)
302:         query_vector = hash_embedding(text or "", dims)
303:     results = store.query(collection, query_vector, k=k)
304:     print(json.dumps(results, indent=2, sort_keys=True))
305: 
306: 
307: def main() -> None:
308:     parser = argparse.ArgumentParser()
309:     sub = parser.add_subparsers(dest="command", required=True)
310: 
311:     sub.add_parser("list-plugins")
312: 
313:     serve_parser = sub.add_parser("serve")
314:     serve_parser.add_argument("--host", default="127.0.0.1")
315:     serve_parser.add_argument("--port", type=int, default=8000)
316: 
317:     run_parser = sub.add_parser("run")
318:     run_parser.add_argument("--file", required=True)
319:     run_parser.add_argument("--plugins", default="auto")
320:     run_parser.add_argument("--settings")
321:     run_parser.add_argument("--run-seed", type=int, default=0)
322: 
323:     eval_parser = sub.add_parser("eval")
324:     eval_parser.add_argument("--report", required=True)
325:     eval_parser.add_argument("--ground-truth", required=True)
326: 
327:     gt_parser = sub.add_parser("make-ground-truth-template")
328:     gt_parser.add_argument("--report", required=True)
329:     gt_parser.add_argument("-o", "--output", required=True)
330: 
331:     backfill_parser = sub.add_parser("backfill")
332:     backfill_parser.add_argument("--plugin", required=True)
333:     backfill_parser.add_argument("--run-seed", type=int, default=0)
334: 
335:     user_parser = sub.add_parser("create-user")
336:     user_parser.add_argument("--email", required=True)
337:     user_parser.add_argument("--password", required=True)
338:     user_parser.add_argument("--name")
339:     user_parser.add_argument("--admin", action="store_true")
340: 
341:     api_key_parser = sub.add_parser("create-api-key")
342:     api_key_parser.add_argument("--email", required=True)
343:     api_key_parser.add_argument("--password", required=True)
344:     api_key_parser.add_argument("--name")
345: 
346:     tenant_parser = sub.add_parser("create-tenant")
347:     tenant_parser.add_argument("--tenant-id", required=True)
348:     tenant_parser.add_argument("--name")
349: 
350:     revoke_key_parser = sub.add_parser("revoke-api-key")
351:     revoke_key_parser.add_argument("--key-id", required=True, type=int)
352: 
353:     disable_user_parser = sub.add_parser("disable-user")
354:     disable_user_parser.add_argument("--email", required=True)
355: 
356:     vector_list_parser = sub.add_parser("vector-list")
357: 
358:     vector_query_parser = sub.add_parser("vector-query")
359:     vector_query_parser.add_argument("--collection", required=True)
360:     vector_query_parser.add_argument("--text")
361:     vector_query_parser.add_argument("--vector")
362:     vector_query_parser.add_argument("--k", type=int, default=10)
363:     vector_query_parser.add_argument("--dimensions", type=int)
364: 
365:     args = parser.parse_args()
366: 
367:     if args.command == "list-plugins":
368:         cmd_list_plugins()
369:     elif args.command == "serve":
370:         cmd_serve(args.host, args.port)
371:     elif args.command == "run":
372:         cmd_run(args.file, args.plugins, args.settings, args.run_seed)
373:     elif args.command == "eval":
374:         cmd_eval(args.report, args.ground_truth)
375:     elif args.command == "make-ground-truth-template":
376:         cmd_make_ground_truth(args.report, args.output)
377:     elif args.command == "backfill":
378:         cmd_backfill(args.plugin, args.run_seed)
379:     elif args.command == "create-user":
380:         cmd_create_user(args.email, args.password, args.name, args.admin)
381:     elif args.command == "create-api-key":
382:         cmd_create_api_key(args.email, args.password, args.name)
383:     elif args.command == "create-tenant":
384:         cmd_create_tenant(args.tenant_id, args.name)
385:     elif args.command == "revoke-api-key":
386:         cmd_revoke_api_key(args.key_id)
387:     elif args.command == "disable-user":
388:         cmd_disable_user(args.email)
389:     elif args.command == "vector-list":
390:         cmd_vector_list()
391:     elif args.command == "vector-query":
392:         cmd_vector_query(
393:             args.collection, args.text, args.vector, args.k, args.dimensions
394:         )
395:     else:
396:         raise SystemExit(2)
397: 
398: 
399: if __name__ == "__main__":
400:     main()
````

## File: tests/fixtures/ground_truth_enertia.yaml
````yaml
  1: strict: true
  2: expected_findings:
  3:   - plugin_id: analysis_capacity_scaling
  4:     kind: capacity_scaling
  5:     where:
  6:       measurement_type: modeled
  7:     min_count: 1
  8:   - plugin_id: analysis_close_cycle_capacity_model
  9:     kind: close_cycle_capacity_model
 10:     where:
 11:       measurement_type: not_applicable
 12:       reason: close_window_not_inferred
 13:     min_count: 6
 14:     max_count: 6
 15: expected_metrics:
 16:   - plugin_id: analysis_close_cycle_contention
 17:     metric: candidates
 18:     value: 0
 19:     tolerance: 0
 20:   - plugin_id: analysis_queue_delay_decomposition
 21:     metric: standalone_runs
 22:     value: 2
 23:     tolerance: 0
 24:   - plugin_id: analysis_queue_delay_decomposition
 25:     metric: sequence_runs
 26:     value: 14
 27:     tolerance: 0
 28:   - plugin_id: analysis_queue_delay_decomposition
 29:     metric: eligible_wait_hours_total
 30:     value: 0.05
 31:     tolerance:
 32:       absolute: 0.001
 33:       relative: 0
 34:   - plugin_id: analysis_queue_delay_decomposition
 35:     metric: eligible_wait_gt_hours_total
 36:     value: 0.0333333333
 37:     tolerance:
 38:       absolute: 0.001
 39:       relative: 0
 40:   - plugin_id: analysis_tail_isolation
 41:     metric: tail_rows
 42:     value: 6
 43:     tolerance: 0
 44:   - plugin_id: analysis_tail_isolation
 45:     metric: tail_hours_total
 46:     value: 0.2
 47:     tolerance:
 48:       absolute: 0.001
 49:       relative: 0
 50:   - plugin_id: analysis_percentile_analysis
 51:     metric: groups
 52:     value: 2
 53:     tolerance: 0
 54:   - plugin_id: analysis_attribution
 55:     metric: total_rows
 56:     value: 16
 57:     tolerance: 0
 58:   - plugin_id: analysis_dependency_resolution_join
 59:     metric: dependency_rows
 60:     value: 12
 61:     tolerance: 0
 62:   - plugin_id: analysis_dependency_resolution_join
 63:     metric: near_zero_ratio
 64:     value: 0.5
 65:     tolerance:
 66:       absolute: 0.001
 67:       relative: 0
 68:   - plugin_id: analysis_sequence_classification
 69:     metric: standalone_runs
 70:     value: 2
 71:     tolerance: 0
 72:   - plugin_id: analysis_sequence_classification
 73:     metric: sequence_runs
 74:     value: 14
 75:     tolerance: 0
 76:   - plugin_id: analysis_process_sequence
 77:     metric: variants
 78:     value: 2
 79:     tolerance: 0
 80:   - plugin_id: analysis_process_sequence
 81:     metric: transitions
 82:     value: 0
 83:     tolerance: 0
 84:   - plugin_id: analysis_chain_makespan
 85:     metric: chains
 86:     value: 16
 87:     tolerance: 0
 88:   - plugin_id: analysis_chain_makespan
 89:     metric: max_makespan_seconds
 90:     value: 300
 91:     tolerance: 0
 92:   - plugin_id: analysis_concurrency_reconstruction
 93:     metric: hosts
 94:     value: 2
 95:     tolerance: 0
 96:   - plugin_id: analysis_concurrency_reconstruction
 97:     metric: peak_concurrency
 98:     value: 2
 99:     tolerance: 0
100:   - plugin_id: analysis_concurrency_reconstruction
101:     metric: avg_concurrency
102:     value: 1.0161290323
103:     tolerance:
104:       absolute: 0.001
105:       relative: 0
106:   - plugin_id: analysis_capacity_scaling
107:     metric: rows
108:     value: 16
109:     tolerance: 0
110:   - plugin_id: analysis_capacity_scaling
111:     metric: host_count
112:     value: 2
113:     tolerance: 0
114:   - plugin_id: analysis_capacity_scaling
115:     metric: scale_factor
116:     value: 1.5
117:     tolerance:
118:       absolute: 0.001
119:       relative: 0
120:   - plugin_id: analysis_capacity_scaling
121:     metric: baseline_wait_hours
122:     value: 0.3666666667
123:     tolerance:
124:       absolute: 0.001
125:       relative: 0
126:   - plugin_id: analysis_capacity_scaling
127:     metric: modeled_wait_hours
128:     value: 0.2444444444
129:     tolerance:
130:       absolute: 0.001
131:       relative: 0
132:   - plugin_id: analysis_capacity_scaling
133:     metric: reduction_hours
134:     value: 0.1222222222
135:     tolerance:
136:       absolute: 0.001
137:       relative: 0
138:   - plugin_id: analysis_determinism_discipline
139:     metric: missing_measurement_type
140:     value: 0
141:     tolerance: 0
142:   - plugin_id: analysis_determinism_discipline
143:     metric: modeled_missing_assumption
144:     value: 0
145:     tolerance: 0
````

## File: tests/fixtures/ground_truth_synth.yaml
````yaml
 1: features:
 2:   - x1
 3:   - x2
 4: changepoints:
 5:   - 10
 6: dependence_shift_pairs:
 7:   - [a, b]
 8: anomalies:
 9:   - 7
10: min_anomaly_hits: 1
11: changepoint_tolerance: 5
````

## File: tests/fixtures/make_synth_data.py
````python
 1: from __future__ import annotations
 2: 
 3: import numpy as np
 4: import pandas as pd
 5: 
 6: rng = np.random.default_rng(42)
 7: 
 8: # Linear data
 9: n = 200
10: x1 = rng.normal(size=n)
11: x2 = rng.normal(size=n)
12: y = 2 * x1 - 0.5 * x2 + rng.normal(scale=0.1, size=n)
13: # Inject outliers
14: outlier_idx = [10, 50, 150]
15: y[outlier_idx] += 5
16: linear = pd.DataFrame({"x1": x1, "x2": x2, "y": y})
17: linear.to_csv("tests/fixtures/synth_linear.csv", index=False)
18: 
19: # Timeseries with mean shift
20: values = np.concatenate(
21:     [rng.normal(loc=0, scale=1, size=100), rng.normal(loc=3, scale=1, size=100)]
22: )
23: timeseries = pd.DataFrame({"value": values})
24: timeseries.to_csv("tests/fixtures/synth_timeseries.csv", index=False)
25: 
26: # Clusters
27: cluster1 = rng.normal(loc=0, scale=0.5, size=(50, 2))
28: cluster2 = rng.normal(loc=3, scale=0.5, size=(50, 2))
29: clusters = pd.DataFrame(np.vstack([cluster1, cluster2]), columns=["x", "y"])
30: clusters.to_csv("tests/fixtures/synth_clusters.csv", index=False)
31: 
32: # Shifted correlation
33: a = rng.normal(size=100)
34: b = rng.normal(size=100)
35: first = pd.DataFrame({"a": a, "b": b})
36: second = pd.DataFrame(
37:     {"a": rng.normal(size=100), "b": a + rng.normal(scale=0.1, size=100)}
38: )
39: shift = pd.concat([first, second], ignore_index=True)
40: shift.to_csv("tests/fixtures/synth_shift_corr.csv", index=False)
````

## File: tests/fixtures/synth_clusters.csv
````
 1: x,y
 2: 0.1,0.2
 3: -0.2,0.1
 4: 0.0,-0.1
 5: 0.3,0.2
 6: -0.1,-0.2
 7: 3.1,3.2
 8: 2.9,3.0
 9: 3.2,2.8
10: 3.0,3.1
11: 2.8,3.3
````

## File: tests/fixtures/synth_linear.csv
````
 1: x1,x2,y
 2: 0.1,0.2,0.5
 3: 0.2,-0.1,0.7
 4: -0.3,0.4,-0.8
 5: 0.5,-0.4,1.4
 6: -0.6,0.3,-1.4
 7: 0.7,-0.2,2.0
 8: -0.8,0.1,-2.1
 9: 0.9,-0.5,6.0
10: -1.0,0.6,-2.6
11: 0.3,-0.3,0.9
````

## File: tests/fixtures/synth_shift_corr.csv
````
 1: a,b
 2: 0.1,0.5
 3: -0.2,0.1
 4: 0.3,-0.4
 5: -0.1,0.2
 6: 0.0,-0.3
 7: 1.0,1.1
 8: 1.2,1.3
 9: 0.8,0.9
10: 1.1,1.0
11: 0.9,0.95
````

## File: tests/fixtures/synth_timeseries.csv
````
 1: value
 2: 0.1
 3: -0.2
 4: 0.0
 5: 0.3
 6: -0.1
 7: 0.2
 8: -0.3
 9: 0.1
10: 0.0
11: -0.2
12: 2.5
13: 3.0
14: 2.8
15: 3.2
16: 2.9
17: 3.1
18: 2.7
19: 3.3
20: 3.0
21: 2.6
````

## File: tests/plugins/test_bocpd_gaussian.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_bocpd_gaussian.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_bocpd_gaussian(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_timeseries.csv")
 9:     ctx = make_context(run_dir, df, {"value_column": "value"})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert len(result.findings) >= 0
````

## File: tests/plugins/test_conformal_feature_prediction.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_conformal_feature_prediction.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_conformal_feature_prediction(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_linear.csv")
 9:     ctx = make_context(run_dir, df, {"alpha": 0.2})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert any(f["kind"] == "anomaly" for f in result.findings)
````

## File: tests/plugins/test_dp_gmm.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_dp_gmm.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_dp_gmm(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_clusters.csv")
 9:     ctx = make_context(run_dir, df, {})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert result.metrics["clusters"] >= 2
````

## File: tests/plugins/test_gaussian_copula_shift.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_gaussian_copula_shift.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_gaussian_copula_shift(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_shift_corr.csv")
 9:     ctx = make_context(run_dir, df, {"max_pairs": 1, "n_permutations": 5})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert any(f["pair"] == ["a", "b"] for f in result.findings)
````

## File: tests/plugins/test_gaussian_knockoffs.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_gaussian_knockoffs.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_gaussian_knockoffs(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_linear.csv")
 9:     ctx = make_context(run_dir, df, {"target_column": "y"})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert any(f["selected"] for f in result.findings)
````

## File: tests/plugins/test_graph_topology_curves.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_graph_topology_curves.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_graph_topology_curves(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_clusters.csv")
 9:     ctx = make_context(run_dir, df, {"max_points": 10, "n_thresholds": 5})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert result.metrics["beta1_peak"] >= 0
````

## File: tests/plugins/test_knockoff_wrapper_rf.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_knockoff_wrapper_rf.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_knockoff_wrapper_rf(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_linear.csv")
 9:     ctx = make_context(run_dir, df, {"target_column": "y", "n_estimators": 10})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert any(f["selected"] for f in result.findings)
````

## File: tests/plugins/test_notears_linear.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_notears_linear.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_notears_linear(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_linear.csv")
 9:     ctx = make_context(run_dir, df, {"weight_threshold": 0.1})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert len(result.findings) >= 0
````

## File: tests/plugins/test_online_conformal_changepoint.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_online_conformal_changepoint.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_online_conformal_changepoint(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_timeseries.csv")
 9:     ctx = make_context(run_dir, df, {"value_column": "value"})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert len(result.findings) >= 0
````

## File: tests/plugins/test_scan_statistics.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.analysis_scan_statistics.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_scan_statistics(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_timeseries.csv")
 9:     ctx = make_context(run_dir, df, {"value_column": "value", "n_permutations": 10})
10:     result = Plugin().run(ctx)
11:     assert result.status in {"ok", "skipped"}
12:     if result.status == "ok":
13:         assert len(result.findings) >= 0
````

## File: tests/conftest.py
````python
 1: from __future__ import annotations
 2: 
 3: from pathlib import Path
 4: 
 5: import pandas as pd
 6: import pytest
 7: 
 8: from statistic_harness.core.dataset_io import DatasetAccessor
 9: from statistic_harness.core.storage import Storage
10: from statistic_harness.core.types import PluginContext
11: from statistic_harness.core.utils import now_iso
12: 
13: 
14: @pytest.fixture()
15: def run_dir(tmp_path: Path) -> Path:
16:     run_dir = tmp_path / "run"
17:     (run_dir / "dataset").mkdir(parents=True, exist_ok=True)
18:     (run_dir / "logs").mkdir(parents=True, exist_ok=True)
19:     return run_dir
20: 
21: 
22: def make_context(
23:     run_dir: Path, df: pd.DataFrame, settings: dict, populate: bool = True
24: ) -> PluginContext:
25:     storage = Storage(run_dir / "state.sqlite")
26:     project_id = "test-project"
27:     dataset_id = "test_dataset"
28:     dataset_version_id = "test_dataset"
29:     table_name = f"dataset_{dataset_version_id}"
30:     storage.ensure_project(project_id, project_id, now_iso())
31:     storage.ensure_dataset(dataset_id, project_id, dataset_id, now_iso())
32: 
33:     with storage.connection() as conn:
34:         storage.ensure_dataset_version(
35:             dataset_version_id, dataset_id, now_iso(), table_name, dataset_id, conn
36:         )
37:         if populate:
38:             columns_meta = []
39:             for idx, (col, dtype) in enumerate(df.dtypes.items(), start=1):
40:                 columns_meta.append(
41:                     {
42:                         "column_id": idx,
43:                         "safe_name": f"c{idx}",
44:                         "original_name": str(col),
45:                         "dtype": str(dtype),
46:                         "sqlite_type": "REAL"
47:                         if pd.api.types.is_numeric_dtype(dtype)
48:                         else "TEXT",
49:                     }
50:                 )
51:             storage.create_dataset_table(table_name, columns_meta, conn)
52:             storage.replace_dataset_columns(dataset_version_id, columns_meta, conn)
53:             safe_columns = [col["safe_name"] for col in columns_meta]
54:             rows = []
55:             for row_index, row in enumerate(df.itertuples(index=False, name=None)):
56:                 rows.append((row_index, None, *row))
57:             storage.insert_dataset_rows(table_name, safe_columns, rows, conn)
58:             storage.update_dataset_version_stats(
59:                 dataset_version_id, len(df), len(columns_meta), conn
60:             )
61: 
62:     accessor = DatasetAccessor(storage, dataset_version_id)
63: 
64:     def logger(msg: str) -> None:
65:         pass
66: 
67:     return PluginContext(
68:         run_id="test-run",
69:         run_dir=run_dir,
70:         settings=settings,
71:         run_seed=42,
72:         logger=logger,
73:         storage=storage,
74:         dataset_loader=accessor.load,
75:         project_id=project_id,
76:         dataset_id=dataset_id,
77:         dataset_version_id=dataset_version_id,
78:         input_hash=dataset_id,
79:     )
````

## File: tests/test_enertia_evaluation.py
````python
 1: from pathlib import Path
 2: 
 3: from statistic_harness.core.evaluation import evaluate_report
 4: from statistic_harness.core.pipeline import Pipeline
 5: 
 6: 
 7: def test_enertia_eventlog_evaluation(tmp_path, monkeypatch):
 8:     appdata = tmp_path / "appdata"
 9:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
10:     pipeline = Pipeline(appdata, Path("plugins"))
11:     run_id = pipeline.run(
12:         Path("tests/fixtures/enertia_eventlog.csv"),
13:         [
14:             "analysis_close_cycle_contention",
15:             "analysis_queue_delay_decomposition",
16:             "analysis_tail_isolation",
17:             "analysis_percentile_analysis",
18:             "analysis_attribution",
19:             "analysis_dependency_resolution_join",
20:             "analysis_sequence_classification",
21:             "analysis_process_sequence",
22:             "analysis_chain_makespan",
23:             "analysis_concurrency_reconstruction",
24:             "analysis_capacity_scaling",
25:             "analysis_close_cycle_capacity_model",
26:             "analysis_determinism_discipline",
27:         ],
28:         {},
29:         123,
30:     )
31:     report_path = appdata / "runs" / run_id / "report.json"
32:     ok, messages = evaluate_report(
33:         report_path, Path("tests/fixtures/ground_truth_enertia.yaml")
34:     )
35:     assert ok, messages
````

## File: tests/test_evaluation.py
````python
  1: import json
  2: from pathlib import Path
  3: 
  4: from statistic_harness.core.evaluation import evaluate_report
  5: 
  6: 
  7: def test_evaluation(tmp_path):
  8:     report = {
  9:         "run_id": "r1",
 10:         "created_at": "now",
 11:         "status": "completed",
 12:         "input": {"filename": "x", "rows": 1, "cols": 1, "inferred_types": {}},
 13:         "lineage": {
 14:             "run": {
 15:                 "run_id": "r1",
 16:                 "created_at": "now",
 17:                 "status": "completed",
 18:                 "run_seed": 0,
 19:             },
 20:             "input": {
 21:                 "upload_id": "",
 22:                 "filename": "x",
 23:                 "canonical_path": "",
 24:                 "input_hash": "",
 25:                 "sha256": "",
 26:                 "size_bytes": 0,
 27:             },
 28:             "dataset": {"dataset_version_id": "dv"},
 29:             "raw_format": None,
 30:             "template": None,
 31:             "plugins": {},
 32:         },
 33:         "plugins": {
 34:             "analysis_gaussian_knockoffs": {
 35:                 "status": "ok",
 36:                 "summary": "",
 37:                 "metrics": {},
 38:                 "findings": [
 39:                     {
 40:                         "kind": "feature_discovery",
 41:                         "feature": "x1",
 42:                         "score": 1.0,
 43:                         "selected": True,
 44:                     },
 45:                     {
 46:                         "kind": "feature_discovery",
 47:                         "feature": "x2",
 48:                         "score": 0.8,
 49:                         "selected": True,
 50:                     },
 51:                 ],
 52:                 "artifacts": [],
 53:                 "error": None,
 54:             },
 55:             "analysis_bocpd_gaussian": {
 56:                 "status": "ok",
 57:                 "summary": "",
 58:                 "metrics": {},
 59:                 "findings": [{"kind": "changepoint", "index": 10, "prob": 0.9}],
 60:                 "artifacts": [],
 61:                 "error": None,
 62:             },
 63:             "analysis_gaussian_copula_shift": {
 64:                 "status": "ok",
 65:                 "summary": "",
 66:                 "metrics": {},
 67:                 "findings": [
 68:                     {
 69:                         "kind": "dependence_shift",
 70:                         "pair": ["a", "b"],
 71:                         "delta": 0.5,
 72:                         "p_value": 0.1,
 73:                     }
 74:                 ],
 75:                 "artifacts": [],
 76:                 "error": None,
 77:             },
 78:             "analysis_conformal_feature_prediction": {
 79:                 "status": "ok",
 80:                 "summary": "",
 81:                 "metrics": {},
 82:                 "findings": [
 83:                     {
 84:                         "kind": "anomaly",
 85:                         "column": "y",
 86:                         "row_index": 7,
 87:                         "score": 1.0,
 88:                         "lower": 0.0,
 89:                         "upper": 2.0,
 90:                     }
 91:                 ],
 92:                 "artifacts": [],
 93:                 "error": None,
 94:             },
 95:         },
 96:     }
 97:     report_path = tmp_path / "report.json"
 98:     report_path.write_text(json.dumps(report), encoding="utf-8")
 99:     ok, messages = evaluate_report(
100:         report_path, Path("tests/fixtures/ground_truth_synth.yaml")
101:     )
102:     assert ok, messages
103: 
104: 
105: def test_evaluation_with_relative_tolerance(tmp_path):
106:     report = {
107:         "run_id": "r2",
108:         "created_at": "now",
109:         "status": "completed",
110:         "input": {"filename": "x", "rows": 1, "cols": 1, "inferred_types": {}},
111:         "lineage": {
112:             "run": {
113:                 "run_id": "r2",
114:                 "created_at": "now",
115:                 "status": "completed",
116:                 "run_seed": 0,
117:             },
118:             "input": {
119:                 "upload_id": "",
120:                 "filename": "x",
121:                 "canonical_path": "",
122:                 "input_hash": "",
123:                 "sha256": "",
124:                 "size_bytes": 0,
125:             },
126:             "dataset": {"dataset_version_id": "dv"},
127:             "raw_format": None,
128:             "template": None,
129:             "plugins": {},
130:         },
131:         "plugins": {
132:             "analysis_gaussian_knockoffs": {
133:                 "status": "ok",
134:                 "summary": "",
135:                 "metrics": {},
136:                 "findings": [
137:                     {
138:                         "kind": "feature_discovery",
139:                         "feature": "x1",
140:                         "score": 1.0,
141:                         "selected": True,
142:                     }
143:                 ],
144:                 "artifacts": [],
145:                 "error": None,
146:             },
147:             "analysis_bocpd_gaussian": {
148:                 "status": "ok",
149:                 "summary": "",
150:                 "metrics": {},
151:                 "findings": [{"kind": "changepoint", "index": 110, "prob": 0.9}],
152:                 "artifacts": [],
153:                 "error": None,
154:             },
155:             "analysis_conformal_feature_prediction": {
156:                 "status": "ok",
157:                 "summary": "",
158:                 "metrics": {},
159:                 "findings": [
160:                     {
161:                         "kind": "anomaly",
162:                         "column": "y",
163:                         "row_index": 55,
164:                         "score": 1.0,
165:                         "lower": 0.0,
166:                         "upper": 2.0,
167:                     }
168:                 ],
169:                 "artifacts": [],
170:                 "error": None,
171:             },
172:         },
173:     }
174:     report_path = tmp_path / "report.json"
175:     report_path.write_text(json.dumps(report), encoding="utf-8")
176:     ok, messages = evaluate_report(
177:         report_path, Path("tests/fixtures/ground_truth_tolerance.yaml")
178:     )
179:     assert ok, messages
180: 
181: 
182: def test_evaluation_expected_findings(tmp_path):
183:     report = {
184:         "run_id": "r3",
185:         "created_at": "now",
186:         "status": "completed",
187:         "input": {"filename": "x", "rows": 1, "cols": 1, "inferred_types": {}},
188:         "lineage": {
189:             "run": {
190:                 "run_id": "r3",
191:                 "created_at": "now",
192:                 "status": "completed",
193:                 "run_seed": 0,
194:             },
195:             "input": {
196:                 "upload_id": "",
197:                 "filename": "x",
198:                 "canonical_path": "",
199:                 "input_hash": "",
200:                 "sha256": "",
201:                 "size_bytes": 0,
202:             },
203:             "dataset": {"dataset_version_id": "dv"},
204:             "raw_format": None,
205:             "template": None,
206:             "plugins": {},
207:         },
208:         "plugins": {
209:             "analysis_process_sequence": {
210:                 "status": "ok",
211:                 "summary": "",
212:                 "metrics": {},
213:                 "findings": [
214:                     {
215:                         "kind": "process_variant",
216:                         "variant": ["qemail", "qpec"],
217:                         "count": 10,
218:                         "fraction": 0.2,
219:                         "columns": ["case_id", "activity"],
220:                     }
221:                 ],
222:                 "artifacts": [],
223:                 "error": None,
224:             }
225:         },
226:     }
227:     report_path = tmp_path / "report.json"
228:     report_path.write_text(json.dumps(report), encoding="utf-8")
229:     truth = {
230:         "expected_findings": [
231:             {
232:                 "plugin_id": "analysis_process_sequence",
233:                 "kind": "process_variant",
234:                 "contains": {"variant": "qemail"},
235:                 "min_count": 1,
236:             }
237:         ]
238:     }
239:     import yaml
240:     truth_path = tmp_path / "ground_truth.yaml"
241:     truth_path.write_text(yaml.safe_dump(truth), encoding="utf-8")
242:     ok, messages = evaluate_report(report_path, truth_path)
243:     assert ok, messages
244: 
245: 
246: def test_evaluation_expected_metrics(tmp_path):
247:     report = {
248:         "run_id": "r4",
249:         "created_at": "now",
250:         "status": "completed",
251:         "input": {"filename": "x", "rows": 1, "cols": 1, "inferred_types": {}},
252:         "lineage": {
253:             "run": {
254:                 "run_id": "r4",
255:                 "created_at": "now",
256:                 "status": "completed",
257:                 "run_seed": 0,
258:             },
259:             "input": {
260:                 "upload_id": "",
261:                 "filename": "x",
262:                 "canonical_path": "",
263:                 "input_hash": "",
264:                 "sha256": "",
265:                 "size_bytes": 0,
266:             },
267:             "dataset": {"dataset_version_id": "dv"},
268:             "raw_format": None,
269:             "template": None,
270:             "plugins": {},
271:         },
272:         "plugins": {
273:             "analysis_queue_delay_decomposition": {
274:                 "status": "ok",
275:                 "summary": "",
276:                 "metrics": {
277:                     "eligible_wait": {"p95": {"value": 120.0, "measurement_type": "measured"}}
278:                 },
279:                 "findings": [],
280:                 "artifacts": [],
281:                 "error": None,
282:             }
283:         },
284:     }
285:     report_path = tmp_path / "report.json"
286:     report_path.write_text(json.dumps(report), encoding="utf-8")
287:     truth = {
288:         "expected_metrics": [
289:             {
290:                 "plugin_id": "analysis_queue_delay_decomposition",
291:                 "metric": "eligible_wait.p95",
292:                 "value": 120.0,
293:                 "tolerance": {"absolute": 0.1, "relative": 0.0},
294:             }
295:         ]
296:     }
297:     import yaml
298:     truth_path = tmp_path / "ground_truth.yaml"
299:     truth_path.write_text(yaml.safe_dump(truth), encoding="utf-8")
300:     ok, messages = evaluate_report(report_path, truth_path)
301:     assert ok, messages
````

## File: tests/test_ingest_tabular.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.ingest_tabular.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_ingest_tabular_csv(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_linear.csv")
 9:     ctx = make_context(
10:         run_dir, df, {"input_file": "tests/fixtures/synth_linear.csv"}, populate=False
11:     )
12:     result = Plugin().run(ctx)
13:     assert result.status == "ok"
14:     version = ctx.storage.get_dataset_version(ctx.dataset_version_id)
15:     assert version
16:     assert int(version["row_count"]) == len(df)
17: 
18: 
19: def test_ingest_tabular_xlsx(run_dir, tmp_path):
20:     df = pd.DataFrame({"a": [1, 2], "b": [3, 4]})
21:     xlsx_path = tmp_path / "data.xlsx"
22:     df.to_excel(xlsx_path, index=False)
23:     ctx = make_context(run_dir, df, {"input_file": str(xlsx_path)}, populate=False)
24:     result = Plugin().run(ctx)
25:     assert result.status == "ok"
26:     version = ctx.storage.get_dataset_version(ctx.dataset_version_id)
27:     assert version
28:     assert int(version["row_count"]) == len(df)
29: 
30: 
31: def test_ingest_tabular_json_lines(run_dir, tmp_path):
32:     path = tmp_path / "data.json"
33:     path.write_text(
34:         '{"a": 1, "b": "x"}\n{"a": 2, "b": "y"}\n',
35:         encoding="utf-8",
36:     )
37:     ctx = make_context(run_dir, pd.DataFrame({"a": [1], "b": ["x"]}), {"input_file": str(path)}, populate=False)
38:     result = Plugin().run(ctx)
39:     assert result.status == "ok"
40:     version = ctx.storage.get_dataset_version(ctx.dataset_version_id)
41:     assert version
42:     assert int(version["row_count"]) == 2
43: 
44: 
45: def test_ingest_tabular_weird_headers(run_dir, tmp_path):
46:     df = pd.DataFrame(
47:         {
48:             "select": [1, 2],
49:             "bad name": [3, 4],
50:             "a;b": [5, 6],
51:         }
52:     )
53:     path = tmp_path / "weird.csv"
54:     df.to_csv(path, index=False)
55:     ctx = make_context(run_dir, df, {"input_file": str(path)}, populate=False)
56:     result = Plugin().run(ctx)
57:     assert result.status == "ok"
58:     columns = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
59:     assert [col["safe_name"] for col in columns] == ["c1", "c2", "c3"]
````

## File: tests/test_llm_prompt_builder.py
````python
 1: import json
 2: 
 3: from plugins.llm_prompt_builder.plugin import Plugin
 4: import pandas as pd
 5: 
 6: from tests.conftest import make_context
 7: 
 8: 
 9: def test_llm_prompt_builder(run_dir):
10:     pii_value = "user@example.com"
11:     report = {
12:         "run_id": "test",
13:         "created_at": "now",
14:         "status": "completed",
15:         "input": {"filename": "x", "rows": 1, "cols": 1, "inferred_types": {}},
16:         "lineage": {
17:             "run": {
18:                 "run_id": "test",
19:                 "created_at": "now",
20:                 "status": "completed",
21:                 "run_seed": 0,
22:             },
23:             "input": {
24:                 "upload_id": "",
25:                 "filename": "x",
26:                 "canonical_path": "",
27:                 "input_hash": "",
28:                 "sha256": "",
29:                 "size_bytes": 0,
30:             },
31:             "dataset": {"dataset_version_id": "dv"},
32:             "raw_format": None,
33:             "template": None,
34:             "plugins": {},
35:         },
36:         "plugins": {
37:             "profile_basic": {
38:                 "status": "ok",
39:                 "summary": "Profiled",
40:                 "metrics": {},
41:                 "findings": [{"kind": "pii_sample", "value": pii_value}],
42:                 "artifacts": [],
43:                 "budget": {
44:                     "row_limit": None,
45:                     "sampled": False,
46:                     "time_limit_ms": None,
47:                     "cpu_limit_ms": None,
48:                 },
49:                 "error": None,
50:             }
51:         },
52:     }
53:     (run_dir / "report.json").write_text(json.dumps(report), encoding="utf-8")
54:     df = pd.DataFrame({"a": [1]})
55:     ctx = make_context(run_dir, df, {})
56:     ctx.storage.upsert_pii_entities(ctx.tenant_id or "default", "email", [pii_value])
57:     result = Plugin().run(ctx)
58:     assert result.status == "ok"
59:     assert any(a.path.endswith("prompt.md") for a in result.artifacts)
60:     prompt_path = next(
61:         a.path for a in result.artifacts if a.path.endswith("prompt.md")
62:     )
63:     prompt_text = (run_dir / prompt_path).read_text(encoding="utf-8")
64:     assert pii_value not in prompt_text
65:     assert "pii:email:" in prompt_text
````

## File: tests/test_pipeline_integration.py
````python
 1: from pathlib import Path
 2: 
 3: from statistic_harness.core.pipeline import Pipeline
 4: 
 5: 
 6: def test_pipeline_integration(tmp_path, monkeypatch):
 7:     appdata = tmp_path / "appdata"
 8:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
 9:     pipeline = Pipeline(appdata, Path("plugins"))
10:     run_id = pipeline.run(
11:         Path("tests/fixtures/synth_linear.csv"), ["profile_basic"], {}, 42
12:     )
13:     run_dir = appdata / "runs" / run_id
14:     assert (run_dir / "report.json").exists()
15:     assert (run_dir / "report.md").exists()
````

## File: tests/test_plugin_discovery.py
````python
 1: from pathlib import Path
 2: 
 3: from statistic_harness.core.plugin_manager import PluginManager
 4: 
 5: 
 6: def test_plugin_discovery():
 7:     manager = PluginManager(Path("plugins"))
 8:     specs = manager.discover()
 9:     ids = {spec.plugin_id for spec in specs}
10:     expected = {
11:         "ingest_tabular",
12:         "profile_basic",
13:         "profile_eventlog",
14:         "planner_basic",
15:         "transform_template",
16:         "transform_normalize_mixed",
17:         "analysis_conformal_feature_prediction",
18:         "analysis_online_conformal_changepoint",
19:         "analysis_gaussian_knockoffs",
20:         "analysis_knockoff_wrapper_rf",
21:         "analysis_notears_linear",
22:         "analysis_bocpd_gaussian",
23:         "analysis_scan_statistics",
24:         "analysis_graph_topology_curves",
25:         "analysis_dp_gmm",
26:         "analysis_gaussian_copula_shift",
27:         "analysis_close_cycle_contention",
28:         "analysis_process_sequence",
29:         "report_bundle",
30:         "llm_prompt_builder",
31:     }
32:     assert expected.issubset(ids)
33:     for spec in specs:
34:         assert spec.config_schema.exists()
35:         assert spec.output_schema.exists()
36:         assert spec.sandbox
````

## File: tests/test_profile_basic.py
````python
 1: import pandas as pd
 2: 
 3: from plugins.profile_basic.plugin import Plugin
 4: from tests.conftest import make_context
 5: 
 6: 
 7: def test_profile_basic(run_dir):
 8:     df = pd.read_csv("tests/fixtures/synth_linear.csv")
 9:     ctx = make_context(run_dir, df, {})
10:     result = Plugin().run(ctx)
11:     assert result.status == "ok"
12:     assert any(a.path.endswith("columns.json") for a in result.artifacts)
13: 
14: 
15: def test_profile_basic_pii_tags(run_dir):
16:     df = pd.DataFrame(
17:         {
18:             "email": ["user@example.com", "admin@example.com"],
19:             "value": [1, 2],
20:         }
21:     )
22:     ctx = make_context(run_dir, df, {})
23:     result = Plugin().run(ctx)
24:     assert result.status == "ok"
25:     columns = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
26:     email_col = next(col for col in columns if col["original_name"] == "email")
27:     assert "email" in (email_col.get("pii_tags") or [])
28:     pii_entities = ctx.storage.fetch_pii_entities(ctx.tenant_id or "default")
29:     raw_values = {entry["raw_value"] for entry in pii_entities}
30:     assert "user@example.com" in raw_values
````

## File: tests/test_quorum_evaluation.py
````python
 1: from pathlib import Path
 2: 
 3: from statistic_harness.core.evaluation import evaluate_report
 4: from statistic_harness.core.pipeline import Pipeline
 5: 
 6: 
 7: def test_quorum_close_cycle_evaluation(tmp_path, monkeypatch):
 8:     appdata = tmp_path / "appdata"
 9:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
10:     pipeline = Pipeline(appdata, Path("plugins"))
11:     run_id = pipeline.run(
12:         Path("tests/fixtures/quorum_close_cycle.csv"),
13:         [
14:             "analysis_close_cycle_contention",
15:             "analysis_queue_delay_decomposition",
16:             "analysis_tail_isolation",
17:             "analysis_percentile_analysis",
18:             "analysis_attribution",
19:             "analysis_dependency_resolution_join",
20:             "analysis_sequence_classification",
21:             "analysis_process_sequence",
22:             "analysis_chain_makespan",
23:             "analysis_concurrency_reconstruction",
24:             "analysis_capacity_scaling",
25:             "analysis_close_cycle_capacity_model",
26:             "analysis_determinism_discipline",
27:         ],
28:         {},
29:         123,
30:     )
31:     report_path = appdata / "runs" / run_id / "report.json"
32:     ok, messages = evaluate_report(
33:         report_path, Path("tests/fixtures/ground_truth_quorum.yaml")
34:     )
35:     assert ok, messages
````

## File: tests/test_report_schema.py
````python
 1: import json
 2: from pathlib import Path
 3: 
 4: from jsonschema import validate
 5: 
 6: from statistic_harness.core.pipeline import Pipeline
 7: 
 8: 
 9: def test_report_schema(tmp_path, monkeypatch):
10:     appdata = tmp_path / "appdata"
11:     monkeypatch.setenv("STAT_HARNESS_APPDATA", str(appdata))
12:     pipeline = Pipeline(appdata, Path("plugins"))
13:     run_id = pipeline.run(
14:         Path("tests/fixtures/synth_linear.csv"), ["profile_basic"], {}, 42
15:     )
16:     report_path = appdata / "runs" / run_id / "report.json"
17:     schema = json.loads(Path("docs/report.schema.json").read_text(encoding="utf-8"))
18:     report = json.loads(report_path.read_text(encoding="utf-8"))
19:     validate(instance=report, schema=schema)
````

## File: tests/test_security_paths.py
````python
 1: import pytest
 2: 
 3: from statistic_harness.core.utils import safe_join
 4: 
 5: 
 6: def test_safe_join_blocks_traversal(tmp_path):
 7:     base = tmp_path / "base"
 8:     base.mkdir()
 9:     with pytest.raises(ValueError):
10:         safe_join(base, "..", "secret.txt")
11: 
12: 
13: def test_safe_join_blocks_prefix_traversal(tmp_path):
14:     base = tmp_path / "base"
15:     base.mkdir()
16:     with pytest.raises(ValueError):
17:         safe_join(base, "..", "base_evil", "secret.txt")
````

## File: AGENTS.md
````markdown
 1: # Statistic Harness — Codex / Agent Instructions
 2: 
 3: ## Non-negotiables (do not ship if violated)
 4: - Do not ship unless: `python -m pytest -q` passes.
 5: - Phase 1 is local-only: NO network calls at runtime (UI is local web only).
 6: - All pipeline steps are plugins/modules:
 7:   - ingest (file parsing)
 8:   - profile/validate
 9:   - analysis (statistical techniques)
10:   - report (human + machine output)
11:   - llm (OFFLINE prompt builder only)
12: - Everything stored is self-contained:
13:   - SQLite (state + results) + filesystem artifacts under `./appdata/`
14:   - `./appdata/` must be gitignored.
15: - Outputs must be BOTH:
16:   - human readable: `report.md`
17:   - machine readable: `report.json` that validates against `docs/report.schema.json`
18: - Determinism:
19:   - Every run has a `run_seed`
20:   - All randomness uses a per-run RNG seeded from `run_seed`
21:   - Tests must set `run_seed` deterministically.
22: - Security:
23:   - Disallow path traversal in downloads and artifact serving
24:   - No `pickle` for untrusted data
25:   - Validate file types and sizes
26:   - Avoid `eval` and shelling out during analysis.
27: 
28: ## Testing requirements
29: - Unit tests per plugin (synthetic fixtures).
30: - Integration test runs the full pipeline and asserts report outputs exist and validate.
31: - Add an evaluator harness:
32:   - takes a `ground_truth.yaml` describing known hidden attributes
33:   - asserts they appear in `report.json` within configured tolerances
34: - If any test fails: do not ship.
35: 
36: ## Style
37: - Python 3.11+.
38: - `src/` layout.
39: - Type hints and clear docstrings.
40: - Fail closed:
41:   - plugin errors do not crash the pipeline
42:   - pipeline still generates a report that includes error summaries.
````

## File: LICENSE
````
 1: MIT License
 2: 
 3: Copyright (c) 2025
 4: 
 5: Permission is hereby granted, free of charge, to any person obtaining a copy
 6: of this software and associated documentation files (the "Software"), to deal
 7: in the Software without restriction, including without limitation the rights
 8: to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 9: copies of the Software, and to permit persons to whom the Software is
10: furnished to do so, subject to the following conditions:
11: 
12: The above copyright notice and this permission notice shall be included in all
13: copies or substantial portions of the Software.
14: 
15: THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
16: IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
17: FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
18: AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
19: LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
20: OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
21: SOFTWARE.
````

## File: pyproject.toml
````toml
 1: [build-system]
 2: requires = ["setuptools>=68", "wheel"]
 3: build-backend = "setuptools.build_meta"
 4: 
 5: [project]
 6: name = "statistic-harness"
 7: version = "0.1.0"
 8: description = "Local-only statistic harness"
 9: readme = "README.md"
10: requires-python = ">=3.11"
11: license = {text = "MIT"}
12: authors = [{name = "ninjra"}]
13: dependencies = [
14:   "fastapi",
15:   "uvicorn",
16:   "jinja2",
17:   "python-multipart",
18:   "pydantic",
19:   "pyyaml",
20:   "numpy",
21:   "pandas",
22:   "openpyxl",
23:   "scikit-learn",
24:   "jsonschema",
25: ]
26: 
27: [project.optional-dependencies]
28: dev = [
29:   "pytest",
30:   "ruff",
31:   "mypy",
32:   "playwright",
33: ]
34: 
35: [project.scripts]
36: stat-harness = "statistic_harness.cli:main"
37: 
38: [tool.setuptools.packages.find]
39: where = ["src"]
````

## File: README.md
````markdown
 1: # Statistic Harness
 2: 
 3: Local-only, plugin-first statistical analysis harness.
 4: 
 5: ## Quickstart
 6: 
 7: WSL/Windows friendly setup (includes dev deps):
 8: 
 9: PowerShell:
10: 
11: ```powershell
12: python -m venv .venv
13: .\.venv\Scripts\Activate.ps1
14: .\scripts\install_dev.ps1
15: stat-harness list-plugins
16: ```
17: 
18: ```bash
19: python -m venv .venv
20: . .venv/bin/activate
21: ./scripts/install_dev.sh
22: stat-harness list-plugins
23: ```
24: 
25: Or, from WSL, just run:
26: 
27: ```bash
28: make dev
29: ```
30: 
31: ## Development
32: 
33: Run tests:
34: 
35: ```bash
36: python -m pytest -q
37: ```
38: 
39: ## Feature Flags / Env
40: 
41: Default behavior is local-only. Optional features are guarded by env flags:
42: 
43: - `STAT_HARNESS_ENABLE_AUTH=1` enables UI/API auth (sessions + API keys).
44: - `STAT_HARNESS_ENABLE_TENANCY=1` enables tenant-aware isolation.
45: - `STAT_HARNESS_ENABLE_VECTOR_STORE=1` enables the sqlite-vec vector store.
46: - `STAT_HARNESS_SQLITE_VEC_PATH=/path/to/vec0.so` loads sqlite-vec if not builtin.
47: - `STAT_HARNESS_MAX_UPLOAD_BYTES` enforces upload size limits.
48: 
49: PII handling: `profile_basic` tags likely PII columns and stores hashed entities.
50: Any LLM/offsystem payloads generated by the prompt builder are anonymized using
51: the PII entity hash table.
````

## File: docs/report.schema.json
````json
  1: {
  2:   "$schema": "http://json-schema.org/draft-07/schema#",
  3:   "title": "Statistic Harness Report",
  4:   "type": "object",
  5:   "required": ["run_id", "created_at", "status", "input", "plugins", "lineage"],
  6:   "properties": {
  7:     "run_id": {"type": "string"},
  8:     "created_at": {"type": "string"},
  9:     "status": {"type": "string"},
 10:     "lineage": {
 11:       "type": "object",
 12:       "required": ["run", "input", "dataset", "plugins"],
 13:       "properties": {
 14:         "run": {
 15:           "type": "object",
 16:           "required": ["run_id", "created_at", "status", "run_seed"],
 17:           "properties": {
 18:             "run_id": {"type": "string"},
 19:             "created_at": {"type": "string"},
 20:             "status": {"type": "string"},
 21:             "run_seed": {"type": "integer"}
 22:           }
 23:         },
 24:         "input": {
 25:           "type": "object",
 26:           "properties": {
 27:             "upload_id": {"type": "string"},
 28:             "filename": {"type": "string"},
 29:             "canonical_path": {"type": "string"},
 30:             "input_hash": {"type": "string"},
 31:             "sha256": {"type": "string"},
 32:             "size_bytes": {"type": "integer"}
 33:           }
 34:         },
 35:         "dataset": {
 36:           "type": "object",
 37:           "required": ["dataset_version_id"],
 38:           "properties": {
 39:             "project_id": {"type": "string"},
 40:             "dataset_id": {"type": "string"},
 41:             "dataset_version_id": {"type": "string"},
 42:             "table_name": {"type": "string"},
 43:             "data_hash": {"type": "string"},
 44:             "row_count": {"type": "integer"},
 45:             "column_count": {"type": "integer"},
 46:             "raw_format_id": {"type": "integer"}
 47:           }
 48:         },
 49:         "raw_format": {
 50:           "anyOf": [
 51:             {"type": "null"},
 52:             {
 53:               "type": "object",
 54:               "properties": {
 55:                 "format_id": {"type": "integer"},
 56:                 "fingerprint": {"type": "string"},
 57:                 "name": {"type": "string"},
 58:                 "created_at": {"type": "string"}
 59:               }
 60:             }
 61:           ]
 62:         },
 63:         "template": {
 64:           "anyOf": [
 65:             {"type": "null"},
 66:             {
 67:               "type": "object",
 68:               "properties": {
 69:                 "template_id": {"type": "integer"},
 70:                 "template_name": {"type": "string"},
 71:                 "template_version": {"type": "string"},
 72:                 "table_name": {"type": "string"},
 73:                 "status": {"type": "string"},
 74:                 "mapping_hash": {"type": "string"},
 75:                 "mapping": {"type": "object"}
 76:               }
 77:             }
 78:           ]
 79:         },
 80:         "plugins": {
 81:           "type": "object",
 82:           "additionalProperties": {
 83:             "type": "object",
 84:             "properties": {
 85:               "plugin_version": {"type": "string"},
 86:               "code_hash": {"type": "string"},
 87:               "settings_hash": {"type": "string"},
 88:               "dataset_hash": {"type": "string"},
 89:               "executed_at": {"type": "string"},
 90:               "status": {"type": "string"},
 91:               "summary": {"type": "string"}
 92:             }
 93:           }
 94:         }
 95:       }
 96:     },
 97:     "input": {
 98:       "type": "object",
 99:       "required": ["filename", "rows", "cols", "inferred_types"],
100:       "properties": {
101:         "filename": {"type": "string"},
102:         "rows": {"type": "integer"},
103:         "cols": {"type": "integer"},
104:         "inferred_types": {"type": "object"}
105:       }
106:     },
107:     "plugins": {
108:       "type": "object",
109:       "additionalProperties": {
110:         "type": "object",
111:         "required": ["status", "summary", "metrics", "findings", "artifacts", "budget", "error"],
112:         "properties": {
113:           "status": {"type": "string"},
114:           "summary": {"type": "string"},
115:           "metrics": {"type": "object"},
116:           "budget": {
117:             "type": "object",
118:             "required": ["row_limit", "sampled", "time_limit_ms", "cpu_limit_ms"],
119:             "properties": {
120:               "row_limit": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
121:               "sampled": {"type": "boolean"},
122:               "time_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
123:               "cpu_limit_ms": {"anyOf": [{"type": "integer"}, {"type": "null"}]},
124:               "notes": {"anyOf": [{"type": "string"}, {"type": "null"}]}
125:             },
126:             "additionalProperties": true
127:           },
128:           "findings": {
129:             "type": "array",
130:             "items": {
131:               "type": "object",
132:               "required": ["evidence", "measurement_type"],
133:               "properties": {
134:                 "evidence": {
135:                   "type": "object",
136:                   "required": [
137:                     "dataset_id",
138:                     "dataset_version_id",
139:                     "row_ids",
140:                     "column_ids",
141:                     "query"
142:                   ],
143:                   "properties": {
144:                     "dataset_id": {"type": "string"},
145:                     "dataset_version_id": {"type": "string"},
146:                     "row_ids": {"type": "array", "items": {"type": "integer"}},
147:                     "column_ids": {"type": "array", "items": {"type": "integer"}},
148:                     "query": {"anyOf": [{"type": "string"}, {"type": "null"}]}
149:                   },
150:                   "additionalProperties": true
151:                 }
152:                 ,
153:                 "measurement_type": {
154:                   "type": "string",
155:                   "enum": ["measured", "modeled", "not_applicable", "error"]
156:                 }
157:               },
158:               "additionalProperties": true
159:             }
160:           },
161:           "artifacts": {
162:             "type": "array",
163:             "items": {
164:               "type": "object",
165:               "required": ["path", "type", "description"],
166:               "properties": {
167:                 "path": {"type": "string"},
168:                 "type": {"type": "string"},
169:                 "description": {"type": "string"}
170:               }
171:             }
172:           },
173:           "error": {
174:             "anyOf": [
175:               {"type": "null"},
176:               {
177:                 "type": "object",
178:                 "required": ["type", "message", "traceback"],
179:                 "properties": {
180:                   "type": {"type": "string"},
181:                   "message": {"type": "string"},
182:                   "traceback": {"type": "string"}
183:                 }
184:               }
185:             ]
186:           }
187:         }
188:       }
189:     },
190:     "known_issues": {
191:       "type": "object",
192:       "properties": {
193:         "scope_type": {"type": "string"},
194:         "scope_value": {"type": "string"},
195:         "strict": {"type": "boolean"},
196:         "notes": {"type": "string"},
197:         "natural_language": {"type": "array"},
198:         "expected_findings": {"type": "array"}
199:       },
200:       "additionalProperties": true
201:     },
202:     "evaluation": {
203:       "type": "object",
204:       "properties": {
205:         "evaluated_at": {"type": "string"},
206:         "result": {"type": "string"},
207:         "ok": {"type": "boolean"},
208:         "messages": {"type": "array"}
209:       },
210:       "additionalProperties": true
211:     },
212:     "recommendations": {
213:       "type": "object",
214:       "properties": {
215:         "status": {"type": "string"},
216:         "summary": {"type": "string"},
217:         "items": {
218:           "type": "array",
219:           "items": {"type": "object"}
220:         }
221:       },
222:       "additionalProperties": true
223:     }
224:   }
225: }
````

## File: plugins/analysis_attribution/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.column_inference import choose_timestamp_column
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: INVALID_STRINGS = {"", "nan", "none", "null"}
 13: 
 14: 
 15: def _pick_column(
 16:     preferred: str | None,
 17:     columns: list[str],
 18:     role_by_name: dict[str, str],
 19:     roles: set[str],
 20:     patterns: list[str],
 21:     lower_names: dict[str, str],
 22: ) -> str | None:
 23:     if preferred and preferred in columns:
 24:         return preferred
 25:     for col in columns:
 26:         if role_by_name.get(col) in roles:
 27:             return col
 28:     for col in columns:
 29:         name = lower_names[col]
 30:         if any(pattern in name for pattern in patterns):
 31:             return col
 32:     return None
 33: 
 34: 
 35: def _pick_timestamp_column(
 36:     preferred: str | None,
 37:     columns: list[str],
 38:     role_by_name: dict[str, str],
 39:     roles: set[str],
 40:     patterns: list[str],
 41:     lower_names: dict[str, str],
 42:     df: pd.DataFrame,
 43: ) -> str | None:
 44:     candidates: list[str] = []
 45:     if preferred and preferred in columns:
 46:         candidates.append(preferred)
 47:     for col in columns:
 48:         if role_by_name.get(col) in roles and col not in candidates:
 49:             candidates.append(col)
 50:     for col in columns:
 51:         name = lower_names[col]
 52:         if any(pattern in name for pattern in patterns) and col not in candidates:
 53:             candidates.append(col)
 54:     if not candidates:
 55:         return None
 56:     return choose_timestamp_column(df, candidates)
 57: 
 58: 
 59: def _normalize_text(value: Any) -> str:
 60:     if value is None:
 61:         return ""
 62:     if isinstance(value, float) and pd.isna(value):
 63:         return ""
 64:     return str(value).strip()
 65: 
 66: 
 67: def _series_from_frame(frame: pd.DataFrame, column: str | None) -> pd.Series | None:
 68:     if not column:
 69:         return None
 70:     data = frame[column]
 71:     if isinstance(data, pd.DataFrame):
 72:         return data.iloc[:, 0]
 73:     return data
 74: 
 75: 
 76: class Plugin:
 77:     def run(self, ctx) -> PluginResult:
 78:         df = ctx.dataset_loader()
 79:         if df.empty:
 80:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 81: 
 82:         columns_meta = []
 83:         role_by_name: dict[str, str] = {}
 84:         if ctx.dataset_version_id:
 85:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 86:             if dataset_template and dataset_template.get("status") == "ready":
 87:                 fields = ctx.storage.fetch_template_fields(
 88:                     int(dataset_template["template_id"])
 89:                 )
 90:                 columns_meta = fields
 91:                 role_by_name = {
 92:                     field["name"]: (field.get("role") or "") for field in fields
 93:                 }
 94:             else:
 95:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 96:                 role_by_name = {
 97:                     col["original_name"]: (col.get("role") or "")
 98:                     for col in columns_meta
 99:                 }
100: 
101:         columns = list(df.columns)
102:         lower_names = {col: str(col).lower() for col in columns}
103: 
104:         process_col = _pick_column(
105:             ctx.settings.get("process_column"),
106:             columns,
107:             role_by_name,
108:             {"process_name", "process_id"},
109:             ["process", "activity", "event", "step", "task", "action", "job"],
110:             lower_names,
111:         )
112:         module_col = _pick_column(
113:             ctx.settings.get("module_column"),
114:             columns,
115:             role_by_name,
116:             {"module_code"},
117:             ["module", "mod"],
118:             lower_names,
119:         )
120:         user_col = _pick_column(
121:             ctx.settings.get("user_column"),
122:             columns,
123:             role_by_name,
124:             {"user_id"},
125:             ["user", "owner", "operator"],
126:             lower_names,
127:         )
128:         queue_col = _pick_timestamp_column(
129:             ctx.settings.get("queue_column"),
130:             columns,
131:             role_by_name,
132:             {"queue_time"},
133:             ["queue", "queued", "enqueue"],
134:             lower_names,
135:             df,
136:         )
137:         eligible_col = _pick_timestamp_column(
138:             ctx.settings.get("eligible_column"),
139:             columns,
140:             role_by_name,
141:             {"eligible", "ready", "available"},
142:             ["eligible", "ready", "available"],
143:             lower_names,
144:             df,
145:         )
146:         start_col = _pick_timestamp_column(
147:             ctx.settings.get("start_column"),
148:             columns,
149:             role_by_name,
150:             {"start_time", "start"},
151:             ["start", "begin"],
152:             lower_names,
153:             df,
154:         )
155: 
156:         if not start_col or (not queue_col and not eligible_col):
157:             return PluginResult(
158:                 "ok",
159:                 "Attribution not applicable",
160:                 {"total_rows": 0},
161:                 [
162:                     {
163:                         "kind": "attribution",
164:                         "measurement_type": "not_applicable",
165:                         "reason": "Missing queue/eligible and start columns.",
166:                         "columns": [
167:                             col
168:                             for col in [queue_col, eligible_col, start_col]
169:                             if col
170:                         ],
171:                     }
172:                 ],
173:                 [],
174:                 None,
175:             )
176: 
177:         selected = [
178:             col
179:             for col in [
180:                 process_col,
181:                 module_col,
182:                 user_col,
183:                 queue_col,
184:                 eligible_col,
185:                 start_col,
186:             ]
187:             if col
188:         ]
189:         selected = list(dict.fromkeys(selected))
190:         work = df.loc[:, selected].copy()
191: 
192:         eligible_series = _series_from_frame(work, eligible_col)
193:         eligible_ts = (
194:             pd.to_datetime(eligible_series, errors="coerce", utc=False)
195:             if eligible_series is not None
196:             else None
197:         )
198:         if eligible_ts is None:
199:             queue_series = _series_from_frame(work, queue_col)
200:             eligible_ts = pd.to_datetime(queue_series, errors="coerce", utc=False)
201:             eligible_basis = "queue"
202:         else:
203:             eligible_basis = "eligible"
204:         start_series = _series_from_frame(work, start_col)
205:         start_ts = pd.to_datetime(start_series, errors="coerce", utc=False)
206:         work["__eligible_ts"] = eligible_ts
207:         work["__start_ts"] = start_ts
208:         work = work.loc[work["__eligible_ts"].notna() & work["__start_ts"].notna()].copy()
209:         if work.empty:
210:             return PluginResult("ok", "No valid timestamps", {"total_rows": 0}, [], [], None)
211: 
212:         wait_sec = (work["__start_ts"] - work["__eligible_ts"]).dt.total_seconds()
213:         wait_sec = wait_sec.clip(lower=0).fillna(0)
214:         work["__eligible_wait_sec"] = wait_sec
215: 
216:         threshold = float(ctx.settings.get("wait_threshold_seconds", 60))
217:         work["__tail_wait_sec"] = work["__eligible_wait_sec"].where(
218:             work["__eligible_wait_sec"] > threshold, 0.0
219:         )
220: 
221:         max_groups = int(ctx.settings.get("max_groups", 5))
222:         max_examples = int(ctx.settings.get("max_examples", 25))
223: 
224:         findings = []
225: 
226:         def _emit_dimension(name: str, col: str | None) -> None:
227:             if not col or col not in work.columns:
228:                 return
229:             temp = work.copy()
230:             temp["__key"] = temp[col].map(_normalize_text)
231:             temp = temp.loc[~temp["__key"].str.lower().isin(INVALID_STRINGS)].copy()
232:             if temp.empty:
233:                 return
234:             grouped = (
235:                 temp.groupby("__key")
236:                 .agg(
237:                     runs=("__key", "size"),
238:                     wait_sec=("__eligible_wait_sec", "sum"),
239:                     tail_wait_sec=("__tail_wait_sec", "sum"),
240:                 )
241:                 .reset_index()
242:             )
243:             grouped = grouped.sort_values(
244:                 ["wait_sec", "__key"], ascending=[False, True]
245:             )
246:             for _, row in grouped.head(max_groups).iterrows():
247:                 key = row["__key"]
248:                 row_ids = temp.loc[temp["__key"] == key].index.tolist()[:max_examples]
249:                 findings.append(
250:                     {
251:                         "kind": "attribution",
252:                         "dimension": name,
253:                         "key": key,
254:                         "runs": int(row["runs"]),
255:                         "eligible_wait_hours": float(row["wait_sec"]) / 3600.0,
256:                         "tail_wait_hours": float(row["tail_wait_sec"]) / 3600.0,
257:                         "threshold_seconds": threshold,
258:                         "eligible_basis": eligible_basis,
259:                         "measurement_type": "measured",
260:                         "row_ids": [int(i) for i in row_ids],
261:                         "columns": [
262:                             col
263:                             for col in [process_col, module_col, user_col]
264:                             if col
265:                         ],
266:                     }
267:                 )
268: 
269:         _emit_dimension("process", process_col)
270:         _emit_dimension("module", module_col)
271:         _emit_dimension("user", user_col)
272: 
273:         artifacts_dir = ctx.artifacts_dir("analysis_attribution")
274:         out_path = artifacts_dir / "attribution.json"
275:         write_json(
276:             out_path,
277:             {
278:                 "summary": {
279:                     "process_column": process_col,
280:                     "module_column": module_col,
281:                     "user_column": user_col,
282:                     "queue_column": queue_col,
283:                     "eligible_column": eligible_col,
284:                     "start_column": start_col,
285:                     "eligible_basis": eligible_basis,
286:                     "threshold_seconds": threshold,
287:                 },
288:                 "total_rows": int(work.shape[0]),
289:             },
290:         )
291:         artifacts = [
292:             PluginArtifact(
293:                 path=str(out_path.relative_to(ctx.run_dir)),
294:                 type="json",
295:                 description="Attribution summary",
296:             )
297:         ]
298: 
299:         return PluginResult(
300:             "ok",
301:             "Attributed eligible wait by dimension",
302:             {"total_rows": int(work.shape[0])},
303:             findings,
304:             artifacts,
305:             None,
306:         )
````

## File: plugins/analysis_close_cycle_capacity_impact/plugin.py
````python
   1: from __future__ import annotations
   2: 
   3: import math
   4: from collections import Counter, defaultdict
   5: from datetime import date, datetime, timedelta
   6: from typing import Any, Iterable
   7: 
   8: import numpy as np
   9: import pandas as pd
  10: 
  11: from statistic_harness.core.types import PluginArtifact, PluginResult
  12: from statistic_harness.core.utils import write_json
  13: 
  14: 
  15: INVALID_STRINGS = {"", "nan", "none", "null"}
  16: 
  17: 
  18: def _normalize_text(value: Any) -> str:
  19:     if value is None:
  20:         return ""
  21:     if isinstance(value, float) and pd.isna(value):
  22:         return ""
  23:     return str(value).strip()
  24: 
  25: 
  26: def _pick_column(
  27:     preferred: str | None,
  28:     columns: list[str],
  29:     role_by_name: dict[str, str],
  30:     roles: set[str],
  31:     patterns: list[str],
  32:     lower_names: dict[str, str],
  33:     exclude: set[str],
  34: ) -> str | None:
  35:     if preferred and preferred in columns:
  36:         return preferred
  37:     for col in columns:
  38:         if col in exclude:
  39:             continue
  40:         if role_by_name.get(col) in roles:
  41:             return col
  42:     for col in columns:
  43:         if col in exclude:
  44:             continue
  45:         name = lower_names[col]
  46:         if any(pattern in name for pattern in patterns):
  47:             return col
  48:     return None
  49: 
  50: 
  51: def _candidate_columns(
  52:     columns: list[str],
  53:     role_by_name: dict[str, str],
  54:     roles: set[str],
  55:     patterns: list[str],
  56:     lower_names: dict[str, str],
  57:     exclude: set[str],
  58: ) -> list[str]:
  59:     candidates: list[str] = []
  60:     for col in columns:
  61:         if col in exclude:
  62:             continue
  63:         if role_by_name.get(col) in roles:
  64:             candidates.append(col)
  65:     for col in columns:
  66:         if col in exclude or col in candidates:
  67:             continue
  68:         name = lower_names[col]
  69:         if any(pattern in name for pattern in patterns):
  70:             candidates.append(col)
  71:     return candidates
  72: 
  73: 
  74: def _score_process_column(name: str, series: pd.Series) -> float:
  75:     score = 0.0
  76:     lower_name = name.lower()
  77:     if lower_name in {"process", "process_id"}:
  78:         score += 3.0
  79:     if lower_name.endswith("_id") or lower_name.endswith("id"):
  80:         score += 1.5
  81:     for token in (
  82:         "queue",
  83:         "status",
  84:         "step",
  85:         "parent",
  86:         "child",
  87:         "hold",
  88:         "lock",
  89:         "schedule",
  90:         "master",
  91:         "dep",
  92:         "ext",
  93:         "attempt",
  94:         "priority",
  95:     ):
  96:         if token in lower_name:
  97:             score -= 2.0
  98: 
  99:     sample = series.dropna()
 100:     if sample.empty:
 101:         return score - 5.0
 102:     if sample.shape[0] > 5000:
 103:         sample = sample.sample(5000, random_state=0)
 104: 
 105:     if pd.api.types.is_numeric_dtype(sample):
 106:         score -= 1.5
 107:     else:
 108:         score += 1.5
 109: 
 110:     sample_str = sample.astype(str).str.strip()
 111:     if not pd.api.types.is_numeric_dtype(sample):
 112:         numeric_like = sample_str.str.match(r"^\d+(\.\d+)?$").mean()
 113:         if numeric_like > 0.8:
 114:             score -= 2.0
 115: 
 116:     unique_ratio = sample.nunique(dropna=True) / max(1, sample.shape[0])
 117:     score += (1.0 - unique_ratio) * 4.0
 118:     if unique_ratio > 0.9:
 119:         score -= 2.0
 120: 
 121:     lengths = sample_str.str.len()
 122:     median_len = float(lengths.median()) if not lengths.empty else 0.0
 123:     if 3 <= median_len <= 20:
 124:         score += 0.5
 125:     elif median_len > 40:
 126:         score -= 0.5
 127: 
 128:     return score
 129: 
 130: 
 131: def _choose_best_process_column(
 132:     candidates: Iterable[str], df: pd.DataFrame
 133: ) -> str | None:
 134:     candidates = list(candidates)
 135:     if not candidates:
 136:         return None
 137:     if len(candidates) == 1:
 138:         return candidates[0]
 139:     scored = []
 140:     for col in candidates:
 141:         scored.append((_score_process_column(str(col), df[col]), col))
 142:     scored.sort(reverse=True, key=lambda item: item[0])
 143:     return scored[0][1]
 144: 
 145: 
 146: def _score_host_column(name: str, series: pd.Series) -> float:
 147:     score = 0.0
 148:     lower_name = name.lower()
 149:     for token in ("host", "server", "node", "instance", "machine"):
 150:         if token in lower_name:
 151:             score += 2.0
 152:     if "process" in lower_name:
 153:         score -= 3.0
 154:     sample = series.dropna()
 155:     if sample.empty:
 156:         return score - 5.0
 157:     if sample.shape[0] > 5000:
 158:         sample = sample.sample(5000, random_state=0)
 159: 
 160:     if pd.api.types.is_numeric_dtype(sample):
 161:         score -= 2.0
 162:     else:
 163:         score += 1.5
 164: 
 165:     sample_str = sample.astype(str).str.strip()
 166:     numeric_like = sample_str.str.match(r"^\d+(\.\d+)?$").mean()
 167:     if numeric_like > 0.8:
 168:         score -= 1.5
 169: 
 170:     unique_ratio = sample.nunique(dropna=True) / max(1, sample.shape[0])
 171:     if unique_ratio < 0.01:
 172:         score += 3.0
 173:     elif unique_ratio < 0.1:
 174:         score += 1.0
 175:     elif unique_ratio > 0.5:
 176:         score -= 1.5
 177: 
 178:     return score
 179: 
 180: 
 181: def _choose_best_host_column(
 182:     candidates: Iterable[str], df: pd.DataFrame
 183: ) -> str | None:
 184:     candidates = list(candidates)
 185:     if not candidates:
 186:         return None
 187:     if len(candidates) == 1:
 188:         return candidates[0]
 189:     scored = []
 190:     for col in candidates:
 191:         scored.append((_score_host_column(str(col), df[col]), col))
 192:     scored.sort(reverse=True, key=lambda item: item[0])
 193:     return scored[0][1]
 194: 
 195: 
 196: def _score_datetime_column(name: str, series: pd.Series, tokens: Iterable[str]) -> float:
 197:     score = 0.0
 198:     lower_name = name.lower()
 199:     if any(token in lower_name for token in tokens):
 200:         score += 2.0
 201: 
 202:     sample = series.dropna()
 203:     if sample.empty:
 204:         return score - 5.0
 205:     if sample.shape[0] > 2000:
 206:         sample = sample.sample(2000, random_state=0)
 207: 
 208:     if pd.api.types.is_numeric_dtype(sample):
 209:         max_val = float(sample.max()) if not sample.empty else 0.0
 210:         if max_val < 1e8:
 211:             score -= 5.0
 212: 
 213:     parsed = pd.to_datetime(sample, errors="coerce", utc=False)
 214:     success = float(parsed.notna().mean())
 215:     score += success * 5.0
 216:     return score
 217: 
 218: 
 219: def _choose_best_datetime_column(
 220:     candidates: Iterable[str], df: pd.DataFrame, tokens: Iterable[str]
 221: ) -> str | None:
 222:     candidates = list(candidates)
 223:     if not candidates:
 224:         return None
 225:     if len(candidates) == 1:
 226:         return candidates[0]
 227:     scored = []
 228:     for col in candidates:
 229:         scored.append((_score_datetime_column(str(col), df[col], tokens), col))
 230:     scored.sort(reverse=True, key=lambda item: item[0])
 231:     best_score, best_col = scored[0]
 232:     if best_score <= 0:
 233:         return None
 234:     return best_col
 235: 
 236: 
 237: def _bucket_floor(ts: pd.Series, bucket_size: str) -> pd.Series:
 238:     if bucket_size == "hour":
 239:         return ts.dt.floor("h")
 240:     return ts.dt.floor("D")
 241: 
 242: 
 243: def _max_concurrent_hosts(
 244:     frame: pd.DataFrame,
 245:     bucket_start: pd.Timestamp,
 246:     bucket_end: pd.Timestamp,
 247: ) -> int:
 248:     events: list[tuple[pd.Timestamp, int]] = []
 249:     grouped = frame.groupby("__host_norm", sort=False)
 250:     for _, host_frame in grouped:
 251:         intervals = host_frame[["__start_ts", "__end_ts"]].sort_values("__start_ts")
 252:         current_start: pd.Timestamp | None = None
 253:         current_end: pd.Timestamp | None = None
 254:         for start_ts, end_ts in intervals.itertuples(index=False, name=None):
 255:             if start_ts is None or end_ts is None:
 256:                 continue
 257:             start_ts = max(start_ts, bucket_start)
 258:             end_ts = min(end_ts, bucket_end)
 259:             if end_ts < start_ts:
 260:                 continue
 261:             if current_start is None:
 262:                 current_start = start_ts
 263:                 current_end = end_ts
 264:                 continue
 265:             if start_ts <= current_end:
 266:                 current_end = max(current_end, end_ts)
 267:             else:
 268:                 events.append((current_start, 1))
 269:                 events.append((current_end, -1))
 270:                 current_start = start_ts
 271:                 current_end = end_ts
 272:         if current_start is not None and current_end is not None:
 273:             events.append((current_start, 1))
 274:             events.append((current_end, -1))
 275: 
 276:     if not events:
 277:         return 0
 278:     events.sort(key=lambda item: (item[0], item[1]))
 279:     current = 0
 280:     max_conc = 0
 281:     for _, delta in events:
 282:         current += delta
 283:         if current > max_conc:
 284:             max_conc = current
 285:     return max_conc
 286: 
 287: 
 288: def _js_divergence(left: dict[str, float], right: dict[str, float]) -> float:
 289:     keys = set(left) | set(right)
 290:     if not keys:
 291:         return 0.0
 292:     eps = 1e-12
 293:     p = []
 294:     q = []
 295:     for key in keys:
 296:         p.append(left.get(key, 0.0) + eps)
 297:         q.append(right.get(key, 0.0) + eps)
 298:     p_sum = sum(p)
 299:     q_sum = sum(q)
 300:     p = [val / p_sum for val in p]
 301:     q = [val / q_sum for val in q]
 302:     m = [(a + b) * 0.5 for a, b in zip(p, q)]
 303: 
 304:     def _kl(a: list[float], b: list[float]) -> float:
 305:         total = 0.0
 306:         for ai, bi in zip(a, b):
 307:             if ai <= 0 or bi <= 0:
 308:                 continue
 309:             total += ai * math.log(ai / bi)
 310:         return total
 311: 
 312:     return 0.5 * _kl(p, m) + 0.5 * _kl(q, m)
 313: 
 314: 
 315: def _bootstrap_ci(
 316:     rng: np.random.Generator,
 317:     hi_values: np.ndarray,
 318:     lo_values: np.ndarray,
 319:     samples: int,
 320:     alpha: float,
 321: ) -> tuple[float | None, float | None]:
 322:     if hi_values.size == 0 or lo_values.size == 0:
 323:         return None, None
 324:     effects = []
 325:     for _ in range(samples):
 326:         hi_sample = rng.choice(hi_values, size=hi_values.size, replace=True)
 327:         lo_sample = rng.choice(lo_values, size=lo_values.size, replace=True)
 328:         lo_med = float(np.median(lo_sample))
 329:         hi_med = float(np.median(hi_sample))
 330:         if lo_med <= 0:
 331:             continue
 332:         effects.append((hi_med / lo_med) - 1.0)
 333:     if not effects:
 334:         return None, None
 335:     effects = np.asarray(effects)
 336:     low = float(np.quantile(effects, alpha / 2.0))
 337:     high = float(np.quantile(effects, 1.0 - alpha / 2.0))
 338:     return low, high
 339: 
 340: 
 341: def _month_key(day: date) -> str:
 342:     return f"{day.year:04d}-{day.month:02d}"
 343: 
 344: 
 345: def _build_calendar_days(start_day: date, end_day: date) -> list[date]:
 346:     days: list[date] = []
 347:     cursor = start_day
 348:     while cursor <= end_day:
 349:         days.append(cursor)
 350:         cursor = cursor + timedelta(days=1)
 351:     return days
 352: 
 353: 
 354: def _calendar_close_window(
 355:     daily: pd.DataFrame, close_start_day: int, close_end_day: int, mode: str
 356: ) -> tuple[set[date], list[dict[str, Any]], set[str]]:
 357:     close_dates: set[date] = set()
 358:     for day in daily["date"].tolist():
 359:         if close_start_day <= close_end_day:
 360:             is_close = close_start_day <= day.day <= close_end_day
 361:         else:
 362:             is_close = day.day >= close_start_day or day.day <= close_end_day
 363:         if is_close:
 364:             close_dates.add(day)
 365:     close_windows = [
 366:         {
 367:             "mode": mode,
 368:             "start_day": close_start_day,
 369:             "end_day": close_end_day,
 370:         }
 371:     ]
 372:     confident_months = {day.strftime("%Y-%m") for day in close_dates}
 373:     return close_dates, close_windows, confident_months
 374: 
 375: 
 376: def _infer_close_windows(
 377:     daily: pd.DataFrame,
 378:     min_days: int,
 379:     max_days: int,
 380:     lookahead_days: int,
 381:     min_confidence: float,
 382:     min_data_ratio: float,
 383: ) -> tuple[set[date], list[dict[str, Any]], set[str]]:
 384:     if daily.empty:
 385:         return set(), [], set()
 386: 
 387:     daily = daily.copy()
 388:     daily["month"] = daily["date"].apply(_month_key)
 389:     daily_map = {
 390:         row["date"]: {
 391:             "count": int(row["count"]),
 392:             "median": float(row["median_ttc"]) if row["median_ttc"] is not None else None,
 393:         }
 394:         for row in daily.to_dict("records")
 395:     }
 396: 
 397:     months = sorted(daily["month"].unique())
 398:     close_windows: list[dict[str, Any]] = []
 399:     close_dates: dict[date, str] = {}
 400:     confident_months: set[str] = set()
 401: 
 402:     for idx, month in enumerate(months):
 403:         year, month_num = [int(part) for part in month.split("-")]
 404:         month_start = date(year, month_num, 1)
 405:         if month_num == 12:
 406:             next_month_start = date(year + 1, 1, 1)
 407:         else:
 408:             next_month_start = date(year, month_num + 1, 1)
 409:         month_end = next_month_start - timedelta(days=1)
 410: 
 411:         month_days = _build_calendar_days(month_start, month_end)
 412:         next_days = _build_calendar_days(next_month_start, next_month_start + timedelta(days=max(0, lookahead_days - 1)))
 413:         extended_days = month_days + next_days
 414: 
 415:         counts = [daily_map.get(day, {}).get("count", 0) for day in month_days]
 416:         medians = [daily_map.get(day, {}).get("median") for day in month_days]
 417:         medians_clean = [val for val in medians if val is not None]
 418:         count_mean = float(np.mean(counts)) if counts else 0.0
 419:         count_std = float(np.std(counts)) if counts else 0.0
 420:         median_mean = float(np.mean(medians_clean)) if medians_clean else 0.0
 421:         median_std = float(np.std(medians_clean)) if medians_clean else 0.0
 422: 
 423:         def _z(val: float, mean: float, std: float) -> float:
 424:             if std <= 0:
 425:                 return 0.0
 426:             return (val - mean) / std
 427: 
 428:         pressures = []
 429:         for day in extended_days:
 430:             record = daily_map.get(day)
 431:             count_val = record.get("count", 0) if record else 0
 432:             median_val = record.get("median") if record else None
 433:             z_count = _z(float(count_val), count_mean, count_std)
 434:             z_median = 0.0
 435:             if median_val is not None:
 436:                 z_median = _z(float(median_val), median_mean, median_std)
 437:             pressures.append(z_count + z_median)
 438: 
 439:         best_score = None
 440:         best_window = None
 441:         second_score = None
 442: 
 443:         for length in range(min_days, max_days + 1):
 444:             if length <= 0 or length > len(extended_days):
 445:                 continue
 446:             for start_idx in range(len(month_days)):
 447:                 end_idx = start_idx + length
 448:                 if end_idx > len(extended_days):
 449:                     continue
 450:                 score = float(sum(pressures[start_idx:end_idx]))
 451:                 if best_score is None or score > best_score:
 452:                     second_score = best_score
 453:                     best_score = score
 454:                     best_window = (start_idx, end_idx)
 455:                 elif second_score is None or score > second_score:
 456:                     second_score = score
 457: 
 458:         confidence = 0.0
 459:         if best_score is not None and best_score > 0:
 460:             if second_score is None:
 461:                 confidence = 1.0
 462:             else:
 463:                 confidence = (best_score - second_score) / abs(best_score)
 464: 
 465:         if best_window is None or confidence < min_confidence:
 466:             close_windows.append(
 467:                 {
 468:                     "month": month,
 469:                     "start": None,
 470:                     "end": None,
 471:                     "length": 0,
 472:                     "confidence": float(confidence),
 473:                     "mode": "infer",
 474:                 }
 475:             )
 476:             continue
 477: 
 478:         start_idx, end_idx = best_window
 479:         window_days = extended_days[start_idx:end_idx]
 480:         data_days = [day for day in window_days if daily_map.get(day, {}).get("count", 0) > 0]
 481:         data_ratio = len(data_days) / max(1, len(window_days))
 482:         if data_ratio < min_data_ratio:
 483:             close_windows.append(
 484:                 {
 485:                     "month": month,
 486:                     "start": None,
 487:                     "end": None,
 488:                     "length": len(window_days),
 489:                     "confidence": float(confidence),
 490:                     "mode": "infer",
 491:                     "reason": "insufficient_data_days",
 492:                 }
 493:             )
 494:             continue
 495: 
 496:         start_day = window_days[0]
 497:         end_day = window_days[-1]
 498:         close_windows.append(
 499:             {
 500:                 "month": month,
 501:                 "start": start_day.isoformat(),
 502:                 "end": end_day.isoformat(),
 503:                 "length": len(window_days),
 504:                 "confidence": float(confidence),
 505:                 "mode": "infer",
 506:             }
 507:         )
 508:         confident_months.add(month)
 509: 
 510:         for day in window_days:
 511:             close_dates.setdefault(day, month)
 512: 
 513:     close_date_set = set(close_dates.keys())
 514:     return close_date_set, close_windows, confident_months
 515: 
 516: 
 517: class Plugin:
 518:     def run(self, ctx) -> PluginResult:
 519:         df = ctx.dataset_loader()
 520:         if df.empty:
 521:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 522: 
 523:         columns_meta = []
 524:         role_by_name: dict[str, str] = {}
 525:         if ctx.dataset_version_id:
 526:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 527:             if dataset_template and dataset_template.get("status") == "ready":
 528:                 fields = ctx.storage.fetch_template_fields(
 529:                     int(dataset_template["template_id"])
 530:                 )
 531:                 columns_meta = fields
 532:                 role_by_name = {
 533:                     field["name"]: (field.get("role") or "") for field in fields
 534:                 }
 535:             else:
 536:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 537:                 role_by_name = {
 538:                     col["original_name"]: (col.get("role") or "")
 539:                     for col in columns_meta
 540:                 }
 541: 
 542:         columns = list(df.columns)
 543:         lower_names = {col: str(col).lower() for col in columns}
 544:         used: set[str] = set()
 545: 
 546:         process_candidates = _candidate_columns(
 547:             columns,
 548:             role_by_name,
 549:             {"process", "activity", "event", "step", "task", "action"},
 550:             ["process", "activity", "event", "step", "task", "action", "job"],
 551:             lower_names,
 552:             used,
 553:         )
 554:         process_col = _choose_best_process_column(process_candidates, df)
 555:         if process_col:
 556:             used.add(process_col)
 557: 
 558:         host_candidates = _candidate_columns(
 559:             columns,
 560:             role_by_name,
 561:             {"server", "host", "node", "instance"},
 562:             ["server", "host", "node", "instance", "machine"],
 563:             lower_names,
 564:             used,
 565:         )
 566:         preferred_host = ctx.settings.get("host_column")
 567:         if preferred_host and preferred_host in columns:
 568:             host_col = preferred_host
 569:         else:
 570:             host_col = _choose_best_host_column(host_candidates, df)
 571:         if host_col:
 572:             used.add(host_col)
 573: 
 574:         start_candidates = _candidate_columns(
 575:             columns,
 576:             role_by_name,
 577:             {"start_time", "start"},
 578:             ["start", "begin"],
 579:             lower_names,
 580:             used,
 581:         )
 582:         preferred_start = ctx.settings.get("start_column")
 583:         if preferred_start and preferred_start in columns:
 584:             start_col = preferred_start
 585:         else:
 586:             start_col = _choose_best_datetime_column(
 587:                 start_candidates, df, ("start", "begin")
 588:             )
 589:         if start_col:
 590:             used.add(start_col)
 591: 
 592:         end_candidates = _candidate_columns(
 593:             columns,
 594:             role_by_name,
 595:             {"end_time", "end", "finish", "complete"},
 596:             ["end", "finish", "complete", "stop"],
 597:             lower_names,
 598:             used,
 599:         )
 600:         preferred_end = ctx.settings.get("end_column")
 601:         if preferred_end and preferred_end in columns:
 602:             end_col = preferred_end
 603:         else:
 604:             end_col = _choose_best_datetime_column(
 605:                 end_candidates, df, ("end", "finish", "complete", "stop")
 606:             )
 607:         if end_col:
 608:             used.add(end_col)
 609: 
 610:         queue_col = _pick_column(
 611:             ctx.settings.get("queue_column"),
 612:             columns,
 613:             role_by_name,
 614:             {"queue", "queued", "enqueue"},
 615:             ["queue", "queued", "enqueue"],
 616:             lower_names,
 617:             used,
 618:         )
 619:         if queue_col:
 620:             used.add(queue_col)
 621: 
 622:         eligible_col = _pick_column(
 623:             ctx.settings.get("eligible_column"),
 624:             columns,
 625:             role_by_name,
 626:             {"eligible", "ready", "available"},
 627:             ["eligible", "ready", "available"],
 628:             lower_names,
 629:             used,
 630:         )
 631:         if eligible_col:
 632:             used.add(eligible_col)
 633: 
 634:         summary = {
 635:             "process_column": process_col,
 636:             "host_column": host_col,
 637:             "start_column": start_col,
 638:             "end_column": end_col,
 639:             "queue_column": queue_col,
 640:             "eligible_column": eligible_col,
 641:         }
 642: 
 643:         def _emit_not_applicable(reason: str) -> PluginResult:
 644:             findings = []
 645:             for metric in ("concurrent", "unique"):
 646:                 findings.append(
 647:                     {
 648:                         "kind": "close_cycle_capacity_impact",
 649:                         "host_metric": metric,
 650:                         "decision": "not_applicable",
 651:                         "measurement_type": "not_applicable",
 652:                         "reason": reason,
 653:                         "columns": [
 654:                             col
 655:                             for col in [
 656:                                 process_col,
 657:                                 host_col,
 658:                                 start_col,
 659:                                 end_col,
 660:                                 queue_col,
 661:                                 eligible_col,
 662:                             ]
 663:                             if col
 664:                         ],
 665:                     }
 666:                 )
 667:             artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_capacity_impact")
 668:             out_path = artifacts_dir / "results.json"
 669:             write_json(out_path, {"summary": summary, "findings": findings})
 670:             artifacts = [
 671:                 PluginArtifact(
 672:                     path=str(out_path.relative_to(ctx.run_dir)),
 673:                     type="json",
 674:                     description="Close-cycle capacity impact summary",
 675:                 )
 676:             ]
 677:             csv_path = artifacts_dir / "results.csv"
 678:             with csv_path.open("w", encoding="utf-8", newline="") as handle:
 679:                 header = [
 680:                     "host_metric",
 681:                     "decision",
 682:                     "effect",
 683:                     "ci_low",
 684:                     "ci_high",
 685:                     "target_reduction",
 686:                     "tolerance",
 687:                     "hi_bucket_count",
 688:                     "lo_bucket_count",
 689:                     "median_rows_hi",
 690:                     "median_rows_lo",
 691:                     "volume_ratio",
 692:                     "process_mix_js",
 693:                     "reason",
 694:                 ]
 695:                 handle.write(",".join(header) + "\n")
 696:                 for item in findings:
 697:                     handle.write(
 698:                         ",".join(
 699:                             [
 700:                                 str(item.get("host_metric")),
 701:                                 str(item.get("decision")),
 702:                                 str(item.get("effect")),
 703:                                 str(item.get("ci_low")),
 704:                                 str(item.get("ci_high")),
 705:                                 str(item.get("target_reduction")),
 706:                                 str(item.get("tolerance")),
 707:                                 str(item.get("hi_bucket_count")),
 708:                                 str(item.get("lo_bucket_count")),
 709:                                 str(item.get("median_rows_hi")),
 710:                                 str(item.get("median_rows_lo")),
 711:                                 str(item.get("volume_ratio")),
 712:                                 str(item.get("process_mix_js")),
 713:                                 str(item.get("reason")),
 714:                             ]
 715:                         )
 716:                         + "\n"
 717:                     )
 718:             artifacts.append(
 719:                 PluginArtifact(
 720:                     path=str(csv_path.relative_to(ctx.run_dir)),
 721:                     type="csv",
 722:                     description="Capacity impact detail table",
 723:                 )
 724:             )
 725: 
 726:             md_path = artifacts_dir / "results.md"
 727:             lines = [
 728:                 "# Close-cycle capacity impact",
 729:                 "",
 730:                 "Summary:",
 731:                 f"- close_window_mode: {summary.get('close_window_mode')}",
 732:                 f"- close_cycle_start_day: {summary.get('close_cycle_start_day')}",
 733:                 f"- close_cycle_end_day: {summary.get('close_cycle_end_day')}",
 734:                 "",
 735:                 "Detected:",
 736:                 "_None_",
 737:                 "",
 738:                 "Not applicable:",
 739:             ]
 740:             for item in findings:
 741:                 lines.append(f"- {item.get('host_metric')}: {item.get('reason')}")
 742:             md_path.write_text("\n".join(lines), encoding="utf-8")
 743:             artifacts.append(
 744:                 PluginArtifact(
 745:                     path=str(md_path.relative_to(ctx.run_dir)),
 746:                     type="markdown",
 747:                     description="Capacity impact summary",
 748:                 )
 749:             )
 750:             return PluginResult(
 751:                 "ok",
 752:                 reason,
 753:                 summary,
 754:                 findings,
 755:                 artifacts,
 756:                 None,
 757:             )
 758: 
 759:         if not start_col or not end_col:
 760:             return _emit_not_applicable("missing_start_end")
 761:         if not host_col:
 762:             return _emit_not_applicable("missing_host_column")
 763: 
 764:         selected_cols = [
 765:             col
 766:             for col in [process_col, host_col, start_col, end_col, queue_col, eligible_col]
 767:             if col
 768:         ]
 769:         work = df.loc[:, selected_cols].copy()
 770: 
 771:         work["__start_ts"] = pd.to_datetime(work[start_col], errors="coerce", utc=False)
 772:         work["__end_ts"] = pd.to_datetime(work[end_col], errors="coerce", utc=False)
 773:         work = work.loc[work["__start_ts"].notna() & work["__end_ts"].notna()].copy()
 774:         if work.empty:
 775:             return _emit_not_applicable("no_valid_timestamps")
 776: 
 777:         work = work.loc[work["__end_ts"] >= work["__start_ts"]].copy()
 778:         if work.empty:
 779:             return _emit_not_applicable("no_non_negative_durations")
 780: 
 781:         work["__ttc_sec"] = (work["__end_ts"] - work["__start_ts"]).dt.total_seconds()
 782:         work = work.loc[work["__ttc_sec"] > 0].copy()
 783:         if work.empty:
 784:             return _emit_not_applicable("no_positive_ttc")
 785: 
 786:         work["__host"] = work[host_col].map(_normalize_text)
 787:         work["__host_norm"] = work["__host"].str.lower()
 788:         work = work.loc[~work["__host_norm"].isin(INVALID_STRINGS)].copy()
 789:         if work.empty:
 790:             return _emit_not_applicable("no_valid_host_values")
 791: 
 792:         if process_col:
 793:             work["__process"] = work[process_col].map(_normalize_text)
 794:             work["__process_norm"] = work["__process"].str.lower()
 795: 
 796:         if queue_col and queue_col in work.columns:
 797:             work["__queue_ts"] = pd.to_datetime(work[queue_col], errors="coerce", utc=False)
 798:             work["__queue_to_end_sec"] = (
 799:                 work["__end_ts"] - work["__queue_ts"]
 800:             ).dt.total_seconds().clip(lower=0)
 801:         else:
 802:             work["__queue_to_end_sec"] = np.nan
 803: 
 804:         if eligible_col and eligible_col in work.columns:
 805:             work["__eligible_ts"] = pd.to_datetime(work[eligible_col], errors="coerce", utc=False)
 806:             work["__eligible_to_end_sec"] = (
 807:                 work["__end_ts"] - work["__eligible_ts"]
 808:             ).dt.total_seconds().clip(lower=0)
 809:         else:
 810:             work["__eligible_to_end_sec"] = np.nan
 811: 
 812:         close_mode = str(ctx.settings.get("close_window_mode", "infer") or "infer").lower()
 813:         if close_mode == "calendar":
 814:             close_mode = "override"
 815:         close_start_day = int(ctx.settings.get("close_cycle_start_day", 20))
 816:         close_end_day = int(ctx.settings.get("close_cycle_end_day", 5))
 817:         min_close_days = int(ctx.settings.get("min_close_days", 5))
 818:         max_close_days = int(ctx.settings.get("max_close_days", 20))
 819:         lookahead_days = int(ctx.settings.get("lookahead_days", 7))
 820:         min_close_confidence = float(ctx.settings.get("min_close_confidence", 0.15))
 821:         min_close_data_ratio = float(ctx.settings.get("min_close_data_ratio", 0.5))
 822: 
 823:         work["__date"] = work["__start_ts"].dt.date
 824:         daily = (
 825:             work.groupby("__date")
 826:             .agg(count=("__date", "size"), median_ttc=("__ttc_sec", "median"))
 827:             .reset_index()
 828:             .rename(columns={"__date": "date"})
 829:         )
 830: 
 831:         close_dates: set[date] = set()
 832:         close_windows: list[dict[str, Any]] = []
 833:         confident_months: set[str] = set()
 834:         fallback_used = False
 835:         fallback_reason: str | None = None
 836: 
 837:         if close_mode == "override":
 838:             close_dates, close_windows, confident_months = _calendar_close_window(
 839:                 daily, close_start_day, close_end_day, "override"
 840:             )
 841:         elif close_mode == "infer_or_default":
 842:             close_dates, close_windows, confident_months = _infer_close_windows(
 843:                 daily,
 844:                 min_close_days,
 845:                 max_close_days,
 846:                 lookahead_days,
 847:                 min_close_confidence,
 848:                 min_close_data_ratio,
 849:             )
 850:             if not close_dates:
 851:                 fallback_used = True
 852:                 fallback_reason = "calendar_default"
 853:                 close_dates, close_windows, confident_months = _calendar_close_window(
 854:                     daily, close_start_day, close_end_day, "fallback"
 855:                 )
 856:         else:
 857:             close_dates, close_windows, confident_months = _infer_close_windows(
 858:                 daily,
 859:                 min_close_days,
 860:                 max_close_days,
 861:                 lookahead_days,
 862:                 min_close_confidence,
 863:                 min_close_data_ratio,
 864:             )
 865: 
 866:         summary.update(
 867:             {
 868:                 "close_window_mode": close_mode,
 869:                 "close_cycle_start_day": close_start_day,
 870:                 "close_cycle_end_day": close_end_day,
 871:                 "min_close_days": min_close_days,
 872:                 "max_close_days": max_close_days,
 873:                 "lookahead_days": lookahead_days,
 874:                 "min_close_confidence": min_close_confidence,
 875:                 "min_close_data_ratio": min_close_data_ratio,
 876:                 "close_windows": close_windows,
 877:                 "close_window_fallback": fallback_used,
 878:                 "close_window_fallback_reason": fallback_reason,
 879:             }
 880:         )
 881: 
 882:         if not close_dates:
 883:             return _emit_not_applicable("close_window_not_inferred")
 884: 
 885:         close_window_source = (
 886:             "fallback"
 887:             if fallback_used
 888:             else ("override" if close_mode == "override" else "infer")
 889:         )
 890: 
 891:         bucket_size = str(ctx.settings.get("bucket_size", "day") or "day").lower()
 892:         min_bucket_rows = int(ctx.settings.get("min_bucket_rows", 50))
 893: 
 894:         work["__bucket_start"] = _bucket_floor(work["__start_ts"], bucket_size)
 895:         if bucket_size == "hour":
 896:             bucket_delta = timedelta(hours=1)
 897:         else:
 898:             bucket_delta = timedelta(days=1)
 899: 
 900:         buckets = []
 901:         grouped = work.groupby("__bucket_start", sort=True)
 902:         for bucket_start, frame in grouped:
 903:             if pd.isna(bucket_start):
 904:                 continue
 905:             bucket_start = pd.Timestamp(bucket_start)
 906:             bucket_end = bucket_start + bucket_delta
 907:             bucket_date = bucket_start.date()
 908:             if bucket_date not in close_dates:
 909:                 close_flag = False
 910:             else:
 911:                 close_flag = True
 912:             row_count = int(frame.shape[0])
 913:             if row_count < min_bucket_rows:
 914:                 continue
 915:             median_ttc = float(frame["__ttc_sec"].median())
 916:             median_qe = float(frame["__queue_to_end_sec"].median()) if frame["__queue_to_end_sec"].notna().any() else None
 917:             median_ee = float(frame["__eligible_to_end_sec"].median()) if frame["__eligible_to_end_sec"].notna().any() else None
 918:             host_unique = int(frame["__host_norm"].nunique())
 919:             host_concurrent = _max_concurrent_hosts(frame, bucket_start, bucket_end)
 920:             buckets.append(
 921:                 {
 922:                     "bucket_start": bucket_start,
 923:                     "bucket_end": bucket_end,
 924:                     "bucket_date": bucket_date,
 925:                     "month": _month_key(bucket_date),
 926:                     "close": close_flag,
 927:                     "rows": row_count,
 928:                     "median_ttc": median_ttc,
 929:                     "median_qe": median_qe,
 930:                     "median_ee": median_ee,
 931:                     "host_unique": host_unique,
 932:                     "host_concurrent": host_concurrent,
 933:                 }
 934:             )
 935: 
 936:         if not buckets:
 937:             return _emit_not_applicable("no_buckets_after_filtering")
 938: 
 939:         bucket_df = pd.DataFrame(buckets)
 940:         close_buckets = bucket_df.loc[bucket_df["close"]].copy()
 941:         close_buckets = close_buckets.loc[close_buckets["month"].isin(confident_months)].copy()
 942:         if close_buckets.empty:
 943:             return _emit_not_applicable("no_close_buckets")
 944: 
 945:         min_buckets_per_group = int(ctx.settings.get("min_buckets_per_group", 5))
 946:         min_buckets_per_month = int(ctx.settings.get("min_buckets_per_month", 3))
 947:         min_months = int(ctx.settings.get("min_months", 2))
 948:         target_reduction = float(ctx.settings.get("target_reduction", 0.30))
 949:         tolerance = float(ctx.settings.get("tolerance", 0.05))
 950:         alpha = float(ctx.settings.get("alpha", 0.01))
 951:         bootstrap_samples = int(ctx.settings.get("bootstrap_samples", 1000))
 952:         max_js = float(ctx.settings.get("max_js_divergence", 0.2))
 953:         min_volume_ratio = float(ctx.settings.get("min_volume_ratio", 0.5))
 954:         max_volume_ratio = float(ctx.settings.get("max_volume_ratio", 2.0))
 955:         max_examples = int(ctx.settings.get("max_examples", 25))
 956: 
 957:         findings = []
 958: 
 959:         for metric in ("concurrent", "unique"):
 960:             if metric == "concurrent":
 961:                 metric_col = "host_concurrent"
 962:             else:
 963:                 metric_col = "host_unique"
 964: 
 965:             hi = close_buckets.loc[close_buckets[metric_col] >= 3].copy()
 966:             lo = close_buckets.loc[close_buckets[metric_col] <= 2].copy()
 967: 
 968:             reasons: list[str] = []
 969:             decision = "not_applicable"
 970: 
 971:             if hi.shape[0] < min_buckets_per_group or lo.shape[0] < min_buckets_per_group:
 972:                 reasons.append("insufficient_buckets")
 973: 
 974:             if process_col is None:
 975:                 reasons.append("missing_process_column")
 976: 
 977:             # Process mix divergence
 978:             js_div = None
 979:             if process_col and hi.shape[0] >= 1 and lo.shape[0] >= 1:
 980:                 hi_bucket_keys = set(hi["bucket_start"].tolist())
 981:                 lo_bucket_keys = set(lo["bucket_start"].tolist())
 982:                 hi_rows = work.loc[work["__bucket_start"].isin(hi_bucket_keys)]
 983:                 lo_rows = work.loc[work["__bucket_start"].isin(lo_bucket_keys)]
 984:                 if hi_rows.empty or lo_rows.empty:
 985:                     reasons.append("insufficient_rows_for_mix")
 986:                 else:
 987:                     hi_counts = (
 988:                         hi_rows["__process_norm"].value_counts(normalize=True).to_dict()
 989:                         if "__process_norm" in hi_rows
 990:                         else {}
 991:                     )
 992:                     lo_counts = (
 993:                         lo_rows["__process_norm"].value_counts(normalize=True).to_dict()
 994:                         if "__process_norm" in lo_rows
 995:                         else {}
 996:                     )
 997:                     js_div = _js_divergence(hi_counts, lo_counts)
 998:                     if js_div > max_js:
 999:                         reasons.append("process_mix_divergence")
1000: 
1001:             # Volume parity guard
1002:             median_rows_hi = float(hi["rows"].median()) if not hi.empty else 0.0
1003:             median_rows_lo = float(lo["rows"].median()) if not lo.empty else 0.0
1004:             volume_ratio = None
1005:             if median_rows_lo > 0:
1006:                 volume_ratio = median_rows_hi / median_rows_lo
1007:                 if volume_ratio < min_volume_ratio or volume_ratio > max_volume_ratio:
1008:                     reasons.append("volume_parity")
1009:             else:
1010:                 reasons.append("volume_parity")
1011: 
1012:             # Time trend guard (per month)
1013:             month_effects = []
1014:             for month, month_frame in close_buckets.groupby("month"):
1015:                 hi_month = month_frame.loc[month_frame[metric_col] >= 3]
1016:                 lo_month = month_frame.loc[month_frame[metric_col] <= 2]
1017:                 if (
1018:                     hi_month.shape[0] < min_buckets_per_month
1019:                     or lo_month.shape[0] < min_buckets_per_month
1020:                 ):
1021:                     continue
1022:                 hi_med = float(hi_month["median_ttc"].median())
1023:                 lo_med = float(lo_month["median_ttc"].median())
1024:                 if lo_med <= 0:
1025:                     continue
1026:                 month_effects.append((hi_med / lo_med) - 1.0)
1027: 
1028:             if len(month_effects) < min_months:
1029:                 reasons.append("insufficient_months")
1030:             else:
1031:                 if any(effect >= 0 for effect in month_effects):
1032:                     reasons.append("month_effect_sign")
1033: 
1034:             # Effect + CI
1035:             effect = None
1036:             ci_low = None
1037:             ci_high = None
1038:             if hi.shape[0] > 0 and lo.shape[0] > 0:
1039:                 hi_vals = hi["median_ttc"].to_numpy(dtype=float)
1040:                 lo_vals = lo["median_ttc"].to_numpy(dtype=float)
1041:                 lo_med = float(np.median(lo_vals))
1042:                 hi_med = float(np.median(hi_vals))
1043:                 if lo_med > 0:
1044:                     effect = (hi_med / lo_med) - 1.0
1045:                 rng = np.random.default_rng(ctx.run_seed ^ (hash(metric_col) & 0xFFFFFFFF))
1046:                 ci_low, ci_high = _bootstrap_ci(
1047:                     rng,
1048:                     hi_vals,
1049:                     lo_vals,
1050:                     bootstrap_samples,
1051:                     alpha,
1052:                 )
1053: 
1054:             if effect is None or ci_low is None or ci_high is None:
1055:                 reasons.append("effect_unavailable")
1056:             else:
1057:                 band_low = -(target_reduction + tolerance)
1058:                 band_high = -(target_reduction - tolerance)
1059:                 if not (ci_low >= band_low and ci_high <= band_high):
1060:                     reasons.append("effect_not_within_tolerance")
1061: 
1062:             if not reasons:
1063:                 decision = "detected"
1064: 
1065:             # Secondary diagnostics (queue/eligible to end)
1066:             qe_effect = None
1067:             ee_effect = None
1068:             qe_ci_low = None
1069:             qe_ci_high = None
1070:             ee_ci_low = None
1071:             ee_ci_high = None
1072: 
1073:             if hi.shape[0] > 0 and lo.shape[0] > 0:
1074:                 if hi["median_qe"].notna().any() and lo["median_qe"].notna().any():
1075:                     hi_qe = hi["median_qe"].dropna().to_numpy(dtype=float)
1076:                     lo_qe = lo["median_qe"].dropna().to_numpy(dtype=float)
1077:                     if hi_qe.size and lo_qe.size:
1078:                         lo_med = float(np.median(lo_qe))
1079:                         if lo_med > 0:
1080:                             qe_effect = (float(np.median(hi_qe)) / lo_med) - 1.0
1081:                         rng = np.random.default_rng(ctx.run_seed ^ (hash(metric_col + "qe") & 0xFFFFFFFF))
1082:                         qe_ci_low, qe_ci_high = _bootstrap_ci(rng, hi_qe, lo_qe, bootstrap_samples, alpha)
1083:                 if hi["median_ee"].notna().any() and lo["median_ee"].notna().any():
1084:                     hi_ee = hi["median_ee"].dropna().to_numpy(dtype=float)
1085:                     lo_ee = lo["median_ee"].dropna().to_numpy(dtype=float)
1086:                     if hi_ee.size and lo_ee.size:
1087:                         lo_med = float(np.median(lo_ee))
1088:                         if lo_med > 0:
1089:                             ee_effect = (float(np.median(hi_ee)) / lo_med) - 1.0
1090:                         rng = np.random.default_rng(ctx.run_seed ^ (hash(metric_col + "ee") & 0xFFFFFFFF))
1091:                         ee_ci_low, ee_ci_high = _bootstrap_ci(rng, hi_ee, lo_ee, bootstrap_samples, alpha)
1092: 
1093:             row_ids = work.index.tolist()[:max_examples]
1094: 
1095:             findings.append(
1096:                 {
1097:                     "kind": "close_cycle_capacity_impact",
1098:                     "host_metric": metric,
1099:                     "decision": decision,
1100:                     "measurement_type": "measured" if decision == "detected" else "not_applicable",
1101:                     "reason": ",".join(reasons) if reasons else "ok",
1102:                     "close_window_mode": close_mode,
1103:                     "close_window_fallback": fallback_used,
1104:                     "close_window_source": close_window_source,
1105:                     "close_window_reason": fallback_reason,
1106:                     "effect": effect,
1107:                     "ci_low": ci_low,
1108:                     "ci_high": ci_high,
1109:                     "target_reduction": target_reduction,
1110:                     "tolerance": tolerance,
1111:                     "alpha": alpha,
1112:                     "hi_bucket_count": int(hi.shape[0]),
1113:                     "lo_bucket_count": int(lo.shape[0]),
1114:                     "median_rows_hi": median_rows_hi,
1115:                     "median_rows_lo": median_rows_lo,
1116:                     "volume_ratio": volume_ratio,
1117:                     "process_mix_js": js_div,
1118:                     "month_effects": month_effects,
1119:                     "qe_effect": qe_effect,
1120:                     "qe_ci_low": qe_ci_low,
1121:                     "qe_ci_high": qe_ci_high,
1122:                     "ee_effect": ee_effect,
1123:                     "ee_ci_low": ee_ci_low,
1124:                     "ee_ci_high": ee_ci_high,
1125:                     "columns": [
1126:                         col
1127:                         for col in [
1128:                             process_col,
1129:                             host_col,
1130:                             start_col,
1131:                             end_col,
1132:                             queue_col,
1133:                             eligible_col,
1134:                         ]
1135:                         if col
1136:                     ],
1137:                     "row_ids": [int(i) for i in row_ids],
1138:                 }
1139:             )
1140: 
1141:         artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_capacity_impact")
1142:         out_path = artifacts_dir / "results.json"
1143:         write_json(out_path, {"summary": summary, "findings": findings})
1144:         artifacts = [
1145:             PluginArtifact(
1146:                 path=str(out_path.relative_to(ctx.run_dir)),
1147:                 type="json",
1148:                 description="Close-cycle capacity impact summary",
1149:             )
1150:         ]
1151: 
1152:         # Human-readable outputs
1153:         csv_path = artifacts_dir / "results.csv"
1154:         with csv_path.open("w", encoding="utf-8", newline="") as handle:
1155:             header = [
1156:                 "host_metric",
1157:                 "decision",
1158:                 "effect",
1159:                 "ci_low",
1160:                 "ci_high",
1161:                 "target_reduction",
1162:                 "tolerance",
1163:                 "hi_bucket_count",
1164:                 "lo_bucket_count",
1165:                 "median_rows_hi",
1166:                 "median_rows_lo",
1167:                 "volume_ratio",
1168:                 "process_mix_js",
1169:                 "reason",
1170:             ]
1171:             handle.write(",".join(header) + "\n")
1172:             for item in findings:
1173:                 handle.write(",".join(
1174:                     [
1175:                         str(item.get("host_metric")),
1176:                         str(item.get("decision")),
1177:                         str(item.get("effect")),
1178:                         str(item.get("ci_low")),
1179:                         str(item.get("ci_high")),
1180:                         str(item.get("target_reduction")),
1181:                         str(item.get("tolerance")),
1182:                         str(item.get("hi_bucket_count")),
1183:                         str(item.get("lo_bucket_count")),
1184:                         str(item.get("median_rows_hi")),
1185:                         str(item.get("median_rows_lo")),
1186:                         str(item.get("volume_ratio")),
1187:                         str(item.get("process_mix_js")),
1188:                         str(item.get("reason")),
1189:                     ]
1190:                 ) + "\n")
1191:         artifacts.append(
1192:             PluginArtifact(
1193:                 path=str(csv_path.relative_to(ctx.run_dir)),
1194:                 type="csv",
1195:                 description="Capacity impact detail table",
1196:             )
1197:         )
1198: 
1199:         md_path = artifacts_dir / "results.md"
1200:         lines = [
1201:             "# Close-cycle capacity impact",
1202:             "",
1203:             "Summary:",
1204:             f"- close_window_mode: {summary.get('close_window_mode')}",
1205:             f"- close_cycle_start_day: {summary.get('close_cycle_start_day')}",
1206:             f"- close_cycle_end_day: {summary.get('close_cycle_end_day')}",
1207:             f"- min_close_days: {summary.get('min_close_days')}",
1208:             f"- max_close_days: {summary.get('max_close_days')}",
1209:             f"- min_close_confidence: {summary.get('min_close_confidence')}",
1210:             f"- target_reduction: {target_reduction}",
1211:             f"- tolerance: {tolerance}",
1212:             f"- alpha: {alpha}",
1213:             "",
1214:             "Detected:",
1215:         ]
1216: 
1217:         detected_rows = [
1218:             item for item in findings if item.get("decision") == "detected"
1219:         ]
1220:         if detected_rows:
1221:             lines.append(
1222:                 "| host_metric | effect | ci_low | ci_high | hi_buckets | lo_buckets |"
1223:             )
1224:             lines.append("| --- | --- | --- | --- | --- | --- |")
1225:             for item in detected_rows:
1226:                 lines.append(
1227:                     "| {host_metric} | {effect:.4f} | {ci_low:.4f} | {ci_high:.4f} | {hi_bucket_count} | {lo_bucket_count} |".format(
1228:                         host_metric=item.get("host_metric"),
1229:                         effect=item.get("effect") or 0.0,
1230:                         ci_low=item.get("ci_low") or 0.0,
1231:                         ci_high=item.get("ci_high") or 0.0,
1232:                         hi_bucket_count=item.get("hi_bucket_count"),
1233:                         lo_bucket_count=item.get("lo_bucket_count"),
1234:                     )
1235:                 )
1236:         else:
1237:             lines.append("_None_")
1238: 
1239:         lines.extend(["", "Not applicable:"])
1240:         for item in findings:
1241:             if item.get("decision") != "detected":
1242:                 lines.append(
1243:                     f"- {item.get('host_metric')}: {item.get('reason')}"
1244:                 )
1245: 
1246:         md_path.write_text("\n".join(lines), encoding="utf-8")
1247:         artifacts.append(
1248:             PluginArtifact(
1249:                 path=str(md_path.relative_to(ctx.run_dir)),
1250:                 type="markdown",
1251:                 description="Capacity impact summary",
1252:             )
1253:         )
1254: 
1255:         metrics = {
1256:             "findings": len(findings),
1257:         }
1258:         return PluginResult(
1259:             "ok",
1260:             "Computed close-cycle capacity impact",
1261:             metrics,
1262:             findings,
1263:             artifacts,
1264:             None,
1265:         )
````

## File: plugins/analysis_close_cycle_capacity_model/config.schema.json
````json
 1: {
 2:   "$schema": "http://json-schema.org/draft-07/schema#",
 3:   "title": "Close Cycle Capacity Model Config",
 4:   "type": "object",
 5:   "additionalProperties": false,
 6:   "properties": {
 7:     "process_column": { "type": ["string", "null"] },
 8:     "host_column": { "type": ["string", "null"] },
 9:     "start_column": { "type": ["string", "null"] },
10:     "end_column": { "type": ["string", "null"] },
11:     "queue_column": { "type": ["string", "null"] },
12:     "eligible_column": { "type": ["string", "null"] },
13:     "close_window_mode": { "type": "string", "enum": ["infer", "override", "infer_or_default", "calendar"] },
14:     "close_cycle_start_day": { "type": "integer", "minimum": 1, "maximum": 31 },
15:     "close_cycle_end_day": { "type": "integer", "minimum": 1, "maximum": 31 },
16:     "min_close_days": { "type": "integer", "minimum": 1 },
17:     "max_close_days": { "type": "integer", "minimum": 1 },
18:     "lookahead_days": { "type": "integer", "minimum": 0 },
19:     "min_close_confidence": { "type": "number", "minimum": 0, "maximum": 1 },
20:     "min_close_data_ratio": { "type": "number", "minimum": 0, "maximum": 1 },
21:     "bucket_size": { "type": "string", "enum": ["day", "hour"] },
22:     "min_bucket_rows": { "type": "integer", "minimum": 0 },
23:     "min_buckets_per_group": { "type": "integer", "minimum": 1 },
24:     "min_months": { "type": "integer", "minimum": 1 },
25:     "baseline_host_count": { "type": ["integer", "null"], "minimum": 1 },
26:     "added_hosts": { "type": "integer", "minimum": 1 },
27:     "baseline_match_mode": { "type": "string", "enum": ["exact", "at_most"] },
28:     "target_reduction": { "type": "number", "minimum": 0, "maximum": 1 },
29:     "tolerance": { "type": "number", "minimum": 0, "maximum": 1 },
30:     "max_examples": { "type": "integer", "minimum": 1 }
31:   }
32: }
````

## File: plugins/analysis_close_cycle_capacity_model/plugin.py
````python
   1: from __future__ import annotations
   2: 
   3: from datetime import date, timedelta
   4: from typing import Any, Iterable
   5: 
   6: import numpy as np
   7: import pandas as pd
   8: 
   9: from statistic_harness.core.types import PluginArtifact, PluginResult
  10: from statistic_harness.core.utils import write_json
  11: 
  12: 
  13: INVALID_STRINGS = {"", "nan", "none", "null"}
  14: 
  15: 
  16: def _normalize_text(value: Any) -> str:
  17:     if value is None:
  18:         return ""
  19:     if isinstance(value, float) and pd.isna(value):
  20:         return ""
  21:     return str(value).strip()
  22: 
  23: 
  24: def _pick_column(
  25:     preferred: str | None,
  26:     columns: list[str],
  27:     role_by_name: dict[str, str],
  28:     roles: set[str],
  29:     patterns: list[str],
  30:     lower_names: dict[str, str],
  31:     exclude: set[str],
  32: ) -> str | None:
  33:     if preferred and preferred in columns:
  34:         return preferred
  35:     for col in columns:
  36:         if col in exclude:
  37:             continue
  38:         if role_by_name.get(col) in roles:
  39:             return col
  40:     for col in columns:
  41:         if col in exclude:
  42:             continue
  43:         name = lower_names[col]
  44:         if any(pattern in name for pattern in patterns):
  45:             return col
  46:     return None
  47: 
  48: 
  49: def _candidate_columns(
  50:     columns: list[str],
  51:     role_by_name: dict[str, str],
  52:     roles: set[str],
  53:     patterns: list[str],
  54:     lower_names: dict[str, str],
  55:     exclude: set[str],
  56: ) -> list[str]:
  57:     candidates: list[str] = []
  58:     for col in columns:
  59:         if col in exclude:
  60:             continue
  61:         if role_by_name.get(col) in roles:
  62:             candidates.append(col)
  63:     for col in columns:
  64:         if col in exclude or col in candidates:
  65:             continue
  66:         name = lower_names[col]
  67:         if any(pattern in name for pattern in patterns):
  68:             candidates.append(col)
  69:     return candidates
  70: 
  71: 
  72: def _score_process_column(name: str, series: pd.Series) -> float:
  73:     score = 0.0
  74:     lower_name = name.lower()
  75:     if lower_name in {"process", "process_id"}:
  76:         score += 3.0
  77:     if lower_name.endswith("_id") or lower_name.endswith("id"):
  78:         score += 1.5
  79:     for token in (
  80:         "queue",
  81:         "status",
  82:         "step",
  83:         "parent",
  84:         "child",
  85:         "hold",
  86:         "lock",
  87:         "schedule",
  88:         "master",
  89:         "dep",
  90:         "ext",
  91:         "attempt",
  92:         "priority",
  93:     ):
  94:         if token in lower_name:
  95:             score -= 2.0
  96: 
  97:     sample = series.dropna()
  98:     if sample.empty:
  99:         return score - 5.0
 100:     if sample.shape[0] > 5000:
 101:         sample = sample.sample(5000, random_state=0)
 102: 
 103:     if pd.api.types.is_numeric_dtype(sample):
 104:         score -= 1.5
 105:     else:
 106:         score += 1.5
 107: 
 108:     sample_str = sample.astype(str).str.strip()
 109:     if not pd.api.types.is_numeric_dtype(sample):
 110:         numeric_like = sample_str.str.match(r"^\d+(\.\d+)?$").mean()
 111:         if numeric_like > 0.8:
 112:             score -= 2.0
 113: 
 114:     unique_ratio = sample.nunique(dropna=True) / max(1, sample.shape[0])
 115:     score += (1.0 - unique_ratio) * 4.0
 116:     if unique_ratio > 0.9:
 117:         score -= 2.0
 118: 
 119:     lengths = sample_str.str.len()
 120:     median_len = float(lengths.median()) if not lengths.empty else 0.0
 121:     if 3 <= median_len <= 20:
 122:         score += 0.5
 123:     elif median_len > 40:
 124:         score -= 0.5
 125: 
 126:     return score
 127: 
 128: 
 129: def _choose_best_process_column(
 130:     candidates: Iterable[str], df: pd.DataFrame
 131: ) -> str | None:
 132:     candidates = list(candidates)
 133:     if not candidates:
 134:         return None
 135:     if len(candidates) == 1:
 136:         return candidates[0]
 137:     scored = []
 138:     for col in candidates:
 139:         scored.append((_score_process_column(str(col), df[col]), col))
 140:     scored.sort(reverse=True, key=lambda item: item[0])
 141:     return scored[0][1]
 142: 
 143: 
 144: def _score_host_column(name: str, series: pd.Series) -> float:
 145:     score = 0.0
 146:     lower_name = name.lower()
 147:     for token in ("host", "server", "node", "instance", "machine"):
 148:         if token in lower_name:
 149:             score += 2.0
 150:     if "process" in lower_name:
 151:         score -= 3.0
 152:     sample = series.dropna()
 153:     if sample.empty:
 154:         return score - 5.0
 155:     if sample.shape[0] > 5000:
 156:         sample = sample.sample(5000, random_state=0)
 157: 
 158:     if pd.api.types.is_numeric_dtype(sample):
 159:         score -= 2.0
 160:     else:
 161:         score += 1.5
 162: 
 163:     sample_str = sample.astype(str).str.strip()
 164:     numeric_like = sample_str.str.match(r"^\d+(\.\d+)?$").mean()
 165:     if numeric_like > 0.8:
 166:         score -= 1.5
 167: 
 168:     unique_ratio = sample.nunique(dropna=True) / max(1, sample.shape[0])
 169:     if unique_ratio < 0.01:
 170:         score += 3.0
 171:     elif unique_ratio < 0.1:
 172:         score += 1.0
 173:     elif unique_ratio > 0.5:
 174:         score -= 1.5
 175: 
 176:     return score
 177: 
 178: 
 179: def _choose_best_host_column(
 180:     candidates: Iterable[str], df: pd.DataFrame
 181: ) -> str | None:
 182:     candidates = list(candidates)
 183:     if not candidates:
 184:         return None
 185:     if len(candidates) == 1:
 186:         return candidates[0]
 187:     scored = []
 188:     for col in candidates:
 189:         scored.append((_score_host_column(str(col), df[col]), col))
 190:     scored.sort(reverse=True, key=lambda item: item[0])
 191:     return scored[0][1]
 192: 
 193: 
 194: def _score_datetime_column(name: str, series: pd.Series, tokens: Iterable[str]) -> float:
 195:     score = 0.0
 196:     lower_name = name.lower()
 197:     if any(token in lower_name for token in tokens):
 198:         score += 2.0
 199: 
 200:     sample = series.dropna()
 201:     if sample.empty:
 202:         return score - 5.0
 203:     if sample.shape[0] > 2000:
 204:         sample = sample.sample(2000, random_state=0)
 205: 
 206:     if pd.api.types.is_numeric_dtype(sample):
 207:         max_val = float(sample.max()) if not sample.empty else 0.0
 208:         if max_val < 1e8:
 209:             score -= 5.0
 210: 
 211:     parsed = pd.to_datetime(sample, errors="coerce", utc=False)
 212:     success = float(parsed.notna().mean())
 213:     score += success * 5.0
 214:     return score
 215: 
 216: 
 217: def _choose_best_datetime_column(
 218:     candidates: Iterable[str], df: pd.DataFrame, tokens: Iterable[str]
 219: ) -> str | None:
 220:     candidates = list(candidates)
 221:     if not candidates:
 222:         return None
 223:     if len(candidates) == 1:
 224:         return candidates[0]
 225:     scored = []
 226:     for col in candidates:
 227:         scored.append((_score_datetime_column(str(col), df[col], tokens), col))
 228:     scored.sort(reverse=True, key=lambda item: item[0])
 229:     best_score, best_col = scored[0]
 230:     if best_score <= 0:
 231:         return None
 232:     return best_col
 233: 
 234: 
 235: def _series_for_column(frame: pd.DataFrame, column: str) -> pd.Series:
 236:     data = frame[column]
 237:     if isinstance(data, pd.DataFrame):
 238:         return data.iloc[:, 0]
 239:     return data
 240: 
 241: 
 242: def _infer_baseline_host_count(values: pd.Series) -> int | None:
 243:     cleaned = values.dropna()
 244:     if cleaned.empty:
 245:         return None
 246:     try:
 247:         cleaned = cleaned.astype(int)
 248:     except (TypeError, ValueError):
 249:         cleaned = cleaned.astype(float).round().astype(int)
 250:     counts = cleaned.value_counts()
 251:     if counts.empty:
 252:         return None
 253:     max_count = int(counts.max())
 254:     candidates = sorted(int(v) for v in counts[counts == max_count].index.tolist())
 255:     return candidates[0] if candidates else None
 256: 
 257: 
 258: def _bucket_floor(ts: pd.Series, bucket_size: str) -> pd.Series:
 259:     if bucket_size == "hour":
 260:         return ts.dt.floor("h")
 261:     return ts.dt.floor("D")
 262: 
 263: 
 264: def _max_concurrent_hosts(
 265:     frame: pd.DataFrame,
 266:     bucket_start: pd.Timestamp,
 267:     bucket_end: pd.Timestamp,
 268: ) -> int:
 269:     events: list[tuple[pd.Timestamp, int]] = []
 270:     grouped = frame.groupby("__host_norm", sort=False)
 271:     for _, host_frame in grouped:
 272:         intervals = host_frame[["__start_ts", "__end_ts"]].sort_values("__start_ts")
 273:         current_start: pd.Timestamp | None = None
 274:         current_end: pd.Timestamp | None = None
 275:         for start_ts, end_ts in intervals.itertuples(index=False, name=None):
 276:             if start_ts is None or end_ts is None:
 277:                 continue
 278:             start_ts = max(start_ts, bucket_start)
 279:             end_ts = min(end_ts, bucket_end)
 280:             if end_ts < start_ts:
 281:                 continue
 282:             if current_start is None:
 283:                 current_start = start_ts
 284:                 current_end = end_ts
 285:                 continue
 286:             if start_ts <= current_end:
 287:                 if end_ts > current_end:
 288:                     current_end = end_ts
 289:             else:
 290:                 events.append((current_start, 1))
 291:                 events.append((current_end, -1))
 292:                 current_start = start_ts
 293:                 current_end = end_ts
 294:         if current_start is not None and current_end is not None:
 295:             events.append((current_start, 1))
 296:             events.append((current_end, -1))
 297: 
 298:     if not events:
 299:         return 0
 300: 
 301:     events.sort(key=lambda item: (item[0], -item[1]))
 302:     current = 0
 303:     peak = 0
 304:     for _, delta in events:
 305:         current += delta
 306:         peak = max(peak, current)
 307:     return peak
 308: 
 309: 
 310: def _month_key(day: date) -> str:
 311:     return f"{day.year:04d}-{day.month:02d}"
 312: 
 313: 
 314: def _build_calendar_days(start_day: date, end_day: date) -> list[date]:
 315:     days: list[date] = []
 316:     cursor = start_day
 317:     while cursor <= end_day:
 318:         days.append(cursor)
 319:         cursor = cursor + timedelta(days=1)
 320:     return days
 321: 
 322: 
 323: def _calendar_close_window(
 324:     daily: pd.DataFrame, close_start_day: int, close_end_day: int, mode: str
 325: ) -> tuple[set[date], list[dict[str, Any]], set[str]]:
 326:     close_dates: set[date] = set()
 327:     for day in daily["date"].tolist():
 328:         if close_start_day <= close_end_day:
 329:             is_close = close_start_day <= day.day <= close_end_day
 330:         else:
 331:             is_close = day.day >= close_start_day or day.day <= close_end_day
 332:         if is_close:
 333:             close_dates.add(day)
 334:     close_windows = [
 335:         {
 336:             "mode": mode,
 337:             "start_day": close_start_day,
 338:             "end_day": close_end_day,
 339:         }
 340:     ]
 341:     confident_months = {day.strftime("%Y-%m") for day in close_dates}
 342:     return close_dates, close_windows, confident_months
 343: 
 344: 
 345: def _infer_close_windows(
 346:     daily: pd.DataFrame,
 347:     min_days: int,
 348:     max_days: int,
 349:     lookahead_days: int,
 350:     min_confidence: float,
 351:     min_data_ratio: float,
 352: ) -> tuple[set[date], list[dict[str, Any]], set[str]]:
 353:     if daily.empty:
 354:         return set(), [], set()
 355: 
 356:     daily = daily.copy()
 357:     daily["month"] = daily["date"].apply(_month_key)
 358:     daily_map = {
 359:         row["date"]: {
 360:             "count": int(row["count"]),
 361:             "median": float(row["median_ttc"]) if row["median_ttc"] is not None else None,
 362:         }
 363:         for row in daily.to_dict("records")
 364:     }
 365: 
 366:     months = sorted(daily["month"].unique())
 367:     close_windows: list[dict[str, Any]] = []
 368:     close_dates: dict[date, str] = {}
 369:     confident_months: set[str] = set()
 370: 
 371:     for month in months:
 372:         year, month_num = [int(part) for part in month.split("-")]
 373:         month_start = date(year, month_num, 1)
 374:         if month_num == 12:
 375:             next_month_start = date(year + 1, 1, 1)
 376:         else:
 377:             next_month_start = date(year, month_num + 1, 1)
 378:         month_end = next_month_start - timedelta(days=1)
 379: 
 380:         month_days = _build_calendar_days(month_start, month_end)
 381:         next_days = _build_calendar_days(
 382:             next_month_start, next_month_start + timedelta(days=max(0, lookahead_days - 1))
 383:         )
 384:         extended_days = month_days + next_days
 385: 
 386:         counts = [daily_map.get(day, {}).get("count", 0) for day in month_days]
 387:         medians = [daily_map.get(day, {}).get("median") for day in month_days]
 388:         medians_clean = [val for val in medians if val is not None]
 389:         count_mean = float(np.mean(counts)) if counts else 0.0
 390:         count_std = float(np.std(counts)) if counts else 0.0
 391:         median_mean = float(np.mean(medians_clean)) if medians_clean else 0.0
 392:         median_std = float(np.std(medians_clean)) if medians_clean else 0.0
 393: 
 394:         def _z(val: float, mean: float, std: float) -> float:
 395:             if std <= 0:
 396:                 return 0.0
 397:             return (val - mean) / std
 398: 
 399:         pressures = []
 400:         for day in extended_days:
 401:             record = daily_map.get(day)
 402:             count_val = record.get("count", 0) if record else 0
 403:             median_val = record.get("median") if record else None
 404:             z_count = _z(float(count_val), count_mean, count_std)
 405:             z_median = 0.0
 406:             if median_val is not None:
 407:                 z_median = _z(float(median_val), median_mean, median_std)
 408:             pressures.append(z_count + z_median)
 409: 
 410:         best_score = None
 411:         best_window = None
 412:         second_score = None
 413: 
 414:         for length in range(min_days, max_days + 1):
 415:             if length <= 0 or length > len(extended_days):
 416:                 continue
 417:             for start_idx in range(len(month_days)):
 418:                 end_idx = start_idx + length
 419:                 if end_idx > len(extended_days):
 420:                     continue
 421:                 score = float(sum(pressures[start_idx:end_idx]))
 422:                 if best_score is None or score > best_score:
 423:                     second_score = best_score
 424:                     best_score = score
 425:                     best_window = (start_idx, end_idx)
 426:                 elif second_score is None or score > second_score:
 427:                     second_score = score
 428: 
 429:         confidence = 0.0
 430:         if best_score is not None and best_score > 0:
 431:             if second_score is None:
 432:                 confidence = 1.0
 433:             else:
 434:                 confidence = (best_score - second_score) / abs(best_score)
 435: 
 436:         if best_window is None or confidence < min_confidence:
 437:             close_windows.append(
 438:                 {
 439:                     "month": month,
 440:                     "start": None,
 441:                     "end": None,
 442:                     "length": 0,
 443:                     "confidence": float(confidence),
 444:                     "mode": "infer",
 445:                 }
 446:             )
 447:             continue
 448: 
 449:         start_idx, end_idx = best_window
 450:         window_days = extended_days[start_idx:end_idx]
 451:         data_days = [day for day in window_days if daily_map.get(day, {}).get("count", 0) > 0]
 452:         data_ratio = len(data_days) / max(1, len(window_days))
 453:         if data_ratio < min_data_ratio:
 454:             close_windows.append(
 455:                 {
 456:                     "month": month,
 457:                     "start": None,
 458:                     "end": None,
 459:                     "length": len(window_days),
 460:                     "confidence": float(confidence),
 461:                     "mode": "infer",
 462:                     "reason": "insufficient_data_days",
 463:                 }
 464:             )
 465:             continue
 466: 
 467:         start_day = window_days[0]
 468:         end_day = window_days[-1]
 469:         close_windows.append(
 470:             {
 471:                 "month": month,
 472:                 "start": start_day.isoformat(),
 473:                 "end": end_day.isoformat(),
 474:                 "length": len(window_days),
 475:                 "confidence": float(confidence),
 476:                 "mode": "infer",
 477:             }
 478:         )
 479:         confident_months.add(month)
 480: 
 481:         for day in window_days:
 482:             close_dates.setdefault(day, month)
 483: 
 484:     close_date_set = set(close_dates.keys())
 485:     return close_date_set, close_windows, confident_months
 486: 
 487: 
 488: class Plugin:
 489:     def run(self, ctx) -> PluginResult:
 490:         df = ctx.dataset_loader()
 491:         if df.empty:
 492:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 493: 
 494:         columns_meta = []
 495:         role_by_name: dict[str, str] = {}
 496:         if ctx.dataset_version_id:
 497:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 498:             if dataset_template and dataset_template.get("status") == "ready":
 499:                 fields = ctx.storage.fetch_template_fields(
 500:                     int(dataset_template["template_id"])
 501:                 )
 502:                 columns_meta = fields
 503:                 role_by_name = {
 504:                     field["name"]: (field.get("role") or "") for field in fields
 505:                 }
 506:             else:
 507:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 508:                 role_by_name = {
 509:                     col["original_name"]: (col.get("role") or "")
 510:                     for col in columns_meta
 511:                 }
 512: 
 513:         columns = list(df.columns)
 514:         lower_names = {col: str(col).lower() for col in columns}
 515:         used: set[str] = set()
 516: 
 517:         process_candidates = _candidate_columns(
 518:             columns,
 519:             role_by_name,
 520:             {"process", "activity", "event", "step", "task", "action"},
 521:             ["process", "activity", "event", "step", "task", "action", "job"],
 522:             lower_names,
 523:             used,
 524:         )
 525:         process_col = _choose_best_process_column(process_candidates, df)
 526:         if process_col:
 527:             used.add(process_col)
 528: 
 529:         host_candidates = _candidate_columns(
 530:             columns,
 531:             role_by_name,
 532:             {"server", "host", "node", "instance"},
 533:             ["server", "host", "node", "instance", "machine"],
 534:             lower_names,
 535:             used,
 536:         )
 537:         preferred_host = ctx.settings.get("host_column")
 538:         if preferred_host and preferred_host in columns:
 539:             host_col = preferred_host
 540:         else:
 541:             host_col = _choose_best_host_column(host_candidates, df)
 542:         if host_col:
 543:             used.add(host_col)
 544: 
 545:         start_candidates = _candidate_columns(
 546:             columns,
 547:             role_by_name,
 548:             {"start_time", "start"},
 549:             ["start", "begin"],
 550:             lower_names,
 551:             used,
 552:         )
 553:         preferred_start = ctx.settings.get("start_column")
 554:         if preferred_start and preferred_start in columns:
 555:             start_col = preferred_start
 556:         else:
 557:             start_col = _choose_best_datetime_column(
 558:                 start_candidates, df, ("start", "begin")
 559:             )
 560:         if start_col:
 561:             used.add(start_col)
 562: 
 563:         end_candidates = _candidate_columns(
 564:             columns,
 565:             role_by_name,
 566:             {"end_time", "end", "finish", "complete"},
 567:             ["end", "finish", "complete", "stop"],
 568:             lower_names,
 569:             used,
 570:         )
 571:         preferred_end = ctx.settings.get("end_column")
 572:         if preferred_end and preferred_end in columns:
 573:             end_col = preferred_end
 574:         else:
 575:             end_col = _choose_best_datetime_column(
 576:                 end_candidates, df, ("end", "finish", "complete", "stop")
 577:             )
 578:         if end_col:
 579:             used.add(end_col)
 580: 
 581:         queue_col = _pick_column(
 582:             ctx.settings.get("queue_column"),
 583:             columns,
 584:             role_by_name,
 585:             {"queue", "queued", "enqueue"},
 586:             ["queue", "queued", "enqueue"],
 587:             lower_names,
 588:             used,
 589:         )
 590:         if queue_col:
 591:             used.add(queue_col)
 592: 
 593:         eligible_col = _pick_column(
 594:             ctx.settings.get("eligible_column"),
 595:             columns,
 596:             role_by_name,
 597:             {"eligible", "ready", "available"},
 598:             ["eligible", "ready", "available"],
 599:             lower_names,
 600:             used,
 601:         )
 602:         eligible_fallback = None
 603:         if eligible_col:
 604:             used.add(eligible_col)
 605:         elif queue_col:
 606:             # If no explicit eligible column is detected, reuse queue timestamp as eligible.
 607:             eligible_col = queue_col
 608:             eligible_fallback = "queue_column"
 609: 
 610:         baseline_host_setting = ctx.settings.get("baseline_host_count", None)
 611:         try:
 612:             baseline_host_setting_val = (
 613:                 int(baseline_host_setting) if baseline_host_setting is not None else None
 614:             )
 615:         except (TypeError, ValueError):
 616:             baseline_host_setting_val = None
 617:         if baseline_host_setting_val is not None and baseline_host_setting_val <= 0:
 618:             baseline_host_setting_val = None
 619:         added_hosts = int(ctx.settings.get("added_hosts", 1))
 620: 
 621:         summary: dict[str, Any] = {
 622:             "process_column": process_col,
 623:             "host_column": host_col,
 624:             "start_column": start_col,
 625:             "end_column": end_col,
 626:             "queue_column": queue_col,
 627:             "eligible_column": eligible_col,
 628:             "eligible_column_fallback": eligible_fallback,
 629:             "baseline_host_setting": baseline_host_setting_val,
 630:             "added_hosts": added_hosts,
 631:         }
 632: 
 633:         metric_types = ["ttc", "queue_to_end", "eligible_to_end"]
 634:         host_metrics = ["concurrent", "unique"]
 635: 
 636:         def _emit_not_applicable(reason: str) -> PluginResult:
 637:             findings = []
 638:             for metric in host_metrics:
 639:                 for metric_type in metric_types:
 640:                     findings.append(
 641:                         {
 642:                             "kind": "close_cycle_capacity_model",
 643:                             "host_metric": metric,
 644:                             "metric_type": metric_type,
 645:                             "decision": "not_applicable",
 646:                             "measurement_type": "not_applicable",
 647:                             "reason": reason,
 648:                             "columns": [
 649:                                 col
 650:                                 for col in [
 651:                                     process_col,
 652:                                     host_col,
 653:                                     start_col,
 654:                                     end_col,
 655:                                     queue_col,
 656:                                     eligible_col,
 657:                                 ]
 658:                                 if col
 659:                             ],
 660:                         }
 661:                     )
 662:             artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_capacity_model")
 663:             out_path = artifacts_dir / "results.json"
 664:             write_json(out_path, {"summary": summary, "findings": findings})
 665:             artifacts = [
 666:                 PluginArtifact(
 667:                     path=str(out_path.relative_to(ctx.run_dir)),
 668:                     type="json",
 669:                     description="Close-cycle capacity model summary",
 670:                 )
 671:             ]
 672:             csv_path = artifacts_dir / "results.csv"
 673:             with csv_path.open("w", encoding="utf-8", newline="") as handle:
 674:                 header = [
 675:                     "host_metric",
 676:                     "metric_type",
 677:                     "decision",
 678:                     "baseline_median_hours",
 679:                     "modeled_median_hours",
 680:                     "effect",
 681:                     "target_reduction",
 682:                     "tolerance",
 683:                     "target_met",
 684:                     "bucket_count",
 685:                     "months",
 686:                     "reason",
 687:                 ]
 688:                 handle.write(",".join(header) + "\n")
 689:                 for item in findings:
 690:                     handle.write(
 691:                         ",".join(
 692:                             [
 693:                                 str(item.get("host_metric")),
 694:                                 str(item.get("metric_type")),
 695:                                 str(item.get("decision")),
 696:                                 str(item.get("baseline_median_hours")),
 697:                                 str(item.get("modeled_median_hours")),
 698:                                 str(item.get("effect")),
 699:                                 str(item.get("target_reduction")),
 700:                                 str(item.get("tolerance")),
 701:                                 str(item.get("target_met")),
 702:                                 str(item.get("bucket_count")),
 703:                                 str(item.get("months")),
 704:                                 str(item.get("reason")),
 705:                             ]
 706:                         )
 707:                         + "\n"
 708:                     )
 709:             artifacts.append(
 710:                 PluginArtifact(
 711:                     path=str(csv_path.relative_to(ctx.run_dir)),
 712:                     type="csv",
 713:                     description="Capacity model detail table",
 714:                 )
 715:             )
 716: 
 717:             md_path = artifacts_dir / "results.md"
 718:             lines = [
 719:                 "# Close-cycle capacity model",
 720:                 "",
 721:                 "Summary:",
 722:                 f"- close_window_mode: {summary.get('close_window_mode')}",
 723:                 f"- close_cycle_start_day: {summary.get('close_cycle_start_day')}",
 724:                 f"- close_cycle_end_day: {summary.get('close_cycle_end_day')}",
 725:                 "",
 726:                 "Modeled findings:",
 727:                 "_None_",
 728:                 "",
 729:                 "Not applicable:",
 730:             ]
 731:             for item in findings:
 732:                 lines.append(
 733:                     f"- {item.get('host_metric')} {item.get('metric_type')}: {item.get('reason')}"
 734:                 )
 735:             md_path.write_text("\n".join(lines), encoding="utf-8")
 736:             artifacts.append(
 737:                 PluginArtifact(
 738:                     path=str(md_path.relative_to(ctx.run_dir)),
 739:                     type="markdown",
 740:                     description="Capacity model summary",
 741:                 )
 742:             )
 743: 
 744:             return PluginResult(
 745:                 "ok",
 746:                 reason,
 747:                 {"findings": len(findings)},
 748:                 findings,
 749:                 artifacts,
 750:                 None,
 751:             )
 752: 
 753:         if not start_col or not end_col:
 754:             return _emit_not_applicable("missing_start_end")
 755:         if not host_col:
 756:             return _emit_not_applicable("missing_host_column")
 757: 
 758:         selected_cols: list[str] = []
 759:         for col in [process_col, host_col, start_col, end_col, queue_col, eligible_col]:
 760:             if col and col not in selected_cols:
 761:                 selected_cols.append(col)
 762:         work = df.loc[:, selected_cols].copy()
 763: 
 764:         work["__start_ts"] = pd.to_datetime(
 765:             _series_for_column(work, start_col), errors="coerce", utc=False
 766:         )
 767:         work["__end_ts"] = pd.to_datetime(
 768:             _series_for_column(work, end_col), errors="coerce", utc=False
 769:         )
 770:         work = work.loc[work["__start_ts"].notna() & work["__end_ts"].notna()].copy()
 771:         if work.empty:
 772:             return _emit_not_applicable("no_valid_timestamps")
 773: 
 774:         work = work.loc[work["__end_ts"] >= work["__start_ts"]].copy()
 775:         if work.empty:
 776:             return _emit_not_applicable("no_non_negative_durations")
 777: 
 778:         work["__service_sec"] = (
 779:             work["__end_ts"] - work["__start_ts"]
 780:         ).dt.total_seconds()
 781:         work = work.loc[work["__service_sec"] > 0].copy()
 782:         if work.empty:
 783:             return _emit_not_applicable("no_positive_ttc")
 784: 
 785:         work["__host"] = _series_for_column(work, host_col).map(_normalize_text)
 786:         work["__host_norm"] = work["__host"].str.lower()
 787:         work = work.loc[~work["__host_norm"].isin(INVALID_STRINGS)].copy()
 788:         if work.empty:
 789:             return _emit_not_applicable("no_valid_host_values")
 790: 
 791:         if queue_col and queue_col in work.columns:
 792:             work["__queue_ts"] = pd.to_datetime(
 793:                 _series_for_column(work, queue_col), errors="coerce", utc=False
 794:             )
 795:             work["__queue_wait_sec"] = (
 796:                 work["__start_ts"] - work["__queue_ts"]
 797:             ).dt.total_seconds().clip(lower=0)
 798:             work["__queue_to_end_sec"] = (
 799:                 work["__end_ts"] - work["__queue_ts"]
 800:             ).dt.total_seconds().clip(lower=0)
 801:         else:
 802:             work["__queue_wait_sec"] = np.nan
 803:             work["__queue_to_end_sec"] = np.nan
 804: 
 805:         if eligible_col and eligible_col in work.columns:
 806:             work["__eligible_ts"] = pd.to_datetime(
 807:                 _series_for_column(work, eligible_col), errors="coerce", utc=False
 808:             )
 809:             work["__eligible_wait_sec"] = (
 810:                 work["__start_ts"] - work["__eligible_ts"]
 811:             ).dt.total_seconds().clip(lower=0)
 812:             work["__eligible_to_end_sec"] = (
 813:                 work["__end_ts"] - work["__eligible_ts"]
 814:             ).dt.total_seconds().clip(lower=0)
 815:         else:
 816:             work["__eligible_wait_sec"] = np.nan
 817:             work["__eligible_to_end_sec"] = np.nan
 818: 
 819:         close_mode = str(ctx.settings.get("close_window_mode", "infer") or "infer").lower()
 820:         if close_mode == "calendar":
 821:             close_mode = "override"
 822:         close_start_day = int(ctx.settings.get("close_cycle_start_day", 20))
 823:         close_end_day = int(ctx.settings.get("close_cycle_end_day", 5))
 824:         min_close_days = int(ctx.settings.get("min_close_days", 5))
 825:         max_close_days = int(ctx.settings.get("max_close_days", 20))
 826:         lookahead_days = int(ctx.settings.get("lookahead_days", 7))
 827:         min_close_confidence = float(ctx.settings.get("min_close_confidence", 0.1))
 828:         min_close_data_ratio = float(ctx.settings.get("min_close_data_ratio", 0.5))
 829: 
 830:         work["__date"] = work["__start_ts"].dt.date
 831:         daily = (
 832:             work.groupby("__date")
 833:             .agg(count=("__date", "size"), median_ttc=("__service_sec", "median"))
 834:             .reset_index()
 835:             .rename(columns={"__date": "date"})
 836:         )
 837: 
 838:         close_dates: set[date] = set()
 839:         close_windows: list[dict[str, Any]] = []
 840:         confident_months: set[str] = set()
 841:         fallback_used = False
 842:         fallback_reason: str | None = None
 843: 
 844:         if close_mode == "override":
 845:             close_dates, close_windows, confident_months = _calendar_close_window(
 846:                 daily, close_start_day, close_end_day, "override"
 847:             )
 848:         elif close_mode == "infer_or_default":
 849:             close_dates, close_windows, confident_months = _infer_close_windows(
 850:                 daily,
 851:                 min_close_days,
 852:                 max_close_days,
 853:                 lookahead_days,
 854:                 min_close_confidence,
 855:                 min_close_data_ratio,
 856:             )
 857:             if not close_dates:
 858:                 fallback_used = True
 859:                 fallback_reason = "calendar_default"
 860:                 close_dates, close_windows, confident_months = _calendar_close_window(
 861:                     daily, close_start_day, close_end_day, "fallback"
 862:                 )
 863:         else:
 864:             close_dates, close_windows, confident_months = _infer_close_windows(
 865:                 daily,
 866:                 min_close_days,
 867:                 max_close_days,
 868:                 lookahead_days,
 869:                 min_close_confidence,
 870:                 min_close_data_ratio,
 871:             )
 872: 
 873:         summary.update(
 874:             {
 875:                 "close_window_mode": close_mode,
 876:                 "close_cycle_start_day": close_start_day,
 877:                 "close_cycle_end_day": close_end_day,
 878:                 "min_close_days": min_close_days,
 879:                 "max_close_days": max_close_days,
 880:                 "lookahead_days": lookahead_days,
 881:                 "min_close_confidence": min_close_confidence,
 882:                 "min_close_data_ratio": min_close_data_ratio,
 883:                 "close_windows": close_windows,
 884:                 "close_window_fallback": fallback_used,
 885:                 "close_window_fallback_reason": fallback_reason,
 886:             }
 887:         )
 888: 
 889:         if not close_dates:
 890:             return _emit_not_applicable("close_window_not_inferred")
 891: 
 892:         close_window_source = (
 893:             "fallback"
 894:             if fallback_used
 895:             else ("override" if close_mode == "override" else "infer")
 896:         )
 897: 
 898:         bucket_size = str(ctx.settings.get("bucket_size", "day") or "day").lower()
 899:         min_bucket_rows = int(ctx.settings.get("min_bucket_rows", 50))
 900: 
 901:         work["__bucket_start"] = _bucket_floor(work["__start_ts"], bucket_size)
 902:         if bucket_size == "hour":
 903:             bucket_delta = timedelta(hours=1)
 904:         else:
 905:             bucket_delta = timedelta(days=1)
 906: 
 907:         buckets = []
 908:         grouped = work.groupby("__bucket_start", sort=True)
 909:         for bucket_start, frame in grouped:
 910:             if pd.isna(bucket_start):
 911:                 continue
 912:             bucket_start = pd.Timestamp(bucket_start)
 913:             bucket_end = bucket_start + bucket_delta
 914:             bucket_date = bucket_start.date()
 915:             close_flag = bucket_date in close_dates
 916:             row_count = int(frame.shape[0])
 917:             if row_count < min_bucket_rows:
 918:                 continue
 919:             median_service = float(frame["__service_sec"].median())
 920:             median_queue_wait = (
 921:                 float(frame["__queue_wait_sec"].median())
 922:                 if frame["__queue_wait_sec"].notna().any()
 923:                 else None
 924:             )
 925:             median_eligible_wait = (
 926:                 float(frame["__eligible_wait_sec"].median())
 927:                 if frame["__eligible_wait_sec"].notna().any()
 928:                 else None
 929:             )
 930:             median_queue_to_end = (
 931:                 float(frame["__queue_to_end_sec"].median())
 932:                 if frame["__queue_to_end_sec"].notna().any()
 933:                 else None
 934:             )
 935:             median_eligible_to_end = (
 936:                 float(frame["__eligible_to_end_sec"].median())
 937:                 if frame["__eligible_to_end_sec"].notna().any()
 938:                 else None
 939:             )
 940:             host_unique = int(frame["__host_norm"].nunique())
 941:             host_concurrent = _max_concurrent_hosts(frame, bucket_start, bucket_end)
 942:             buckets.append(
 943:                 {
 944:                     "bucket_start": bucket_start,
 945:                     "bucket_end": bucket_end,
 946:                     "bucket_date": bucket_date,
 947:                     "month": _month_key(bucket_date),
 948:                     "close": close_flag,
 949:                     "rows": row_count,
 950:                     "median_service": median_service,
 951:                     "median_queue_wait": median_queue_wait,
 952:                     "median_eligible_wait": median_eligible_wait,
 953:                     "median_queue_to_end": median_queue_to_end,
 954:                     "median_eligible_to_end": median_eligible_to_end,
 955:                     "host_unique": host_unique,
 956:                     "host_concurrent": host_concurrent,
 957:                 }
 958:             )
 959: 
 960:         if not buckets:
 961:             return _emit_not_applicable("no_buckets_after_filtering")
 962: 
 963:         bucket_df = pd.DataFrame(buckets)
 964:         close_buckets = bucket_df.loc[bucket_df["close"]].copy()
 965:         close_buckets = close_buckets.loc[
 966:             close_buckets["month"].isin(confident_months)
 967:         ].copy()
 968:         if close_buckets.empty:
 969:             return _emit_not_applicable("no_close_buckets")
 970: 
 971:         baseline_match_mode = str(
 972:             ctx.settings.get("baseline_match_mode", "exact") or "exact"
 973:         ).lower()
 974:         min_buckets_per_group = int(ctx.settings.get("min_buckets_per_group", 5))
 975:         min_months = int(ctx.settings.get("min_months", 1))
 976:         target_reduction = float(ctx.settings.get("target_reduction", 0.30))
 977:         tolerance = float(ctx.settings.get("tolerance", 0.05))
 978:         max_examples = int(ctx.settings.get("max_examples", 25))
 979: 
 980:         if added_hosts <= 0:
 981:             return _emit_not_applicable("invalid_added_hosts")
 982: 
 983:         findings = []
 984: 
 985:         def _within_tolerance(effect: float | None) -> bool | None:
 986:             if effect is None:
 987:                 return None
 988:             band_low = -(target_reduction + tolerance)
 989:             band_high = -(target_reduction - tolerance)
 990:             return band_low <= effect <= band_high
 991: 
 992:         for metric in host_metrics:
 993:             metric_col = "host_concurrent" if metric == "concurrent" else "host_unique"
 994:             baseline_host_count = baseline_host_setting_val
 995:             baseline_host_source = "config"
 996:             if baseline_host_count is None:
 997:                 baseline_host_count = _infer_baseline_host_count(close_buckets[metric_col])
 998:                 baseline_host_source = "inferred_mode"
 999:             if not baseline_host_count or baseline_host_count <= 0:
1000:                 for metric_type in metric_types:
1001:                     findings.append(
1002:                         {
1003:                             "kind": "close_cycle_capacity_model",
1004:                             "host_metric": metric,
1005:                             "metric_type": metric_type,
1006:                             "decision": "not_applicable",
1007:                             "measurement_type": "not_applicable",
1008:                             "reason": "invalid_baseline_host_count",
1009:                             "close_window_mode": close_mode,
1010:                             "close_window_fallback": fallback_used,
1011:                             "close_window_source": close_window_source,
1012:                             "close_window_reason": fallback_reason,
1013:                             "baseline_host_count": baseline_host_count,
1014:                             "baseline_host_source": baseline_host_source,
1015:                             "added_hosts": added_hosts,
1016:                             "scale_factor": None,
1017:                             "target_reduction": target_reduction,
1018:                             "tolerance": tolerance,
1019:                             "columns": [
1020:                                 col
1021:                                 for col in [
1022:                                     process_col,
1023:                                     host_col,
1024:                                     start_col,
1025:                                     end_col,
1026:                                     queue_col,
1027:                                     eligible_col,
1028:                                 ]
1029:                                 if col
1030:                             ],
1031:                         }
1032:                     )
1033:                 continue
1034: 
1035:             scale_factor = (baseline_host_count + added_hosts) / float(baseline_host_count)
1036:             assumption = (
1037:                 "Queue/eligible waits scale inversely with capacity; service time unchanged; "
1038:                 f"modeled factor {scale_factor:.3f}."
1039:             )
1040:             baseline_mode_effective = baseline_match_mode
1041:             baseline_fallback = None
1042:             if baseline_match_mode == "at_most":
1043:                 baseline_mask = close_buckets[metric_col] <= baseline_host_count
1044:             else:
1045:                 baseline_mask = close_buckets[metric_col] == baseline_host_count
1046: 
1047:             baseline = close_buckets.loc[baseline_mask].copy()
1048:             reasons = []
1049:             if baseline.shape[0] < min_buckets_per_group:
1050:                 reasons.append("insufficient_buckets")
1051: 
1052:             months = sorted(baseline["month"].unique().tolist())
1053:             if len(months) < min_months:
1054:                 reasons.append("insufficient_months")
1055: 
1056:             if reasons and baseline_match_mode == "exact":
1057:                 alt_mask = close_buckets[metric_col] <= baseline_host_count
1058:                 alt_baseline = close_buckets.loc[alt_mask].copy()
1059:                 alt_reasons = []
1060:                 if alt_baseline.shape[0] < min_buckets_per_group:
1061:                     alt_reasons.append("insufficient_buckets")
1062:                 alt_months = sorted(alt_baseline["month"].unique().tolist())
1063:                 if len(alt_months) < min_months:
1064:                     alt_reasons.append("insufficient_months")
1065:                 if not alt_reasons:
1066:                     baseline = alt_baseline
1067:                     reasons = []
1068:                     baseline_mode_effective = "at_most"
1069:                     baseline_fallback = "at_most"
1070:                     months = alt_months
1071:             if reasons and not close_buckets.empty:
1072:                 # Final fallback: use all close-cycle buckets when baseline host count not observed.
1073:                 baseline = close_buckets.copy()
1074:                 months = sorted(baseline["month"].unique().tolist())
1075:                 if len(months) >= min_months:
1076:                     reasons = []
1077:                     baseline_mode_effective = "all"
1078:                     baseline_fallback = "all_close_buckets"
1079: 
1080:             if reasons:
1081:                 for metric_type in metric_types:
1082:                     findings.append(
1083:                         {
1084:                             "kind": "close_cycle_capacity_model",
1085:                             "host_metric": metric,
1086:                             "metric_type": metric_type,
1087:                             "decision": "not_applicable",
1088:                             "measurement_type": "not_applicable",
1089:                             "reason": ",".join(reasons),
1090:                             "close_window_mode": close_mode,
1091:                             "close_window_fallback": fallback_used,
1092:                             "close_window_source": close_window_source,
1093:                             "close_window_reason": fallback_reason,
1094:                             "baseline_host_count": baseline_host_count,
1095:                             "baseline_host_source": baseline_host_source,
1096:                             "added_hosts": added_hosts,
1097:                             "scale_factor": scale_factor,
1098:                             "target_reduction": target_reduction,
1099:                             "tolerance": tolerance,
1100:                             "baseline_match_mode": baseline_mode_effective,
1101:                             "baseline_match_fallback": baseline_fallback,
1102:                             "columns": [
1103:                                 col
1104:                                 for col in [
1105:                                     process_col,
1106:                                     host_col,
1107:                                     start_col,
1108:                                     end_col,
1109:                                     queue_col,
1110:                                     eligible_col,
1111:                                 ]
1112:                                 if col
1113:                             ],
1114:                         }
1115:                     )
1116:                 continue
1117: 
1118:             baseline_service = float(baseline["median_service"].median())
1119:             queue_waits = baseline["median_queue_wait"].dropna()
1120:             eligible_waits = baseline["median_eligible_wait"].dropna()
1121: 
1122:             baseline_queue_wait = (
1123:                 float(queue_waits.median()) if not queue_waits.empty else None
1124:             )
1125:             baseline_eligible_wait = (
1126:                 float(eligible_waits.median()) if not eligible_waits.empty else None
1127:             )
1128: 
1129:             baseline_queue_to_end = None
1130:             if baseline_queue_wait is not None:
1131:                 baseline_queue_to_end = baseline_queue_wait + baseline_service
1132:             else:
1133:                 direct = baseline["median_queue_to_end"].dropna()
1134:                 if not direct.empty:
1135:                     baseline_queue_to_end = float(direct.median())
1136: 
1137:             baseline_eligible_to_end = None
1138:             if baseline_eligible_wait is not None:
1139:                 baseline_eligible_to_end = baseline_eligible_wait + baseline_service
1140:             else:
1141:                 direct = baseline["median_eligible_to_end"].dropna()
1142:                 if not direct.empty:
1143:                     baseline_eligible_to_end = float(direct.median())
1144: 
1145:             modeled_service = baseline_service
1146:             modeled_queue_to_end = (
1147:                 baseline_queue_wait / scale_factor + baseline_service
1148:                 if baseline_queue_wait is not None
1149:                 else None
1150:             )
1151:             modeled_eligible_to_end = (
1152:                 baseline_eligible_wait / scale_factor + baseline_service
1153:                 if baseline_eligible_wait is not None
1154:                 else None
1155:             )
1156: 
1157:             row_ids = work.index.tolist()[:max_examples]
1158: 
1159:             def _emit_metric(metric_type: str, baseline_val: float | None, modeled_val: float | None, reason_hint: str | None) -> None:
1160:                 if baseline_val is None or baseline_val <= 0:
1161:                     findings.append(
1162:                         {
1163:                             "kind": "close_cycle_capacity_model",
1164:                             "host_metric": metric,
1165:                             "metric_type": metric_type,
1166:                             "decision": "not_applicable",
1167:                             "measurement_type": "not_applicable",
1168:                             "reason": reason_hint or "missing_metric",
1169:                             "close_window_mode": close_mode,
1170:                             "close_window_fallback": fallback_used,
1171:                             "close_window_source": close_window_source,
1172:                             "close_window_reason": fallback_reason,
1173:                             "baseline_host_count": baseline_host_count,
1174:                             "added_hosts": added_hosts,
1175:                             "scale_factor": scale_factor,
1176:                             "target_reduction": target_reduction,
1177:                             "tolerance": tolerance,
1178:                             "bucket_count": int(baseline.shape[0]),
1179:                             "months": months,
1180:                             "columns": [
1181:                                 col
1182:                                 for col in [
1183:                                     process_col,
1184:                                     host_col,
1185:                                     start_col,
1186:                                     end_col,
1187:                                     queue_col,
1188:                                     eligible_col,
1189:                                 ]
1190:                                 if col
1191:                             ],
1192:                         }
1193:                     )
1194:                     return
1195: 
1196:                 effect = None
1197:                 if modeled_val is not None and baseline_val > 0:
1198:                     effect = (modeled_val / baseline_val) - 1.0
1199: 
1200:                 findings.append(
1201:                     {
1202:                         "kind": "close_cycle_capacity_model",
1203:                         "host_metric": metric,
1204:                         "metric_type": metric_type,
1205:                         "decision": "modeled",
1206:                         "measurement_type": "modeled",
1207:                         "reason": "fallback_calendar_default" if fallback_used else "ok",
1208:                         "close_window_mode": close_mode,
1209:                         "close_window_fallback": fallback_used,
1210:                         "close_window_source": close_window_source,
1211:                         "close_window_reason": fallback_reason,
1212:                         "assumptions": [assumption],
1213:                         "scope": {
1214:                             "host_metric": metric,
1215:                             "metric_type": metric_type,
1216:                             "close_window_mode": close_mode,
1217:                             "close_window_source": close_window_source,
1218:                         },
1219:                         "baseline_median_sec": baseline_val,
1220:                         "modeled_median_sec": modeled_val,
1221:                         "effect": effect,
1222:                         "target_reduction": target_reduction,
1223:                         "tolerance": tolerance,
1224:                         "baseline_match_mode": baseline_mode_effective,
1225:                         "baseline_match_fallback": baseline_fallback,
1226:                         "target_met": _within_tolerance(effect),
1227:                         "baseline_host_count": baseline_host_count,
1228:                         "baseline_host_source": baseline_host_source,
1229:                         "added_hosts": added_hosts,
1230:                         "scale_factor": scale_factor,
1231:                         "host_count_baseline": baseline_host_count,
1232:                         "host_count_modeled": (
1233:                             baseline_host_count + added_hosts
1234:                             if baseline_host_count is not None
1235:                             else None
1236:                         ),
1237:                         "bucket_count": int(baseline.shape[0]),
1238:                         "months": months,
1239:                         "row_ids": [int(i) for i in row_ids],
1240:                         "columns": [
1241:                             col
1242:                             for col in [
1243:                                 process_col,
1244:                                 host_col,
1245:                                 start_col,
1246:                                 end_col,
1247:                                 queue_col,
1248:                                 eligible_col,
1249:                             ]
1250:                             if col
1251:                         ],
1252:                     }
1253:                 )
1254: 
1255:             _emit_metric("ttc", baseline_service, modeled_service, None)
1256:             _emit_metric(
1257:                 "queue_to_end",
1258:                 baseline_queue_to_end,
1259:                 modeled_queue_to_end,
1260:                 "missing_queue_column",
1261:             )
1262:             _emit_metric(
1263:                 "eligible_to_end",
1264:                 baseline_eligible_to_end,
1265:                 modeled_eligible_to_end,
1266:                 "missing_eligible_column",
1267:             )
1268: 
1269:         artifacts_dir = ctx.artifacts_dir("analysis_close_cycle_capacity_model")
1270:         out_path = artifacts_dir / "results.json"
1271:         write_json(out_path, {"summary": summary, "findings": findings})
1272:         artifacts = [
1273:             PluginArtifact(
1274:                 path=str(out_path.relative_to(ctx.run_dir)),
1275:                 type="json",
1276:                 description="Close-cycle capacity model summary",
1277:             )
1278:         ]
1279: 
1280:         csv_path = artifacts_dir / "results.csv"
1281:         with csv_path.open("w", encoding="utf-8", newline="") as handle:
1282:             header = [
1283:                 "host_metric",
1284:                 "metric_type",
1285:                 "decision",
1286:                 "baseline_median_hours",
1287:                 "modeled_median_hours",
1288:                 "effect",
1289:                 "target_reduction",
1290:                 "tolerance",
1291:                 "target_met",
1292:                 "bucket_count",
1293:                 "months",
1294:                 "reason",
1295:             ]
1296:             handle.write(",".join(header) + "\n")
1297:             for item in findings:
1298:                 baseline_hours = (
1299:                     float(item.get("baseline_median_sec")) / 3600.0
1300:                     if item.get("baseline_median_sec") is not None
1301:                     else None
1302:                 )
1303:                 modeled_hours = (
1304:                     float(item.get("modeled_median_sec")) / 3600.0
1305:                     if item.get("modeled_median_sec") is not None
1306:                     else None
1307:                 )
1308:                 handle.write(
1309:                     ",".join(
1310:                         [
1311:                             str(item.get("host_metric")),
1312:                             str(item.get("metric_type")),
1313:                             str(item.get("decision")),
1314:                             str(baseline_hours),
1315:                             str(modeled_hours),
1316:                             str(item.get("effect")),
1317:                             str(item.get("target_reduction")),
1318:                             str(item.get("tolerance")),
1319:                             str(item.get("target_met")),
1320:                             str(item.get("bucket_count")),
1321:                             str(item.get("months")),
1322:                             str(item.get("reason")),
1323:                         ]
1324:                     )
1325:                     + "\n"
1326:                 )
1327:         artifacts.append(
1328:             PluginArtifact(
1329:                 path=str(csv_path.relative_to(ctx.run_dir)),
1330:                 type="csv",
1331:                 description="Capacity model detail table",
1332:             )
1333:         )
1334: 
1335:         md_path = artifacts_dir / "results.md"
1336:         lines = [
1337:             "# Close-cycle capacity model",
1338:             "",
1339:             "Summary:",
1340:             f"- close_window_mode: {summary.get('close_window_mode')}",
1341:             f"- close_cycle_start_day: {summary.get('close_cycle_start_day')}",
1342:             f"- close_cycle_end_day: {summary.get('close_cycle_end_day')}",
1343:             f"- baseline_host_count: {baseline_host_count}",
1344:             f"- added_hosts: {added_hosts}",
1345:             f"- scale_factor: {scale_factor:.3f}",
1346:             f"- target_reduction: {target_reduction}",
1347:             f"- tolerance: {tolerance}",
1348:             "",
1349:             "Modeled findings:",
1350:         ]
1351: 
1352:         modeled_rows = [
1353:             item for item in findings if item.get("decision") == "modeled"
1354:         ]
1355:         if modeled_rows:
1356:             lines.append(
1357:                 "| host_metric | metric_type | baseline_hours | modeled_hours | effect | target_met | buckets |"
1358:             )
1359:             lines.append("| --- | --- | --- | --- | --- | --- | --- |")
1360:             for item in modeled_rows:
1361:                 baseline_hours = (
1362:                     float(item.get("baseline_median_sec")) / 3600.0
1363:                     if item.get("baseline_median_sec") is not None
1364:                     else 0.0
1365:                 )
1366:                 modeled_hours = (
1367:                     float(item.get("modeled_median_sec")) / 3600.0
1368:                     if item.get("modeled_median_sec") is not None
1369:                     else 0.0
1370:                 )
1371:                 effect = item.get("effect")
1372:                 effect_str = f"{effect:.4f}" if effect is not None else ""
1373:                 lines.append(
1374:                     "| {host_metric} | {metric_type} | {baseline_hours:.2f} | {modeled_hours:.2f} | {effect} | {target_met} | {bucket_count} |".format(
1375:                         host_metric=item.get("host_metric"),
1376:                         metric_type=item.get("metric_type"),
1377:                         baseline_hours=baseline_hours,
1378:                         modeled_hours=modeled_hours,
1379:                         effect=effect_str,
1380:                         target_met=item.get("target_met"),
1381:                         bucket_count=item.get("bucket_count"),
1382:                     )
1383:                 )
1384:         else:
1385:             lines.append("_None_")
1386: 
1387:         lines.extend(["", "Not applicable:"])
1388:         for item in findings:
1389:             if item.get("decision") != "modeled":
1390:                 lines.append(
1391:                     f"- {item.get('host_metric')} {item.get('metric_type')}: {item.get('reason')}"
1392:                 )
1393: 
1394:         md_path.write_text("\n".join(lines), encoding="utf-8")
1395:         artifacts.append(
1396:             PluginArtifact(
1397:                 path=str(md_path.relative_to(ctx.run_dir)),
1398:                 type="markdown",
1399:                 description="Capacity model summary",
1400:             )
1401:         )
1402: 
1403:         metrics = {
1404:             "findings": len(findings),
1405:             "modeled_findings": len(modeled_rows),
1406:         }
1407:         return PluginResult(
1408:             "ok",
1409:             "Computed close-cycle capacity model",
1410:             metrics,
1411:             findings,
1412:             artifacts,
1413:             None,
1414:         )
````

## File: plugins/analysis_conformal_feature_prediction/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: 
 4: import numpy as np
 5: from sklearn.linear_model import Ridge
 6: 
 7: from statistic_harness.core.types import PluginArtifact, PluginResult
 8: from statistic_harness.core.utils import write_json
 9: 
10: 
11: class Plugin:
12:     def run(self, ctx) -> PluginResult:
13:         df = ctx.dataset_loader()
14:         numeric = df.select_dtypes(include="number")
15:         if numeric.empty:
16:             return PluginResult("skipped", "No numeric columns", {}, [], [], None)
17:         numeric = numeric.dropna(axis=0, how="any")
18:         if numeric.empty:
19:             return PluginResult(
20:                 "skipped", "No complete numeric rows", {"count": 0}, [], [], None
21:             )
22: 
23:         max_cols = int(ctx.settings.get("max_target_cols", 5))
24:         alpha = float(ctx.settings.get("alpha", 0.1))
25:         findings = []
26:         artifacts = []
27:         artifacts_dir = ctx.artifacts_dir("analysis_conformal_feature_prediction")
28: 
29:         for target in list(numeric.columns)[:max_cols]:
30:             y = numeric[target].to_numpy()
31:             X = numeric.drop(columns=[target]).to_numpy()
32:             if X.size == 0:
33:                 continue
34:             n = len(y)
35:             n_train = int(0.6 * n)
36:             n_calib = int(0.2 * n)
37:             train_idx = slice(0, n_train)
38:             calib_idx = slice(n_train, n_train + n_calib)
39:             test_idx = slice(n_train + n_calib, n)
40: 
41:             model = Ridge(alpha=1.0)
42:             model.fit(X[train_idx], y[train_idx])
43:             calib_pred = model.predict(X[calib_idx])
44:             resid = np.abs(y[calib_idx] - calib_pred)
45:             q = np.quantile(resid, 1 - alpha)
46:             test_pred = model.predict(X[test_idx])
47:             lower = test_pred - q
48:             upper = test_pred + q
49:             test_y = y[test_idx]
50:             anomaly_mask = (test_y < lower) | (test_y > upper)
51:             test_indices = np.arange(n)[test_idx]
52: 
53:             for idx, is_anom, lo, hi, score in zip(
54:                 test_indices, anomaly_mask, lower, upper, np.abs(test_y - test_pred)
55:             ):
56:                 if is_anom:
57:                     findings.append(
58:                         {
59:                             "kind": "anomaly",
60:                             "column": target,
61:                             "row_index": int(idx),
62:                             "score": float(score),
63:                             "lower": float(lo),
64:                             "upper": float(hi),
65:                         }
66:                     )
67: 
68:         anomalies_path = artifacts_dir / "anomalies.json"
69:         write_json(anomalies_path, findings)
70:         artifacts.append(
71:             PluginArtifact(
72:                 path=str(anomalies_path.relative_to(ctx.run_dir)),
73:                 type="json",
74:                 description="Anomalies",
75:             )
76:         )
77: 
78:         if not findings:
79:             return PluginResult(
80:                 "skipped",
81:                 "No anomalies detected",
82:                 {"count": 0},
83:                 [],
84:                 artifacts,
85:                 None,
86:             )
87: 
88:         return PluginResult(
89:             "ok",
90:             "Computed conformal anomalies",
91:             {"count": len(findings)},
92:             findings,
93:             artifacts,
94:             None,
95:         )
````

## File: plugins/analysis_dependency_resolution_join/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.column_inference import infer_timestamp_series
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: INVALID_STRINGS = {"", "nan", "none", "null"}
 13: MAX_SAMPLE_ROWS = 50000
 14: MIN_DATETIME_RATIO = 0.15
 15: MIN_OVERLAP_RATIO = 0.02
 16: MIN_OVERLAP_COUNT = 10
 17: 
 18: 
 19: def _normalize_text(value: Any) -> str:
 20:     if value is None:
 21:         return ""
 22:     if isinstance(value, float) and pd.isna(value):
 23:         return ""
 24:     return str(value).strip()
 25: 
 26: 
 27: def _candidate_columns(
 28:     preferred: str | None,
 29:     columns: list[str],
 30:     role_by_name: dict[str, str],
 31:     roles: set[str],
 32:     patterns: list[str],
 33:     lower_names: dict[str, str],
 34: ) -> list[str]:
 35:     seen: set[str] = set()
 36:     candidates: list[str] = []
 37:     if preferred and preferred in columns and preferred not in seen:
 38:         candidates.append(preferred)
 39:         seen.add(preferred)
 40:     for col in columns:
 41:         if role_by_name.get(col) in roles and col not in seen:
 42:             candidates.append(col)
 43:             seen.add(col)
 44:     for col in columns:
 45:         if col in seen:
 46:             continue
 47:         name = lower_names[col]
 48:         if any(pattern in name for pattern in patterns):
 49:             candidates.append(col)
 50:             seen.add(col)
 51:     return candidates
 52: 
 53: 
 54: def _sample_series(series: pd.Series, max_rows: int = MAX_SAMPLE_ROWS) -> pd.Series:
 55:     if len(series) > max_rows:
 56:         return series.head(max_rows)
 57:     return series
 58: 
 59: 
 60: def _normalize_values(series: pd.Series) -> pd.Series:
 61:     sample = _sample_series(series)
 62:     values = sample.dropna().astype(str).str.strip()
 63:     if values.empty:
 64:         return values
 65:     values = values[~values.str.lower().isin(INVALID_STRINGS)]
 66:     return values
 67: 
 68: 
 69: def _best_datetime_column(
 70:     candidates: list[str], df: pd.DataFrame, min_ratio: float = MIN_DATETIME_RATIO
 71: ) -> str | None:
 72:     best_col = None
 73:     best_score = 0.0
 74:     for col in candidates:
 75:         info = infer_timestamp_series(df[col], name_hint=col, sample_size=MAX_SAMPLE_ROWS)
 76:         if not info.valid or info.parse_ratio < min_ratio:
 77:             continue
 78:         if info.score > best_score:
 79:             best_score = info.score
 80:             best_col = col
 81:     if best_score <= 0.0:
 82:         return None
 83:     return best_col
 84: 
 85: 
 86: def _best_dependency_pair(
 87:     dep_candidates: list[str],
 88:     id_candidates: list[str],
 89:     df: pd.DataFrame,
 90: ) -> tuple[str | None, str | None]:
 91:     row_count = len(df)
 92:     min_overlap_count = 1 if row_count < 500 else MIN_OVERLAP_COUNT
 93:     min_overlap_ratio = 0.0 if row_count < 500 else MIN_OVERLAP_RATIO
 94:     id_values: dict[str, set[str]] = {}
 95:     for col in id_candidates:
 96:         values = _normalize_values(df[col])
 97:         if not values.empty:
 98:             id_values[col] = set(values.tolist())
 99:     if not id_values:
100:         return None, None
101: 
102:     best_dep = None
103:     best_id = None
104:     best_ratio = 0.0
105:     for dep_col in dep_candidates:
106:         dep_values = _normalize_values(df[dep_col])
107:         if dep_values.empty:
108:             continue
109:         dep_set = set(dep_values.tolist())
110:         if len(dep_set) < min_overlap_count:
111:             continue
112:         for id_col, id_set in id_values.items():
113:             if id_col == dep_col:
114:                 continue
115:             if not id_set:
116:                 continue
117:             overlap = dep_set & id_set
118:             if len(overlap) < min_overlap_count:
119:                 continue
120:             ratio = len(overlap) / max(len(dep_set), 1)
121:             if ratio > best_ratio:
122:                 best_ratio = ratio
123:                 best_dep = dep_col
124:                 best_id = id_col
125:     if best_ratio < min_overlap_ratio:
126:         return None, None
127:     return best_dep, best_id
128: 
129: 
130: class Plugin:
131:     def run(self, ctx) -> PluginResult:
132:         df = ctx.dataset_loader()
133:         if df.empty:
134:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
135: 
136:         columns_meta = []
137:         role_by_name: dict[str, str] = {}
138:         if ctx.dataset_version_id:
139:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
140:             if dataset_template and dataset_template.get("status") == "ready":
141:                 fields = ctx.storage.fetch_template_fields(
142:                     int(dataset_template["template_id"])
143:                 )
144:                 columns_meta = fields
145:                 role_by_name = {
146:                     field["name"]: (field.get("role") or "") for field in fields
147:                 }
148:             else:
149:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
150:                 role_by_name = {
151:                     col["original_name"]: (col.get("role") or "")
152:                     for col in columns_meta
153:                 }
154: 
155:         columns = list(df.columns)
156:         lower_names = {col: str(col).lower() for col in columns}
157: 
158:         dep_candidates = _candidate_columns(
159:             ctx.settings.get("dependency_column"),
160:             columns,
161:             role_by_name,
162:             {"dependency_id"},
163:             ["dep", "dependency", "parent", "prereq", "preced"],
164:             lower_names,
165:         )
166:         id_candidates = _candidate_columns(
167:             ctx.settings.get("id_column"),
168:             columns,
169:             role_by_name,
170:             {"process_id", "id"},
171:             ["process_id", "proc_id", "queue_id", "job_id", "run_id", "id"],
172:             lower_names,
173:         )
174:         start_candidates = _candidate_columns(
175:             ctx.settings.get("start_column"),
176:             columns,
177:             role_by_name,
178:             {"start_time", "start"},
179:             ["start", "begin"],
180:             lower_names,
181:         )
182:         end_candidates = _candidate_columns(
183:             ctx.settings.get("end_column"),
184:             columns,
185:             role_by_name,
186:             {"end_time", "end"},
187:             ["end", "finish", "complete", "stop"],
188:             lower_names,
189:         )
190: 
191:         dep_col, id_col = _best_dependency_pair(dep_candidates, id_candidates, df)
192:         start_col = _best_datetime_column(start_candidates, df)
193:         end_col = _best_datetime_column(end_candidates, df)
194: 
195:         if not dep_col or not id_col or not start_col or not end_col:
196:             return PluginResult(
197:                 "ok",
198:                 "Dependency join not applicable",
199:                 {
200:                     "dependency_rows": 0,
201:                     "near_zero_ratio": 0.0,
202:                 },
203:                 [
204:                     {
205:                         "kind": "dependency_lag_summary",
206:                         "measurement_type": "not_applicable",
207:                         "reason": "Missing dependency/id/start/end columns.",
208:                         "columns": [
209:                             col
210:                             for col in [dep_col, id_col, start_col, end_col]
211:                             if col
212:                         ],
213:                     }
214:                 ],
215:                 [],
216:                 None,
217:             )
218: 
219:         work = df.loc[:, [dep_col, id_col, start_col, end_col]].copy()
220:         work["__dep"] = work[dep_col].map(_normalize_text)
221:         work["__id"] = work[id_col].map(_normalize_text)
222:         work["__start_ts"] = pd.to_datetime(work[start_col], errors="coerce", utc=False)
223:         work["__end_ts"] = pd.to_datetime(work[end_col], errors="coerce", utc=False)
224: 
225:         work = work.loc[~work["__dep"].str.lower().isin(INVALID_STRINGS)].copy()
226:         if work.empty:
227:             return PluginResult(
228:                 "ok",
229:                 "No dependency rows found",
230:                 {"dependency_rows": 0, "near_zero_ratio": 0.0},
231:                 [
232:                     {
233:                         "kind": "dependency_lag_summary",
234:                         "measurement_type": "not_applicable",
235:                         "reason": "No dependency ids present.",
236:                         "columns": [dep_col, id_col, start_col, end_col],
237:                     }
238:                 ],
239:                 [],
240:                 None,
241:             )
242: 
243:         end_lookup = (
244:             work.loc[work["__id"].astype(bool) & work["__end_ts"].notna()]
245:             .groupby("__id")["__end_ts"]
246:             .max()
247:         )
248:         work["__dep_end_ts"] = work["__dep"].map(end_lookup)
249:         work = work.loc[work["__dep_end_ts"].notna() & work["__start_ts"].notna()].copy()
250:         if work.empty:
251:             return PluginResult(
252:                 "ok",
253:                 "No dependency end timestamps found",
254:                 {"dependency_rows": 0, "near_zero_ratio": 0.0},
255:                 [
256:                     {
257:                         "kind": "dependency_lag_summary",
258:                         "measurement_type": "not_applicable",
259:                         "reason": "No matching dependency end times.",
260:                         "columns": [dep_col, id_col, start_col, end_col],
261:                     }
262:                 ],
263:                 [],
264:                 None,
265:             )
266: 
267:         lag_sec = (work["__start_ts"] - work["__dep_end_ts"]).dt.total_seconds()
268:         lag_sec = lag_sec.clip(lower=0).fillna(0)
269:         work["__lag_sec"] = lag_sec
270: 
271:         threshold = float(ctx.settings.get("near_zero_threshold_seconds", 2))
272:         near_zero = work["__lag_sec"] <= threshold
273:         near_zero_ratio = float(near_zero.mean()) if len(work) else 0.0
274: 
275:         p50 = float(work["__lag_sec"].quantile(0.5))
276:         p95 = float(work["__lag_sec"].quantile(0.95))
277:         p99 = float(work["__lag_sec"].quantile(0.99))
278: 
279:         max_examples = int(ctx.settings.get("max_examples", 25))
280:         example_rows = work.loc[near_zero].index.tolist()[:max_examples]
281: 
282:         findings = [
283:             {
284:                 "kind": "dependency_lag_summary",
285:                 "dependency_rows": int(len(work)),
286:                 "near_zero_ratio": near_zero_ratio,
287:                 "near_zero_threshold_seconds": threshold,
288:                 "lag_p50_seconds": p50,
289:                 "lag_p95_seconds": p95,
290:                 "lag_p99_seconds": p99,
291:                 "measurement_type": "measured",
292:                 "row_ids": [int(i) for i in example_rows],
293:                 "columns": [dep_col, id_col, start_col, end_col],
294:             }
295:         ]
296: 
297:         artifacts_dir = ctx.artifacts_dir("analysis_dependency_resolution_join")
298:         out_path = artifacts_dir / "dependency_lag.json"
299:         write_json(
300:             out_path,
301:             {
302:                 "summary": {
303:                     "dependency_column": dep_col,
304:                     "id_column": id_col,
305:                     "start_column": start_col,
306:                     "end_column": end_col,
307:                     "near_zero_threshold_seconds": threshold,
308:                 },
309:                 "stats": {
310:                     "dependency_rows": int(len(work)),
311:                     "near_zero_ratio": near_zero_ratio,
312:                     "lag_p50_seconds": p50,
313:                     "lag_p95_seconds": p95,
314:                     "lag_p99_seconds": p99,
315:                 },
316:             },
317:         )
318:         artifacts = [
319:             PluginArtifact(
320:                 path=str(out_path.relative_to(ctx.run_dir)),
321:                 type="json",
322:                 description="Dependency lag summary",
323:             )
324:         ]
325: 
326:         return PluginResult(
327:             "ok",
328:             "Computed dependency lag distribution",
329:             {
330:                 "dependency_rows": int(len(work)),
331:                 "near_zero_ratio": near_zero_ratio,
332:             },
333:             findings,
334:             artifacts,
335:             None,
336:         )
````

## File: plugins/analysis_gaussian_copula_shift/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: import time
  4: 
  5: import numpy as np
  6: 
  7: from statistic_harness.core.types import PluginArtifact, PluginResult
  8: from statistic_harness.core.stat_controls import benjamini_hochberg, confidence_from_p
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: class Plugin:
 13:     def run(self, ctx) -> PluginResult:
 14:         df = ctx.dataset_loader()
 15:         numeric = df.select_dtypes(include="number")
 16:         if numeric.shape[1] < 2:
 17:             return PluginResult(
 18:                 "skipped", "Not enough numeric columns", {}, [], [], None
 19:             )
 20:         values = numeric.to_numpy()
 21:         n = values.shape[0]
 22:         max_rows = ctx.settings.get("max_rows")
 23:         row_limit = ctx.budget.get("row_limit") or max_rows
 24:         sampled = False
 25:         if row_limit and n > int(row_limit):
 26:             rng = np.random.default_rng(ctx.run_seed)
 27:             idx = rng.choice(n, size=int(row_limit), replace=False)
 28:             values = values[idx]
 29:             n = values.shape[0]
 30:             sampled = True
 31:         split = n // 2
 32:         if split == 0:
 33:             return PluginResult(
 34:                 "skipped", "Not enough rows for split", {}, [], [], None
 35:             )
 36:         first = values[:split]
 37:         second = values[split:]
 38: 
 39:         def inv_norm(p: np.ndarray) -> np.ndarray:
 40:             # Approximation by Peter John Acklam, implemented for array.
 41:             a = [
 42:                 -3.969683028665376e01,
 43:                 2.209460984245205e02,
 44:                 -2.759285104469687e02,
 45:                 1.383577518672690e02,
 46:                 -3.066479806614716e01,
 47:                 2.506628277459239e00,
 48:             ]
 49:             b = [
 50:                 -5.447609879822406e01,
 51:                 1.615858368580409e02,
 52:                 -1.556989798598866e02,
 53:                 6.680131188771972e01,
 54:                 -1.328068155288572e01,
 55:             ]
 56:             c = [
 57:                 -7.784894002430293e-03,
 58:                 -3.223964580411365e-01,
 59:                 -2.400758277161838e00,
 60:                 -2.549732539343734e00,
 61:                 4.374664141464968e00,
 62:                 2.938163982698783e00,
 63:             ]
 64:             d = [
 65:                 7.784695709041462e-03,
 66:                 3.224671290700398e-01,
 67:                 2.445134137142996e00,
 68:                 3.754408661907416e00,
 69:             ]
 70:             plow = 0.02425
 71:             phigh = 1 - plow
 72:             x = np.zeros_like(p)
 73:             lower = p < plow
 74:             upper = p > phigh
 75:             mid = (~lower) & (~upper)
 76:             q = np.sqrt(-2 * np.log(p[lower]))
 77:             x[lower] = (
 78:                 ((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5]
 79:             ) / ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1)
 80:             q = p[mid] - 0.5
 81:             r = q * q
 82:             x[mid] = (
 83:                 (((((a[0] * r + a[1]) * r + a[2]) * r + a[3]) * r + a[4]) * r + a[5])
 84:                 * q
 85:                 / (((((b[0] * r + b[1]) * r + b[2]) * r + b[3]) * r + b[4]) * r + 1)
 86:             )
 87:             q = np.sqrt(-2 * np.log(1 - p[upper]))
 88:             x[upper] = -(
 89:                 (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])
 90:                 / ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1)
 91:             )
 92:             return x
 93: 
 94:         def to_gaussian(data: np.ndarray) -> np.ndarray:
 95:             ranks = np.argsort(np.argsort(data, axis=0), axis=0) + 1
 96:             u = ranks / (data.shape[0] + 1)
 97:             u = np.clip(u, 1e-6, 1 - 1e-6)
 98:             return inv_norm(u)
 99: 
100:         z1 = to_gaussian(first)
101:         z2 = to_gaussian(second)
102:         corr1 = np.corrcoef(z1, rowvar=False)
103:         corr2 = np.corrcoef(z2, rowvar=False)
104:         delta = np.abs(corr1 - corr2)
105: 
106:         pairs = []
107:         for i in range(delta.shape[0]):
108:             for j in range(i + 1, delta.shape[1]):
109:                 pairs.append(
110:                     (i, j, (numeric.columns[i], numeric.columns[j]), float(delta[i, j]))
111:                 )
112:         pairs.sort(key=lambda x: x[3], reverse=True)
113:         max_pairs = int(ctx.settings.get("max_pairs", 5))
114:         selected = pairs[:max_pairs]
115: 
116:         rng = np.random.default_rng(ctx.run_seed)
117:         findings = []
118:         time_limit_ms = ctx.budget.get("time_limit_ms")
119:         start_time = time.perf_counter()
120:         time_limit_hit = False
121:         perms_run = 0
122:         for i, j, (a, b), score in selected:
123:             p_value = 1.0
124:             perm_scores = []
125:             for _ in range(int(ctx.settings.get("n_permutations", 100))):
126:                 if time_limit_ms and (time.perf_counter() - start_time) * 1000 > int(
127:                     time_limit_ms
128:                 ):
129:                     time_limit_hit = True
130:                     break
131:                 perm = rng.permutation(values)
132:                 perm_first = perm[:split]
133:                 perm_second = perm[split:]
134:                 z1p = to_gaussian(perm_first)
135:                 z2p = to_gaussian(perm_second)
136:                 delta_p = np.abs(
137:                     np.corrcoef(z1p, rowvar=False) - np.corrcoef(z2p, rowvar=False)
138:                 )
139:                 perm_scores.append(delta_p[i, j])
140:                 perms_run += 1
141:             if time_limit_hit:
142:                 break
143:             if perm_scores:
144:                 p_value = float((np.array(perm_scores) >= score).mean())
145:             findings.append(
146:                 {
147:                     "kind": "dependence_shift",
148:                     "pair": [a, b],
149:                     "delta": score,
150:                     "p_value": p_value,
151:                 }
152:             )
153:         if time_limit_hit:
154:             ctx.logger("analysis_gaussian_copula_shift hit time limit")
155:         if findings:
156:             q_values = benjamini_hochberg([f["p_value"] for f in findings])
157:             for finding, q_value in zip(findings, q_values):
158:                 finding["q_value"] = q_value
159:                 finding["confidence"] = confidence_from_p(q_value)
160: 
161:         artifacts_dir = ctx.artifacts_dir("analysis_gaussian_copula_shift")
162:         out_path = artifacts_dir / "summary.json"
163:         write_json(out_path, findings)
164:         artifacts = [
165:             PluginArtifact(
166:                 path=str(out_path.relative_to(ctx.run_dir)),
167:                 type="json",
168:                 description="Dependence shift",
169:             )
170:         ]
171:         summary = "Computed copula shift"
172:         if time_limit_hit:
173:             summary = "Computed copula shift (time limit hit)"
174:         return PluginResult(
175:             "ok",
176:             summary,
177:             {
178:                 "pairs": len(findings),
179:                 "rows_used": n,
180:                 "sampled": sampled,
181:                 "permutations_run": perms_run,
182:                 "time_limit_hit": time_limit_hit,
183:             },
184:             findings,
185:             artifacts,
186:             None,
187:         )
````

## File: plugins/analysis_gaussian_copula_shift/plugin.yaml
````yaml
 1: id: analysis_gaussian_copula_shift
 2: name: Gaussian Copula Shift
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_multi_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Dependence shift detection
12:   defaults:
13:     segment: halves
14:     max_pairs: 5
15:     n_permutations: 100
16:     max_rows: 20000
17:     budget:
18:       time_limit_ms: 60000
19: config_schema: config.schema.json
20: output_schema: output.schema.json
21: sandbox:
22:   no_network: true
23:   fs_allowlist:
24:   - appdata
25:   - plugins
````

## File: plugins/analysis_knockoff_wrapper_rf/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import numpy as np
 4: from sklearn.ensemble import RandomForestRegressor
 5: 
 6: from statistic_harness.core.types import PluginArtifact, PluginResult
 7: from statistic_harness.core.utils import write_json
 8: 
 9: 
10: class Plugin:
11:     def run(self, ctx) -> PluginResult:
12:         df = ctx.dataset_loader()
13:         numeric = df.select_dtypes(include="number")
14:         if numeric.shape[1] < 2:
15:             return PluginResult(
16:                 "skipped", "Not enough numeric columns", {}, [], [], None
17:             )
18:         numeric = numeric.dropna(axis=0, how="any")
19:         if numeric.empty:
20:             return PluginResult(
21:                 "skipped", "No complete numeric rows", {}, [], [], None
22:             )
23:         target_column = ctx.settings.get("target_column") or numeric.columns[-1]
24:         y = numeric[target_column].to_numpy()
25:         X = numeric.drop(columns=[target_column])
26:         rng = np.random.default_rng(ctx.run_seed)
27:         rf = RandomForestRegressor(
28:             n_estimators=int(ctx.settings.get("n_estimators", 50)),
29:             random_state=ctx.run_seed,
30:         )
31:         rf.fit(X, y)
32:         importances = rf.feature_importances_
33:         knockoff = X.apply(lambda col: rng.permutation(col.to_numpy()))
34:         rf_knock = RandomForestRegressor(
35:             n_estimators=int(ctx.settings.get("n_estimators", 50)),
36:             random_state=ctx.run_seed,
37:         )
38:         rf_knock.fit(knockoff, y)
39:         knock_imp = rf_knock.feature_importances_
40: 
41:         scores = importances - knock_imp
42:         threshold = np.quantile(scores, 1 - float(ctx.settings.get("fdr_q", 0.1)))
43:         findings = []
44:         for feature, score in zip(X.columns, scores):
45:             selected = bool(score >= threshold)
46:             findings.append(
47:                 {
48:                     "kind": "feature_discovery",
49:                     "feature": feature,
50:                     "score": float(score),
51:                     "selected": selected,
52:                 }
53:             )
54: 
55:         artifacts_dir = ctx.artifacts_dir("analysis_knockoff_wrapper_rf")
56:         selection_path = artifacts_dir / "selection.json"
57:         write_json(selection_path, findings)
58:         artifacts = [
59:             PluginArtifact(
60:                 path=str(selection_path.relative_to(ctx.run_dir)),
61:                 type="json",
62:                 description="Selection",
63:             )
64:         ]
65:         selected_count = sum(1 for f in findings if f["selected"])
66:         return PluginResult(
67:             "ok",
68:             "Computed RF knockoff selection",
69:             {"selected": selected_count},
70:             findings,
71:             artifacts,
72:             None,
73:         )
````

## File: plugins/analysis_percentile_analysis/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.column_inference import choose_timestamp_column
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: def _pick_column(
 13:     preferred: str | None,
 14:     columns: list[str],
 15:     role_by_name: dict[str, str],
 16:     roles: set[str],
 17:     patterns: list[str],
 18:     lower_names: dict[str, str],
 19: ) -> str | None:
 20:     if preferred and preferred in columns:
 21:         return preferred
 22:     for col in columns:
 23:         if role_by_name.get(col) in roles:
 24:             return col
 25:     for col in columns:
 26:         name = lower_names[col]
 27:         if any(pattern in name for pattern in patterns):
 28:             return col
 29:     return None
 30: 
 31: 
 32: def _pick_timestamp_column(
 33:     preferred: str | None,
 34:     columns: list[str],
 35:     role_by_name: dict[str, str],
 36:     roles: set[str],
 37:     patterns: list[str],
 38:     lower_names: dict[str, str],
 39:     df: pd.DataFrame,
 40: ) -> str | None:
 41:     candidates: list[str] = []
 42:     if preferred and preferred in columns:
 43:         candidates.append(preferred)
 44:     for col in columns:
 45:         if role_by_name.get(col) in roles and col not in candidates:
 46:             candidates.append(col)
 47:     for col in columns:
 48:         name = lower_names[col]
 49:         if any(pattern in name for pattern in patterns) and col not in candidates:
 50:             candidates.append(col)
 51:     if not candidates:
 52:         return None
 53:     return choose_timestamp_column(df, candidates)
 54: 
 55: 
 56: def _series_from_frame(frame: pd.DataFrame, column: str | None) -> pd.Series | None:
 57:     if not column:
 58:         return None
 59:     data = frame[column]
 60:     if isinstance(data, pd.DataFrame):
 61:         return data.iloc[:, 0]
 62:     return data
 63: 
 64: 
 65: class Plugin:
 66:     def run(self, ctx) -> PluginResult:
 67:         df = ctx.dataset_loader()
 68:         if df.empty:
 69:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 70: 
 71:         columns_meta = []
 72:         role_by_name: dict[str, str] = {}
 73:         if ctx.dataset_version_id:
 74:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 75:             if dataset_template and dataset_template.get("status") == "ready":
 76:                 fields = ctx.storage.fetch_template_fields(
 77:                     int(dataset_template["template_id"])
 78:                 )
 79:                 columns_meta = fields
 80:                 role_by_name = {
 81:                     field["name"]: (field.get("role") or "") for field in fields
 82:                 }
 83:             else:
 84:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 85:                 role_by_name = {
 86:                     col["original_name"]: (col.get("role") or "")
 87:                     for col in columns_meta
 88:                 }
 89: 
 90:         columns = list(df.columns)
 91:         lower_names = {col: str(col).lower() for col in columns}
 92: 
 93:         process_col = _pick_column(
 94:             ctx.settings.get("process_column"),
 95:             columns,
 96:             role_by_name,
 97:             {"process_name", "process_id"},
 98:             ["process", "activity", "event", "step", "task", "action", "job"],
 99:             lower_names,
100:         )
101:         module_col = _pick_column(
102:             ctx.settings.get("module_column"),
103:             columns,
104:             role_by_name,
105:             {"module_code"},
106:             ["module", "mod"],
107:             lower_names,
108:         )
109:         queue_col = _pick_timestamp_column(
110:             ctx.settings.get("queue_column"),
111:             columns,
112:             role_by_name,
113:             {"queue_time"},
114:             ["queue", "queued", "enqueue"],
115:             lower_names,
116:             df,
117:         )
118:         eligible_col = _pick_timestamp_column(
119:             ctx.settings.get("eligible_column"),
120:             columns,
121:             role_by_name,
122:             {"eligible", "ready", "available"},
123:             ["eligible", "ready", "available"],
124:             lower_names,
125:             df,
126:         )
127:         start_col = _pick_timestamp_column(
128:             ctx.settings.get("start_column"),
129:             columns,
130:             role_by_name,
131:             {"start_time", "start"},
132:             ["start", "begin"],
133:             lower_names,
134:             df,
135:         )
136:         end_col = _pick_timestamp_column(
137:             ctx.settings.get("end_column"),
138:             columns,
139:             role_by_name,
140:             {"end_time", "end"},
141:             ["end", "finish", "complete", "stop"],
142:             lower_names,
143:             df,
144:         )
145: 
146:         if not start_col or (not queue_col and not eligible_col):
147:             return PluginResult(
148:                 "ok",
149:                 "Percentile analysis not applicable",
150:                 {"groups": 0},
151:                 [
152:                     {
153:                         "kind": "percentile_stats",
154:                         "measurement_type": "not_applicable",
155:                         "reason": "Missing queue/eligible and start columns.",
156:                         "columns": [
157:                             col
158:                             for col in [queue_col, eligible_col, start_col, end_col]
159:                             if col
160:                         ],
161:                     }
162:                 ],
163:                 [],
164:                 None,
165:             )
166: 
167:         selected = [
168:             col
169:             for col in [
170:                 process_col,
171:                 module_col,
172:                 queue_col,
173:                 eligible_col,
174:                 start_col,
175:                 end_col,
176:             ]
177:             if col
178:         ]
179:         selected = list(dict.fromkeys(selected))
180:         work = df.loc[:, selected].copy()
181:         eligible_series = _series_from_frame(work, eligible_col)
182:         eligible_ts = (
183:             pd.to_datetime(eligible_series, errors="coerce", utc=False)
184:             if eligible_series is not None
185:             else None
186:         )
187:         if eligible_ts is None:
188:             queue_series = _series_from_frame(work, queue_col)
189:             eligible_ts = pd.to_datetime(queue_series, errors="coerce", utc=False)
190:             eligible_basis = "queue"
191:         else:
192:             eligible_basis = "eligible"
193:         start_series = _series_from_frame(work, start_col)
194:         start_ts = pd.to_datetime(start_series, errors="coerce", utc=False)
195:         end_series = _series_from_frame(work, end_col)
196:         end_ts = (
197:             pd.to_datetime(end_series, errors="coerce", utc=False)
198:             if end_series is not None
199:             else None
200:         )
201:         work["__eligible_ts"] = eligible_ts
202:         work["__start_ts"] = start_ts
203:         if end_ts is not None:
204:             work["__end_ts"] = end_ts
205: 
206:         work = work.loc[work["__eligible_ts"].notna() & work["__start_ts"].notna()].copy()
207:         if work.empty:
208:             return PluginResult("ok", "No valid timestamps", {"groups": 0}, [], [], None)
209: 
210:         work["__eligible_wait_sec"] = (
211:             work["__start_ts"] - work["__eligible_ts"]
212:         ).dt.total_seconds().clip(lower=0).fillna(0)
213:         if end_ts is not None:
214:             work["__completion_sec"] = (
215:                 work["__end_ts"] - work["__start_ts"]
216:             ).dt.total_seconds().clip(lower=0).fillna(0)
217: 
218:         group_cols: list[str] = []
219:         if process_col:
220:             group_cols.append(process_col)
221:         if module_col:
222:             group_cols.append(module_col)
223: 
224:         max_groups = int(ctx.settings.get("max_groups", 10))
225:         findings = []
226: 
227:         if group_cols:
228:             grouped = work.groupby(group_cols, dropna=False)
229:         else:
230:             grouped = [("overall", work)]
231: 
232:         for key, frame in grouped:
233:             if isinstance(key, tuple):
234:                 label = {
235:                     "process": key[0] if process_col else None,
236:                     "module": key[1] if module_col and len(key) > 1 else None,
237:                 }
238:             else:
239:                 label = {"process": key if process_col else None, "module": None}
240: 
241:             eligible_quantiles = frame["__eligible_wait_sec"].quantile([0.5, 0.95, 0.99])
242:             completion_quantiles = None
243:             if "__completion_sec" in frame:
244:                 completion_quantiles = frame["__completion_sec"].quantile([0.5, 0.95, 0.99])
245: 
246:             finding = {
247:                 "kind": "percentile_stats",
248:                 "process": label.get("process"),
249:                 "module": label.get("module"),
250:                 "eligible_wait_p50": float(eligible_quantiles.loc[0.5]),
251:                 "eligible_wait_p95": float(eligible_quantiles.loc[0.95]),
252:                 "eligible_wait_p99": float(eligible_quantiles.loc[0.99]),
253:                 "eligible_basis": eligible_basis,
254:                 "measurement_type": "measured",
255:                 "columns": [
256:                     col
257:                     for col in [process_col, module_col, queue_col, eligible_col, start_col, end_col]
258:                     if col
259:                 ],
260:             }
261:             if completion_quantiles is not None:
262:                 finding.update(
263:                     {
264:                         "completion_p50": float(completion_quantiles.loc[0.5]),
265:                         "completion_p95": float(completion_quantiles.loc[0.95]),
266:                         "completion_p99": float(completion_quantiles.loc[0.99]),
267:                     }
268:                 )
269:             findings.append(finding)
270:             if len(findings) >= max_groups:
271:                 break
272: 
273:         artifacts_dir = ctx.artifacts_dir("analysis_percentile_analysis")
274:         out_path = artifacts_dir / "percentiles.json"
275:         write_json(
276:             out_path,
277:             {
278:                 "summary": {
279:                     "process_column": process_col,
280:                     "module_column": module_col,
281:                     "queue_column": queue_col,
282:                     "eligible_column": eligible_col,
283:                     "start_column": start_col,
284:                     "end_column": end_col,
285:                     "eligible_basis": eligible_basis,
286:                 },
287:                 "groups": len(findings),
288:             },
289:         )
290:         artifacts = [
291:             PluginArtifact(
292:                 path=str(out_path.relative_to(ctx.run_dir)),
293:                 type="json",
294:                 description="Percentile analysis summary",
295:             )
296:         ]
297: 
298:         return PluginResult(
299:             "ok",
300:             "Computed percentile statistics",
301:             {"groups": len(findings)},
302:             findings,
303:             artifacts,
304:             None,
305:         )
````

## File: plugins/analysis_queue_delay_decomposition/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.column_inference import infer_timestamp_series
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: INVALID_STRINGS = {"", "nan", "none", "null"}
 13: MAX_SAMPLE_ROWS = 50000
 14: MIN_PROCESS_ALPHA_RATIO = 0.15
 15: MIN_DATETIME_RATIO = 0.15
 16: 
 17: 
 18: def _normalize_process(value: Any) -> str:
 19:     if value is None:
 20:         return ""
 21:     if isinstance(value, float) and pd.isna(value):
 22:         return ""
 23:     return str(value).strip()
 24: 
 25: 
 26: def _candidate_columns(
 27:     preferred: str | None,
 28:     columns: list[str],
 29:     role_by_name: dict[str, str],
 30:     roles: set[str],
 31:     patterns: list[str],
 32:     lower_names: dict[str, str],
 33:     exclude: set[str],
 34: ) -> list[str]:
 35:     seen: set[str] = set()
 36:     candidates: list[str] = []
 37:     if preferred and preferred in columns and preferred not in exclude and preferred not in seen:
 38:         candidates.append(preferred)
 39:         seen.add(preferred)
 40:     for col in columns:
 41:         if col in exclude or col in seen:
 42:             continue
 43:         if role_by_name.get(col) in roles:
 44:             candidates.append(col)
 45:             seen.add(col)
 46:     for col in columns:
 47:         if col in exclude or col in seen:
 48:             continue
 49:         name = lower_names[col]
 50:         if any(pattern in name for pattern in patterns):
 51:             candidates.append(col)
 52:             seen.add(col)
 53:     return candidates
 54: 
 55: 
 56: def _parse_list(value: Any) -> list[str]:
 57:     if value is None:
 58:         return []
 59:     if isinstance(value, str):
 60:         return [item.strip() for item in value.split(",") if item.strip()]
 61:     if isinstance(value, (list, tuple, set)):
 62:         return [str(item).strip() for item in value if str(item).strip()]
 63:     return []
 64: 
 65: 
 66: def _sample_series(series: pd.Series, max_rows: int = MAX_SAMPLE_ROWS) -> pd.Series:
 67:     if len(series) > max_rows:
 68:         return series.head(max_rows)
 69:     return series
 70: 
 71: 
 72: def _normalize_values(series: pd.Series) -> pd.Series:
 73:     sample = _sample_series(series)
 74:     values = sample.dropna().astype(str).str.strip()
 75:     if values.empty:
 76:         return values
 77:     values = values[~values.str.lower().isin(INVALID_STRINGS)]
 78:     return values
 79: 
 80: 
 81: def _score_process_column(series: pd.Series) -> tuple[float, float]:
 82:     values = _normalize_values(series)
 83:     if values.empty:
 84:         return -1.0, 0.0
 85:     alpha_ratio = float(values.str.contains(r"[A-Za-z]", regex=True).mean())
 86:     digit_ratio = float(values.str.match(r"^\\d+(\\.0+)?$").mean())
 87:     unique_ratio = float(values.nunique() / max(len(values), 1))
 88:     score = alpha_ratio * 3.0 - digit_ratio * 2.0 + min(unique_ratio, 1.0) * 0.2
 89:     return score, alpha_ratio
 90: 
 91: 
 92: def _best_process_column(candidates: list[str], df: pd.DataFrame) -> str | None:
 93:     best_col = None
 94:     best_score = -1.0
 95:     best_alpha = 0.0
 96:     for col in candidates:
 97:         score, alpha = _score_process_column(df[col])
 98:         if score > best_score:
 99:             best_col = col
100:             best_score = score
101:             best_alpha = alpha
102:     if best_col is None or best_alpha < MIN_PROCESS_ALPHA_RATIO:
103:         return None
104:     return best_col
105: 
106: 
107: def _best_datetime_column(
108:     candidates: list[str], df: pd.DataFrame, min_ratio: float = MIN_DATETIME_RATIO
109: ) -> str | None:
110:     best_col = None
111:     best_score = 0.0
112:     for col in candidates:
113:         info = infer_timestamp_series(df[col], name_hint=col, sample_size=MAX_SAMPLE_ROWS)
114:         if not info.valid or info.parse_ratio < min_ratio:
115:             continue
116:         if info.score > best_score:
117:             best_score = info.score
118:             best_col = col
119:     return best_col
120: 
121: 
122: class Plugin:
123:     def run(self, ctx) -> PluginResult:
124:         df = ctx.dataset_loader()
125:         if df.empty:
126:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
127: 
128:         columns_meta = []
129:         role_by_name: dict[str, str] = {}
130:         if ctx.dataset_version_id:
131:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
132:             if dataset_template and dataset_template.get("status") == "ready":
133:                 fields = ctx.storage.fetch_template_fields(
134:                     int(dataset_template["template_id"])
135:                 )
136:                 columns_meta = fields
137:                 role_by_name = {
138:                     field["name"]: (field.get("role") or "") for field in fields
139:                 }
140:             else:
141:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
142:                 role_by_name = {
143:                     col["original_name"]: (col.get("role") or "")
144:                     for col in columns_meta
145:                 }
146: 
147:         columns = list(df.columns)
148:         lower_names = {col: str(col).lower() for col in columns}
149:         used: set[str] = set()
150: 
151:         process_candidates = _candidate_columns(
152:             ctx.settings.get("process_column"),
153:             columns,
154:             role_by_name,
155:             {"process", "activity", "event", "step", "task", "action"},
156:             ["process", "activity", "event", "step", "task", "action", "job"],
157:             lower_names,
158:             used,
159:         )
160:         process_col = _best_process_column(process_candidates, df)
161:         if process_col:
162:             used.add(process_col)
163: 
164:         queue_candidates = _candidate_columns(
165:             ctx.settings.get("queue_column"),
166:             columns,
167:             role_by_name,
168:             {"queue", "queued", "enqueue"},
169:             ["queue", "queued", "enqueue"],
170:             lower_names,
171:             used,
172:         )
173:         queue_col = _best_datetime_column(queue_candidates, df)
174:         if queue_col:
175:             used.add(queue_col)
176: 
177:         eligible_candidates = _candidate_columns(
178:             ctx.settings.get("eligible_column"),
179:             columns,
180:             role_by_name,
181:             {"eligible", "ready", "available"},
182:             ["eligible", "ready", "available"],
183:             lower_names,
184:             used,
185:         )
186:         eligible_col = _best_datetime_column(eligible_candidates, df)
187:         if eligible_col:
188:             used.add(eligible_col)
189: 
190:         start_candidates = _candidate_columns(
191:             ctx.settings.get("start_column"),
192:             columns,
193:             role_by_name,
194:             {"start"},
195:             ["start", "begin"],
196:             lower_names,
197:             used,
198:         )
199:         start_col = _best_datetime_column(start_candidates, df)
200:         if start_col:
201:             used.add(start_col)
202: 
203:         end_candidates = _candidate_columns(
204:             ctx.settings.get("end_column"),
205:             columns,
206:             role_by_name,
207:             {"end", "finish", "complete"},
208:             ["end", "finish", "complete", "stop"],
209:             lower_names,
210:             used,
211:         )
212:         end_col = _best_datetime_column(end_candidates, df)
213:         if end_col:
214:             used.add(end_col)
215: 
216:         if not process_col:
217:             return PluginResult(
218:                 "skipped", "No process/activity column detected", {}, [], [], None
219:             )
220:         if not start_col or (not queue_col and not eligible_col):
221:             return PluginResult(
222:                 "skipped",
223:                 "Queue/start timestamps required for delay decomposition",
224:                 {},
225:                 [],
226:                 [],
227:                 None,
228:             )
229: 
230:         dep_cols = _parse_list(ctx.settings.get("dependency_columns"))
231:         if not dep_cols:
232:             dep_tokens = ("dep", "dependency", "parent", "master", "preced")
233:             dep_cols = [
234:                 col for col in columns if any(tok in lower_names[col] for tok in dep_tokens)
235:             ]
236: 
237:         selected_cols: list[str] = []
238:         for col in [process_col, queue_col, eligible_col, start_col, end_col, *dep_cols]:
239:             if col and col in columns and col not in selected_cols:
240:                 selected_cols.append(col)
241:         work = df.loc[:, selected_cols].copy()
242: 
243:         work["__process"] = work[process_col].map(_normalize_process)
244:         work["__process_norm"] = work["__process"].str.lower()
245:         work = work.loc[~work["__process_norm"].isin(INVALID_STRINGS)].copy()
246:         if work.empty:
247:             return PluginResult("skipped", "No valid process values", {}, [], [], None)
248: 
249:         queue_ts = pd.to_datetime(work[queue_col], errors="coerce", utc=False) if queue_col else None
250:         eligible_ts = pd.to_datetime(work[eligible_col], errors="coerce", utc=False) if eligible_col else None
251:         start_ts = pd.to_datetime(work[start_col], errors="coerce", utc=False)
252:         if eligible_ts is None:
253:             eligible_ts = queue_ts
254:             eligible_basis = "queue"
255:         else:
256:             eligible_basis = "eligible"
257: 
258:         if queue_ts is None and eligible_ts is None:
259:             return PluginResult("skipped", "No queue/eligible timestamps", {}, [], [], None)
260: 
261:         work["__start_ts"] = start_ts
262:         work["__queue_ts"] = queue_ts if queue_ts is not None else eligible_ts
263:         work["__eligible_ts"] = eligible_ts
264:         work = work.loc[work["__start_ts"].notna() & work["__eligible_ts"].notna()].copy()
265:         if work.empty:
266:             return PluginResult("skipped", "No valid timestamps found", {}, [], [], None)
267: 
268:         wait_pre = (work["__eligible_ts"] - work["__queue_ts"]).dt.total_seconds()
269:         wait_pre = wait_pre.clip(lower=0).fillna(0)
270:         eligible_wait = (work["__start_ts"] - work["__eligible_ts"]).dt.total_seconds()
271:         eligible_wait = eligible_wait.clip(lower=0).fillna(0)
272: 
273:         work["__wait_pre_sec"] = wait_pre
274:         work["__eligible_wait_sec"] = eligible_wait
275: 
276:         threshold = float(ctx.settings.get("wait_threshold_seconds", 60))
277:         work["__eligible_wait_gt_sec"] = eligible_wait.where(eligible_wait > threshold, 0.0)
278:         wait_to_start = (work["__start_ts"] - work["__queue_ts"]).dt.total_seconds()
279:         wait_to_start = wait_to_start.clip(lower=0).fillna(0)
280:         work["__wait_to_start_sec"] = wait_to_start
281:         work["__wait_to_start_gt_sec"] = wait_to_start.where(wait_to_start > threshold, 0.0)
282: 
283:         close_start = int(ctx.settings.get("close_cycle_start_day", 20))
284:         close_end = int(ctx.settings.get("close_cycle_end_day", 5))
285:         work["__day"] = work["__start_ts"].dt.day
286:         if close_start <= close_end:
287:             work["__close"] = (work["__day"] >= close_start) & (work["__day"] <= close_end)
288:         else:
289:             work["__close"] = (work["__day"] >= close_start) | (work["__day"] <= close_end)
290: 
291:         sequence_mask = pd.Series(False, index=work.index)
292:         for col in dep_cols:
293:             if col not in work.columns:
294:                 continue
295:             series = work[col]
296:             mask = series.notna()
297:             if mask.any():
298:                 text = series.astype(str).str.strip().str.lower()
299:                 mask = mask & (~text.isin(INVALID_STRINGS))
300:             sequence_mask |= mask
301:         work["__sequence"] = sequence_mask
302:         work["__standalone"] = ~sequence_mask
303: 
304:         exclude_list = _parse_list(ctx.settings.get("exclude_processes"))
305:         if not exclude_list:
306:             exclude_list = ["qlongjob"]
307:         exclude_processes = {p.lower() for p in exclude_list}
308:         work["__excluded"] = work["__process_norm"].isin(exclude_processes)
309: 
310:         standalone = work.loc[work["__standalone"] & ~work["__excluded"]].copy()
311:         if standalone.empty:
312:             return PluginResult(
313:                 "ok",
314:                 "No standalone rows after filtering",
315:                 {
316:                     "standalone_runs": 0,
317:                     "sequence_runs": int(sequence_mask.sum()),
318:                     "eligible_basis": eligible_basis,
319:                 },
320:                 [],
321:                 [],
322:                 None,
323:             )
324: 
325:         busy_periods: list[dict[str, Any]] = []
326:         bucket_ts = standalone["__queue_ts"].dt.floor("h")
327:         grouped = (
328:             standalone.groupby(bucket_ts)
329:             .agg(
330:                 rows_total=("__queue_ts", "size"),
331:                 rows_over_threshold=("__wait_to_start_gt_sec", lambda series: int((series > 0).sum())),
332:                 wait_to_start_hours_total=("__wait_to_start_gt_sec", "sum"),
333:             )
334:             .reset_index()
335:             .rename(columns={"__queue_ts": "bucket_start"})
336:         )
337:         if not grouped.empty:
338:             grouped["wait_to_start_hours_total"] = grouped["wait_to_start_hours_total"] / 3600.0
339:             grouped["bucket_end"] = grouped["bucket_start"] + pd.Timedelta(hours=1)
340:             grouped["weekday"] = grouped["bucket_start"].dt.day_name()
341:             grouped["weekend"] = grouped["bucket_start"].dt.dayofweek >= 5
342:             grouped["hour"] = grouped["bucket_start"].dt.hour
343:             grouped["after_hours"] = (grouped["hour"] < 8) | (grouped["hour"] >= 18)
344:             grouped = grouped.sort_values(
345:                 ["wait_to_start_hours_total", "bucket_start"],
346:                 ascending=[False, True],
347:             )
348:             for _, row in grouped.iterrows():
349:                 busy_periods.append(
350:                     {
351:                         "period_start": row["bucket_start"].isoformat(),
352:                         "period_end": row["bucket_end"].isoformat(),
353:                         "wait_to_start_hours_total": float(row["wait_to_start_hours_total"]),
354:                         "rows_total": int(row["rows_total"]),
355:                         "rows_over_threshold": int(row["rows_over_threshold"]),
356:                         "weekday": row["weekday"],
357:                         "weekend": bool(row["weekend"]),
358:                         "after_hours": bool(row["after_hours"]),
359:                     }
360:                 )
361: 
362:         agg = (
363:             standalone.groupby(["__process_norm", "__close"], dropna=False)
364:             .agg(
365:                 runs=("__process_norm", "size"),
366:                 eligible_wait_sec=("__eligible_wait_sec", "sum"),
367:                 eligible_wait_gt_sec=("__eligible_wait_gt_sec", "sum"),
368:                 wait_pre_sec=("__wait_pre_sec", "sum"),
369:             )
370:             .reset_index()
371:         )
372: 
373:         stats: dict[str, dict[str, float]] = {}
374:         for _, row in agg.iterrows():
375:             proc = row["__process_norm"]
376:             close_flag = bool(row["__close"])
377:             entry = stats.setdefault(proc, {})
378:             suffix = "close" if close_flag else "open"
379:             entry[f"runs_{suffix}"] = float(row["runs"])
380:             entry[f"eligible_wait_sec_{suffix}"] = float(row["eligible_wait_sec"])
381:             entry[f"eligible_wait_gt_sec_{suffix}"] = float(row["eligible_wait_gt_sec"])
382:             entry[f"wait_pre_sec_{suffix}"] = float(row["wait_pre_sec"])
383: 
384:         process_labels = (
385:             standalone.groupby("__process_norm")["__process"]
386:             .agg(lambda series: series.value_counts().index[0])
387:             .to_dict()
388:         )
389: 
390:         summaries = []
391:         for proc, values in stats.items():
392:             runs_close = values.get("runs_close", 0.0)
393:             runs_open = values.get("runs_open", 0.0)
394:             eligible_wait_sec_close = values.get("eligible_wait_sec_close", 0.0)
395:             eligible_wait_sec_open = values.get("eligible_wait_sec_open", 0.0)
396:             eligible_wait_gt_sec_close = values.get("eligible_wait_gt_sec_close", 0.0)
397:             eligible_wait_gt_sec_open = values.get("eligible_wait_gt_sec_open", 0.0)
398:             wait_pre_sec_close = values.get("wait_pre_sec_close", 0.0)
399:             wait_pre_sec_open = values.get("wait_pre_sec_open", 0.0)
400:             summaries.append(
401:                 {
402:                     "process": process_labels.get(proc, proc),
403:                     "process_norm": proc,
404:                     "runs_total": runs_close + runs_open,
405:                     "runs_close": runs_close,
406:                     "runs_open": runs_open,
407:                     "eligible_wait_hours_total": (eligible_wait_sec_close + eligible_wait_sec_open) / 3600.0,
408:                     "eligible_wait_hours_close": eligible_wait_sec_close / 3600.0,
409:                     "eligible_wait_hours_open": eligible_wait_sec_open / 3600.0,
410:                     "eligible_wait_gt_hours_total": (eligible_wait_gt_sec_close + eligible_wait_gt_sec_open) / 3600.0,
411:                     "eligible_wait_gt_hours_close": eligible_wait_gt_sec_close / 3600.0,
412:                     "eligible_wait_gt_hours_open": eligible_wait_gt_sec_open / 3600.0,
413:                     "wait_pre_hours_total": (wait_pre_sec_close + wait_pre_sec_open) / 3600.0,
414:                 }
415:             )
416: 
417:         summaries = sorted(
418:             summaries,
419:             key=lambda item: (-(item["eligible_wait_hours_total"]), item["process_norm"]),
420:         )
421: 
422:         min_hours = float(ctx.settings.get("min_total_hours", 1))
423:         max_findings = int(ctx.settings.get("max_process_findings", 5))
424:         target_process = str(ctx.settings.get("target_process") or "qemail").strip().lower()
425:         findings = []
426:         columns_used = [
427:             col
428:             for col in [process_col, queue_col, eligible_col, start_col, end_col]
429:             if col
430:         ]
431: 
432:         def build_finding(entry: dict[str, float]) -> dict[str, Any]:
433:             return {
434:                 "kind": "eligible_wait_process_stats",
435:                 "process": entry["process"],
436:                 "process_norm": entry["process_norm"],
437:                 "runs_total": int(entry["runs_total"]),
438:                 "runs_close": int(entry["runs_close"]),
439:                 "runs_open": int(entry["runs_open"]),
440:                 "eligible_wait_hours_total": float(entry["eligible_wait_hours_total"]),
441:                 "eligible_wait_hours_close": float(entry["eligible_wait_hours_close"]),
442:                 "eligible_wait_hours_open": float(entry["eligible_wait_hours_open"]),
443:                 "eligible_wait_gt_hours_total": float(entry["eligible_wait_gt_hours_total"]),
444:                 "eligible_wait_gt_hours_close": float(entry["eligible_wait_gt_hours_close"]),
445:                 "eligible_wait_gt_hours_open": float(entry["eligible_wait_gt_hours_open"]),
446:                 "wait_pre_hours_total": float(entry["wait_pre_hours_total"]),
447:                 "close_cycle_start_day": close_start,
448:                 "close_cycle_end_day": close_end,
449:                 "eligible_basis": eligible_basis,
450:                 "columns": columns_used,
451:             }
452: 
453:         target_entry = None
454:         for entry in summaries:
455:             if target_process and entry["process_norm"] == target_process:
456:                 target_entry = entry
457:                 break
458:         if target_entry:
459:             finding = build_finding(target_entry)
460:             finding["measurement_type"] = "measured"
461:             target_rows = work.loc[
462:                 (work["__process_norm"] == target_process) & work["__close"]
463:             ].index.tolist()
464:             finding["row_ids"] = [int(i) for i in target_rows[: int(ctx.settings.get("max_examples", 25))]]
465:             findings.append(finding)
466: 
467:         for entry in summaries:
468:             if entry["eligible_wait_hours_total"] < min_hours:
469:                 continue
470:             if target_process and entry["process_norm"] == target_process:
471:                 continue
472:             finding = build_finding(entry)
473:             finding["measurement_type"] = "measured"
474:             findings.append(finding)
475:             if len(findings) >= max_findings + (1 if target_entry else 0):
476:                 break
477: 
478:         total_runs = int(standalone.shape[0])
479:         sequence_runs = int(work["__sequence"].sum())
480: 
481:         total_eligible_wait_hours = float(standalone["__eligible_wait_sec"].sum() / 3600.0)
482:         total_eligible_wait_gt_hours = float(standalone["__eligible_wait_gt_sec"].sum() / 3600.0)
483:         total_close_hours = float(
484:             standalone.loc[standalone["__close"], "__eligible_wait_sec"].sum() / 3600.0
485:         )
486:         total_open_hours = float(
487:             standalone.loc[~standalone["__close"], "__eligible_wait_sec"].sum() / 3600.0
488:         )
489:         total_close_gt_hours = float(
490:             standalone.loc[standalone["__close"], "__eligible_wait_gt_sec"].sum()
491:             / 3600.0
492:         )
493:         total_open_gt_hours = float(
494:             standalone.loc[~standalone["__close"], "__eligible_wait_gt_sec"].sum()
495:             / 3600.0
496:         )
497: 
498:         impact_finding = None
499:         if target_entry:
500:             remaining_hours = total_eligible_wait_hours - float(
501:                 target_entry["eligible_wait_hours_total"]
502:             )
503:             remaining_gt_hours = total_eligible_wait_gt_hours - float(
504:                 target_entry["eligible_wait_gt_hours_total"]
505:             )
506:             remaining_close_hours = total_close_hours - float(
507:                 target_entry["eligible_wait_hours_close"]
508:             )
509:             remaining_open_hours = total_open_hours - float(
510:                 target_entry["eligible_wait_hours_open"]
511:             )
512:             remaining_close_gt_hours = total_close_gt_hours - float(
513:                 target_entry["eligible_wait_gt_hours_close"]
514:             )
515:             remaining_open_gt_hours = total_open_gt_hours - float(
516:                 target_entry["eligible_wait_gt_hours_open"]
517:             )
518:             impact_finding = {
519:                 "kind": "eligible_wait_impact",
520:                 "process": target_entry["process"],
521:                 "eligible_wait_hours_total": total_eligible_wait_hours,
522:                 "eligible_wait_hours_without_target": remaining_hours,
523:                 "eligible_wait_hours_close_total": total_close_hours,
524:                 "eligible_wait_hours_close_without_target": remaining_close_hours,
525:                 "eligible_wait_hours_open_total": total_open_hours,
526:                 "eligible_wait_hours_open_without_target": remaining_open_hours,
527:                 "eligible_wait_gt_hours_total": total_eligible_wait_gt_hours,
528:                 "eligible_wait_gt_hours_without_target": remaining_gt_hours,
529:                 "eligible_wait_gt_hours_close_total": total_close_gt_hours,
530:                 "eligible_wait_gt_hours_close_without_target": remaining_close_gt_hours,
531:                 "eligible_wait_gt_hours_open_total": total_open_gt_hours,
532:                 "eligible_wait_gt_hours_open_without_target": remaining_open_gt_hours,
533:                 "measurement_type": "measured",
534:                 "columns": columns_used,
535:             }
536:             findings.append(impact_finding)
537: 
538:             scale_factor = float(ctx.settings.get("capacity_scale_factor", 0.6667))
539:             if remaining_gt_hours > 0 and scale_factor > 0:
540:                 modeled = remaining_gt_hours * scale_factor
541:                 modeled_close = remaining_close_gt_hours * scale_factor
542:                 modeled_open = remaining_open_gt_hours * scale_factor
543:                 assumptions = [
544:                     "capacity-proportional scaling on >threshold eligible-wait",
545:                 ]
546:                 scope = {
547:                     "metric": "eligible_wait_gt_hours",
548:                     "close_cycle_start_day": close_start,
549:                     "close_cycle_end_day": close_end,
550:                     "eligible_basis": eligible_basis,
551:                 }
552:                 findings.append(
553:                     {
554:                         "kind": "capacity_scale_model",
555:                         "process": target_entry["process"],
556:                         "eligible_wait_gt_hours_without_target": remaining_gt_hours,
557:                         "eligible_wait_gt_hours_modeled": modeled,
558:                         "eligible_wait_gt_hours_close_without_target": remaining_close_gt_hours,
559:                         "eligible_wait_gt_hours_close_modeled": modeled_close,
560:                         "eligible_wait_gt_hours_open_without_target": remaining_open_gt_hours,
561:                         "eligible_wait_gt_hours_open_modeled": modeled_open,
562:                         "scale_factor": scale_factor,
563:                         "host_count_baseline": None,
564:                         "host_count_modeled": None,
565:                         "measurement_type": "modeled",
566:                         "assumptions": assumptions,
567:                         "scope": scope,
568:                         "columns": columns_used,
569:                     }
570:                 )
571: 
572:         artifacts_dir = ctx.artifacts_dir("analysis_queue_delay_decomposition")
573:         out_path = artifacts_dir / "results.json"
574:         write_json(
575:             out_path,
576:             {
577:                 "summary": {
578:                     "process_column": process_col,
579:                     "queue_column": queue_col,
580:                     "eligible_column": eligible_col,
581:                     "start_column": start_col,
582:                     "end_column": end_col,
583:                     "dependency_columns": dep_cols,
584:                     "eligible_basis": eligible_basis,
585:                     "close_cycle_start_day": close_start,
586:                     "close_cycle_end_day": close_end,
587:                     "wait_threshold_seconds": threshold,
588:                     "busy_period_bucket": "hour",
589:                     "busy_period_basis": "queue_to_start",
590:                 },
591:                 "process_stats": summaries,
592:                 "totals": {
593:                     "standalone_runs": total_runs,
594:                     "sequence_runs": sequence_runs,
595:                     "eligible_wait_hours_total": total_eligible_wait_hours,
596:                     "eligible_wait_gt_hours_total": total_eligible_wait_gt_hours,
597:                 },
598:                 "busy_periods": busy_periods,
599:             },
600:         )
601:         artifacts = [
602:             PluginArtifact(
603:                 path=str(out_path.relative_to(ctx.run_dir)),
604:                 type="json",
605:                 description="Queue delay decomposition results",
606:             )
607:         ]
608: 
609:         return PluginResult(
610:             "ok",
611:             "Computed queue delay decomposition",
612:             {
613:                 "standalone_runs": total_runs,
614:                 "sequence_runs": sequence_runs,
615:                 "eligible_wait_hours_total": total_eligible_wait_hours,
616:                 "eligible_wait_gt_hours_total": total_eligible_wait_gt_hours,
617:                 "eligible_basis": eligible_basis,
618:                 "process_column": process_col,
619:             },
620:             findings,
621:             artifacts,
622:             None,
623:         )
````

## File: plugins/analysis_scan_statistics/plugin.py
````python
 1: from __future__ import annotations
 2: 
 3: import numpy as np
 4: 
 5: from statistic_harness.core.types import PluginArtifact, PluginResult
 6: from statistic_harness.core.stat_controls import benjamini_hochberg, confidence_from_p
 7: from statistic_harness.core.utils import write_json
 8: 
 9: 
10: class Plugin:
11:     def run(self, ctx) -> PluginResult:
12:         df = ctx.dataset_loader()
13:         value_col = ctx.settings.get("value_column")
14:         if not value_col:
15:             numeric = df.select_dtypes(include="number")
16:             if numeric.empty:
17:                 return PluginResult("skipped", "No numeric columns", {}, [], [], None)
18:             value_col = numeric.columns[0]
19:         series = df[value_col].to_numpy()
20:         n = len(series)
21:         max_rows = int(ctx.settings.get("max_rows", 5000))
22:         sampled = False
23:         if n > max_rows > 0:
24:             idx = np.linspace(0, n - 1, max_rows, dtype=int)
25:             series = series[idx]
26:             n = len(series)
27:             sampled = True
28:         min_window = int(ctx.settings.get("min_window", 5))
29:         max_window = int(ctx.settings.get("max_window", 20))
30:         n_perm = int(ctx.settings.get("n_permutations", 100))
31:         rng = np.random.default_rng(ctx.run_seed)
32: 
33:         best = None
34:         for window in range(min_window, min(max_window, n) + 1):
35:             for start in range(0, n - window + 1):
36:                 end = start + window
37:                 segment = series[start:end]
38:                 score = (segment.mean() - series.mean()) / (series.std() + 1e-8)
39:                 if best is None or score > best["score"]:
40:                     best = {"start": start, "end": end, "score": float(score)}
41: 
42:         p_value = 1.0
43:         if best:
44:             perm_scores = []
45:             for _ in range(n_perm):
46:                 perm = rng.permutation(series)
47:                 segment = perm[best["start"] : best["end"]]
48:                 perm_score = (segment.mean() - perm.mean()) / (perm.std() + 1e-8)
49:                 perm_scores.append(perm_score)
50:             p_value = float((np.array(perm_scores) >= best["score"]).mean())
51: 
52:         results = []
53:         if best:
54:             results.append(
55:                 {
56:                     "kind": "cluster",
57:                     "start": int(best["start"]),
58:                     "end": int(best["end"]),
59:                     "score": float(best["score"]),
60:                     "p_value": p_value,
61:                 }
62:             )
63:         if results:
64:             q_values = benjamini_hochberg([r["p_value"] for r in results])
65:             for finding, q_value in zip(results, q_values):
66:                 finding["q_value"] = q_value
67:                 finding["confidence"] = confidence_from_p(q_value)
68: 
69:         artifacts_dir = ctx.artifacts_dir("analysis_scan_statistics")
70:         out_path = artifacts_dir / "results.json"
71:         write_json(out_path, results)
72:         artifacts = [
73:             PluginArtifact(
74:                 path=str(out_path.relative_to(ctx.run_dir)),
75:                 type="json",
76:                 description="Scan results",
77:             )
78:         ]
79:         return PluginResult(
80:             "ok",
81:             "Computed scan statistics",
82:             {"count": len(results), "sampled": sampled, "rows_used": n},
83:             results,
84:             artifacts,
85:             None,
86:         )
````

## File: plugins/analysis_scan_statistics/plugin.yaml
````yaml
 1: id: analysis_scan_statistics
 2: name: Scan Statistics
 3: version: 0.1.0
 4: type: analysis
 5: entrypoint: plugin.py:Plugin
 6: capabilities:
 7: - needs_numeric
 8: depends_on:
 9: - ingest_tabular
10: settings:
11:   description: Window scan statistics
12:   defaults:
13:     value_column: null
14:     min_window: 5
15:     max_window: 20
16:     n_permutations: 100
17:     max_rows: 5000
18: config_schema: config.schema.json
19: output_schema: output.schema.json
20: sandbox:
21:   no_network: true
22:   fs_allowlist:
23:   - appdata
24:   - plugins
````

## File: plugins/analysis_tail_isolation/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from statistic_harness.core.column_inference import choose_timestamp_column
  8: from statistic_harness.core.types import PluginArtifact, PluginResult
  9: from statistic_harness.core.utils import write_json
 10: 
 11: 
 12: INVALID_STRINGS = {"", "nan", "none", "null"}
 13: 
 14: 
 15: def _pick_column(
 16:     preferred: str | None,
 17:     columns: list[str],
 18:     role_by_name: dict[str, str],
 19:     roles: set[str],
 20:     patterns: list[str],
 21:     lower_names: dict[str, str],
 22: ) -> str | None:
 23:     if preferred and preferred in columns:
 24:         return preferred
 25:     for col in columns:
 26:         if role_by_name.get(col) in roles:
 27:             return col
 28:     for col in columns:
 29:         name = lower_names[col]
 30:         if any(pattern in name for pattern in patterns):
 31:             return col
 32:     return None
 33: 
 34: 
 35: def _pick_timestamp_column(
 36:     preferred: str | None,
 37:     columns: list[str],
 38:     role_by_name: dict[str, str],
 39:     roles: set[str],
 40:     patterns: list[str],
 41:     lower_names: dict[str, str],
 42:     df: pd.DataFrame,
 43: ) -> str | None:
 44:     candidates: list[str] = []
 45:     if preferred and preferred in columns:
 46:         candidates.append(preferred)
 47:     for col in columns:
 48:         if role_by_name.get(col) in roles and col not in candidates:
 49:             candidates.append(col)
 50:     for col in columns:
 51:         name = lower_names[col]
 52:         if any(pattern in name for pattern in patterns) and col not in candidates:
 53:             candidates.append(col)
 54:     if not candidates:
 55:         return None
 56:     return choose_timestamp_column(df, candidates)
 57: 
 58: 
 59: def _normalize_text(value: Any) -> str:
 60:     if value is None:
 61:         return ""
 62:     if isinstance(value, float) and pd.isna(value):
 63:         return ""
 64:     return str(value).strip()
 65: 
 66: 
 67: def _series_from_frame(frame: pd.DataFrame, column: str | None) -> pd.Series | None:
 68:     if not column:
 69:         return None
 70:     data = frame[column]
 71:     if isinstance(data, pd.DataFrame):
 72:         return data.iloc[:, 0]
 73:     return data
 74: 
 75: 
 76: class Plugin:
 77:     def run(self, ctx) -> PluginResult:
 78:         df = ctx.dataset_loader()
 79:         if df.empty:
 80:             return PluginResult("skipped", "Empty dataset", {}, [], [], None)
 81: 
 82:         columns_meta = []
 83:         role_by_name: dict[str, str] = {}
 84:         if ctx.dataset_version_id:
 85:             dataset_template = ctx.storage.fetch_dataset_template(ctx.dataset_version_id)
 86:             if dataset_template and dataset_template.get("status") == "ready":
 87:                 fields = ctx.storage.fetch_template_fields(
 88:                     int(dataset_template["template_id"])
 89:                 )
 90:                 columns_meta = fields
 91:                 role_by_name = {
 92:                     field["name"]: (field.get("role") or "") for field in fields
 93:                 }
 94:             else:
 95:                 columns_meta = ctx.storage.fetch_dataset_columns(ctx.dataset_version_id)
 96:                 role_by_name = {
 97:                     col["original_name"]: (col.get("role") or "")
 98:                     for col in columns_meta
 99:                 }
100: 
101:         columns = list(df.columns)
102:         lower_names = {col: str(col).lower() for col in columns}
103: 
104:         process_col = _pick_column(
105:             ctx.settings.get("process_column"),
106:             columns,
107:             role_by_name,
108:             {"process_name", "process_id"},
109:             ["process", "activity", "event", "step", "task", "action", "job"],
110:             lower_names,
111:         )
112:         module_col = _pick_column(
113:             ctx.settings.get("module_column"),
114:             columns,
115:             role_by_name,
116:             {"module_code"},
117:             ["module", "mod"],
118:             lower_names,
119:         )
120:         user_col = _pick_column(
121:             ctx.settings.get("user_column"),
122:             columns,
123:             role_by_name,
124:             {"user_id"},
125:             ["user", "owner", "operator"],
126:             lower_names,
127:         )
128:         sequence_col = _pick_column(
129:             ctx.settings.get("sequence_column"),
130:             columns,
131:             role_by_name,
132:             {"master_id"},
133:             ["master", "sequence", "chain", "workflow", "batch", "group", "case"],
134:             lower_names,
135:         )
136:         queue_col = _pick_timestamp_column(
137:             ctx.settings.get("queue_column"),
138:             columns,
139:             role_by_name,
140:             {"queue_time"},
141:             ["queue", "queued", "enqueue"],
142:             lower_names,
143:             df,
144:         )
145:         eligible_col = _pick_timestamp_column(
146:             ctx.settings.get("eligible_column"),
147:             columns,
148:             role_by_name,
149:             {"eligible", "ready", "available"},
150:             ["eligible", "ready", "available"],
151:             lower_names,
152:             df,
153:         )
154:         start_col = _pick_timestamp_column(
155:             ctx.settings.get("start_column"),
156:             columns,
157:             role_by_name,
158:             {"start_time", "start"},
159:             ["start", "begin"],
160:             lower_names,
161:             df,
162:         )
163: 
164:         if not start_col or (not queue_col and not eligible_col):
165:             return PluginResult(
166:                 "ok",
167:                 "Tail isolation not applicable",
168:                 {"tail_rows": 0, "tail_hours_total": 0.0},
169:                 [
170:                     {
171:                         "kind": "tail_isolation",
172:                         "measurement_type": "not_applicable",
173:                         "reason": "Missing queue/eligible and start columns.",
174:                         "columns": [
175:                             col
176:                             for col in [queue_col, eligible_col, start_col]
177:                             if col
178:                         ],
179:                     }
180:                 ],
181:                 [],
182:                 None,
183:             )
184: 
185:         selected = [
186:             col
187:             for col in [
188:                 process_col,
189:                 module_col,
190:                 user_col,
191:                 sequence_col,
192:                 queue_col,
193:                 eligible_col,
194:                 start_col,
195:             ]
196:             if col
197:         ]
198:         selected = list(dict.fromkeys(selected))
199:         work = df.loc[:, selected].copy()
200: 
201:         eligible_series = _series_from_frame(work, eligible_col)
202:         eligible_ts = (
203:             pd.to_datetime(eligible_series, errors="coerce", utc=False)
204:             if eligible_series is not None
205:             else None
206:         )
207:         if eligible_ts is None:
208:             queue_series = _series_from_frame(work, queue_col)
209:             eligible_ts = pd.to_datetime(queue_series, errors="coerce", utc=False)
210:             eligible_basis = "queue"
211:         else:
212:             eligible_basis = "eligible"
213:         start_series = _series_from_frame(work, start_col)
214:         start_ts = pd.to_datetime(start_series, errors="coerce", utc=False)
215:         work["__eligible_ts"] = eligible_ts
216:         work["__start_ts"] = start_ts
217:         work = work.loc[work["__eligible_ts"].notna() & work["__start_ts"].notna()].copy()
218:         if work.empty:
219:             return PluginResult("ok", "No valid timestamps", {"tail_rows": 0}, [], [], None)
220: 
221:         wait_sec = (work["__start_ts"] - work["__eligible_ts"]).dt.total_seconds()
222:         wait_sec = wait_sec.clip(lower=0).fillna(0)
223:         work["__eligible_wait_sec"] = wait_sec
224: 
225:         threshold = float(ctx.settings.get("wait_threshold_seconds", 60))
226:         tail = work["__eligible_wait_sec"] > threshold
227:         tail_work = work.loc[tail].copy()
228: 
229:         if tail_work.empty:
230:             return PluginResult(
231:                 "ok",
232:                 "No tail rows above threshold",
233:                 {"tail_rows": 0, "tail_hours_total": 0.0},
234:                 [
235:                     {
236:                         "kind": "tail_isolation",
237:                         "measurement_type": "not_applicable",
238:                         "reason": "No rows exceed the tail threshold.",
239:                         "threshold_seconds": threshold,
240:                         "columns": [
241:                             col
242:                             for col in [process_col, module_col, user_col, sequence_col]
243:                             if col
244:                         ],
245:                     }
246:                 ],
247:                 [],
248:                 None,
249:             )
250: 
251:         max_groups = int(ctx.settings.get("max_groups", 5))
252:         max_examples = int(ctx.settings.get("max_examples", 25))
253: 
254:         findings = []
255: 
256:         def _emit_dimension(name: str, col: str | None) -> None:
257:             if not col or col not in tail_work.columns:
258:                 return
259:             temp = tail_work.copy()
260:             temp["__key"] = temp[col].map(_normalize_text)
261:             temp = temp.loc[~temp["__key"].str.lower().isin(INVALID_STRINGS)].copy()
262:             if temp.empty:
263:                 return
264:             grouped = (
265:                 temp.groupby("__key")
266:                 .agg(
267:                     tail_rows=("__key", "size"),
268:                     tail_wait_sec=("__eligible_wait_sec", "sum"),
269:                 )
270:                 .reset_index()
271:             )
272:             grouped = grouped.sort_values(
273:                 ["tail_wait_sec", "__key"], ascending=[False, True]
274:             )
275:             for _, row in grouped.head(max_groups).iterrows():
276:                 key = row["__key"]
277:                 row_ids = temp.loc[temp["__key"] == key].index.tolist()[:max_examples]
278:                 findings.append(
279:                     {
280:                         "kind": "tail_isolation",
281:                         "dimension": name,
282:                         "key": key,
283:                         "tail_rows": int(row["tail_rows"]),
284:                         "tail_wait_hours": float(row["tail_wait_sec"]) / 3600.0,
285:                         "threshold_seconds": threshold,
286:                         "eligible_basis": eligible_basis,
287:                         "measurement_type": "measured",
288:                         "row_ids": [int(i) for i in row_ids],
289:                         "columns": [
290:                             col
291:                             for col in [process_col, module_col, user_col, sequence_col]
292:                             if col
293:                         ],
294:                     }
295:                 )
296: 
297:         _emit_dimension("process", process_col)
298:         _emit_dimension("module", module_col)
299:         _emit_dimension("user", user_col)
300:         _emit_dimension("sequence", sequence_col)
301: 
302:         tail_hours = float(tail_work["__eligible_wait_sec"].sum() / 3600.0)
303: 
304:         artifacts_dir = ctx.artifacts_dir("analysis_tail_isolation")
305:         out_path = artifacts_dir / "tail_isolation.json"
306:         write_json(
307:             out_path,
308:             {
309:                 "summary": {
310:                     "process_column": process_col,
311:                     "module_column": module_col,
312:                     "user_column": user_col,
313:                     "sequence_column": sequence_col,
314:                     "queue_column": queue_col,
315:                     "eligible_column": eligible_col,
316:                     "start_column": start_col,
317:                     "eligible_basis": eligible_basis,
318:                     "threshold_seconds": threshold,
319:                 },
320:                 "tail_rows": int(tail_work.shape[0]),
321:                 "tail_hours_total": tail_hours,
322:             },
323:         )
324:         artifacts = [
325:             PluginArtifact(
326:                 path=str(out_path.relative_to(ctx.run_dir)),
327:                 type="json",
328:                 description="Tail isolation summary",
329:             )
330:         ]
331: 
332:         return PluginResult(
333:             "ok",
334:             "Isolated tail-eligible wait segments",
335:             {
336:                 "tail_rows": int(tail_work.shape[0]),
337:                 "tail_hours_total": tail_hours,
338:             },
339:             findings,
340:             artifacts,
341:             None,
342:         )
````

## File: plugins/profile_basic/plugin.py
````python
  1: from __future__ import annotations
  2: 
  3: import json
  4: import re
  5: from typing import Any
  6: 
  7: import pandas as pd
  8: 
  9: from statistic_harness.core.column_inference import infer_timestamp_series
 10: from statistic_harness.core.types import PluginArtifact, PluginResult
 11: from statistic_harness.core.utils import DEFAULT_TENANT_ID, write_json
 12: 
 13: 
 14: class Plugin:
 15:     def run(self, ctx) -> PluginResult:
 16:         df = ctx.dataset_loader()
 17:         columns = []
 18:         numeric = df.select_dtypes(include="number")
 19:         role_by_name: dict[str, str] = {}
 20: 
 21:         def infer_role(col_name: str, series: pd.Series) -> str | None:
 22:             lname = col_name.lower()
 23:             if "param" in lname or "parameter" in lname or "params" in lname:
 24:                 return "parameter"
 25:             if "meta" in lname or "config" in lname:
 26:                 return "parameter"
 27:             ts_info = infer_timestamp_series(series, name_hint=col_name, sample_size=500)
 28:             if ts_info.valid and (
 29:                 "time" in lname
 30:                 or "date" in lname
 31:                 or "timestamp" in lname
 32:                 or ts_info.score >= 2.5
 33:             ):
 34:                 return "timestamp"
 35:             if lname.endswith("id") or " id" in lname:
 36:                 return "id"
 37:             if pd.api.types.is_numeric_dtype(series):
 38:                 return "numeric"
 39:             return None
 40: 
 41:         for col in df.columns:
 42:             series = df[col]
 43:             role = infer_role(col, series)
 44:             if role:
 45:                 role_by_name[col] = role
 46:             entry = {
 47:                 "name": col,
 48:                 "dtype": str(series.dtype),
 49:                 "missing_pct": float(series.isna().mean()),
 50:                 "unique": int(series.nunique()),
 51:             }
 52:             if pd.api.types.is_numeric_dtype(series):
 53:                 entry.update(
 54:                     {
 55:                         "min": float(series.min()),
 56:                         "max": float(series.max()),
 57:                         "mean": float(series.mean()),
 58:                         "std": float(series.std(ddof=0)),
 59:                     }
 60:                 )
 61:             columns.append(entry)
 62: 
 63:         pii_patterns = {
 64:             "email": re.compile(
 65:                 r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$"
 66:             ),
 67:             "ssn": re.compile(r"^\d{3}-?\d{2}-?\d{4}$"),
 68:             "phone": re.compile(r"^\+?\d[\d\s().-]{6,}\d$"),
 69:             "address": re.compile(
 70:                 r"\b\d{1,5}\s+\w+(?:\s+\w+){0,4}\s+"
 71:                 r"(st|street|ave|avenue|rd|road|blvd|lane|ln|dr|drive|ct|court)\b",
 72:                 re.IGNORECASE,
 73:             ),
 74:         }
 75:         pii_tags_by_name: dict[str, list[str]] = {}
 76:         sample_size = int(ctx.settings.get("pii_sample_size", 200))
 77:         threshold = float(ctx.settings.get("pii_match_threshold", 0.6))
 78:         for col in df.columns:
 79:             series = df[col]
 80:             if series.empty:
 81:                 continue
 82:             values = series.dropna().astype(str)
 83:             if values.empty:
 84:                 continue
 85:             sample = values.head(sample_size)
 86:             tags: list[str] = []
 87:             for tag, pattern in pii_patterns.items():
 88:                 matches = sample.str.match(pattern, na=False)
 89:                 if matches.any():
 90:                     ratio = float(matches.mean())
 91:                     if ratio >= threshold:
 92:                         tags.append(tag)
 93:             if tags:
 94:                 pii_tags_by_name[col] = tags
 95: 
 96:         artifacts = []
 97:         artifacts_dir = ctx.artifacts_dir("profile_basic")
 98:         columns_path = artifacts_dir / "columns.json"
 99:         write_json(columns_path, columns)
100:         artifacts.append(
101:             PluginArtifact(
102:                 path=str(columns_path.relative_to(ctx.run_dir)),
103:                 type="json",
104:                 description="Column stats",
105:             )
106:         )
107: 
108:         max_cols = int(ctx.settings.get("max_corr_cols", 10))
109:         if numeric.shape[1] > 1 and numeric.shape[1] <= max_cols:
110:             corr = numeric.corr()
111:             corr_path = artifacts_dir / "correlation.csv"
112:             corr.to_csv(corr_path)
113:             artifacts.append(
114:                 PluginArtifact(
115:                     path=str(corr_path.relative_to(ctx.run_dir)),
116:                     type="csv",
117:                     description="Correlation",
118:                 )
119:             )
120: 
121:         if ctx.dataset_version_id:
122:             ctx.storage.update_dataset_column_roles(
123:                 ctx.dataset_version_id, role_by_name
124:             )
125:             if pii_tags_by_name:
126:                 ctx.storage.update_dataset_column_pii_tags(
127:                     ctx.dataset_version_id, pii_tags_by_name
128:                 )
129:                 tenant_id = ctx.tenant_id or DEFAULT_TENANT_ID
130:                 for col, tags in pii_tags_by_name.items():
131:                     values = (
132:                         df[col]
133:                         .dropna()
134:                         .astype(str)
135:                         .unique()
136:                         .tolist()
137:                     )
138:                     values = sorted({str(v) for v in values})
139:                     for tag in tags:
140:                         ctx.storage.upsert_pii_entities(tenant_id, tag, values)
141: 
142:         def normalize_kv_pairs(kv_pairs: list[tuple[str, str]]) -> tuple[str, list[tuple[str, str]]]:
143:             cleaned = []
144:             for key, value in kv_pairs:
145:                 key = str(key).strip().lower()
146:                 value = str(value).strip()
147:                 if key:
148:                     cleaned.append((key, value))
149:             if not cleaned:
150:                 return "", []
151:             cleaned = sorted(set(cleaned))
152:             canonical = ";".join(f"{k}={v}" for k, v in cleaned)
153:             return canonical, cleaned
154: 
155:         def parse_parameter_value(value: Any) -> tuple[str, list[tuple[str, str]]] | None:
156:             if value is None or (isinstance(value, float) and pd.isna(value)):
157:                 return None
158:             raw = str(value).strip()
159:             if not raw:
160:                 return None
161:             if raw.startswith("{") and raw.endswith("}"):
162:                 try:
163:                     data = json.loads(raw)
164:                     if isinstance(data, dict):
165:                         kv_pairs = [(str(k), str(v)) for k, v in data.items()]
166:                         canonical, cleaned = normalize_kv_pairs(kv_pairs)
167:                         if canonical:
168:                             return canonical, cleaned
169:                 except json.JSONDecodeError:
170:                     pass
171:             tokens = re.split(r"[;|,\\n]+", raw)
172:             kv_pairs = []
173:             for token in tokens:
174:                 token = token.strip()
175:                 if not token:
176:                     continue
177:                 if "=" in token:
178:                     key, value = token.split("=", 1)
179:                 elif ":" in token:
180:                     key, value = token.split(":", 1)
181:                 else:
182:                     continue
183:                 kv_pairs.append((key, value))
184:             if kv_pairs:
185:                 canonical, cleaned = normalize_kv_pairs(kv_pairs)
186:                 if canonical:
187:                     return canonical, cleaned
188:             canonical, cleaned = normalize_kv_pairs([("raw", raw)])
189:             if canonical:
190:                 return canonical, cleaned
191:             return None
192: 
193:         parameter_columns = [
194:             col
195:             for col in df.columns
196:             if role_by_name.get(col) == "parameter"
197:             or (
198:                 df[col].dtype == object
199:                 and df[col]
200:                 .dropna()
201:                 .astype(str)
202:                 .head(20)
203:                 .str.contains(r"[=:]")
204:                 .any()
205:             )
206:         ]
207: 
208:         if parameter_columns and ctx.dataset_version_id:
209:             cache: dict[str, int] = {}
210:             links: list[tuple[int, int]] = []
211:             edges: list[tuple[int, int, str, dict[str, Any] | None, float | None]] = []
212:             with ctx.storage.connection() as conn:
213:                 dataset_entity_id = ctx.storage.ensure_entity(
214:                     "dataset_version", ctx.dataset_version_id, conn
215:                 )
216:                 for col in parameter_columns:
217:                     series = df[col]
218:                     for row_index, value in series.items():
219:                         parsed = parse_parameter_value(value)
220:                         if not parsed:
221:                             continue
222:                         canonical, kv_pairs = parsed
223:                         if canonical not in cache:
224:                             entity_id = ctx.storage.get_or_create_parameter_entity(
225:                                 canonical, conn
226:                             )
227:                             ctx.storage.insert_parameter_kv(
228:                                 entity_id, kv_pairs, conn
229:                             )
230:                             cache[canonical] = entity_id
231:                             param_entity_id = ctx.storage.ensure_entity(
232:                                 "parameter", canonical, conn
233:                             )
234:                             edges.append(
235:                                 (
236:                                     dataset_entity_id,
237:                                     param_entity_id,
238:                                     "uses_parameter",
239:                                     {"column": col},
240:                                     None,
241:                                 )
242:                             )
243:                         links.append((int(row_index), cache[canonical]))
244:                         if len(links) >= 1000:
245:                             ctx.storage.insert_row_parameter_links(
246:                                 ctx.dataset_version_id, links, conn
247:                             )
248:                             links = []
249:                 if links:
250:                     ctx.storage.insert_row_parameter_links(
251:                         ctx.dataset_version_id, links, conn
252:                     )
253:                 if edges:
254:                     ctx.storage.add_edges(edges, conn)
255: 
256:         return PluginResult(
257:             status="ok",
258:             summary="Profiled dataset",
259:             metrics={"columns": len(columns)},
260:             findings=[],
261:             artifacts=artifacts,
262:             error=None,
263:         )
````

## File: src/statistic_harness/core/planner.py
````python
  1: from __future__ import annotations
  2: 
  3: from typing import Any
  4: 
  5: import pandas as pd
  6: 
  7: from .column_inference import infer_timestamp_series
  8: from .utils import quote_identifier
  9: 
 10: from .plugin_manager import PluginSpec
 11: from .storage import Storage
 12: 
 13: 
 14: def _is_numeric_dtype(dtype: str | None) -> bool:
 15:     if not dtype:
 16:         return False
 17:     lowered = dtype.lower()
 18:     return "int" in lowered or "float" in lowered or "double" in lowered
 19: 
 20: 
 21: def _infer_dataset_features(
 22:     storage: Storage, dataset_version_id: str
 23: ) -> dict[str, Any]:
 24:     dataset_template = storage.fetch_dataset_template(dataset_version_id)
 25:     if dataset_template and dataset_template.get("status") == "ready":
 26:         fields = storage.fetch_template_fields(int(dataset_template["template_id"]))
 27:         numeric_cols = [field for field in fields if _is_numeric_dtype(field.get("dtype"))]
 28:         roles = {field["name"]: (field.get("role") or "") for field in fields}
 29:         names = [field["name"].lower() for field in fields]
 30:     else:
 31:         columns = storage.fetch_dataset_columns(dataset_version_id)
 32:         numeric_cols = [col for col in columns if _is_numeric_dtype(col.get("dtype"))]
 33:         roles = {col["original_name"]: (col.get("role") or "") for col in columns}
 34:         names = [col["original_name"].lower() for col in columns]
 35: 
 36:     role_values = [str(role).lower() for role in roles.values() if role]
 37:     has_timestamp = any(
 38:         role == "timestamp" or "time" in role or "date" in role for role in role_values
 39:     )
 40:     if not has_timestamp:
 41:         has_timestamp = any("time" in name or "date" in name for name in names)
 42: 
 43:     if not has_timestamp:
 44:         columns = storage.fetch_dataset_columns(dataset_version_id)
 45:         if columns:
 46:             safe_cols = [col.get("safe_name") for col in columns if col.get("safe_name")]
 47:             table_row = storage.get_dataset_version_context(dataset_version_id)
 48:             if safe_cols and table_row and table_row.get("table_name"):
 49:                 quoted = ", ".join(quote_identifier(col) for col in safe_cols)
 50:                 sql = (
 51:                     f"SELECT {quoted} FROM {quote_identifier(table_row['table_name'])} "
 52:                     "ORDER BY row_index LIMIT ?"
 53:                 )
 54:                 with storage.connection() as conn:
 55:                     df = pd.read_sql_query(sql, conn, params=(200,))
 56:                 df = df.rename(
 57:                     columns={
 58:                         col["safe_name"]: col["original_name"]
 59:                         for col in columns
 60:                         if col.get("safe_name") in df.columns
 61:                     }
 62:                 )
 63:                 for col in df.columns:
 64:                     info = infer_timestamp_series(df[col], name_hint=col, sample_size=200)
 65:                     if info.valid:
 66:                         has_timestamp = True
 67:                         break
 68: 
 69:     has_id = any("id" in role for role in role_values)
 70:     if not has_id:
 71:         has_id = any(name.endswith("id") or " id" in name for name in names)
 72: 
 73:     has_activity = any("activity" in name or "event" in name or "step" in name for name in names)
 74:     has_eventlog = has_id and has_activity
 75: 
 76:     host_tokens = ("host", "server", "node", "instance", "machine")
 77:     has_host = any(
 78:         any(token in role for token in host_tokens) for role in role_values
 79:     )
 80:     if not has_host:
 81:         has_host = any(any(token in name for token in host_tokens) for name in names)
 82: 
 83:     return {
 84:         "numeric_count": len(numeric_cols),
 85:         "has_numeric": len(numeric_cols) > 0,
 86:         "has_multi_numeric": len(numeric_cols) > 1,
 87:         "has_timestamp": has_timestamp,
 88:         "has_eventlog": has_eventlog,
 89:         "has_host": has_host,
 90:     }
 91: 
 92: 
 93: def _capabilities_satisfied(capabilities: list[str], features: dict[str, Any]) -> bool:
 94:     for cap in capabilities:
 95:         if cap == "needs_numeric" and not features["has_numeric"]:
 96:             return False
 97:         if cap == "needs_multi_numeric" and not features["has_multi_numeric"]:
 98:             return False
 99:         if cap == "needs_timestamp" and not features["has_timestamp"]:
100:             return False
101:         if cap == "needs_eventlog" and not features["has_eventlog"]:
102:             return False
103:     return True
104: 
105: 
106: def select_plugins(
107:     specs: list[PluginSpec], storage: Storage, dataset_version_id: str
108: ) -> list[str]:
109:     features = _infer_dataset_features(storage, dataset_version_id)
110:     dataset_template = storage.fetch_dataset_template(dataset_version_id)
111:     template_ready = bool(dataset_template and dataset_template.get("status") == "ready")
112:     selected = []
113:     spec_ids = {spec.plugin_id for spec in specs}
114:     for spec in specs:
115:         if spec.type != "analysis":
116:             continue
117:         if not _capabilities_satisfied(spec.capabilities, features):
118:             continue
119:         selected.append(spec.plugin_id)
120:     if (
121:         "analysis_close_cycle_capacity_model" in spec_ids
122:         and "analysis_close_cycle_capacity_model" not in selected
123:         and features.get("has_timestamp")
124:         and features.get("has_host")
125:     ):
126:         selected.append("analysis_close_cycle_capacity_model")
127:     if not template_ready:
128:         for spec in specs:
129:             if spec.plugin_id == "transform_normalize_mixed":
130:                 selected.append(spec.plugin_id)
131:                 break
132:     return sorted(set(selected))
````

## File: tests/fixtures/ground_truth_quorum.yaml
````yaml
  1: strict: true
  2: expected_findings:
  3:   - plugin_id: analysis_capacity_scaling
  4:     kind: capacity_scaling
  5:     where:
  6:       measurement_type: modeled
  7:     min_count: 1
  8:   - plugin_id: analysis_close_cycle_capacity_model
  9:     kind: close_cycle_capacity_model
 10:     where:
 11:       measurement_type: not_applicable
 12:       reason: no_buckets_after_filtering
 13:     min_count: 6
 14:     max_count: 6
 15:   - plugin_id: analysis_close_cycle_contention
 16:     kind: close_cycle_contention
 17:     where:
 18:       process: qemail
 19:     min_count: 1
 20:     max_count: 1
 21: expected_metrics:
 22:   - plugin_id: analysis_close_cycle_contention
 23:     metric: candidates
 24:     value: 1
 25:     tolerance: 0
 26:   - plugin_id: analysis_queue_delay_decomposition
 27:     metric: standalone_runs
 28:     value: 45
 29:     tolerance: 0
 30:   - plugin_id: analysis_queue_delay_decomposition
 31:     metric: sequence_runs
 32:     value: 0
 33:     tolerance: 0
 34:   - plugin_id: analysis_queue_delay_decomposition
 35:     metric: eligible_wait_hours_total
 36:     value: 0.75
 37:     tolerance:
 38:       absolute: 0.001
 39:       relative: 0
 40:   - plugin_id: analysis_queue_delay_decomposition
 41:     metric: eligible_wait_gt_hours_total
 42:     value: 0.0
 43:     tolerance:
 44:       absolute: 0.001
 45:       relative: 0
 46:   - plugin_id: analysis_tail_isolation
 47:     metric: tail_rows
 48:     value: 0
 49:     tolerance: 0
 50:   - plugin_id: analysis_tail_isolation
 51:     metric: tail_hours_total
 52:     value: 0.0
 53:     tolerance:
 54:       absolute: 0.001
 55:       relative: 0
 56:   - plugin_id: analysis_percentile_analysis
 57:     metric: groups
 58:     value: 2
 59:     tolerance: 0
 60:   - plugin_id: analysis_attribution
 61:     metric: total_rows
 62:     value: 45
 63:     tolerance: 0
 64:   - plugin_id: analysis_dependency_resolution_join
 65:     metric: dependency_rows
 66:     value: 0
 67:     tolerance: 0
 68:   - plugin_id: analysis_dependency_resolution_join
 69:     metric: near_zero_ratio
 70:     value: 0.0
 71:     tolerance:
 72:       absolute: 0.001
 73:       relative: 0
 74:   - plugin_id: analysis_sequence_classification
 75:     metric: standalone_runs
 76:     value: 45
 77:     tolerance: 0
 78:   - plugin_id: analysis_sequence_classification
 79:     metric: sequence_runs
 80:     value: 0
 81:     tolerance: 0
 82:   - plugin_id: analysis_process_sequence
 83:     metric: variants
 84:     value: 2
 85:     tolerance: 0
 86:   - plugin_id: analysis_process_sequence
 87:     metric: transitions
 88:     value: 0
 89:     tolerance: 0
 90:   - plugin_id: analysis_chain_makespan
 91:     metric: chains
 92:     value: 45
 93:     tolerance: 0
 94:   - plugin_id: analysis_chain_makespan
 95:     metric: max_makespan_seconds
 96:     value: 180
 97:     tolerance: 0
 98:   - plugin_id: analysis_concurrency_reconstruction
 99:     metric: hosts
100:     value: 2
101:     tolerance: 0
102:   - plugin_id: analysis_concurrency_reconstruction
103:     metric: peak_concurrency
104:     value: 1
105:     tolerance: 0
106:   - plugin_id: analysis_capacity_scaling
107:     metric: rows
108:     value: 45
109:     tolerance: 0
110:   - plugin_id: analysis_capacity_scaling
111:     metric: host_count
112:     value: 2
113:     tolerance: 0
114:   - plugin_id: analysis_capacity_scaling
115:     metric: scale_factor
116:     value: 1.5
117:     tolerance:
118:       absolute: 0.001
119:       relative: 0
120:   - plugin_id: analysis_capacity_scaling
121:     metric: baseline_wait_hours
122:     value: 0.75
123:     tolerance:
124:       absolute: 0.001
125:       relative: 0
126:   - plugin_id: analysis_capacity_scaling
127:     metric: modeled_wait_hours
128:     value: 0.5
129:     tolerance:
130:       absolute: 0.001
131:       relative: 0
132:   - plugin_id: analysis_capacity_scaling
133:     metric: reduction_hours
134:     value: 0.25
135:     tolerance:
136:       absolute: 0.001
137:       relative: 0
138:   - plugin_id: analysis_determinism_discipline
139:     metric: missing_measurement_type
140:     value: 0
141:     tolerance: 0
142:   - plugin_id: analysis_determinism_discipline
143:     metric: modeled_missing_assumption
144:     value: 0
145:     tolerance: 0
````

## File: src/statistic_harness/core/report.py
````python
   1: from __future__ import annotations
   2: 
   3: import csv
   4: import json
   5: import re
   6: import os
   7: from pathlib import Path
   8: from typing import Any
   9: 
  10: from jsonschema import validate
  11: import yaml
  12: 
  13: from .stat_controls import confidence_from_p
  14: 
  15: 
  16: def _matches_expected(
  17:     item: dict[str, Any],
  18:     where: dict[str, Any] | None,
  19:     contains: dict[str, Any] | None,
  20: ) -> bool:
  21:     if where:
  22:         for key, expected in where.items():
  23:             actual = item.get(key)
  24:             if actual != expected:
  25:                 return False
  26:     if contains:
  27:         for key, expected in contains.items():
  28:             actual = item.get(key)
  29:             if isinstance(actual, str):
  30:                 if str(expected) not in actual:
  31:                     return False
  32:             elif isinstance(actual, (list, tuple, set)):
  33:                 if isinstance(expected, (list, tuple, set)):
  34:                     if not set(expected).issubset(set(actual)):
  35:                         return False
  36:                 else:
  37:                     if expected not in actual:
  38:                         return False
  39:             else:
  40:                 return False
  41:     return True
  42: 
  43: 
  44: def _collect_findings_for_plugin(
  45:     report: dict[str, Any], plugin_id: str | None, kind: str | None = None
  46: ) -> list[dict[str, Any]]:
  47:     findings: list[dict[str, Any]] = []
  48:     plugins = report.get("plugins", {}) or {}
  49:     for pid, plugin in plugins.items():
  50:         if plugin_id and pid != plugin_id:
  51:             continue
  52:         for item in plugin.get("findings", []) or []:
  53:             if kind and item.get("kind") != kind:
  54:                 continue
  55:             if isinstance(item, dict):
  56:                 findings.append(item)
  57:     return findings
  58: 
  59: 
  60: def _process_hint(where: dict[str, Any] | None) -> str:
  61:     if not where:
  62:         return ""
  63:     for key in ("process", "process_norm", "process_name", "activity"):
  64:         value = where.get(key)
  65:         if isinstance(value, str) and value.strip():
  66:             return value.strip()
  67:     return ""
  68: 
  69: 
  70: def _recommendation_text(status: str, label: str, process_hint: str) -> str:
  71:     suffix = f" (process {process_hint})" if process_hint else ""
  72:     if status == "confirmed":
  73:         return f"Act on {label}{suffix}."
  74:     if status == "over_limit":
  75:         return f"Investigate excess occurrences of {label}{suffix}."
  76:     if status in {"missing", "below_min"}:
  77:         return f"Missing evidence for {label}{suffix}; check inputs and re-run."
  78:     return f"Review {label}{suffix}."
  79: 
  80: 
  81: def _capacity_scale_recommendation(
  82:     kind: str | None, matched: list[dict[str, Any]], label: str, process_hint: str
  83: ) -> tuple[str | None, dict[str, Any]]:
  84:     if not kind or not matched:
  85:         return None, {}
  86:     suffix = f" (process {process_hint})" if process_hint else ""
  87:     item = matched[0]
  88:     meta: dict[str, Any] = {"action": None, "modeled_delta": None}
  89:     if kind == "capacity_scale_model":
  90:         base = item.get("eligible_wait_gt_hours_without_target")
  91:         modeled = item.get("eligible_wait_gt_hours_modeled")
  92:         scale = item.get("scale_factor")
  93:         if isinstance(base, (int, float)) and isinstance(modeled, (int, float)):
  94:             delta = float(base) - float(modeled)
  95:             scale_text = f" scale_factor={scale:.3f}" if isinstance(scale, (int, float)) else ""
  96:             meta = {"action": "add_one_server", "modeled_delta": delta}
  97:             return (
  98:                 f"Add one server{suffix}: modeled >threshold eligible-wait drops "
  99:                 f"from {float(base):.3f}h to {float(modeled):.3f}h (Δ {delta:.3f}h){scale_text}."
 100:             ), meta
 101:     if kind == "capacity_scaling":
 102:         base = item.get("baseline_wait_hours")
 103:         modeled = item.get("modeled_wait_hours")
 104:         reduction = item.get("reduction_hours")
 105:         scale = item.get("scale_factor")
 106:         if isinstance(base, (int, float)) and isinstance(modeled, (int, float)):
 107:             reduction_val = reduction if isinstance(reduction, (int, float)) else float(base) - float(modeled)
 108:             scale_text = f" scale_factor={scale:.3f}" if isinstance(scale, (int, float)) else ""
 109:             meta = {"action": "add_one_server", "modeled_delta": float(reduction_val)}
 110:             return (
 111:                 f"Add one server{suffix}: eligible-wait drops from "
 112:                 f"{float(base):.3f}h to {float(modeled):.3f}h (Δ {float(reduction_val):.3f}h){scale_text}."
 113:             ), meta
 114:     if kind == "close_cycle_capacity_impact":
 115:         effect = item.get("effect")
 116:         decision = str(item.get("decision") or "")
 117:         if isinstance(effect, (int, float)) and decision == "detected":
 118:             pct = abs(float(effect)) * 100.0
 119:             meta = {"action": "add_one_server", "modeled_delta": None}
 120:             return f"Add one server{suffix}: median close-cycle improves by ~{pct:.1f}% (measured).", meta
 121:     return None, {}
 122: 
 123: 
 124: def _dedupe_recommendations(items: list[dict[str, Any]]) -> list[dict[str, Any]]:
 125:     merged: dict[tuple[str, float], dict[str, Any]] = {}
 126:     passthrough: list[dict[str, Any]] = []
 127:     for item in items:
 128:         action = item.get("action")
 129:         delta = item.get("modeled_delta")
 130:         if not action or delta is None:
 131:             passthrough.append(item)
 132:             continue
 133:         try:
 134:             delta_key = round(float(delta), 3)
 135:         except (TypeError, ValueError):
 136:             passthrough.append(item)
 137:             continue
 138:         key = (str(action), delta_key)
 139:         if key not in merged:
 140:             merged_item = dict(item)
 141:             merged_item["merged_titles"] = [item.get("title") or ""]
 142:             merged[key] = merged_item
 143:             continue
 144:         current = merged[key]
 145:         current["merged_titles"].append(item.get("title") or "")
 146:         current["observed_count"] = int(current.get("observed_count") or 0) + int(
 147:             item.get("observed_count") or 0
 148:         )
 149:         if isinstance(current.get("evidence"), list) and isinstance(
 150:             item.get("evidence"), list
 151:         ):
 152:             merged_evidence = current["evidence"] + item["evidence"]
 153:             current["evidence"] = merged_evidence[:3]
 154:     return list(merged.values()) + passthrough
 155: 
 156: 
 157: def _build_recommendations(report: dict[str, Any]) -> dict[str, Any]:
 158:     known = report.get("known_issues")
 159:     if not isinstance(known, dict):
 160:         return {
 161:             "status": "no_known_issues",
 162:             "summary": "No known issues attached; recommendations not generated.",
 163:             "items": [],
 164:         }
 165:     expected = known.get("expected_findings") or []
 166:     if not isinstance(expected, list) or not expected:
 167:         return {
 168:             "status": "no_expected_findings",
 169:             "summary": "Known issues attached but no expected findings provided.",
 170:             "items": [],
 171:         }
 172: 
 173:     items: list[dict[str, Any]] = []
 174:     for issue in expected:
 175:         if not isinstance(issue, dict):
 176:             continue
 177:         plugin_id = issue.get("plugin_id")
 178:         kind = issue.get("kind")
 179:         where = issue.get("where") if isinstance(issue.get("where"), dict) else None
 180:         contains = (
 181:             issue.get("contains") if isinstance(issue.get("contains"), dict) else None
 182:         )
 183:         min_count = issue.get("min_count")
 184:         max_count = issue.get("max_count")
 185:         title = issue.get("title") or issue.get("description") or ""
 186:         if not title:
 187:             label = f"{plugin_id or 'any'}:{kind or 'finding'}"
 188:         else:
 189:             label = title
 190: 
 191:         findings = _collect_findings_for_plugin(report, plugin_id, kind)
 192:         matched = [f for f in findings if _matches_expected(f, where, contains)]
 193:         count = len(matched)
 194: 
 195:         status = "confirmed"
 196:         if count == 0:
 197:             status = "missing"
 198:         if min_count is not None:
 199:             try:
 200:                 if count < int(min_count):
 201:                     status = "below_min"
 202:             except (TypeError, ValueError):
 203:                 pass
 204:         if max_count is not None:
 205:             try:
 206:                 if count > int(max_count):
 207:                     status = "over_limit"
 208:             except (TypeError, ValueError):
 209:                 pass
 210: 
 211:         process_hint = _process_hint(where)
 212:         recommendation, meta = _capacity_scale_recommendation(
 213:             kind, matched, label, process_hint
 214:         )
 215:         if not recommendation:
 216:             recommendation = _recommendation_text(status, label, process_hint)
 217:             meta = {"action": None, "modeled_delta": None}
 218: 
 219:         evidence: list[dict[str, Any]] = []
 220:         for item in matched[:3]:
 221:             snippet: dict[str, Any] = {"kind": item.get("kind")}
 222:             for key in ("feature", "pair", "row_index", "index", "score", "metric"):
 223:                 if key in item:
 224:                     snippet[key] = item.get(key)
 225:             evidence.append(snippet)
 226: 
 227:         items.append(
 228:             {
 229:                 "title": label,
 230:                 "status": status,
 231:                 "recommendation": recommendation,
 232:                 "plugin_id": plugin_id,
 233:                 "kind": kind,
 234:                 "where": where,
 235:                 "contains": contains,
 236:                 "expected": {"min_count": min_count, "max_count": max_count},
 237:                 "observed_count": count,
 238:                 "evidence": evidence,
 239:                 "action": meta.get("action"),
 240:                 "modeled_delta": meta.get("modeled_delta"),
 241:             }
 242:         )
 243:     deduped = _dedupe_recommendations(items)
 244:     return {
 245:         "status": "ok",
 246:         "summary": f"Generated {len(deduped)} recommendation(s) from known issues.",
 247:         "items": deduped,
 248:     }
 249: 
 250: 
 251: def _build_executive_summary(report: dict[str, Any]) -> list[str]:
 252:     plugins = report.get("plugins", {}) or {}
 253:     queue_plugin = plugins.get("analysis_queue_delay_decomposition")
 254:     if not isinstance(queue_plugin, dict):
 255:         return []
 256:     findings = queue_plugin.get("findings") or []
 257:     qemail_stats = None
 258:     impact = None
 259:     scale = None
 260:     for item in findings:
 261:         if not isinstance(item, dict):
 262:             continue
 263:         kind = item.get("kind")
 264:         if kind == "eligible_wait_process_stats" and item.get("process_norm") == "qemail":
 265:             qemail_stats = item
 266:         elif kind == "eligible_wait_impact":
 267:             impact = item
 268:         elif kind == "capacity_scale_model":
 269:             scale = item
 270: 
 271:     lines: list[str] = []
 272:     if qemail_stats and impact:
 273:         q_gt = qemail_stats.get("eligible_wait_gt_hours_total")
 274:         total_gt = impact.get("eligible_wait_gt_hours_total")
 275:         runs_total = qemail_stats.get("runs_total")
 276:         if isinstance(q_gt, (int, float)) and isinstance(total_gt, (int, float)) and total_gt:
 277:             share = (float(q_gt) / float(total_gt)) * 100.0
 278:             runs_text = f" across {int(runs_total):,} runs" if isinstance(runs_total, (int, float)) else ""
 279:             lines.append(
 280:                 "QEMAIL is a major close-cycle drag: "
 281:                 f"{float(q_gt):.2f}h of >threshold eligible wait out of "
 282:                 f"{float(total_gt):.2f}h total ({share:.1f}%).{runs_text}"
 283:             )
 284: 
 285:     if scale:
 286:         base = scale.get("eligible_wait_gt_hours_without_target")
 287:         modeled = scale.get("eligible_wait_gt_hours_modeled")
 288:         if isinstance(base, (int, float)) and isinstance(modeled, (int, float)) and base:
 289:             delta = float(base) - float(modeled)
 290:             pct = (delta / float(base)) * 100.0 if base else 0.0
 291:             lines.append(
 292:                 "QPEC+1 recommended (modeled): "
 293:                 f">threshold eligible wait drops from {float(base):.2f}h to "
 294:                 f"{float(modeled):.2f}h (Δ {delta:.2f}h, {pct:.1f}%)."
 295:             )
 296: 
 297:     return lines
 298: 
 299: 
 300: def _evaluate_known_issues(report: dict[str, Any]) -> list[dict[str, Any]]:
 301:     known = report.get("known_issues")
 302:     if not isinstance(known, dict):
 303:         return []
 304:     expected = known.get("expected_findings") or []
 305:     if not isinstance(expected, list) or not expected:
 306:         return []
 307:     evaluations: list[dict[str, Any]] = []
 308:     for issue in expected:
 309:         if not isinstance(issue, dict):
 310:             continue
 311:         plugin_id = issue.get("plugin_id")
 312:         kind = issue.get("kind")
 313:         where = issue.get("where") if isinstance(issue.get("where"), dict) else None
 314:         contains = issue.get("contains") if isinstance(issue.get("contains"), dict) else None
 315:         min_count = issue.get("min_count")
 316:         max_count = issue.get("max_count")
 317:         title = issue.get("title") or issue.get("description") or ""
 318:         label = title or f"{plugin_id or 'any'}:{kind or 'finding'}"
 319: 
 320:         findings = _collect_findings_for_plugin(report, plugin_id, kind)
 321:         matched = [f for f in findings if _matches_expected(f, where, contains)]
 322:         count = len(matched)
 323: 
 324:         status = "confirmed"
 325:         if count == 0:
 326:             status = "missing"
 327:         if min_count is not None:
 328:             try:
 329:                 if count < int(min_count):
 330:                     status = "below_min"
 331:             except (TypeError, ValueError):
 332:                 pass
 333:         if max_count is not None:
 334:             try:
 335:                 if count > int(max_count):
 336:                     status = "over_limit"
 337:             except (TypeError, ValueError):
 338:                 pass
 339: 
 340:         evaluations.append(
 341:             {
 342:                 "issue": issue,
 343:                 "label": label,
 344:                 "plugin_id": plugin_id,
 345:                 "kind": kind,
 346:                 "where": where,
 347:                 "contains": contains,
 348:                 "min_count": min_count,
 349:                 "max_count": max_count,
 350:                 "matched": matched,
 351:                 "count": count,
 352:                 "status": status,
 353:                 "process_hint": _process_hint(where),
 354:             }
 355:         )
 356:     return evaluations
 357: 
 358: 
 359: def _format_issue_value(value: Any, digits: int = 3) -> str:
 360:     if value is None:
 361:         return "n/a"
 362:     if isinstance(value, bool):
 363:         return "true" if value else "false"
 364:     if isinstance(value, (int, float)):
 365:         if isinstance(value, int):
 366:             return f"{value}"
 367:         if float(value).is_integer():
 368:             return f"{int(value)}"
 369:         return f"{float(value):.{digits}f}"
 370:     return str(value)
 371: 
 372: 
 373: def _artifact_paths(report: dict[str, Any], plugin_id: str | None) -> list[str]:
 374:     if not plugin_id:
 375:         return []
 376:     plugin = report.get("plugins", {}).get(plugin_id) or {}
 377:     artifacts = plugin.get("artifacts") or []
 378:     paths = []
 379:     for artifact in artifacts:
 380:         if isinstance(artifact, dict):
 381:             path = artifact.get("path")
 382:             if isinstance(path, str) and path:
 383:                 paths.append(path)
 384:     return paths
 385: 
 386: 
 387: def _metric_spec(kind: str | None) -> dict[str, Any]:
 388:     return {
 389:         "eligible_wait_process_stats": {
 390:             "name": "Eligible wait > threshold (hours)",
 391:             "definition": "Total hours of eligible wait above the threshold for the process.",
 392:             "baseline_field": "eligible_wait_gt_hours_total",
 393:             "observed_field": "eligible_wait_gt_hours_total",
 394:             "denominator_field": "runs_total",
 395:         },
 396:         "eligible_wait_impact": {
 397:             "name": "Eligible wait > threshold (hours)",
 398:             "definition": "Total hours of eligible wait above the threshold (all runs).",
 399:             "baseline_field": "eligible_wait_gt_hours_total",
 400:             "observed_field": "eligible_wait_gt_hours_without_target",
 401:             "denominator_field": "runs_total",
 402:         },
 403:         "capacity_scale_model": {
 404:             "name": "Eligible wait > threshold (hours)",
 405:             "definition": "Modeled eligible wait above the threshold after capacity scaling.",
 406:             "baseline_field": "eligible_wait_gt_hours_without_target",
 407:             "observed_field": "eligible_wait_gt_hours_modeled",
 408:             "denominator_field": "runs_total",
 409:         },
 410:         "capacity_scaling": {
 411:             "name": "Eligible wait (hours)",
 412:             "definition": "Modeled eligible wait hours after capacity scaling.",
 413:             "baseline_field": "baseline_wait_hours",
 414:             "observed_field": "modeled_wait_hours",
 415:             "denominator_field": "rows",
 416:         },
 417:         "close_cycle_capacity_model": {
 418:             "name": "Close-cycle median duration (seconds)",
 419:             "definition": "Modeled median close-cycle duration under added capacity.",
 420:             "baseline_field": "baseline_median_sec",
 421:             "observed_field": "modeled_median_sec",
 422:             "denominator_field": "bucket_count",
 423:         },
 424:         "close_cycle_capacity_impact": {
 425:             "name": "Close-cycle median duration (seconds)",
 426:             "definition": "Measured close-cycle median duration shift.",
 427:             "baseline_field": "baseline_median_sec",
 428:             "observed_field": "modeled_median_sec",
 429:             "denominator_field": "bucket_count",
 430:         },
 431:         "close_cycle_revenue_compression": {
 432:             "name": "Close-cycle span (days)",
 433:             "definition": "Median close-cycle span by revenue month.",
 434:             "baseline_field": "baseline_span_days_median",
 435:             "observed_field": "baseline_span_days_median",
 436:             "denominator_field": "months",
 437:         },
 438:     }.get(kind or "", {"name": "Finding count", "definition": "Count of matching findings."})
 439: 
 440: 
 441: def _denominator_text(item: dict[str, Any], report: dict[str, Any], spec: dict[str, Any]) -> str:
 442:     field = spec.get("denominator_field")
 443:     value = item.get(field) if isinstance(field, str) else None
 444:     parts: list[str] = []
 445:     if value is not None:
 446:         parts.append(f"{field}={_format_issue_value(value)}")
 447:     if "close_cycle_start_day" in item and "close_cycle_end_day" in item:
 448:         parts.append(
 449:             f"close_window=day{item.get('close_cycle_start_day')}-day{item.get('close_cycle_end_day')}"
 450:         )
 451:     if "close_window_mode" in item:
 452:         parts.append(f"close_window_mode={item.get('close_window_mode')}")
 453:     if not parts:
 454:         rows = report.get("input", {}).get("rows")
 455:         if rows is not None:
 456:             parts.append(f"rows={rows}")
 457:     return ", ".join(parts) if parts else "n/a"
 458: 
 459: 
 460: def _impact_hours(item: dict[str, Any]) -> float:
 461:     for field in (
 462:         "eligible_wait_gt_hours_total",
 463:         "eligible_wait_hours_total",
 464:         "baseline_wait_hours",
 465:         "reduction_hours",
 466:     ):
 467:         value = item.get(field)
 468:         if isinstance(value, (int, float)):
 469:             return float(value)
 470:     baseline_days = item.get("baseline_span_days_median")
 471:     if isinstance(baseline_days, (int, float)):
 472:         return float(baseline_days) * 24.0
 473:     baseline_sec = item.get("baseline_median_sec")
 474:     if isinstance(baseline_sec, (int, float)):
 475:         return float(baseline_sec) / 3600.0
 476:     return 0.0
 477: 
 478: 
 479: def _confidence_weight(item: dict[str, Any], issue: dict[str, Any]) -> float:
 480:     for key in ("confidence_weight", "confidence"):
 481:         value = issue.get(key) if isinstance(issue, dict) else None
 482:         if isinstance(value, (int, float)):
 483:             return max(0.0, min(1.0, float(value)))
 484:     value = item.get("confidence")
 485:     if isinstance(value, (int, float)):
 486:         return max(0.0, min(1.0, float(value)))
 487:     p_value = item.get("p_value")
 488:     if isinstance(p_value, (int, float)):
 489:         return confidence_from_p(float(p_value))
 490:     return 0.5
 491: 
 492: 
 493: def _controllability_weight(kind: str | None, issue: dict[str, Any]) -> float:
 494:     override = issue.get("controllability_weight") if isinstance(issue, dict) else None
 495:     if isinstance(override, (int, float)):
 496:         return max(0.0, min(1.0, float(override)))
 497:     mapping = {
 498:         "capacity_scale_model": 0.9,
 499:         "capacity_scaling": 0.9,
 500:         "close_cycle_capacity_model": 0.8,
 501:         "close_cycle_capacity_impact": 0.8,
 502:         "eligible_wait_process_stats": 0.7,
 503:         "eligible_wait_impact": 0.7,
 504:         "close_cycle_revenue_compression": 0.6,
 505:     }
 506:     return mapping.get(kind or "", 0.5)
 507: 
 508: 
 509: def _issue_cards(report: dict[str, Any]) -> list[dict[str, Any]]:
 510:     cards: list[dict[str, Any]] = []
 511:     for evaluation in _evaluate_known_issues(report):
 512:         matched = evaluation.get("matched") or []
 513:         item = matched[0] if matched else {}
 514:         spec = _metric_spec(evaluation.get("kind"))
 515:         metric_name = spec.get("name", "Metric")
 516:         definition = spec.get("definition", "")
 517:         baseline_field = spec.get("baseline_field")
 518:         observed_field = spec.get("observed_field")
 519:         baseline_val = item.get(baseline_field) if baseline_field else None
 520:         observed_val = item.get(observed_field) if observed_field else None
 521:         target_val = None
 522:         if evaluation.get("kind") == "capacity_scale_model" and isinstance(
 523:             baseline_val, (int, float)
 524:         ):
 525:             scale = item.get("scale_factor")
 526:             if isinstance(scale, (int, float)):
 527:                 target_val = float(baseline_val) * float(scale)
 528:         if evaluation.get("kind") == "capacity_scaling" and isinstance(
 529:             baseline_val, (int, float)
 530:         ):
 531:             scale = item.get("scale_factor")
 532:             if isinstance(scale, (int, float)):
 533:                 target_val = float(baseline_val) / float(scale)
 534:         if evaluation.get("kind") == "close_cycle_capacity_model":
 535:             target_reduction = item.get("target_reduction")
 536:             if isinstance(target_reduction, (int, float)) and isinstance(
 537:                 baseline_val, (int, float)
 538:             ):
 539:                 target_val = float(baseline_val) * (1.0 + float(target_reduction))
 540:         if evaluation.get("kind") == "close_cycle_revenue_compression":
 541:             target_val = item.get("target_days")
 542: 
 543:         status = evaluation.get("status")
 544:         pass_fail_reason = "REVIEW"
 545:         if status == "confirmed":
 546:             pass_fail_reason = "PASS: expected evidence present."
 547:         elif status == "missing":
 548:             pass_fail_reason = "FAIL: no matching findings."
 549:         elif status == "below_min":
 550:             pass_fail_reason = "FAIL: observed below minimum threshold."
 551:         elif status == "over_limit":
 552:             pass_fail_reason = "FAIL: observed above maximum threshold."
 553: 
 554:         denominator = _denominator_text(item, report, spec)
 555:         artifacts = _artifact_paths(report, evaluation.get("plugin_id"))
 556:         impact_hours = _impact_hours(item)
 557:         confidence_weight = _confidence_weight(item, evaluation.get("issue") or {})
 558:         controllability_weight = _controllability_weight(
 559:             evaluation.get("kind"), evaluation.get("issue") or {}
 560:         )
 561:         relevance_score = impact_hours * confidence_weight * controllability_weight
 562: 
 563:         cards.append(
 564:             {
 565:                 "title": evaluation.get("label"),
 566:                 "plugin_id": evaluation.get("plugin_id"),
 567:                 "kind": evaluation.get("kind"),
 568:                 "metric_name": metric_name,
 569:                 "definition": definition,
 570:                 "denominator": denominator,
 571:         "baseline": _format_issue_value(baseline_val),
 572:         "target_threshold": _format_issue_value(target_val),
 573:         "observed": _format_issue_value(observed_val),
 574:                 "status": status,
 575:                 "reason": pass_fail_reason,
 576:                 "artifact_paths": artifacts,
 577:                 "impact_hours": impact_hours,
 578:                 "confidence_weight": confidence_weight,
 579:                 "controllability_weight": controllability_weight,
 580:                 "relevance_score": relevance_score,
 581:             }
 582:         )
 583:     return cards
 584: 
 585: 
 586: def _waterfall_summary(report: dict[str, Any]) -> dict[str, Any] | None:
 587:     plugins = report.get("plugins", {}) or {}
 588:     queue_plugin = plugins.get("analysis_queue_delay_decomposition")
 589:     if not isinstance(queue_plugin, dict):
 590:         return None
 591:     findings = queue_plugin.get("findings") or []
 592:     impact = None
 593:     qemail = None
 594:     scale = None
 595:     for item in findings:
 596:         if not isinstance(item, dict):
 597:             continue
 598:         kind = item.get("kind")
 599:         if kind == "eligible_wait_impact":
 600:             impact = item
 601:         elif kind == "capacity_scale_model":
 602:             scale = item
 603:         elif kind == "eligible_wait_process_stats" and item.get("process_norm") == "qemail":
 604:             qemail = item
 605:     if impact is None:
 606:         return None
 607:     if qemail is None:
 608:         candidates = [
 609:             f
 610:             for f in findings
 611:             if isinstance(f, dict) and f.get("kind") == "eligible_wait_process_stats"
 612:         ]
 613:         if candidates:
 614:             qemail = max(
 615:                 candidates,
 616:                 key=lambda f: float(f.get("eligible_wait_gt_hours_total") or 0.0),
 617:             )
 618:     total = impact.get("eligible_wait_gt_hours_total")
 619:     qemail_val = qemail.get("eligible_wait_gt_hours_total") if qemail else None
 620:     remainder = impact.get("eligible_wait_gt_hours_without_target")
 621:     modeled = scale.get("eligible_wait_gt_hours_modeled") if scale else None
 622:     return {
 623:         "total": total,
 624:         "qemail": qemail_val,
 625:         "remainder": remainder,
 626:         "modeled": modeled,
 627:         "scale_factor": scale.get("scale_factor") if scale else None,
 628:     }
 629: 
 630: 
 631: def _load_artifact_json(run_dir: Path, path: str | None) -> dict[str, Any] | None:
 632:     if not path:
 633:         return None
 634:     target = run_dir / path
 635:     if not target.exists():
 636:         return None
 637:     try:
 638:         return json.loads(target.read_text(encoding="utf-8"))
 639:     except json.JSONDecodeError:
 640:         return None
 641: 
 642: 
 643: def _queue_delay_results(report: dict[str, Any], run_dir: Path) -> dict[str, Any] | None:
 644:     plugin = report.get("plugins", {}).get("analysis_queue_delay_decomposition")
 645:     if not isinstance(plugin, dict):
 646:         return None
 647:     artifacts = plugin.get("artifacts") or []
 648:     for artifact in artifacts:
 649:         if not isinstance(artifact, dict):
 650:             continue
 651:         path = artifact.get("path")
 652:         if isinstance(path, str) and path.endswith("results.json"):
 653:             payload = _load_artifact_json(run_dir, path)
 654:             if payload:
 655:                 return payload
 656:     return None
 657: 
 658: 
 659: def _dedupe_recommendations_text(items: list[dict[str, Any]]) -> list[dict[str, Any]]:
 660:     seen: set[tuple[str, str]] = set()
 661:     deduped: list[dict[str, Any]] = []
 662:     for item in items:
 663:         text = str(item.get("recommendation") or "").strip()
 664:         delta = item.get("modeled_delta")
 665:         delta_text = _format_issue_value(delta) if delta is not None else ""
 666:         key = (text, delta_text)
 667:         if key in seen:
 668:             continue
 669:         seen.add(key)
 670:         deduped.append(item)
 671:     return deduped
 672: 
 673: 
 674: def _write_csv(path: Path, rows: list[dict[str, Any]], headers: list[str]) -> None:
 675:     path.parent.mkdir(parents=True, exist_ok=True)
 676:     with path.open("w", encoding="utf-8", newline="") as handle:
 677:         writer = csv.DictWriter(handle, fieldnames=headers)
 678:         writer.writeheader()
 679:         for row in rows:
 680:             writer.writerow({key: row.get(key, "") for key in headers})
 681: 
 682: 
 683: def _collapse_findings(
 684:     findings: list[Any], max_examples: int = 10
 685: ) -> dict[str, Any]:
 686:     if not isinstance(findings, list) or not findings:
 687:         return {"count": 0, "unique_count": 0, "top_examples": []}
 688:     deduped: dict[str, dict[str, Any]] = {}
 689:     for item in findings:
 690:         key = json_dumps(item) if isinstance(item, (dict, list)) else str(item)
 691:         if key not in deduped:
 692:             deduped[key] = {"count": 0, "example": item}
 693:         deduped[key]["count"] += 1
 694:     ordered = sorted(
 695:         deduped.values(), key=lambda entry: entry["count"], reverse=True
 696:     )
 697:     total = len(findings)
 698:     return {
 699:         "count": total,
 700:         "unique_count": len(deduped),
 701:         "top_examples": ordered[:max_examples],
 702:     }
 703: 
 704: 
 705: def _plugin_summary_rows(report: dict[str, Any]) -> tuple[list[tuple[str, int, str]], list[tuple[str, int, str]]]:
 706:     rows: list[tuple[str, int, str]] = []
 707:     plugins = report.get("plugins", {}) or {}
 708:     for plugin_id, data in plugins.items():
 709:         if not isinstance(data, dict):
 710:             continue
 711:         findings = data.get("findings") or []
 712:         summary = (data.get("summary") or "").strip()
 713:         rows.append((plugin_id, len(findings), summary))
 714:     rows.sort()
 715:     yes_rows = [row for row in rows if row[1] > 0]
 716:     no_rows = [row for row in rows if row[1] == 0]
 717:     return yes_rows, no_rows
 718: 
 719: 
 720: def _format_plugin_table(rows: list[tuple[str, int, str]]) -> list[str]:
 721:     lines = ["| Plugin | Findings | One-line summary |", "|---|---:|---|"]
 722:     for plugin_id, count, summary in rows:
 723:         lines.append(f"| `{plugin_id}` | {count} | {summary} |")
 724:     return lines
 725: 
 726: 
 727: def _load_known_issues_fallback(run_dir: Path) -> dict[str, Any] | None:
 728:     known_dir = run_dir.parent.parent / "known_issues"
 729:     if not known_dir.exists():
 730:         return None
 731:     payloads: list[dict[str, Any]] = []
 732:     for path in sorted(known_dir.glob("*.yaml")):
 733:         try:
 734:             data = yaml.safe_load(path.read_text(encoding="utf-8"))
 735:         except Exception:
 736:             continue
 737:         if not isinstance(data, dict):
 738:             continue
 739:         if data.get("expected_findings"):
 740:             payloads.append(data)
 741:     if not payloads:
 742:         return None
 743:     expected: list[dict[str, Any]] = []
 744:     strict_values: list[bool] = []
 745:     notes: list[str] = []
 746:     for data in payloads:
 747:         strict_values.append(bool(data.get("strict", False)))
 748:         note = str(data.get("notes") or "").strip()
 749:         if note:
 750:             notes.append(note)
 751:         expected.extend(data.get("expected_findings") or [])
 752:     note_text = "Fallback merged from appdata/known_issues"
 753:     if notes:
 754:         note_text = f"{note_text} | " + " | ".join(notes)
 755:     return {
 756:         "scope_type": "fallback",
 757:         "scope_value": "appdata/known_issues",
 758:         "strict": all(strict_values) if strict_values else False,
 759:         "notes": note_text,
 760:         "natural_language": [],
 761:         "expected_findings": expected,
 762:     }
 763: 
 764: from .storage import Storage
 765: from .utils import json_dumps, now_iso, read_json, write_json
 766: 
 767: 
 768: def build_report(
 769:     storage: Storage, run_id: str, run_dir: Path, schema_path: Path
 770: ) -> dict[str, Any]:
 771:     run_row = storage.fetch_run(run_id)
 772:     if not run_row or not run_row.get("dataset_version_id"):
 773:         raise ValueError("Run dataset version not found")
 774:     upload_row = (
 775:         storage.fetch_upload(run_row["upload_id"])
 776:         if run_row.get("upload_id")
 777:         else None
 778:     )
 779:     from .dataset_io import DatasetAccessor
 780: 
 781:     accessor = DatasetAccessor(storage, run_row["dataset_version_id"])
 782:     info = accessor.info()
 783: 
 784:     plugin_rows = storage.fetch_plugin_results(run_id)
 785:     plugins: dict[str, Any] = {}
 786: 
 787:     def _ensure_measurement(findings: list[Any]) -> list[Any]:
 788:         for item in findings:
 789:             if isinstance(item, dict) and "measurement_type" not in item:
 790:                 item["measurement_type"] = "measured"
 791:         return findings
 792: 
 793:     def _canonicalize_payload(payload: Any) -> Any:
 794:         try:
 795:             return json.loads(json_dumps(payload))
 796:         except TypeError:
 797:             return payload
 798: 
 799:     def _sort_payload_list(items: list[Any]) -> list[Any]:
 800:         try:
 801:             return sorted(items, key=lambda item: json_dumps(item))
 802:         except Exception:
 803:             return items
 804: 
 805:     for row in sorted(plugin_rows, key=lambda item: item["plugin_id"]):
 806:         findings = json.loads(row["findings_json"])
 807:         if isinstance(findings, list):
 808:             findings = _ensure_measurement(findings)
 809:             findings = _sort_payload_list(findings)
 810:         artifacts = json.loads(row["artifacts_json"])
 811:         if isinstance(artifacts, list):
 812:             artifacts = _sort_payload_list(artifacts)
 813:         budget = None
 814:         if "budget_json" in row.keys() and row.get("budget_json"):
 815:             try:
 816:                 budget = json.loads(row["budget_json"])
 817:             except json.JSONDecodeError:
 818:                 budget = None
 819:         if not isinstance(budget, dict):
 820:             budget = {
 821:                 "row_limit": None,
 822:                 "sampled": False,
 823:                 "time_limit_ms": None,
 824:                 "cpu_limit_ms": None,
 825:             }
 826:         plugins[row["plugin_id"]] = {
 827:             "status": row["status"],
 828:             "summary": row["summary"],
 829:             "metrics": _canonicalize_payload(json.loads(row["metrics_json"])),
 830:             "findings": findings,
 831:             "artifacts": artifacts,
 832:             "budget": _canonicalize_payload(budget),
 833:             "error": json.loads(row["error_json"]) if row["error_json"] else None,
 834:         }
 835: 
 836:     dataset_version = storage.get_dataset_version(run_row["dataset_version_id"])
 837:     dataset_context = storage.get_dataset_version_context(run_row["dataset_version_id"])
 838:     project_row = None
 839:     if dataset_context and dataset_context.get("project_id"):
 840:         project_row = storage.fetch_project(dataset_context["project_id"])
 841:     dataset_template = storage.fetch_dataset_template(run_row["dataset_version_id"])
 842:     raw_format = None
 843:     raw_format_id = None
 844:     if dataset_version:
 845:         raw_format_id = dataset_version.get("raw_format_id")
 846:     if raw_format_id:
 847:         with storage.connection() as conn:
 848:             cur = conn.execute(
 849:                 """
 850:                 SELECT format_id, fingerprint, name, created_at
 851:                 FROM raw_formats
 852:                 WHERE format_id = ?
 853:                 """,
 854:                 (raw_format_id,),
 855:             )
 856:             row = cur.fetchone()
 857:             raw_format = dict(row) if row else None
 858:     if raw_format:
 859:         raw_format = {
 860:             "format_id": int(raw_format.get("format_id") or raw_format_id or 0),
 861:             "fingerprint": raw_format.get("fingerprint") or "",
 862:             "name": raw_format.get("name") or "",
 863:             "created_at": raw_format.get("created_at") or "",
 864:         }
 865: 
 866:     mapping = None
 867:     if dataset_template and dataset_template.get("mapping_json"):
 868:         try:
 869:             mapping = json.loads(dataset_template["mapping_json"])
 870:         except json.JSONDecodeError:
 871:             mapping = {}
 872: 
 873:     dataset_block = {
 874:         "dataset_version_id": run_row.get("dataset_version_id") or "unknown",
 875:     }
 876:     if dataset_context:
 877:         if dataset_context.get("project_id"):
 878:             dataset_block["project_id"] = dataset_context["project_id"]
 879:         if dataset_context.get("dataset_id"):
 880:             dataset_block["dataset_id"] = dataset_context["dataset_id"]
 881:         if dataset_context.get("table_name"):
 882:             dataset_block["table_name"] = dataset_context["table_name"]
 883:     if dataset_version:
 884:         if dataset_version.get("data_hash"):
 885:             dataset_block["data_hash"] = dataset_version["data_hash"]
 886:         if dataset_version.get("row_count") is not None:
 887:             dataset_block["row_count"] = int(dataset_version["row_count"])
 888:         if dataset_version.get("column_count") is not None:
 889:             dataset_block["column_count"] = int(dataset_version["column_count"])
 890:         if dataset_version.get("raw_format_id"):
 891:             dataset_block["raw_format_id"] = int(dataset_version["raw_format_id"])
 892: 
 893:     def _string_or_empty(value: Any) -> str:
 894:         return value if isinstance(value, str) else ""
 895: 
 896:     lineage_plugins: dict[str, Any] = {}
 897:     for row in plugin_rows:
 898:         lineage_plugins[row["plugin_id"]] = {
 899:             "plugin_version": _string_or_empty(row.get("plugin_version")),
 900:             "code_hash": _string_or_empty(row.get("code_hash")),
 901:             "settings_hash": _string_or_empty(row.get("settings_hash")),
 902:             "dataset_hash": _string_or_empty(row.get("dataset_hash")),
 903:             "executed_at": _string_or_empty(row.get("executed_at")),
 904:             "status": _string_or_empty(row.get("status")),
 905:             "summary": _string_or_empty(row.get("summary")),
 906:         }
 907: 
 908:     template_block = None
 909:     if dataset_template:
 910:         template_block = {
 911:             "template_id": int(dataset_template["template_id"]),
 912:             "table_name": dataset_template.get("table_name") or "",
 913:             "status": dataset_template.get("status") or "",
 914:             "mapping_hash": dataset_template.get("mapping_hash") or "",
 915:             "mapping": mapping if isinstance(mapping, dict) else {},
 916:         }
 917:         if dataset_template.get("template_name"):
 918:             template_block["template_name"] = dataset_template["template_name"]
 919:         if dataset_template.get("template_version"):
 920:             template_block["template_version"] = dataset_template["template_version"]
 921: 
 922:     known_block = None
 923:     known_scope_type = ""
 924:     known_scope_value = ""
 925:     if project_row and project_row.get("erp_type"):
 926:         known_scope_type = "erp_type"
 927:         known_scope_value = str(project_row.get("erp_type") or "unknown").strip() or "unknown"
 928:         known_block = storage.fetch_known_issues(known_scope_value, known_scope_type)
 929:     if not known_block and upload_row and upload_row.get("sha256"):
 930:         known_scope_type = "sha256"
 931:         known_scope_value = str(upload_row.get("sha256") or "")
 932:         if known_scope_value:
 933:             known_block = storage.fetch_known_issues(known_scope_value, known_scope_type)
 934:     if not known_block and dataset_block.get("data_hash"):
 935:         data_hash = str(dataset_block.get("data_hash") or "")
 936:         if re.fullmatch(r"[a-f0-9]{64}", data_hash):
 937:             known_scope_type = "sha256"
 938:             known_scope_value = data_hash
 939:             known_block = storage.fetch_known_issues(known_scope_value, known_scope_type)
 940: 
 941:     known_payload = None
 942:     if known_block:
 943:         known_payload = {
 944:             "scope_type": known_block.get("scope_type") or known_scope_type,
 945:             "scope_value": known_block.get("scope_value") or known_scope_value,
 946:             "strict": bool(known_block.get("strict", True)),
 947:             "notes": known_block.get("notes") or "",
 948:             "natural_language": known_block.get("natural_language") or [],
 949:             "expected_findings": known_block.get("expected_findings") or [],
 950:         }
 951: 
 952:     report = {
 953:         "run_id": run_id,
 954:         "created_at": now_iso(),
 955:         "status": "completed",
 956:         "input": {
 957:             "filename": run_row.get("input_filename") or "unknown",
 958:             **info,
 959:         },
 960:         "lineage": {
 961:             "run": {
 962:                 "run_id": run_id,
 963:                 "created_at": run_row.get("created_at") or "",
 964:                 "status": run_row.get("status") or "",
 965:                 "run_seed": int(run_row.get("run_seed") or 0),
 966:             },
 967:             "input": {
 968:                 "upload_id": run_row.get("upload_id") or "",
 969:                 "filename": run_row.get("input_filename") or "unknown",
 970:                 "canonical_path": run_row.get("canonical_path") or "",
 971:                 "input_hash": run_row.get("input_hash") or "",
 972:                 "sha256": upload_row.get("sha256") if upload_row else "",
 973:                 "size_bytes": int(upload_row.get("size_bytes") or 0)
 974:                 if upload_row
 975:                 else 0,
 976:             },
 977:             "dataset": dataset_block,
 978:             "raw_format": raw_format,
 979:             "template": template_block,
 980:             "plugins": lineage_plugins,
 981:         },
 982:         "plugins": plugins,
 983:     }
 984:     if not known_payload:
 985:         known_payload = _load_known_issues_fallback(run_dir)
 986:     if known_payload:
 987:         report["known_issues"] = known_payload
 988:     report["recommendations"] = _build_recommendations(report)
 989:     evaluation_path = run_dir / "evaluation.json"
 990:     if evaluation_path.exists():
 991:         try:
 992:             report["evaluation"] = json.loads(evaluation_path.read_text(encoding="utf-8"))
 993:         except json.JSONDecodeError:
 994:             report["evaluation"] = None
 995: 
 996:     schema = read_json(schema_path)
 997:     validate(instance=report, schema=schema)
 998:     return report
 999: 
1000: 
1001: def write_report(report: dict[str, Any], run_dir: Path) -> None:
1002:     report_path = run_dir / "report.json"
1003:     write_json(report_path, report)
1004: 
1005:     lines = ["# Statistic Harness Report", ""]
1006:     lines.append("## Decision")
1007:     lines.append("")
1008:     lines.append(f"Run ID: {report.get('run_id')}")
1009:     lines.append(f"Created: {report.get('created_at')}")
1010:     lines.append(f"Status: {report.get('status')}")
1011:     lines.append(f"Rows: {report.get('input', {}).get('rows')}")
1012:     lines.append(f"Cols: {report.get('input', {}).get('cols')}")
1013:     lines.append("")
1014: 
1015:     exec_summary = _build_executive_summary(report)
1016:     lines.append("### Executive Summary")
1017:     if exec_summary:
1018:         for entry in exec_summary:
1019:             lines.append(f"- {entry}")
1020:     else:
1021:         lines.append("No executive summary available.")
1022:     lines.append("")
1023: 
1024:     cards = _issue_cards(report)
1025:     cards_sorted = sorted(cards, key=lambda c: c.get("relevance_score", 0.0), reverse=True)
1026:     top_n = int(os.environ.get("STAT_HARNESS_DECISION_TOP_N", "5"))
1027:     decision_cards = cards_sorted[:top_n] if top_n > 0 else []
1028: 
1029:     lines.append("### Decision Items")
1030:     if decision_cards:
1031:         lines.append("| Issue | Score | Status | Reason |")
1032:         lines.append("|---|---:|---|---|")
1033:         for card in decision_cards:
1034:             title = card.get("title") or "Issue"
1035:             score = _format_issue_value(card.get("relevance_score"), digits=3)
1036:             status = card.get("status") or "unknown"
1037:             reason = card.get("reason") or ""
1038:             lines.append(f"| {title} | {score} | {status} | {reason} |")
1039:     else:
1040:         lines.append("No decision items available.")
1041:     lines.append("")
1042: 
1043:     lines.append("### Waterfall Summary")
1044:     waterfall = _waterfall_summary(report)
1045:     if waterfall and all(
1046:         isinstance(waterfall.get(key), (int, float)) for key in ("total", "qemail", "remainder")
1047:     ):
1048:         total = float(waterfall["total"])
1049:         qemail = float(waterfall["qemail"])
1050:         remainder = float(waterfall["remainder"])
1051:         modeled = waterfall.get("modeled")
1052:         scale = waterfall.get("scale_factor")
1053:         total_text = _format_issue_value(total)
1054:         qemail_text = _format_issue_value(qemail)
1055:         remainder_text = _format_issue_value(remainder)
1056:         lines.append(f"Total over-threshold eligible wait: {total_text}h")
1057:         lines.append(f"QEMAIL contribution: {qemail_text}h")
1058:         lines.append(
1059:             f"Remainder without QEMAIL: {remainder_text}h = {total_text}h - {qemail_text}h"
1060:         )
1061:         if isinstance(modeled, (int, float)):
1062:             modeled_text = _format_issue_value(float(modeled))
1063:             if isinstance(scale, (int, float)):
1064:                 scale_text = _format_issue_value(float(scale))
1065:                 lines.append(
1066:                     f"Modeled after add one server: {modeled_text}h = {remainder_text}h × {scale_text}"
1067:                 )
1068:             else:
1069:                 lines.append(f"Modeled after add one server: {modeled_text}h")
1070:     else:
1071:         lines.append("No waterfall summary available.")
1072:     lines.append("")
1073: 
1074:     recommendations = report.get("recommendations")
1075:     lines.append("### Recommendations")
1076:     if isinstance(recommendations, dict):
1077:         summary = recommendations.get("summary") or ""
1078:         if summary:
1079:             lines.append(summary)
1080:         items = recommendations.get("items") or []
1081:         if items:
1082:             lines.append("| Status | Recommendation |")
1083:             lines.append("|---|---|")
1084:             for item in items:
1085:                 if not isinstance(item, dict):
1086:                     continue
1087:                 status = item.get("status") or "unknown"
1088:                 rec = item.get("recommendation") or "Recommendation"
1089:                 lines.append(f"| {status} | {rec} |")
1090:         else:
1091:             lines.append("No recommendations available.")
1092:     else:
1093:         lines.append("No recommendations available.")
1094:     lines.append("")
1095: 
1096:     if decision_cards:
1097:         lines.append("### Issue Cards (Top)")
1098:         for card in decision_cards:
1099:             lines.append(f"#### {card.get('title')}")
1100:             lines.append(
1101:                 f"Metric: {card.get('metric_name')} — {card.get('definition')}"
1102:             )
1103:             lines.append(f"Denominator: {card.get('denominator')}")
1104:             lines.append(f"Baseline: {card.get('baseline')}")
1105:             lines.append(f"Target Threshold: {card.get('target_threshold')}")
1106:             lines.append(f"Observed: {card.get('observed')}")
1107:             lines.append(f"Decision: {card.get('reason')}")
1108:             artifacts = ", ".join(card.get("artifact_paths") or []) or "n/a"
1109:             lines.append(f"Evidence Artifacts: {artifacts}")
1110:             lines.append("")
1111: 
1112:     lines.append("## Appendix")
1113:     lines.append("")
1114:     lines.append("### Issue Cards (Full)")
1115:     if cards:
1116:         for card in cards:
1117:             lines.append(f"#### {card.get('title')}")
1118:             lines.append(
1119:                 f"Metric: {card.get('metric_name')} — {card.get('definition')}"
1120:             )
1121:             lines.append(f"Denominator: {card.get('denominator')}")
1122:             lines.append(f"Baseline: {card.get('baseline')}")
1123:             lines.append(f"Target Threshold: {card.get('target_threshold')}")
1124:             lines.append(f"Observed: {card.get('observed')}")
1125:             lines.append(f"Decision: {card.get('reason')}")
1126:             artifacts = ", ".join(card.get("artifact_paths") or []) or "n/a"
1127:             lines.append(f"Evidence Artifacts: {artifacts}")
1128:             lines.append("")
1129:     else:
1130:         lines.append("No issue cards available.")
1131:         lines.append("")
1132: 
1133:     lines.append("### Plugin Dumps")
1134:     for plugin_id in sorted(report.get("plugins", {}).keys()):
1135:         data = report["plugins"][plugin_id]
1136:         lines.append(f"#### {plugin_id}")
1137:         lines.append("```json")
1138:         lines.append(json_dumps(data))
1139:         lines.append("```")
1140:         lines.append("")
1141:     (run_dir / "report.md").write_text("\n".join(lines), encoding="utf-8")
1142: 
1143:     _write_business_summary(report, run_dir)
1144:     _write_engineering_summary(report, run_dir)
1145:     _write_appendix_raw(report, run_dir)
1146:     _write_slide_kit(report, run_dir)
1147: 
1148: 
1149: def _format_value(value: Any) -> str:
1150:     if isinstance(value, float):
1151:         return f"{value:.4f}"
1152:     if isinstance(value, (int, bool)):
1153:         return str(value)
1154:     if isinstance(value, list):
1155:         preview = ", ".join(str(v) for v in value[:5])
1156:         if len(value) > 5:
1157:             preview += ", ..."
1158:         return f"[{preview}]"
1159:     if isinstance(value, dict):
1160:         return "{...}"
1161:     text = str(value)
1162:     if len(text) > 80:
1163:         return text[:77] + "..."
1164:     return text
1165: 
1166: 
1167: def _format_metrics(metrics: dict[str, Any]) -> list[str]:
1168:     if not isinstance(metrics, dict):
1169:         return []
1170:     items: list[tuple[str, Any]] = []
1171:     for key, value in metrics.items():
1172:         if isinstance(value, (int, float, str, bool)):
1173:             items.append((key, value))
1174:     items = sorted(items, key=lambda item: item[0])
1175:     return [f"{key}: {_format_value(value)}" for key, value in items]
1176: 
1177: 
1178: def _format_findings(findings: list[Any]) -> list[str]:
1179:     rendered: list[str] = []
1180:     for finding in findings:
1181:         if not isinstance(finding, dict):
1182:             rendered.append(_format_value(finding))
1183:             continue
1184:         kind = finding.get("kind", "finding")
1185:         measurement = finding.get("measurement_type", "measured")
1186:         parts = [f"kind={kind}", f"measurement={measurement}"]
1187:         key_fields = [
1188:             "role",
1189:             "column",
1190:             "process",
1191:             "process_norm",
1192:             "process_id",
1193:             "process_name",
1194:             "module",
1195:             "module_cd",
1196:             "user",
1197:             "user_id",
1198:             "dimension",
1199:             "key",
1200:             "sequence",
1201:             "host",
1202:             "feature",
1203:             "metric",
1204:         ]
1205:         for field in key_fields:
1206:             if field in finding and finding[field] not in (None, ""):
1207:                 parts.append(f"{field}={_format_value(finding[field])}")
1208:         numeric_fields = [
1209:             key
1210:             for key, value in finding.items()
1211:             if isinstance(value, (int, float))
1212:             and key not in {"row_index"}
1213:             and (
1214:                 key.endswith("_sec")
1215:                 or key.endswith("_hours")
1216:                 or key.endswith("_count")
1217:                 or key.endswith("_runs")
1218:                 or key.endswith("_ratio")
1219:                 or key.endswith("_pct")
1220:                 or key in {"p50", "p95", "p99", "mean", "min", "max", "score"}
1221:             )
1222:         ]
1223:         for key in sorted(numeric_fields)[:6]:
1224:             parts.append(f"{key}={_format_value(finding[key])}")
1225:         evidence = finding.get("evidence")
1226:         if isinstance(evidence, dict):
1227:             row_ids = evidence.get("row_ids")
1228:             col_ids = evidence.get("column_ids")
1229:             if isinstance(row_ids, list) and row_ids:
1230:                 parts.append(f"rows={len(row_ids)}")
1231:             if isinstance(col_ids, list) and col_ids:
1232:                 parts.append(f"cols={len(col_ids)}")
1233:             query = evidence.get("query")
1234:             if query:
1235:                 parts.append(f"query={_format_value(query)}")
1236:         rendered.append(", ".join(parts))
1237:     return rendered
1238: 
1239: 
1240: def _matches_expected(
1241:     item: dict[str, Any], where: dict[str, Any] | None, contains: dict[str, Any] | None
1242: ) -> bool:
1243:     if where:
1244:         for key, expected in where.items():
1245:             if item.get(key) != expected:
1246:                 return False
1247:     if contains:
1248:         for key, expected in contains.items():
1249:             actual = item.get(key)
1250:             if isinstance(actual, str):
1251:                 if str(expected) not in actual:
1252:                     return False
1253:             elif isinstance(actual, (list, tuple, set)):
1254:                 if isinstance(expected, (list, tuple, set)):
1255:                     if not set(expected).issubset(set(actual)):
1256:                         return False
1257:                 else:
1258:                     if expected not in actual:
1259:                         return False
1260:             else:
1261:                 return False
1262:     return True
1263: 
1264: 
1265: def _format_known_issue_checks(
1266:     report: dict[str, Any], expected: list[dict[str, Any]]
1267: ) -> list[str]:
1268:     items: list[str] = []
1269:     for entry in expected:
1270:         if not isinstance(entry, dict):
1271:             continue
1272:         plugin_id = entry.get("plugin_id")
1273:         kind = entry.get("kind")
1274:         if not kind:
1275:             continue
1276:         where = entry.get("where") or {}
1277:         contains = entry.get("contains") or {}
1278:         min_count = int(entry.get("min_count", 1))
1279:         max_count = entry.get("max_count")
1280:         candidates = []
1281:         for pid, plugin in report.get("plugins", {}).items():
1282:             if plugin_id and pid != plugin_id:
1283:                 continue
1284:             for item in plugin.get("findings", []):
1285:                 if item.get("kind") == kind:
1286:                     candidates.append(item)
1287:         matches = [item for item in candidates if _matches_expected(item, where, contains)]
1288:         status = "PASS" if len(matches) >= min_count and (max_count is None or len(matches) <= int(max_count)) else "FAIL"
1289:         detail = f"{len(matches)} match(es)"
1290:         title = entry.get("title") or entry.get("description") or ""
1291:         if title:
1292:             title = title.strip()
1293:         context = f"{kind} ({plugin_id or '*'})"
1294:         if title:
1295:             context = f"{title} :: {context}"
1296:         items.append(f"{status} - {context} - {detail}")
1297:     return items
````

## File: src/statistic_harness/core/storage.py
````python
   1: from __future__ import annotations
   2: 
   3: import hashlib
   4: import json
   5: import sqlite3
   6: from contextlib import contextmanager
   7: from dataclasses import asdict
   8: from pathlib import Path
   9: from typing import Any, Iterable
  10: 
  11: from .migrations import run_migrations
  12: from .types import PluginResult
  13: from .utils import (
  14:     DEFAULT_TENANT_ID,
  15:     ensure_dir,
  16:     json_dumps,
  17:     now_iso,
  18:     quote_identifier,
  19:     scope_key,
  20: )
  21: 
  22: 
  23: class Storage:
  24:     def __init__(self, db_path: Path, tenant_id: str | None = None) -> None:
  25:         ensure_dir(db_path.parent)
  26:         self.db_path = db_path
  27:         self.tenant_id = tenant_id or DEFAULT_TENANT_ID
  28:         with self.connection() as conn:
  29:             run_migrations(conn)
  30: 
  31:     def _tenant_id(self) -> str:
  32:         return self.tenant_id or DEFAULT_TENANT_ID
  33: 
  34:     def _scoped_key(self, scope_type: str, scope_value: str) -> str:
  35:         tenant_id = self._tenant_id()
  36:         if tenant_id == DEFAULT_TENANT_ID:
  37:             return scope_key(scope_type, scope_value)
  38:         return scope_key(f"{tenant_id}:{scope_type}", scope_value)
  39: 
  40:     def _scoped_value(self, value: str) -> str:
  41:         tenant_id = self._tenant_id()
  42:         if tenant_id == DEFAULT_TENANT_ID:
  43:             return value
  44:         prefix = f"{tenant_id}__"
  45:         if value.startswith(prefix):
  46:             return value
  47:         return f"{prefix}{value}"
  48: 
  49:     def _connect(self) -> sqlite3.Connection:
  50:         conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=30.0)
  51:         conn.row_factory = sqlite3.Row
  52:         conn.execute("PRAGMA foreign_keys = ON")
  53:         conn.execute("PRAGMA journal_mode = WAL")
  54:         conn.execute("PRAGMA synchronous = NORMAL")
  55:         conn.execute("PRAGMA temp_store = MEMORY")
  56:         conn.execute("PRAGMA busy_timeout = 30000")
  57:         return conn
  58: 
  59:     @contextmanager
  60:     def connection(self) -> Iterable[sqlite3.Connection]:
  61:         conn = self._connect()
  62:         try:
  63:             yield conn
  64:             conn.commit()
  65:         except Exception:
  66:             conn.rollback()
  67:             raise
  68:         finally:
  69:             conn.close()
  70: 
  71:     def create_run(
  72:         self,
  73:         run_id: str,
  74:         created_at: str,
  75:         status: str,
  76:         upload_id: str,
  77:         input_filename: str,
  78:         canonical_path: str,
  79:         settings: dict[str, Any],
  80:         error: dict[str, Any] | None,
  81:         run_seed: int = 0,
  82:         project_id: str | None = None,
  83:         dataset_id: str | None = None,
  84:         dataset_version_id: str | None = None,
  85:         input_hash: str | None = None,
  86:     ) -> None:
  87:         tenant_id = self._tenant_id()
  88:         with self.connection() as conn:
  89:             conn.execute(
  90:                 """
  91:                 INSERT INTO runs
  92:                 (run_id, tenant_id, created_at, status, upload_id, input_filename, canonical_path, settings_json, error_json,
  93:                  run_seed, project_id, dataset_id, dataset_version_id, input_hash)
  94:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
  95:                 """,
  96:                 (
  97:                     run_id,
  98:                     tenant_id,
  99:                     created_at,
 100:                     status,
 101:                     upload_id,
 102:                     input_filename,
 103:                     canonical_path,
 104:                     json_dumps(settings),
 105:                     json_dumps(error) if error else None,
 106:                     int(run_seed),
 107:                     project_id,
 108:                     dataset_id,
 109:                     dataset_version_id,
 110:                     input_hash,
 111:                 ),
 112:             )
 113: 
 114:     def update_run_status(
 115:         self, run_id: str, status: str, error: dict[str, Any] | None = None
 116:     ) -> None:
 117:         tenant_id = self._tenant_id()
 118:         with self.connection() as conn:
 119:             conn.execute(
 120:                 "UPDATE runs SET status = ?, error_json = ? WHERE run_id = ? AND tenant_id = ?",
 121:                 (status, json_dumps(error) if error else None, run_id, tenant_id),
 122:             )
 123: 
 124:     def save_plugin_result(
 125:         self,
 126:         run_id: str,
 127:         plugin_id: str,
 128:         plugin_version: str | None,
 129:         executed_at: str,
 130:         code_hash: str | None,
 131:         settings_hash: str | None,
 132:         dataset_hash: str | None,
 133:         result: PluginResult,
 134:     ) -> None:
 135:         tenant_id = self._tenant_id()
 136:         artifacts = [asdict(a) for a in result.artifacts]
 137:         error_payload = asdict(result.error) if result.error else None
 138:         budget_payload = result.budget if isinstance(result.budget, dict) else {}
 139:         with self.connection() as conn:
 140:             conn.execute(
 141:                 """
 142:                 INSERT INTO plugin_results_v2
 143:                 (run_id, tenant_id, plugin_id, plugin_version, executed_at, code_hash, settings_hash, dataset_hash,
 144:                  status, summary, metrics_json, findings_json, artifacts_json, error_json, budget_json)
 145:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
 146:                 """,
 147:                 (
 148:                     run_id,
 149:                     tenant_id,
 150:                     plugin_id,
 151:                     plugin_version,
 152:                     executed_at,
 153:                     code_hash,
 154:                     settings_hash,
 155:                     dataset_hash,
 156:                     result.status,
 157:                     result.summary,
 158:                     json_dumps(result.metrics),
 159:                     json_dumps(result.findings),
 160:                     json_dumps(artifacts),
 161:                     json_dumps(error_payload) if error_payload else None,
 162:                     json_dumps(budget_payload),
 163:                 ),
 164:             )
 165: 
 166:     def insert_plugin_execution(
 167:         self,
 168:         run_id: str,
 169:         plugin_id: str,
 170:         plugin_version: str | None,
 171:         started_at: str | None,
 172:         completed_at: str | None,
 173:         duration_ms: int | None,
 174:         status: str,
 175:         exit_code: int | None,
 176:         cpu_user: float | None,
 177:         cpu_system: float | None,
 178:         max_rss: int | None,
 179:         warnings_count: int | None,
 180:         stdout: str | None,
 181:         stderr: str | None,
 182:     ) -> None:
 183:         tenant_id = self._tenant_id()
 184:         with self.connection() as conn:
 185:             conn.execute(
 186:                 """
 187:                 INSERT INTO plugin_executions
 188:                 (run_id, tenant_id, plugin_id, plugin_version, started_at, completed_at, duration_ms,
 189:                  status, exit_code, cpu_user, cpu_system, max_rss, warnings_count, stdout, stderr)
 190:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
 191:                 """,
 192:                 (
 193:                     run_id,
 194:                     tenant_id,
 195:                     plugin_id,
 196:                     plugin_version,
 197:                     started_at,
 198:                     completed_at,
 199:                     duration_ms,
 200:                     status,
 201:                     exit_code,
 202:                     cpu_user,
 203:                     cpu_system,
 204:                     max_rss,
 205:                     warnings_count,
 206:                     stdout,
 207:                     stderr,
 208:                 ),
 209:             )
 210: 
 211:     def start_plugin_execution(
 212:         self,
 213:         run_id: str,
 214:         plugin_id: str,
 215:         plugin_version: str | None,
 216:         started_at: str,
 217:         status: str = "running",
 218:     ) -> int:
 219:         tenant_id = self._tenant_id()
 220:         with self.connection() as conn:
 221:             cur = conn.execute(
 222:                 """
 223:                 INSERT INTO plugin_executions
 224:                 (run_id, tenant_id, plugin_id, plugin_version, started_at, status)
 225:                 VALUES (?, ?, ?, ?, ?, ?)
 226:                 """,
 227:                 (run_id, tenant_id, plugin_id, plugin_version, started_at, status),
 228:             )
 229:             return int(cur.lastrowid)
 230: 
 231:     def update_plugin_execution(
 232:         self,
 233:         execution_id: int,
 234:         completed_at: str | None,
 235:         duration_ms: int | None,
 236:         status: str,
 237:         exit_code: int | None,
 238:         cpu_user: float | None,
 239:         cpu_system: float | None,
 240:         max_rss: int | None,
 241:         warnings_count: int | None,
 242:         stdout: str | None,
 243:         stderr: str | None,
 244:     ) -> None:
 245:         tenant_id = self._tenant_id()
 246:         with self.connection() as conn:
 247:             conn.execute(
 248:                 """
 249:                 UPDATE plugin_executions
 250:                 SET completed_at = ?, duration_ms = ?, status = ?, exit_code = ?,
 251:                     cpu_user = ?, cpu_system = ?, max_rss = ?, warnings_count = ?,
 252:                     stdout = ?, stderr = ?
 253:                 WHERE execution_id = ? AND tenant_id = ?
 254:                 """,
 255:                 (
 256:                     completed_at,
 257:                     duration_ms,
 258:                     status,
 259:                     exit_code,
 260:                     cpu_user,
 261:                     cpu_system,
 262:                     max_rss,
 263:                     warnings_count,
 264:                     stdout,
 265:                     stderr,
 266:                     execution_id,
 267:                     tenant_id,
 268:                 ),
 269:             )
 270: 
 271:     def fetch_plugin_executions(self, run_id: str) -> list[dict[str, Any]]:
 272:         tenant_id = self._tenant_id()
 273:         with self.connection() as conn:
 274:             cur = conn.execute(
 275:                 """
 276:                 SELECT *
 277:                 FROM plugin_executions
 278:                 WHERE run_id = ? AND tenant_id = ?
 279:                 ORDER BY execution_id ASC
 280:                 """,
 281:                 (run_id, tenant_id),
 282:             )
 283:             return [dict(row) for row in cur.fetchall()]
 284: 
 285:     def fetch_plugin_results(self, run_id: str) -> list[dict[str, Any]]:
 286:         tenant_id = self._tenant_id()
 287:         with self.connection() as conn:
 288:             cur = conn.execute(
 289:                 """
 290:                 SELECT pr.*
 291:                 FROM plugin_results_v2 pr
 292:                 JOIN (
 293:                     SELECT plugin_id, MAX(result_id) AS max_id
 294:                     FROM plugin_results_v2
 295:                     WHERE run_id = ? AND tenant_id = ?
 296:                     GROUP BY plugin_id
 297:                 ) latest
 298:                 ON pr.plugin_id = latest.plugin_id AND pr.result_id = latest.max_id
 299:                 WHERE pr.run_id = ? AND pr.tenant_id = ?
 300:                 """,
 301:                 (run_id, tenant_id, run_id, tenant_id),
 302:             )
 303:             return [dict(row) for row in cur.fetchall()]
 304: 
 305:     def fetch_run(self, run_id: str) -> dict[str, Any] | None:
 306:         tenant_id = self._tenant_id()
 307:         with self.connection() as conn:
 308:             cur = conn.execute(
 309:                 "SELECT * FROM runs WHERE run_id = ? AND tenant_id = ?",
 310:                 (run_id, tenant_id),
 311:             )
 312:             row = cur.fetchone()
 313:             return dict(row) if row else None
 314: 
 315:     def fetch_upload(self, upload_id: str) -> dict[str, Any] | None:
 316:         tenant_id = self._tenant_id()
 317:         with self.connection() as conn:
 318:             cur = conn.execute(
 319:                 "SELECT * FROM uploads WHERE upload_id = ? AND tenant_id = ?",
 320:                 (upload_id, tenant_id),
 321:             )
 322:             row = cur.fetchone()
 323:             return dict(row) if row else None
 324: 
 325:     def fetch_upload_by_sha256(self, sha256: str) -> dict[str, Any] | None:
 326:         tenant_id = self._tenant_id()
 327:         with self.connection() as conn:
 328:             cur = conn.execute(
 329:                 "SELECT * FROM uploads WHERE sha256 = ? AND tenant_id = ? ORDER BY created_at ASC LIMIT 1",
 330:                 (sha256, tenant_id),
 331:             )
 332:             row = cur.fetchone()
 333:             return dict(row) if row else None
 334: 
 335:     def list_uploads(self, limit: int = 50) -> list[dict[str, Any]]:
 336:         tenant_id = self._tenant_id()
 337:         with self.connection() as conn:
 338:             cur = conn.execute(
 339:                 """
 340:                 SELECT upload_id, filename, size_bytes, sha256, created_at
 341:                 FROM uploads
 342:                 WHERE tenant_id = ?
 343:                 ORDER BY created_at DESC
 344:                 LIMIT ?
 345:                 """,
 346:                 (tenant_id, int(limit)),
 347:             )
 348:             return [dict(row) for row in cur.fetchall()]
 349: 
 350:     def fetch_known_issues(
 351:         self, scope_value: str, scope_type: str = "sha256"
 352:     ) -> dict[str, Any] | None:
 353:         tenant_id = self._tenant_id()
 354:         key = self._scoped_key(scope_type, scope_value)
 355:         with self.connection() as conn:
 356:             cur = conn.execute(
 357:                 "SELECT * FROM known_issue_sets WHERE sha256 = ? AND tenant_id = ?",
 358:                 (key, tenant_id),
 359:             )
 360:             set_row = cur.fetchone()
 361:             if not set_row:
 362:                 return None
 363:             set_row = dict(set_row)
 364:             cur = conn.execute(
 365:                 """
 366:                 SELECT * FROM known_issues
 367:                 WHERE set_id = ?
 368:                 ORDER BY issue_id
 369:                 """,
 370:                 (set_row["set_id"],),
 371:             )
 372:             issues = []
 373:             for row in cur.fetchall():
 374:                 row = dict(row)
 375:                 entry: dict[str, Any] = {"kind": row["kind"]}
 376:                 if row.get("plugin_id"):
 377:                     entry["plugin_id"] = row["plugin_id"]
 378:                 if row.get("title"):
 379:                     entry["title"] = row["title"]
 380:                 if row.get("description"):
 381:                     entry["description"] = row["description"]
 382:                 if row.get("source_text"):
 383:                     entry["source_text"] = row["source_text"]
 384:                 if row.get("where_json"):
 385:                     try:
 386:                         entry["where"] = json.loads(row["where_json"])
 387:                     except json.JSONDecodeError:
 388:                         pass
 389:                 if row.get("contains_json"):
 390:                     try:
 391:                         entry["contains"] = json.loads(row["contains_json"])
 392:                     except json.JSONDecodeError:
 393:                         pass
 394:                 if row.get("min_count") is not None:
 395:                     entry["min_count"] = int(row["min_count"])
 396:                 if row.get("max_count") is not None:
 397:                     entry["max_count"] = int(row["max_count"])
 398:                 issues.append(entry)
 399:             return {
 400:                 "set_id": int(set_row["set_id"]),
 401:                 "sha256": set_row.get("sha256") or "",
 402:                 "scope_type": set_row.get("scope_type") or "sha256",
 403:                 "scope_value": set_row.get("scope_value") or set_row.get("sha256") or "",
 404:                 "upload_id": set_row.get("upload_id"),
 405:                 "strict": bool(set_row.get("strict", 0)),
 406:                 "notes": set_row.get("notes") or "",
 407:                 "natural_language": json.loads(set_row["nl_json"])
 408:                 if set_row.get("nl_json")
 409:                 else [],
 410:                 "expected_findings": issues,
 411:             }
 412: 
 413:     def upsert_known_issue_set(
 414:         self,
 415:         scope_value: str,
 416:         scope_type: str,
 417:         upload_id: str | None,
 418:         strict: bool,
 419:         notes: str,
 420:         natural_language: list[dict[str, Any]] | None = None,
 421:         conn: sqlite3.Connection | None = None,
 422:     ) -> int:
 423:         tenant_id = self._tenant_id()
 424:         key = self._scoped_key(scope_type, scope_value)
 425:         if conn is None:
 426:             with self.connection() as temp:
 427:                 return self.upsert_known_issue_set(
 428:                     scope_value,
 429:                     scope_type,
 430:                     upload_id,
 431:                     strict,
 432:                     notes,
 433:                     natural_language,
 434:                     temp,
 435:                 )
 436:         created_at = now_iso()
 437:         updated_at = created_at
 438:         nl_json = json_dumps(natural_language or []) if natural_language else None
 439:         conn.execute(
 440:             """
 441:             INSERT INTO known_issue_sets
 442:             (sha256, tenant_id, upload_id, strict, notes, created_at, updated_at, scope_type, scope_value, nl_json)
 443:             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
 444:             ON CONFLICT(sha256) DO UPDATE SET
 445:               tenant_id = excluded.tenant_id,
 446:               upload_id = excluded.upload_id,
 447:               strict = excluded.strict,
 448:               notes = excluded.notes,
 449:               updated_at = excluded.updated_at,
 450:               scope_type = excluded.scope_type,
 451:               scope_value = excluded.scope_value,
 452:               nl_json = excluded.nl_json
 453:             """,
 454:             (
 455:                 key,
 456:                 tenant_id,
 457:                 upload_id,
 458:                 1 if strict else 0,
 459:                 notes,
 460:                 created_at,
 461:                 updated_at,
 462:                 scope_type,
 463:                 scope_value,
 464:                 nl_json,
 465:             ),
 466:         )
 467:         cur = conn.execute(
 468:             "SELECT set_id FROM known_issue_sets WHERE sha256 = ? AND tenant_id = ?",
 469:             (key, tenant_id),
 470:         )
 471:         row = cur.fetchone()
 472:         if not row:
 473:             raise ValueError("Known issue set not found after upsert")
 474:         return int(row["set_id"])
 475: 
 476:     def replace_known_issues(
 477:         self,
 478:         set_id: int,
 479:         issues: list[dict[str, Any]],
 480:         conn: sqlite3.Connection | None = None,
 481:     ) -> None:
 482:         if conn is None:
 483:             with self.connection() as temp:
 484:                 self.replace_known_issues(set_id, issues, temp)
 485:                 return
 486:         conn.execute("DELETE FROM known_issues WHERE set_id = ?", (set_id,))
 487:         if not issues:
 488:             return
 489:         created_at = now_iso()
 490:         rows = []
 491:         for issue in issues:
 492:             rows.append(
 493:                 (
 494:                     set_id,
 495:                     issue.get("title"),
 496:                     issue.get("description"),
 497:                     issue.get("plugin_id"),
 498:                     issue.get("kind"),
 499:                     json_dumps(issue.get("where")) if issue.get("where") else None,
 500:                     json_dumps(issue.get("contains")) if issue.get("contains") else None,
 501:                     issue.get("min_count"),
 502:                     issue.get("max_count"),
 503:                     issue.get("source_text"),
 504:                     created_at,
 505:                     created_at,
 506:                 )
 507:             )
 508:         conn.executemany(
 509:             """
 510:             INSERT INTO known_issues
 511:             (set_id, title, description, plugin_id, kind, where_json, contains_json, min_count, max_count, source_text, created_at, updated_at)
 512:             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
 513:             """,
 514:             rows,
 515:         )
 516: 
 517:     def get_or_create_parameter_entity(
 518:         self, canonical_text: str, conn: sqlite3.Connection | None = None
 519:     ) -> int:
 520:         if conn is None:
 521:             with self.connection() as temp:
 522:                 return self.get_or_create_parameter_entity(canonical_text, temp)
 523:         conn.execute(
 524:             "INSERT OR IGNORE INTO parameter_entities (canonical_text) VALUES (?)",
 525:             (canonical_text,),
 526:         )
 527:         cur = conn.execute(
 528:             "SELECT entity_id FROM parameter_entities WHERE canonical_text = ?",
 529:             (canonical_text,),
 530:         )
 531:         row = cur.fetchone()
 532:         if not row:
 533:             raise ValueError("Parameter entity not found")
 534:         return int(row[0])
 535: 
 536:     def insert_parameter_kv(
 537:         self,
 538:         entity_id: int,
 539:         kv_pairs: list[tuple[str, str]],
 540:         conn: sqlite3.Connection | None = None,
 541:     ) -> None:
 542:         if not kv_pairs:
 543:             return
 544:         if conn is None:
 545:             with self.connection() as temp:
 546:                 self.insert_parameter_kv(entity_id, kv_pairs, temp)
 547:                 return
 548:         conn.executemany(
 549:             """
 550:             INSERT OR IGNORE INTO parameter_kv (entity_id, key, value)
 551:             VALUES (?, ?, ?)
 552:             """,
 553:             [(entity_id, key, value) for key, value in kv_pairs],
 554:         )
 555: 
 556:     def insert_row_parameter_links(
 557:         self,
 558:         dataset_version_id: str,
 559:         links: list[tuple[int, int]],
 560:         conn: sqlite3.Connection | None = None,
 561:     ) -> None:
 562:         if not links:
 563:             return
 564:         if conn is None:
 565:             with self.connection() as temp:
 566:                 self.insert_row_parameter_links(dataset_version_id, links, temp)
 567:                 return
 568:         conn.executemany(
 569:             """
 570:             INSERT OR IGNORE INTO row_parameter_link
 571:             (dataset_version_id, row_index, entity_id)
 572:             VALUES (?, ?, ?)
 573:             """,
 574:             [(dataset_version_id, row_index, entity_id) for row_index, entity_id in links],
 575:         )
 576: 
 577:     def ensure_entity(
 578:         self, entity_type: str, key: str, conn: sqlite3.Connection | None = None
 579:     ) -> int:
 580:         if conn is None:
 581:             with self.connection() as temp:
 582:                 return self.ensure_entity(entity_type, key, temp)
 583:         conn.execute(
 584:             "INSERT OR IGNORE INTO entities (type, key) VALUES (?, ?)",
 585:             (entity_type, key),
 586:         )
 587:         cur = conn.execute(
 588:             "SELECT entity_id FROM entities WHERE type = ? AND key = ?",
 589:             (entity_type, key),
 590:         )
 591:         row = cur.fetchone()
 592:         if not row:
 593:             raise ValueError("Entity not found")
 594:         return int(row[0])
 595: 
 596:     def add_edges(
 597:         self,
 598:         edges: list[tuple[int, int, str, dict[str, Any] | None, float | None]],
 599:         conn: sqlite3.Connection | None = None,
 600:     ) -> None:
 601:         if not edges:
 602:             return
 603:         if conn is None:
 604:             with self.connection() as temp:
 605:                 self.add_edges(edges, temp)
 606:                 return
 607:         conn.executemany(
 608:             """
 609:             INSERT INTO edges
 610:             (src_entity_id, dst_entity_id, kind, evidence_json, score)
 611:             VALUES (?, ?, ?, ?, ?)
 612:             """,
 613:             [
 614:                 (
 615:                     src,
 616:                     dst,
 617:                     kind,
 618:                     json_dumps(evidence) if evidence else None,
 619:                     score,
 620:                 )
 621:                 for src, dst, kind, evidence, score in edges
 622:             ],
 623:         )
 624: 
 625:     def create_upload(
 626:         self,
 627:         upload_id: str,
 628:         filename: str,
 629:         size_bytes: int,
 630:         sha256: str,
 631:         created_at: str,
 632:     ) -> None:
 633:         tenant_id = self._tenant_id()
 634:         with self.connection() as conn:
 635:             conn.execute(
 636:                 """
 637:                 INSERT OR REPLACE INTO uploads
 638:                 (upload_id, tenant_id, filename, size_bytes, sha256, created_at)
 639:                 VALUES (?, ?, ?, ?, ?, ?)
 640:                 """,
 641:                 (upload_id, tenant_id, filename, size_bytes, sha256, created_at),
 642:             )
 643: 
 644:     def ensure_project(self, project_id: str, fingerprint: str, created_at: str) -> None:
 645:         tenant_id = self._tenant_id()
 646:         with self.connection() as conn:
 647:             conn.execute(
 648:                 """
 649:                 INSERT OR IGNORE INTO projects
 650:                 (project_id, tenant_id, fingerprint, created_at, erp_type)
 651:                 VALUES (?, ?, ?, ?, ?)
 652:                 """,
 653:                 (project_id, tenant_id, fingerprint, created_at, "unknown"),
 654:             )
 655: 
 656:     def ensure_dataset(
 657:         self, dataset_id: str, project_id: str, fingerprint: str, created_at: str
 658:     ) -> None:
 659:         tenant_id = self._tenant_id()
 660:         with self.connection() as conn:
 661:             conn.execute(
 662:                 """
 663:                 INSERT OR IGNORE INTO datasets
 664:                 (dataset_id, tenant_id, project_id, fingerprint, created_at)
 665:                 VALUES (?, ?, ?, ?, ?)
 666:                 """,
 667:                 (dataset_id, tenant_id, project_id, fingerprint, created_at),
 668:             )
 669: 
 670:     def get_dataset_version(
 671:         self, dataset_version_id: str, conn: sqlite3.Connection | None = None
 672:     ) -> dict[str, Any] | None:
 673:         tenant_id = self._tenant_id()
 674:         if conn is None:
 675:             with self.connection() as temp:
 676:                 return self.get_dataset_version(dataset_version_id, temp)
 677:         cur = conn.execute(
 678:             "SELECT * FROM dataset_versions WHERE dataset_version_id = ? AND tenant_id = ?",
 679:             (dataset_version_id, tenant_id),
 680:         )
 681:         row = cur.fetchone()
 682:         return dict(row) if row else None
 683: 
 684:     def get_dataset_version_context(
 685:         self, dataset_version_id: str, conn: sqlite3.Connection | None = None
 686:     ) -> dict[str, Any] | None:
 687:         tenant_id = self._tenant_id()
 688:         if conn is None:
 689:             with self.connection() as temp:
 690:                 return self.get_dataset_version_context(dataset_version_id, temp)
 691:         cur = conn.execute(
 692:             """
 693:             SELECT dv.dataset_version_id,
 694:                    dv.dataset_id,
 695:                    dv.table_name,
 696:                    dv.data_hash,
 697:                    d.project_id
 698:             FROM dataset_versions dv
 699:             JOIN datasets d ON d.dataset_id = dv.dataset_id
 700:             WHERE dv.dataset_version_id = ? AND dv.tenant_id = ? AND d.tenant_id = ?
 701:             """,
 702:             (dataset_version_id, tenant_id, tenant_id),
 703:         )
 704:         row = cur.fetchone()
 705:         return dict(row) if row else None
 706: 
 707:     def ensure_dataset_version(
 708:         self,
 709:         dataset_version_id: str,
 710:         dataset_id: str,
 711:         created_at: str,
 712:         table_name: str,
 713:         data_hash: str | None = None,
 714:         conn: sqlite3.Connection | None = None,
 715:     ) -> None:
 716:         tenant_id = self._tenant_id()
 717:         if conn is None:
 718:             with self.connection() as temp:
 719:                 self.ensure_dataset_version(
 720:                     dataset_version_id,
 721:                     dataset_id,
 722:                     created_at,
 723:                     table_name,
 724:                     data_hash,
 725:                     temp,
 726:                 )
 727:                 return
 728:         conn.execute(
 729:             """
 730:             INSERT OR IGNORE INTO dataset_versions
 731:             (dataset_version_id, tenant_id, dataset_id, created_at, table_name, data_hash)
 732:             VALUES (?, ?, ?, ?, ?, ?)
 733:             """,
 734:             (dataset_version_id, tenant_id, dataset_id, created_at, table_name, data_hash),
 735:         )
 736: 
 737:     def update_dataset_version_stats(
 738:         self,
 739:         dataset_version_id: str,
 740:         row_count: int,
 741:         column_count: int,
 742:         conn: sqlite3.Connection | None = None,
 743:     ) -> None:
 744:         tenant_id = self._tenant_id()
 745:         if conn is None:
 746:             with self.connection() as temp:
 747:                 self.update_dataset_version_stats(
 748:                     dataset_version_id, row_count, column_count, temp
 749:                 )
 750:                 return
 751:         conn.execute(
 752:             """
 753:             UPDATE dataset_versions
 754:             SET row_count = ?, column_count = ?
 755:             WHERE dataset_version_id = ? AND tenant_id = ?
 756:             """,
 757:             (row_count, column_count, dataset_version_id, tenant_id),
 758:         )
 759: 
 760:     def set_dataset_raw_format(
 761:         self,
 762:         dataset_version_id: str,
 763:         format_id: int,
 764:         conn: sqlite3.Connection | None = None,
 765:     ) -> None:
 766:         tenant_id = self._tenant_id()
 767:         if conn is None:
 768:             with self.connection() as temp:
 769:                 self.set_dataset_raw_format(dataset_version_id, format_id, temp)
 770:                 return
 771:         conn.execute(
 772:             """
 773:             UPDATE dataset_versions
 774:             SET raw_format_id = ?
 775:             WHERE dataset_version_id = ? AND tenant_id = ?
 776:             """,
 777:             (format_id, dataset_version_id, tenant_id),
 778:         )
 779: 
 780:     def replace_dataset_columns(
 781:         self,
 782:         dataset_version_id: str,
 783:         columns: list[dict[str, Any]],
 784:         conn: sqlite3.Connection | None = None,
 785:     ) -> None:
 786:         tenant_id = self._tenant_id()
 787:         if conn is None:
 788:             with self.connection() as temp:
 789:                 self.replace_dataset_columns(dataset_version_id, columns, temp)
 790:                 return
 791:         conn.execute(
 792:             "DELETE FROM dataset_columns WHERE dataset_version_id = ? AND tenant_id = ?",
 793:             (dataset_version_id, tenant_id),
 794:         )
 795:         conn.executemany(
 796:             """
 797:             INSERT INTO dataset_columns
 798:             (dataset_version_id, tenant_id, column_id, safe_name, original_name, dtype, role, pii_tags_json)
 799:             VALUES (?, ?, ?, ?, ?, ?, ?, ?)
 800:             """,
 801:             [
 802:                 (
 803:                     dataset_version_id,
 804:                     tenant_id,
 805:                     col["column_id"],
 806:                     col["safe_name"],
 807:                     col["original_name"],
 808:                     col.get("dtype"),
 809:                     col.get("role"),
 810:                     json_dumps(col.get("pii_tags")) if col.get("pii_tags") else None,
 811:                 )
 812:                 for col in columns
 813:             ],
 814:         )
 815: 
 816:     def update_dataset_column_roles(
 817:         self,
 818:         dataset_version_id: str,
 819:         role_by_name: dict[str, str],
 820:         conn: sqlite3.Connection | None = None,
 821:     ) -> None:
 822:         tenant_id = self._tenant_id()
 823:         if not role_by_name:
 824:             return
 825:         if conn is None:
 826:             with self.connection() as temp:
 827:                 self.update_dataset_column_roles(dataset_version_id, role_by_name, temp)
 828:                 return
 829:         conn.executemany(
 830:             """
 831:             UPDATE dataset_columns
 832:             SET role = ?
 833:             WHERE dataset_version_id = ? AND original_name = ? AND tenant_id = ?
 834:             """,
 835:             [
 836:                 (role, dataset_version_id, name, tenant_id)
 837:                 for name, role in role_by_name.items()
 838:             ],
 839:         )
 840: 
 841:     def fetch_dataset_columns(
 842:         self, dataset_version_id: str, conn: sqlite3.Connection | None = None
 843:     ) -> list[dict[str, Any]]:
 844:         tenant_id = self._tenant_id()
 845:         if conn is None:
 846:             with self.connection() as temp:
 847:                 return self.fetch_dataset_columns(dataset_version_id, temp)
 848:         cur = conn.execute(
 849:             """
 850:             SELECT column_id, safe_name, original_name, dtype, role, pii_tags_json
 851:             FROM dataset_columns
 852:             WHERE dataset_version_id = ? AND tenant_id = ?
 853:             ORDER BY column_id
 854:             """,
 855:             (dataset_version_id, tenant_id),
 856:         )
 857:         rows = []
 858:         for row in cur.fetchall():
 859:             entry = dict(row)
 860:             tags = entry.get("pii_tags_json")
 861:             if tags:
 862:                 try:
 863:                     entry["pii_tags"] = json.loads(tags)
 864:                 except json.JSONDecodeError:
 865:                     entry["pii_tags"] = []
 866:             rows.append(entry)
 867:         return rows
 868: 
 869:     def update_dataset_column_pii_tags(
 870:         self,
 871:         dataset_version_id: str,
 872:         tags_by_name: dict[str, list[str]],
 873:         conn: sqlite3.Connection | None = None,
 874:     ) -> None:
 875:         tenant_id = self._tenant_id()
 876:         if not tags_by_name:
 877:             return
 878:         if conn is None:
 879:             with self.connection() as temp:
 880:                 self.update_dataset_column_pii_tags(
 881:                     dataset_version_id, tags_by_name, temp
 882:                 )
 883:                 return
 884:         conn.executemany(
 885:             """
 886:             UPDATE dataset_columns
 887:             SET pii_tags_json = ?
 888:             WHERE dataset_version_id = ? AND original_name = ? AND tenant_id = ?
 889:             """,
 890:             [
 891:                 (json_dumps(tags), dataset_version_id, name, tenant_id)
 892:                 for name, tags in tags_by_name.items()
 893:             ],
 894:         )
 895: 
 896:     def ensure_pii_salt(self, tenant_id: str) -> str:
 897:         with self.connection() as conn:
 898:             cur = conn.execute(
 899:                 "SELECT salt FROM pii_salts WHERE tenant_id = ?",
 900:                 (tenant_id,),
 901:             )
 902:             row = cur.fetchone()
 903:             if row and row["salt"]:
 904:                 return str(row["salt"])
 905:             salt = hashlib.sha256(
 906:                 f"{tenant_id}:{now_iso()}".encode("utf-8")
 907:             ).hexdigest()
 908:             conn.execute(
 909:                 "INSERT OR REPLACE INTO pii_salts (tenant_id, salt, created_at) VALUES (?, ?, ?)",
 910:                 (tenant_id, salt, now_iso()),
 911:             )
 912:             return salt
 913: 
 914:     def upsert_pii_entities(
 915:         self,
 916:         tenant_id: str,
 917:         pii_type: str,
 918:         raw_values: list[str],
 919:         conn: sqlite3.Connection | None = None,
 920:     ) -> None:
 921:         if not raw_values:
 922:             return
 923:         if conn is None:
 924:             with self.connection() as temp:
 925:                 self.upsert_pii_entities(tenant_id, pii_type, raw_values, temp)
 926:                 return
 927:         salt = self.ensure_pii_salt(tenant_id)
 928:         rows = []
 929:         for value in raw_values:
 930:             value = str(value)
 931:             value_hash = hashlib.sha256(
 932:                 f"{salt}:{value}".encode("utf-8")
 933:             ).hexdigest()
 934:             rows.append((tenant_id, pii_type, value, value_hash, now_iso()))
 935:         conn.executemany(
 936:             """
 937:             INSERT OR IGNORE INTO pii_entities
 938:             (tenant_id, pii_type, raw_value, value_hash, created_at)
 939:             VALUES (?, ?, ?, ?, ?)
 940:             """,
 941:             rows,
 942:         )
 943: 
 944:     def fetch_pii_entities(self, tenant_id: str) -> list[dict[str, Any]]:
 945:         with self.connection() as conn:
 946:             cur = conn.execute(
 947:                 "SELECT pii_type, raw_value, value_hash FROM pii_entities WHERE tenant_id = ?",
 948:                 (tenant_id,),
 949:             )
 950:             return [dict(row) for row in cur.fetchall()]
 951: 
 952:     def fetch_row_trace(
 953:         self,
 954:         dataset_version_id: str,
 955:         row_index: int,
 956:         source_dataset_version_id: str | None = None,
 957:         max_rows: int = 50,
 958:     ) -> dict[str, Any]:
 959:         with self.connection() as conn:
 960:             version = self.get_dataset_version(dataset_version_id, conn)
 961:             if not version:
 962:                 raise ValueError("Dataset version not found")
 963:             table_name = version["table_name"]
 964:             dataset_template = self.fetch_dataset_template(dataset_version_id)
 965:             template_fields: list[dict[str, Any]] | None = None
 966:             name_lookup: dict[str, str] = {}
 967:             if dataset_template:
 968:                 template_id = int(dataset_template["template_id"])
 969:                 template_fields = self.fetch_template_fields(template_id)
 970:                 name_lookup = {
 971:                     field["safe_name"]: field["name"] for field in template_fields
 972:                 }
 973:             else:
 974:                 columns = self.fetch_dataset_columns(dataset_version_id, conn)
 975:                 name_lookup = {
 976:                     col["safe_name"]: col["original_name"] for col in columns
 977:                 }
 978: 
 979:             col_info = conn.execute(
 980:                 f"PRAGMA table_info({quote_identifier(table_name)})"
 981:             ).fetchall()
 982:             table_cols = [row["name"] for row in col_info]
 983:             has_dataset_id = "dataset_version_id" in table_cols
 984: 
 985:             select_cols = ["row_id", "row_index", "row_json"]
 986:             if has_dataset_id:
 987:                 select_cols.append("dataset_version_id")
 988:             select_cols.extend(
 989:                 [col for col in name_lookup.keys() if col in table_cols]
 990:             )
 991:             if not select_cols:
 992:                 return {
 993:                     "dataset_version_id": dataset_version_id,
 994:                     "table_name": table_name,
 995:                     "rows": [],
 996:                     "parameters": {},
 997:                 }
 998: 
 999:             scope = None
1000:             if dataset_template and dataset_template.get("mapping_json"):
1001:                 try:
1002:                     mapping = json.loads(dataset_template["mapping_json"])
1003:                     scope = mapping.get("scope")
1004:                 except json.JSONDecodeError:
1005:                     scope = None
1006: 
1007:             conditions = ["row_index = ?"]
1008:             params: list[Any] = [int(row_index)]
1009:             if has_dataset_id:
1010:                 if source_dataset_version_id:
1011:                     conditions.append("dataset_version_id = ?")
1012:                     params.append(source_dataset_version_id)
1013:                 elif scope != "all":
1014:                     conditions.append("dataset_version_id = ?")
1015:                     params.append(dataset_version_id)
1016: 
1017:             where = " AND ".join(conditions)
1018:             sql = (
1019:                 f"SELECT {', '.join(quote_identifier(c) for c in select_cols)} "
1020:                 f"FROM {quote_identifier(table_name)} WHERE {where} "
1021:                 "ORDER BY row_index LIMIT ?"
1022:             )
1023:             params.append(int(max_rows))
1024:             cur = conn.execute(sql, params)
1025:             raw_rows = [dict(row) for row in cur.fetchall()]
1026: 
1027:             rows: list[dict[str, Any]] = []
1028:             row_params: dict[str, list[dict[str, Any]]] = {}
1029:             for row in raw_rows:
1030:                 values: dict[str, Any] = {}
1031:                 for safe_name, friendly in name_lookup.items():
1032:                     if safe_name in row:
1033:                         values[friendly] = row.get(safe_name)
1034:                 row_key = f"{row.get('dataset_version_id', dataset_version_id)}:{row.get('row_index')}"
1035:                 rows.append(
1036:                     {
1037:                         "row_id": row.get("row_id"),
1038:                         "row_index": row.get("row_index"),
1039:                         "dataset_version_id": row.get(
1040:                             "dataset_version_id", dataset_version_id
1041:                         ),
1042:                         "row_json": row.get("row_json"),
1043:                         "values": values,
1044:                     }
1045:                 )
1046:                 row_params[row_key] = []
1047: 
1048:             if rows:
1049:                 by_dataset: dict[str, list[int]] = {}
1050:                 for row in rows:
1051:                     key = row.get("dataset_version_id") or dataset_version_id
1052:                     by_dataset.setdefault(key, []).append(int(row["row_index"]))
1053:                 for dv_id, indices in by_dataset.items():
1054:                     placeholders = ", ".join(["?"] * len(indices))
1055:                     params = [dv_id, *indices]
1056:                     link_rows = conn.execute(
1057:                         f"""
1058:                         SELECT rpl.row_index, pe.entity_id, pe.canonical_text
1059:                         FROM row_parameter_link rpl
1060:                         JOIN parameter_entities pe ON pe.entity_id = rpl.entity_id
1061:                         WHERE rpl.dataset_version_id = ? AND rpl.row_index IN ({placeholders})
1062:                         """,
1063:                         params,
1064:                     ).fetchall()
1065:                     entity_ids = sorted(
1066:                         {int(row["entity_id"]) for row in link_rows}
1067:                     )
1068:                     kv_map: dict[int, list[dict[str, Any]]] = {}
1069:                     if entity_ids:
1070:                         kv_placeholders = ", ".join(["?"] * len(entity_ids))
1071:                         kv_rows = conn.execute(
1072:                             f"""
1073:                             SELECT entity_id, key, value
1074:                             FROM parameter_kv
1075:                             WHERE entity_id IN ({kv_placeholders})
1076:                             """,
1077:                             entity_ids,
1078:                         ).fetchall()
1079:                         for kv in kv_rows:
1080:                             kv_map.setdefault(int(kv["entity_id"]), []).append(
1081:                                 {"key": kv["key"], "value": kv["value"]}
1082:                             )
1083:                     for link in link_rows:
1084:                         row_key = f"{dv_id}:{link['row_index']}"
1085:                         row_params.setdefault(row_key, []).append(
1086:                             {
1087:                                 "entity_id": int(link["entity_id"]),
1088:                                 "canonical": link["canonical_text"],
1089:                                 "kv": kv_map.get(int(link["entity_id"]), []),
1090:                             }
1091:                         )
1092: 
1093:             for row in rows:
1094:                 row_key = f"{row.get('dataset_version_id', dataset_version_id)}:{row.get('row_index')}"
1095:                 row["parameters"] = row_params.get(row_key, [])
1096: 
1097:             return {
1098:                 "dataset_version_id": dataset_version_id,
1099:                 "table_name": table_name,
1100:                 "rows": rows,
1101:                 "parameters": row_params,
1102:             }
1103: 
1104:     def list_projects(self) -> list[dict[str, Any]]:
1105:         tenant_id = self._tenant_id()
1106:         with self.connection() as conn:
1107:             cur = conn.execute(
1108:                 """
1109:                 SELECT p.project_id, p.fingerprint, p.name, p.created_at, p.erp_type,
1110:                        (SELECT COUNT(*) FROM datasets d
1111:                         WHERE d.project_id = p.project_id AND d.tenant_id = p.tenant_id) AS dataset_count
1112:                 FROM projects p
1113:                 WHERE p.tenant_id = ?
1114:                 ORDER BY p.created_at DESC
1115:                 """
1116:                 ,
1117:                 (tenant_id,),
1118:             )
1119:             return [dict(row) for row in cur.fetchall()]
1120: 
1121:     def list_runs_by_project(self, project_id: str, limit: int = 25) -> list[dict[str, Any]]:
1122:         tenant_id = self._tenant_id()
1123:         with self.connection() as conn:
1124:             cur = conn.execute(
1125:                 """
1126:                 SELECT run_id, created_at, status, input_filename, upload_id, dataset_version_id
1127:                 FROM runs
1128:                 WHERE project_id = ? AND tenant_id = ?
1129:                 ORDER BY created_at DESC
1130:                 LIMIT ?
1131:                 """,
1132:                 (project_id, tenant_id, int(limit)),
1133:             )
1134:             return [dict(row) for row in cur.fetchall()]
1135: 
1136:     def fetch_project(self, project_id: str) -> dict[str, Any] | None:
1137:         tenant_id = self._tenant_id()
1138:         with self.connection() as conn:
1139:             cur = conn.execute(
1140:                 "SELECT project_id, fingerprint, name, created_at, erp_type FROM projects WHERE project_id = ? AND tenant_id = ?",
1141:                 (project_id, tenant_id),
1142:             )
1143:             row = cur.fetchone()
1144:             return dict(row) if row else None
1145: 
1146:     def update_project_name(self, project_id: str, name: str) -> None:
1147:         tenant_id = self._tenant_id()
1148:         with self.connection() as conn:
1149:             conn.execute(
1150:                 "UPDATE projects SET name = ? WHERE project_id = ? AND tenant_id = ?",
1151:                 (name, project_id, tenant_id),
1152:             )
1153: 
1154:     def fetch_project_plugin_settings(self, project_id: str) -> dict[str, dict[str, Any]]:
1155:         tenant_id = self._tenant_id()
1156:         with self.connection() as conn:
1157:             cur = conn.execute(
1158:                 """
1159:                 SELECT plugin_id, settings_json
1160:                 FROM project_plugin_settings
1161:                 WHERE project_id = ? AND tenant_id = ?
1162:                 """,
1163:                 (project_id, tenant_id),
1164:             )
1165:             settings: dict[str, dict[str, Any]] = {}
1166:             for row in cur.fetchall():
1167:                 payload = {}
1168:                 if row["settings_json"]:
1169:                     try:
1170:                         payload = json.loads(row["settings_json"])
1171:                     except json.JSONDecodeError:
1172:                         payload = {}
1173:                 settings[str(row["plugin_id"])] = payload
1174:             return settings
1175: 
1176:     def replace_project_plugin_settings(
1177:         self, project_id: str, settings: dict[str, dict[str, Any]]
1178:     ) -> None:
1179:         tenant_id = self._tenant_id()
1180:         with self.connection() as conn:
1181:             conn.execute(
1182:                 "DELETE FROM project_plugin_settings WHERE project_id = ? AND tenant_id = ?",
1183:                 (project_id, tenant_id),
1184:             )
1185:             if not settings:
1186:                 return
1187:             created_at = now_iso()
1188:             rows = []
1189:             for plugin_id, payload in settings.items():
1190:                 rows.append(
1191:                     (
1192:                         project_id,
1193:                         tenant_id,
1194:                         plugin_id,
1195:                         json_dumps(payload) if payload is not None else None,
1196:                         created_at,
1197:                         created_at,
1198:                     )
1199:                 )
1200:             conn.executemany(
1201:                 """
1202:                 INSERT INTO project_plugin_settings
1203:                 (project_id, tenant_id, plugin_id, settings_json, created_at, updated_at)
1204:                 VALUES (?, ?, ?, ?, ?, ?)
1205:                 """,
1206:                 rows,
1207:             )
1208: 
1209:     def update_project_erp_type(self, project_id: str, erp_type: str) -> None:
1210:         tenant_id = self._tenant_id()
1211:         with self.connection() as conn:
1212:             conn.execute(
1213:                 "UPDATE projects SET erp_type = ? WHERE project_id = ? AND tenant_id = ?",
1214:                 (erp_type, project_id, tenant_id),
1215:             )
1216: 
1217:     def fetch_project_role_overrides(self, project_id: str) -> dict[str, str]:
1218:         tenant_id = self._tenant_id()
1219:         with self.connection() as conn:
1220:             cur = conn.execute(
1221:                 """
1222:                 SELECT role, column_name
1223:                 FROM project_role_overrides
1224:                 WHERE project_id = ? AND tenant_id = ?
1225:                 """,
1226:                 (project_id, tenant_id),
1227:             )
1228:             return {row["role"]: row["column_name"] for row in cur.fetchall()}
1229: 
1230:     def replace_project_role_overrides(
1231:         self, project_id: str, overrides: dict[str, str]
1232:     ) -> None:
1233:         tenant_id = self._tenant_id()
1234:         created_at = now_iso()
1235:         with self.connection() as conn:
1236:             conn.execute(
1237:                 "DELETE FROM project_role_overrides WHERE project_id = ? AND tenant_id = ?",
1238:                 (project_id, tenant_id),
1239:             )
1240:             rows = [
1241:                 (
1242:                     project_id,
1243:                     tenant_id,
1244:                     role,
1245:                     column_name,
1246:                     created_at,
1247:                     created_at,
1248:                 )
1249:                 for role, column_name in overrides.items()
1250:                 if column_name
1251:             ]
1252:             if rows:
1253:                 conn.executemany(
1254:                     """
1255:                     INSERT INTO project_role_overrides
1256:                     (project_id, tenant_id, role, column_name, created_at, updated_at)
1257:                     VALUES (?, ?, ?, ?, ?, ?)
1258:                     """,
1259:                     rows,
1260:                 )
1261: 
1262:     def replace_dataset_role_candidates(
1263:         self, dataset_version_id: str, candidates: list[dict[str, Any]]
1264:     ) -> None:
1265:         tenant_id = self._tenant_id()
1266:         with self.connection() as conn:
1267:             conn.execute(
1268:                 "DELETE FROM dataset_role_candidates WHERE dataset_version_id = ? AND tenant_id = ?",
1269:                 (dataset_version_id, tenant_id),
1270:             )
1271:             if not candidates:
1272:                 return
1273:             created_at = now_iso()
1274:             rows = [
1275:                 (
1276:                     dataset_version_id,
1277:                     tenant_id,
1278:                     int(item["column_id"]),
1279:                     item["role"],
1280:                     float(item["score"]),
1281:                     json_dumps(item.get("reasons") or []),
1282:                     created_at,
1283:                 )
1284:                 for item in candidates
1285:             ]
1286:             conn.executemany(
1287:                 """
1288:                 INSERT INTO dataset_role_candidates
1289:                 (dataset_version_id, tenant_id, column_id, role, score, reasons_json, created_at)
1290:                 VALUES (?, ?, ?, ?, ?, ?, ?)
1291:                 """,
1292:                 rows,
1293:             )
1294: 
1295:     def fetch_dataset_role_candidates(
1296:         self, dataset_version_id: str
1297:     ) -> list[dict[str, Any]]:
1298:         tenant_id = self._tenant_id()
1299:         with self.connection() as conn:
1300:             cur = conn.execute(
1301:                 """
1302:                 SELECT column_id, role, score, reasons_json
1303:                 FROM dataset_role_candidates
1304:                 WHERE dataset_version_id = ? AND tenant_id = ?
1305:                 ORDER BY role, score DESC
1306:                 """,
1307:                 (dataset_version_id, tenant_id),
1308:             )
1309:             rows = []
1310:             for row in cur.fetchall():
1311:                 entry = dict(row)
1312:                 if entry.get("reasons_json"):
1313:                     try:
1314:                         entry["reasons"] = json.loads(entry["reasons_json"])
1315:                     except json.JSONDecodeError:
1316:                         entry["reasons"] = []
1317:                 rows.append(entry)
1318:             return rows
1319: 
1320:     def select_role_assignments(
1321:         self, dataset_version_id: str, project_id: str | None = None
1322:     ) -> dict[str, dict[str, Any]]:
1323:         overrides = (
1324:             self.fetch_project_role_overrides(project_id)
1325:             if project_id
1326:             else {}
1327:         )
1328:         columns = self.fetch_dataset_columns(dataset_version_id)
1329:         by_name = {col["original_name"]: col for col in columns}
1330:         assignments: dict[str, dict[str, Any]] = {}
1331:         for role, column_name in overrides.items():
1332:             col = by_name.get(column_name)
1333:             if col:
1334:                 assignments[role] = col
1335:         if assignments and len(assignments) == len(overrides):
1336:             return assignments
1337: 
1338:         with self.connection() as conn:
1339:             cur = conn.execute(
1340:                 """
1341:                 SELECT column_id, role, score
1342:                 FROM dataset_role_candidates
1343:                 WHERE dataset_version_id = ? AND tenant_id = ?
1344:                 ORDER BY role, score DESC
1345:                 """,
1346:                 (dataset_version_id, self._tenant_id()),
1347:             )
1348:             role_best: dict[str, tuple[int, float]] = {}
1349:             for row in cur.fetchall():
1350:                 role = row["role"]
1351:                 if role in assignments:
1352:                     continue
1353:                 if role not in role_best:
1354:                     role_best[role] = (int(row["column_id"]), float(row["score"]))
1355:         if role_best:
1356:             columns_by_id = {col["column_id"]: col for col in columns}
1357:             for role, (col_id, _) in role_best.items():
1358:                 col = columns_by_id.get(col_id)
1359:                 if col:
1360:                     assignments[role] = col
1361:         return assignments
1362: 
1363:     def list_dataset_versions_by_project(self, project_id: str) -> list[dict[str, Any]]:
1364:         tenant_id = self._tenant_id()
1365:         with self.connection() as conn:
1366:             cur = conn.execute(
1367:                 """
1368:                 SELECT dv.dataset_version_id, dv.dataset_id, dv.created_at, dv.table_name,
1369:                        dv.row_count, dv.column_count, dv.data_hash, dv.raw_format_id,
1370:                        (SELECT r.input_filename FROM runs r
1371:                         WHERE r.dataset_version_id = dv.dataset_version_id
1372:                           AND r.tenant_id = ?
1373:                         ORDER BY r.created_at ASC LIMIT 1) AS source_filename,
1374:                        (SELECT r.created_at FROM runs r
1375:                         WHERE r.dataset_version_id = dv.dataset_version_id
1376:                           AND r.tenant_id = ?
1377:                         ORDER BY r.created_at ASC LIMIT 1) AS first_run_at,
1378:                        (SELECT r.created_at FROM runs r
1379:                         WHERE r.dataset_version_id = dv.dataset_version_id
1380:                           AND r.tenant_id = ?
1381:                         ORDER BY r.created_at DESC LIMIT 1) AS last_run_at
1382:                 FROM dataset_versions dv
1383:                 JOIN datasets d ON d.dataset_id = dv.dataset_id
1384:                 WHERE d.project_id = ? AND dv.tenant_id = ? AND d.tenant_id = ?
1385:                 ORDER BY dv.created_at DESC
1386:                 """,
1387:                 (tenant_id, tenant_id, tenant_id, project_id, tenant_id, tenant_id),
1388:             )
1389:             return [dict(row) for row in cur.fetchall()]
1390: 
1391:     def list_dataset_versions(self) -> list[dict[str, Any]]:
1392:         tenant_id = self._tenant_id()
1393:         with self.connection() as conn:
1394:             cur = conn.execute(
1395:                 """
1396:                 SELECT dataset_version_id, dataset_id, created_at, table_name, row_count,
1397:                        column_count, data_hash, raw_format_id
1398:                 FROM dataset_versions
1399:                 WHERE tenant_id = ?
1400:                 ORDER BY created_at DESC
1401:                 """,
1402:                 (tenant_id,),
1403:             )
1404:             return [dict(row) for row in cur.fetchall()]
1405: 
1406:     def fetch_latest_plugin_results_for_dataset(
1407:         self, dataset_version_id: str
1408:     ) -> list[dict[str, Any]]:
1409:         tenant_id = self._tenant_id()
1410:         with self.connection() as conn:
1411:             cur = conn.execute(
1412:                 """
1413:                 SELECT pr.*
1414:                 FROM plugin_results_v2 pr
1415:                 JOIN runs r ON r.run_id = pr.run_id
1416:                 JOIN (
1417:                     SELECT pr2.plugin_id AS plugin_id, MAX(pr2.result_id) AS max_id
1418:                     FROM plugin_results_v2 pr2
1419:                     JOIN runs r2 ON r2.run_id = pr2.run_id
1420:                     WHERE r2.dataset_version_id = ? AND r2.tenant_id = ? AND pr2.tenant_id = ?
1421:                     GROUP BY pr2.plugin_id
1422:                 ) latest
1423:                 ON pr.plugin_id = latest.plugin_id AND pr.result_id = latest.max_id
1424:                 WHERE r.dataset_version_id = ? AND r.tenant_id = ? AND pr.tenant_id = ?
1425:                 """,
1426:                 (dataset_version_id, tenant_id, tenant_id, dataset_version_id, tenant_id, tenant_id),
1427:             )
1428:             return [dict(row) for row in cur.fetchall()]
1429: 
1430:     def fetch_deliveries_for_dataset(
1431:         self, dataset_version_id: str
1432:     ) -> list[dict[str, Any]]:
1433:         tenant_id = self._tenant_id()
1434:         with self.connection() as conn:
1435:             cur = conn.execute(
1436:                 """
1437:                 SELECT *
1438:                 FROM deliveries
1439:                 WHERE dataset_version_id = ? AND tenant_id = ?
1440:                 ORDER BY delivered_at DESC
1441:                 """,
1442:                 (dataset_version_id, tenant_id),
1443:             )
1444:             return [dict(row) for row in cur.fetchall()]
1445: 
1446:     def record_delivery(
1447:         self,
1448:         project_id: str,
1449:         dataset_version_id: str,
1450:         plugin_id: str,
1451:         plugin_version: str | None,
1452:         code_hash: str | None,
1453:         dataset_hash: str | None,
1454:         delivered_at: str,
1455:         notes: str | None = None,
1456:     ) -> None:
1457:         tenant_id = self._tenant_id()
1458:         with self.connection() as conn:
1459:             conn.execute(
1460:                 """
1461:                 INSERT OR IGNORE INTO deliveries
1462:                 (project_id, tenant_id, dataset_version_id, plugin_id, plugin_version, code_hash, dataset_hash, delivered_at, notes)
1463:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
1464:                 """,
1465:                 (
1466:                     project_id,
1467:                     tenant_id,
1468:                     dataset_version_id,
1469:                     plugin_id,
1470:                     plugin_version,
1471:                     code_hash,
1472:                     dataset_hash,
1473:                     delivered_at,
1474:                     notes,
1475:                 ),
1476:             )
1477: 
1478:     def enqueue_analysis_job(
1479:         self,
1480:         dataset_version_id: str,
1481:         plugin_id: str,
1482:         plugin_version: str | None,
1483:         code_hash: str | None,
1484:         settings_hash: str | None,
1485:         run_seed: int,
1486:         created_at: str,
1487:     ) -> None:
1488:         tenant_id = self._tenant_id()
1489:         with self.connection() as conn:
1490:             conn.execute(
1491:                 """
1492:                 INSERT OR IGNORE INTO analysis_jobs
1493:                 (dataset_version_id, tenant_id, plugin_id, plugin_version, code_hash, settings_hash, run_seed, status, created_at)
1494:                 VALUES (?, ?, ?, ?, ?, ?, ?, 'queued', ?)
1495:                 """,
1496:                 (
1497:                     dataset_version_id,
1498:                     tenant_id,
1499:                     plugin_id,
1500:                     plugin_version,
1501:                     code_hash,
1502:                     settings_hash,
1503:                     int(run_seed),
1504:                     created_at,
1505:                 ),
1506:             )
1507: 
1508:     def list_analysis_jobs(self, status: str = "queued") -> list[dict[str, Any]]:
1509:         tenant_id = self._tenant_id()
1510:         with self.connection() as conn:
1511:             cur = conn.execute(
1512:                 "SELECT * FROM analysis_jobs WHERE status = ? AND tenant_id = ? ORDER BY created_at",
1513:                 (status, tenant_id),
1514:             )
1515:             return [dict(row) for row in cur.fetchall()]
1516: 
1517:     def update_analysis_job_status(
1518:         self,
1519:         job_id: int,
1520:         status: str,
1521:         started_at: str | None = None,
1522:         completed_at: str | None = None,
1523:         error: dict[str, Any] | None = None,
1524:     ) -> None:
1525:         tenant_id = self._tenant_id()
1526:         with self.connection() as conn:
1527:             conn.execute(
1528:                 """
1529:                 UPDATE analysis_jobs
1530:                 SET status = ?, started_at = COALESCE(?, started_at),
1531:                     completed_at = COALESCE(?, completed_at),
1532:                     error_json = ?
1533:                 WHERE job_id = ? AND tenant_id = ?
1534:                 """,
1535:                 (
1536:                     status,
1537:                     started_at,
1538:                     completed_at,
1539:                     json_dumps(error) if error else None,
1540:                     job_id,
1541:                     tenant_id,
1542:                 ),
1543:             )
1544: 
1545:     def create_dataset_table(
1546:         self,
1547:         table_name: str,
1548:         columns: list[dict[str, Any]],
1549:         conn: sqlite3.Connection | None = None,
1550:     ) -> None:
1551:         if conn is None:
1552:             with self.connection() as temp:
1553:                 self.create_dataset_table(table_name, columns, temp)
1554:                 return
1555:         safe_table = quote_identifier(table_name)
1556:         col_defs = []
1557:         for col in columns:
1558:             safe_name = quote_identifier(col["safe_name"])
1559:             col_type = col.get("sqlite_type", "TEXT")
1560:             col_defs.append(f"{safe_name} {col_type}")
1561:         ddl = (
1562:             f"CREATE TABLE IF NOT EXISTS {safe_table} ("
1563:             "row_id INTEGER PRIMARY KEY AUTOINCREMENT, "
1564:             "row_index INTEGER NOT NULL, "
1565:             "row_json TEXT, "
1566:             + ", ".join(col_defs)
1567:             + ")"
1568:         )
1569:         conn.execute(ddl)
1570: 
1571:     def create_template_table(
1572:         self,
1573:         table_name: str,
1574:         fields: list[dict[str, Any]],
1575:         conn: sqlite3.Connection | None = None,
1576:     ) -> None:
1577:         if conn is None:
1578:             with self.connection() as temp:
1579:                 self.create_template_table(table_name, fields, temp)
1580:                 return
1581:         safe_table = quote_identifier(table_name)
1582:         col_defs = []
1583:         for field in fields:
1584:             safe_name = quote_identifier(field["safe_name"])
1585:             col_type = field.get("sqlite_type", "TEXT")
1586:             col_defs.append(f"{safe_name} {col_type}")
1587:         ddl = (
1588:             f"CREATE TABLE IF NOT EXISTS {safe_table} ("
1589:             "row_id INTEGER PRIMARY KEY AUTOINCREMENT, "
1590:             "dataset_version_id TEXT NOT NULL, "
1591:             "row_index INTEGER NOT NULL, "
1592:             "row_json TEXT, "
1593:             + ", ".join(col_defs)
1594:             + ")"
1595:         )
1596:         conn.execute(ddl)
1597:         conn.execute(
1598:             f"CREATE INDEX IF NOT EXISTS idx_{table_name}_dataset ON {safe_table}(dataset_version_id)"
1599:         )
1600: 
1601:     def add_append_only_triggers(
1602:         self, table_name: str, conn: sqlite3.Connection | None = None
1603:     ) -> None:
1604:         if conn is None:
1605:             with self.connection() as temp:
1606:                 self.add_append_only_triggers(table_name, temp)
1607:                 return
1608:         safe_table = quote_identifier(table_name)
1609:         conn.execute(
1610:             f"""
1611:             CREATE TRIGGER IF NOT EXISTS {table_name}_no_delete
1612:             BEFORE DELETE ON {safe_table}
1613:             BEGIN
1614:                 SELECT RAISE(ABORT, 'raw data is append-only');
1615:             END;
1616:             """
1617:         )
1618:         conn.execute(
1619:             f"""
1620:             CREATE TRIGGER IF NOT EXISTS {table_name}_no_update
1621:             BEFORE UPDATE ON {safe_table}
1622:             BEGIN
1623:                 SELECT RAISE(ABORT, 'raw data is append-only');
1624:             END;
1625:             """
1626:         )
1627: 
1628:     def insert_dataset_rows(
1629:         self,
1630:         table_name: str,
1631:         safe_columns: list[str],
1632:         rows: list[tuple[Any, ...]],
1633:         conn: sqlite3.Connection | None = None,
1634:     ) -> None:
1635:         if not rows:
1636:             return
1637:         if conn is None:
1638:             with self.connection() as temp:
1639:                 self.insert_dataset_rows(table_name, safe_columns, rows, temp)
1640:                 return
1641:         quoted_cols = [quote_identifier(col) for col in safe_columns]
1642:         cols = ["row_index", "row_json"] + quoted_cols
1643:         placeholders = ", ".join(["?"] * len(cols))
1644:         sql = (
1645:             f"INSERT INTO {quote_identifier(table_name)} "
1646:             f"({', '.join(cols)}) VALUES ({placeholders})"
1647:         )
1648:         conn.executemany(sql, rows)
1649: 
1650:     def insert_template_rows(
1651:         self,
1652:         table_name: str,
1653:         safe_columns: list[str],
1654:         rows: list[tuple[Any, ...]],
1655:         conn: sqlite3.Connection | None = None,
1656:     ) -> None:
1657:         if not rows:
1658:             return
1659:         if conn is None:
1660:             with self.connection() as temp:
1661:                 self.insert_template_rows(table_name, safe_columns, rows, temp)
1662:                 return
1663:         quoted_cols = [quote_identifier(col) for col in safe_columns]
1664:         cols = ["dataset_version_id", "row_index", "row_json"] + quoted_cols
1665:         placeholders = ", ".join(["?"] * len(cols))
1666:         sql = (
1667:             f"INSERT INTO {quote_identifier(table_name)} "
1668:             f"({', '.join(cols)}) VALUES ({placeholders})"
1669:         )
1670:         conn.executemany(sql, rows)
1671: 
1672:     def close(self) -> None:
1673:         return
1674: 
1675:     def count_users(self) -> int:
1676:         with self.connection() as conn:
1677:             cur = conn.execute("SELECT COUNT(*) FROM users")
1678:             return int(cur.fetchone()[0])
1679: 
1680:     def list_tenants(self) -> list[dict[str, Any]]:
1681:         with self.connection() as conn:
1682:             cur = conn.execute(
1683:                 "SELECT tenant_id, name, created_at, is_default FROM tenants ORDER BY created_at"
1684:             )
1685:             return [dict(row) for row in cur.fetchall()]
1686: 
1687:     def create_tenant(self, tenant_id: str, name: str | None, created_at: str) -> None:
1688:         with self.connection() as conn:
1689:             conn.execute(
1690:                 """
1691:                 INSERT OR IGNORE INTO tenants (tenant_id, name, created_at, is_default)
1692:                 VALUES (?, ?, ?, 0)
1693:                 """,
1694:                 (tenant_id, name, created_at),
1695:             )
1696: 
1697:     def create_user(
1698:         self,
1699:         email: str,
1700:         password_hash: str,
1701:         name: str | None,
1702:         is_admin: bool,
1703:         created_at: str,
1704:     ) -> int:
1705:         normalized = email.strip().lower()
1706:         with self.connection() as conn:
1707:             cur = conn.execute(
1708:                 """
1709:                 INSERT INTO users (email, name, password_hash, is_admin, created_at)
1710:                 VALUES (?, ?, ?, ?, ?)
1711:                 """,
1712:                 (normalized, name, password_hash, 1 if is_admin else 0, created_at),
1713:             )
1714:             return int(cur.lastrowid)
1715: 
1716:     def fetch_user_by_email(self, email: str) -> dict[str, Any] | None:
1717:         normalized = email.strip().lower()
1718:         with self.connection() as conn:
1719:             cur = conn.execute(
1720:                 """
1721:                 SELECT user_id, email, name, password_hash, is_admin, created_at, disabled_at
1722:                 FROM users
1723:                 WHERE email = ?
1724:                 """,
1725:                 (normalized,),
1726:             )
1727:             row = cur.fetchone()
1728:             return dict(row) if row else None
1729: 
1730:     def fetch_user_by_id(self, user_id: int) -> dict[str, Any] | None:
1731:         with self.connection() as conn:
1732:             cur = conn.execute(
1733:                 """
1734:                 SELECT user_id, email, name, password_hash, is_admin, created_at, disabled_at
1735:                 FROM users
1736:                 WHERE user_id = ?
1737:                 """,
1738:                 (int(user_id),),
1739:             )
1740:             row = cur.fetchone()
1741:             return dict(row) if row else None
1742: 
1743:     def ensure_membership(
1744:         self, user_id: int, role: str, created_at: str, tenant_id: str | None = None
1745:     ) -> None:
1746:         tenant_id = tenant_id or self._tenant_id()
1747:         with self.connection() as conn:
1748:             conn.execute(
1749:                 """
1750:                 INSERT OR IGNORE INTO tenant_memberships
1751:                 (tenant_id, user_id, role, created_at)
1752:                 VALUES (?, ?, ?, ?)
1753:                 """,
1754:                 (tenant_id, int(user_id), role, created_at),
1755:             )
1756: 
1757:     def fetch_membership(
1758:         self, user_id: int, tenant_id: str | None = None
1759:     ) -> dict[str, Any] | None:
1760:         tenant_id = tenant_id or self._tenant_id()
1761:         with self.connection() as conn:
1762:             cur = conn.execute(
1763:                 """
1764:                 SELECT membership_id, tenant_id, user_id, role, created_at
1765:                 FROM tenant_memberships
1766:                 WHERE tenant_id = ? AND user_id = ?
1767:                 """,
1768:                 (tenant_id, int(user_id)),
1769:             )
1770:             row = cur.fetchone()
1771:             return dict(row) if row else None
1772: 
1773:     def list_users_for_tenant(self, tenant_id: str | None = None) -> list[dict[str, Any]]:
1774:         tenant_id = tenant_id or self._tenant_id()
1775:         with self.connection() as conn:
1776:             cur = conn.execute(
1777:                 """
1778:                 SELECT u.user_id, u.email, u.name, u.is_admin, u.created_at, u.disabled_at,
1779:                        m.role
1780:                 FROM tenant_memberships m
1781:                 JOIN users u ON u.user_id = m.user_id
1782:                 WHERE m.tenant_id = ?
1783:                 ORDER BY u.created_at
1784:                 """,
1785:                 (tenant_id,),
1786:             )
1787:             return [dict(row) for row in cur.fetchall()]
1788: 
1789:     def create_session(
1790:         self,
1791:         user_id: int,
1792:         token_hash: str,
1793:         created_at: str,
1794:         expires_at: str,
1795:         tenant_id: str | None = None,
1796:     ) -> int:
1797:         tenant_id = tenant_id or self._tenant_id()
1798:         with self.connection() as conn:
1799:             cur = conn.execute(
1800:                 """
1801:                 INSERT INTO user_sessions
1802:                 (user_id, tenant_id, token_hash, created_at, last_seen_at, expires_at)
1803:                 VALUES (?, ?, ?, ?, ?, ?)
1804:                 """,
1805:                 (
1806:                     int(user_id),
1807:                     tenant_id,
1808:                     token_hash,
1809:                     created_at,
1810:                     created_at,
1811:                     expires_at,
1812:                 ),
1813:             )
1814:             return int(cur.lastrowid)
1815: 
1816:     def fetch_session_by_hash(
1817:         self, token_hash: str, tenant_id: str | None = None
1818:     ) -> dict[str, Any] | None:
1819:         tenant_id = tenant_id or self._tenant_id()
1820:         with self.connection() as conn:
1821:             cur = conn.execute(
1822:                 """
1823:                 SELECT s.session_id, s.user_id, s.tenant_id, s.token_hash, s.created_at,
1824:                        s.last_seen_at, s.expires_at, s.revoked_at,
1825:                        u.email, u.name, u.is_admin, u.disabled_at
1826:                 FROM user_sessions s
1827:                 JOIN users u ON u.user_id = s.user_id
1828:                 WHERE s.token_hash = ? AND s.tenant_id = ?
1829:                 """,
1830:                 (token_hash, tenant_id),
1831:             )
1832:             row = cur.fetchone()
1833:             return dict(row) if row else None
1834: 
1835:     def touch_session(self, session_id: int, seen_at: str) -> None:
1836:         with self.connection() as conn:
1837:             conn.execute(
1838:                 "UPDATE user_sessions SET last_seen_at = ? WHERE session_id = ?",
1839:                 (seen_at, int(session_id)),
1840:             )
1841: 
1842:     def rotate_session(
1843:         self, session_id: int, token_hash: str, seen_at: str, expires_at: str
1844:     ) -> None:
1845:         with self.connection() as conn:
1846:             conn.execute(
1847:                 """
1848:                 UPDATE user_sessions
1849:                 SET token_hash = ?, last_seen_at = ?, expires_at = ?, revoked_at = NULL
1850:                 WHERE session_id = ?
1851:                 """,
1852:                 (token_hash, seen_at, expires_at, int(session_id)),
1853:             )
1854: 
1855:     def revoke_session(self, token_hash: str) -> None:
1856:         with self.connection() as conn:
1857:             conn.execute(
1858:                 "UPDATE user_sessions SET revoked_at = ? WHERE token_hash = ?",
1859:                 (now_iso(), token_hash),
1860:             )
1861: 
1862:     def create_api_key(
1863:         self,
1864:         user_id: int,
1865:         key_hash: str,
1866:         name: str | None,
1867:         created_at: str,
1868:         tenant_id: str | None = None,
1869:     ) -> int:
1870:         tenant_id = tenant_id or self._tenant_id()
1871:         with self.connection() as conn:
1872:             cur = conn.execute(
1873:                 """
1874:                 INSERT INTO api_keys
1875:                 (user_id, tenant_id, name, key_hash, created_at)
1876:                 VALUES (?, ?, ?, ?, ?)
1877:                 """,
1878:                 (int(user_id), tenant_id, name, key_hash, created_at),
1879:             )
1880:             return int(cur.lastrowid)
1881: 
1882:     def fetch_api_key_by_hash(
1883:         self, key_hash: str, tenant_id: str | None = None
1884:     ) -> dict[str, Any] | None:
1885:         tenant_id = tenant_id or self._tenant_id()
1886:         with self.connection() as conn:
1887:             cur = conn.execute(
1888:                 """
1889:                 SELECT k.key_id, k.user_id, k.tenant_id, k.name, k.key_hash, k.created_at,
1890:                        k.last_used_at, k.revoked_at,
1891:                        u.email, u.name AS user_name, u.is_admin, u.disabled_at
1892:                 FROM api_keys k
1893:                 JOIN users u ON u.user_id = k.user_id
1894:                 WHERE k.key_hash = ? AND k.tenant_id = ?
1895:                 """,
1896:                 (key_hash, tenant_id),
1897:             )
1898:             row = cur.fetchone()
1899:             return dict(row) if row else None
1900: 
1901:     def list_api_keys(self, tenant_id: str | None = None) -> list[dict[str, Any]]:
1902:         tenant_id = tenant_id or self._tenant_id()
1903:         with self.connection() as conn:
1904:             cur = conn.execute(
1905:                 """
1906:                 SELECT k.key_id, k.user_id, k.tenant_id, k.name, k.created_at,
1907:                        k.last_used_at, k.revoked_at, u.email
1908:                 FROM api_keys k
1909:                 JOIN users u ON u.user_id = k.user_id
1910:                 WHERE k.tenant_id = ?
1911:                 ORDER BY k.created_at DESC
1912:                 """,
1913:                 (tenant_id,),
1914:             )
1915:             return [dict(row) for row in cur.fetchall()]
1916: 
1917:     def touch_api_key(self, key_id: int, seen_at: str) -> None:
1918:         with self.connection() as conn:
1919:             conn.execute(
1920:                 "UPDATE api_keys SET last_used_at = ? WHERE key_id = ?",
1921:                 (seen_at, int(key_id)),
1922:             )
1923: 
1924:     def revoke_api_key(self, key_id: int) -> None:
1925:         with self.connection() as conn:
1926:             conn.execute(
1927:                 "UPDATE api_keys SET revoked_at = ? WHERE key_id = ?",
1928:                 (now_iso(), int(key_id)),
1929:             )
1930: 
1931:     def disable_user(self, user_id: int) -> None:
1932:         with self.connection() as conn:
1933:             conn.execute(
1934:                 "UPDATE users SET disabled_at = ? WHERE user_id = ?",
1935:                 (now_iso(), int(user_id)),
1936:             )
1937: 
1938:     def revoke_user_sessions(self, user_id: int) -> None:
1939:         with self.connection() as conn:
1940:             conn.execute(
1941:                 "UPDATE user_sessions SET revoked_at = ? WHERE user_id = ?",
1942:                 (now_iso(), int(user_id)),
1943:             )
1944: 
1945:     def revoke_user_api_keys(self, user_id: int) -> None:
1946:         with self.connection() as conn:
1947:             conn.execute(
1948:                 "UPDATE api_keys SET revoked_at = ? WHERE user_id = ?",
1949:                 (now_iso(), int(user_id)),
1950:             )
1951: 
1952:     def ensure_raw_format(
1953:         self, fingerprint: str, name: str | None = None, created_at: str | None = None
1954:     ) -> int:
1955:         tenant_id = self._tenant_id()
1956:         scoped_fingerprint = self._scoped_value(fingerprint)
1957:         with self.connection() as conn:
1958:             conn.execute(
1959:                 """
1960:                 INSERT OR IGNORE INTO raw_formats (fingerprint, tenant_id, name, created_at)
1961:                 VALUES (?, ?, ?, ?)
1962:                 """,
1963:                 (scoped_fingerprint, tenant_id, name, created_at),
1964:             )
1965:             cur = conn.execute(
1966:                 "SELECT format_id FROM raw_formats WHERE fingerprint = ? AND tenant_id = ?",
1967:                 (scoped_fingerprint, tenant_id),
1968:             )
1969:             row = cur.fetchone()
1970:             if not row:
1971:                 raise ValueError("Raw format not found")
1972:             return int(row[0])
1973: 
1974:     def list_raw_formats(self) -> list[dict[str, Any]]:
1975:         tenant_id = self._tenant_id()
1976:         with self.connection() as conn:
1977:             cur = conn.execute(
1978:                 """
1979:                 SELECT rf.format_id, rf.fingerprint, rf.name, rf.created_at,
1980:                        (SELECT COUNT(*) FROM dataset_versions dv
1981:                         WHERE dv.raw_format_id = rf.format_id AND dv.tenant_id = rf.tenant_id) AS dataset_count
1982:                 FROM raw_formats rf
1983:                 WHERE rf.tenant_id = ?
1984:                 ORDER BY rf.created_at DESC
1985:                 """,
1986:                 (tenant_id,),
1987:             )
1988:             return [dict(row) for row in cur.fetchall()]
1989: 
1990:     def add_raw_format_mapping(
1991:         self,
1992:         format_id: int,
1993:         template_id: int,
1994:         mapping_json: str,
1995:         mapping_hash: str,
1996:         notes: str | None,
1997:         created_at: str,
1998:     ) -> None:
1999:         tenant_id = self._tenant_id()
2000:         with self.connection() as conn:
2001:             conn.execute(
2002:                 """
2003:                 INSERT OR IGNORE INTO raw_format_mappings
2004:                 (format_id, tenant_id, template_id, mapping_json, mapping_hash, notes, created_at)
2005:                 VALUES (?, ?, ?, ?, ?, ?, ?)
2006:                 """,
2007:                 (format_id, tenant_id, template_id, mapping_json, mapping_hash, notes, created_at),
2008:             )
2009: 
2010:     def list_raw_format_mappings(self, format_id: int) -> list[dict[str, Any]]:
2011:         tenant_id = self._tenant_id()
2012:         with self.connection() as conn:
2013:             cur = conn.execute(
2014:                 """
2015:                 SELECT mapping_id, template_id, mapping_json, mapping_hash, notes, created_at
2016:                 FROM raw_format_mappings
2017:                 WHERE format_id = ? AND tenant_id = ?
2018:                 ORDER BY created_at DESC
2019:                 """,
2020:                 (format_id, tenant_id),
2021:             )
2022:             return [dict(row) for row in cur.fetchall()]
2023: 
2024:     def add_raw_format_note(
2025:         self, format_id: int, note: str, created_at: str
2026:     ) -> None:
2027:         tenant_id = self._tenant_id()
2028:         with self.connection() as conn:
2029:             conn.execute(
2030:                 """
2031:                 INSERT INTO raw_format_notes (format_id, tenant_id, note, created_at)
2032:                 VALUES (?, ?, ?, ?)
2033:                 """,
2034:                 (format_id, tenant_id, note, created_at),
2035:             )
2036: 
2037:     def list_raw_format_notes(self, format_id: int) -> list[dict[str, Any]]:
2038:         tenant_id = self._tenant_id()
2039:         with self.connection() as conn:
2040:             cur = conn.execute(
2041:                 """
2042:                 SELECT note_id, note, created_at
2043:                 FROM raw_format_notes
2044:                 WHERE format_id = ? AND tenant_id = ?
2045:                 ORDER BY created_at DESC
2046:                 """,
2047:                 (format_id, tenant_id),
2048:             )
2049:             return [dict(row) for row in cur.fetchall()]
2050: 
2051:     def create_template(
2052:         self,
2053:         name: str,
2054:         fields: list[dict[str, Any]],
2055:         description: str | None,
2056:         version: str | None,
2057:         created_at: str,
2058:     ) -> int:
2059:         tenant_id = self._tenant_id()
2060:         scoped_name = self._scoped_value(name)
2061:         table_name = f"template_{scoped_name.lower().replace(' ', '_').replace(':', '_')}"
2062:         with self.connection() as conn:
2063:             conn.execute(
2064:                 """
2065:                 INSERT INTO templates (name, tenant_id, description, version, created_at, table_name)
2066:                 VALUES (?, ?, ?, ?, ?, ?)
2067:                 """,
2068:                 (scoped_name, tenant_id, description, version, created_at, table_name),
2069:             )
2070:             cur = conn.execute(
2071:                 "SELECT template_id FROM templates WHERE name = ? AND tenant_id = ?",
2072:                 (scoped_name, tenant_id),
2073:             )
2074:             row = cur.fetchone()
2075:             if not row:
2076:                 raise ValueError("Template not created")
2077:             template_id = int(row[0])
2078:             prepared_fields = []
2079:             for idx, field in enumerate(fields, start=1):
2080:                 prepared_fields.append(
2081:                     {
2082:                         "field_id": idx,
2083:                         "safe_name": f"f{idx}",
2084:                         "name": field["name"],
2085:                         "dtype": field.get("dtype"),
2086:                         "role": field.get("role"),
2087:                         "required": 1 if field.get("required") else 0,
2088:                         "sqlite_type": field.get("sqlite_type", "TEXT"),
2089:                     }
2090:                 )
2091:             conn.executemany(
2092:                 """
2093:                 INSERT INTO template_fields
2094:                 (template_id, tenant_id, field_id, safe_name, name, dtype, role, required)
2095:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?)
2096:                 """,
2097:                 [
2098:                     (
2099:                         template_id,
2100:                         tenant_id,
2101:                         f["field_id"],
2102:                         f["safe_name"],
2103:                         f["name"],
2104:                         f.get("dtype"),
2105:                         f.get("role"),
2106:                         f.get("required", 0),
2107:                     )
2108:                     for f in prepared_fields
2109:                 ],
2110:             )
2111:             self.create_template_table(table_name, prepared_fields, conn)
2112:         return template_id
2113: 
2114:     def list_templates(self) -> list[dict[str, Any]]:
2115:         tenant_id = self._tenant_id()
2116:         with self.connection() as conn:
2117:             cur = conn.execute(
2118:                 "SELECT * FROM templates WHERE tenant_id = ? ORDER BY created_at DESC",
2119:                 (tenant_id,),
2120:             )
2121:             return [dict(row) for row in cur.fetchall()]
2122: 
2123:     def fetch_template(self, template_id: int) -> dict[str, Any] | None:
2124:         tenant_id = self._tenant_id()
2125:         with self.connection() as conn:
2126:             cur = conn.execute(
2127:                 "SELECT * FROM templates WHERE template_id = ? AND tenant_id = ?",
2128:                 (template_id, tenant_id),
2129:             )
2130:             row = cur.fetchone()
2131:             return dict(row) if row else None
2132: 
2133:     def fetch_template_fields(self, template_id: int) -> list[dict[str, Any]]:
2134:         tenant_id = self._tenant_id()
2135:         with self.connection() as conn:
2136:             cur = conn.execute(
2137:                 """
2138:                 SELECT field_id, safe_name, name, dtype, role, required
2139:                 FROM template_fields
2140:                 WHERE template_id = ? AND tenant_id = ?
2141:                 ORDER BY field_id
2142:                 """,
2143:                 (template_id, tenant_id),
2144:             )
2145:             return [dict(row) for row in cur.fetchall()]
2146: 
2147:     def upsert_dataset_template(
2148:         self,
2149:         dataset_version_id: str,
2150:         template_id: int,
2151:         mapping_json: str,
2152:         mapping_hash: str,
2153:         status: str,
2154:         created_at: str,
2155:         updated_at: str,
2156:     ) -> None:
2157:         tenant_id = self._tenant_id()
2158:         with self.connection() as conn:
2159:             conn.execute(
2160:                 """
2161:                 INSERT INTO dataset_templates
2162:                 (dataset_version_id, tenant_id, template_id, mapping_json, mapping_hash, status, created_at, updated_at)
2163:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?)
2164:                 ON CONFLICT(dataset_version_id, template_id)
2165:                 DO UPDATE SET mapping_json = excluded.mapping_json,
2166:                               mapping_hash = excluded.mapping_hash,
2167:                               status = excluded.status,
2168:                               updated_at = excluded.updated_at
2169:                 """,
2170:                 (
2171:                     dataset_version_id,
2172:                     tenant_id,
2173:                     template_id,
2174:                     mapping_json,
2175:                     mapping_hash,
2176:                     status,
2177:                     created_at,
2178:                     updated_at,
2179:                 ),
2180:             )
2181: 
2182:     def fetch_dataset_template(
2183:         self, dataset_version_id: str
2184:     ) -> dict[str, Any] | None:
2185:         tenant_id = self._tenant_id()
2186:         with self.connection() as conn:
2187:             cur = conn.execute(
2188:                 """
2189:                 SELECT dt.*, t.table_name, t.name AS template_name, t.version AS template_version
2190:                 FROM dataset_templates dt
2191:                 JOIN templates t ON t.template_id = dt.template_id
2192:                 WHERE dt.dataset_version_id = ? AND dt.tenant_id = ? AND t.tenant_id = ?
2193:                 ORDER BY dt.updated_at DESC
2194:                 LIMIT 1
2195:                 """,
2196:                 (dataset_version_id, tenant_id, tenant_id),
2197:             )
2198:             row = cur.fetchone()
2199:             return dict(row) if row else None
2200: 
2201:     def ensure_template_aggregate_dataset(
2202:         self,
2203:         template_id: int,
2204:         created_at: str,
2205:         filters: dict[str, Any] | None = None,
2206:     ) -> str:
2207:         template = self.fetch_template(template_id)
2208:         if not template:
2209:             raise ValueError("Template not found")
2210:         mapping: dict[str, Any] = {"scope": "all"}
2211:         if filters:
2212:             mapping["filters"] = filters
2213:         mapping_hash = hashlib.sha256(json_dumps(mapping).encode("utf-8")).hexdigest()
2214:         if filters:
2215:             dataset_version_id = f"template_{template_id}_all_{mapping_hash[:8]}"
2216:         else:
2217:             dataset_version_id = f"template_{template_id}_all"
2218:         project_id = f"template_{template_id}_meta"
2219:         fingerprint = dataset_version_id
2220:         self.ensure_project(project_id, fingerprint, created_at)
2221:         self.ensure_dataset(dataset_version_id, project_id, fingerprint, created_at)
2222:         self.ensure_dataset_version(
2223:             dataset_version_id,
2224:             dataset_version_id,
2225:             created_at,
2226:             template["table_name"],
2227:             f"template:{template_id}:all:{mapping_hash}",
2228:         )
2229:         self.upsert_dataset_template(
2230:             dataset_version_id,
2231:             template_id,
2232:             json_dumps(mapping),
2233:             mapping_hash,
2234:             "ready",
2235:             created_at,
2236:             created_at,
2237:         )
2238:         return dataset_version_id
2239: 
2240:     def record_template_conversion(
2241:         self,
2242:         dataset_version_id: str,
2243:         template_id: int,
2244:         status: str,
2245:         started_at: str | None,
2246:         completed_at: str | None,
2247:         mapping_hash: str,
2248:         row_count: int = 0,
2249:         error: dict[str, Any] | None = None,
2250:     ) -> None:
2251:         tenant_id = self._tenant_id()
2252:         with self.connection() as conn:
2253:             conn.execute(
2254:                 """
2255:                 INSERT INTO template_conversions
2256:                 (dataset_version_id, tenant_id, template_id, status, started_at, completed_at, error_json, mapping_hash, row_count)
2257:                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
2258:                 """,
2259:                 (
2260:                     dataset_version_id,
2261:                     tenant_id,
2262:                     template_id,
2263:                     status,
2264:                     started_at,
2265:                     completed_at,
2266:                     json_dumps(error) if error else None,
2267:                     mapping_hash,
2268:                     int(row_count),
2269:                 ),
2270:             )
2271: 
2272:     def trace_from_entity(
2273:         self, entity_type: str, key: str, max_depth: int = 5
2274:     ) -> dict[str, Any]:
2275:         with self.connection() as conn:
2276:             cur = conn.execute(
2277:                 """
2278:                 WITH RECURSIVE trace(entity_id, type, key, depth) AS (
2279:                     SELECT entity_id, type, key, 0
2280:                     FROM entities
2281:                     WHERE type = ? AND key = ?
2282:                     UNION ALL
2283:                     SELECT e.dst_entity_id, ent.type, ent.key, trace.depth + 1
2284:                     FROM edges e
2285:                     JOIN trace ON e.src_entity_id = trace.entity_id
2286:                     JOIN entities ent ON ent.entity_id = e.dst_entity_id
2287:                     WHERE trace.depth < ?
2288:                 )
2289:                 SELECT entity_id, type, key, depth FROM trace
2290:                 """,
2291:                 (entity_type, key, max_depth),
2292:             )
2293:             nodes = [dict(row) for row in cur.fetchall()]
2294:             node_ids = [row["entity_id"] for row in nodes]
2295:             if not node_ids:
2296:                 return {"nodes": [], "edges": []}
2297:             placeholders = ", ".join(["?"] * len(node_ids))
2298:             edges_cur = conn.execute(
2299:                 f"""
2300:                 SELECT src_entity_id, dst_entity_id, kind, evidence_json, score
2301:                 FROM edges
2302:                 WHERE src_entity_id IN ({placeholders})
2303:                 """,
2304:                 node_ids,
2305:             )
2306:             return {"nodes": nodes, "edges": [dict(row) for row in edges_cur.fetchall()]}
````

## File: src/statistic_harness/ui/server.py
````python
   1: from __future__ import annotations
   2: 
   3: import base64
   4: import hashlib
   5: import hmac
   6: import json
   7: import os
   8: import re
   9: import sqlite3
  10: import shutil
  11: import struct
  12: import uuid
  13: from dataclasses import dataclass
  14: from datetime import datetime, timedelta, timezone
  15: from pathlib import Path
  16: 
  17: import yaml
  18: from fastapi import (
  19:     BackgroundTasks,
  20:     FastAPI,
  21:     File,
  22:     Form,
  23:     HTTPException,
  24:     Request,
  25:     Query,
  26:     UploadFile,
  27: )
  28: from fastapi.responses import FileResponse, HTMLResponse, JSONResponse
  29: from starlette.responses import RedirectResponse
  30: from fastapi.staticfiles import StaticFiles
  31: from fastapi.templating import Jinja2Templates
  32: 
  33: from statistic_harness.core import evaluation as eval_core
  34: from statistic_harness.core.evaluation import evaluate_report
  35: from statistic_harness.core.known_issue_compiler import compile_known_issues
  36: from statistic_harness.core.auth import (
  37:     AuthUser,
  38:     generate_api_key,
  39:     generate_session_token,
  40:     hash_password,
  41:     hash_token,
  42:     is_api_key,
  43:     normalize_email,
  44:     verify_password,
  45: )
  46: from statistic_harness.core.pipeline import Pipeline
  47: from statistic_harness.core.report import build_report, write_report
  48: from statistic_harness.core.tenancy import get_tenant_context
  49: from statistic_harness.core.utils import (
  50:     DEFAULT_TENANT_ID,
  51:     auth_enabled,
  52:     file_sha256,
  53:     json_dumps,
  54:     max_upload_bytes,
  55:     now_iso,
  56:     safe_join,
  57:     scope_key,
  58:     stable_hash,
  59:     vector_store_enabled,
  60: )
  61: from statistic_harness.core.vector_store import VectorStore, hash_embedding
  62: 
  63: app = FastAPI()
  64: BASE_DIR = Path(__file__).resolve().parent
  65: TEMPLATES = Jinja2Templates(directory=str(BASE_DIR / "templates"))
  66: app.mount("/static", StaticFiles(directory=str(BASE_DIR / "static")), name="static")
  67: 
  68: TENANT_CTX = get_tenant_context()
  69: APPDATA_DIR = TENANT_CTX.tenant_root
  70: pipeline = Pipeline(TENANT_CTX.appdata_root, Path("plugins"), tenant_id=TENANT_CTX.tenant_id)
  71: KNOWN_ISSUES_DIR = APPDATA_DIR / "known_issues"
  72: AUTH_ENABLED = auth_enabled()
  73: SESSION_COOKIE_NAME = "stat_harness_session"
  74: 
  75: 
  76: def _session_ttl_hours() -> int:
  77:     raw = os.environ.get("STAT_HARNESS_SESSION_TTL_HOURS", "").strip()
  78:     if not raw:
  79:         return 24
  80:     try:
  81:         value = int(raw)
  82:     except ValueError:
  83:         return 24
  84:     return max(1, value)
  85: 
  86: 
  87: def _expires_at(hours: int) -> str:
  88:     return (datetime.now(timezone.utc) + timedelta(hours=hours)).isoformat()
  89: 
  90: 
  91: def _is_expired(expires_at: str | None) -> bool:
  92:     if not expires_at:
  93:         return False
  94:     try:
  95:         return datetime.fromisoformat(expires_at) <= datetime.now(timezone.utc)
  96:     except ValueError:
  97:         return False
  98: 
  99: 
 100: @dataclass
 101: class AuthResult:
 102:     user: AuthUser
 103:     session_id: int | None = None
 104:     rotated_token: str | None = None
 105:     api_key_id: int | None = None
 106: 
 107: 
 108: def _extract_token(request: Request) -> tuple[str | None, str | None]:
 109:     auth_header = request.headers.get("Authorization", "").strip()
 110:     if auth_header.lower().startswith("bearer "):
 111:         token = auth_header.split(" ", 1)[1].strip()
 112:         return token, "header"
 113:     api_key = request.headers.get("X-API-Key", "").strip()
 114:     if api_key:
 115:         return api_key, "api_key"
 116:     cookie_token = request.cookies.get(SESSION_COOKIE_NAME, "").strip()
 117:     if cookie_token:
 118:         return cookie_token, "cookie"
 119:     return None, None
 120: 
 121: 
 122: def _current_tenant_id() -> str:
 123:     return TENANT_CTX.tenant_id
 124: 
 125: 
 126: def _authenticate_request(request: Request) -> AuthResult | None:
 127:     token, source = _extract_token(request)
 128:     if not token:
 129:         return None
 130:     now = now_iso()
 131:     tenant_id = _current_tenant_id()
 132:     if source == "api_key" or is_api_key(token):
 133:         key_hash = hash_token(token)
 134:         row = pipeline.storage.fetch_api_key_by_hash(key_hash, tenant_id=tenant_id)
 135:         if not row or row.get("revoked_at") or row.get("disabled_at"):
 136:             return None
 137:         membership = pipeline.storage.fetch_membership(int(row["user_id"]), tenant_id)
 138:         if not membership:
 139:             return None
 140:         pipeline.storage.touch_api_key(int(row["key_id"]), now)
 141:         user = AuthUser(
 142:             user_id=int(row["user_id"]),
 143:             email=str(row["email"]),
 144:             name=row.get("user_name"),
 145:             is_admin=bool(row.get("is_admin")),
 146:             tenant_id=tenant_id,
 147:         )
 148:         return AuthResult(user=user, api_key_id=int(row["key_id"]))
 149: 
 150:     token_hash = hash_token(token)
 151:     session = pipeline.storage.fetch_session_by_hash(token_hash, tenant_id=tenant_id)
 152:     if not session or session.get("revoked_at") or session.get("disabled_at"):
 153:         return None
 154:     if _is_expired(session.get("expires_at")):
 155:         return None
 156:     membership = pipeline.storage.fetch_membership(int(session["user_id"]), tenant_id)
 157:     if not membership:
 158:         return None
 159: 
 160:     rotated_token = None
 161:     if source == "cookie":
 162:         rotated_token = generate_session_token()
 163:         pipeline.storage.rotate_session(
 164:             int(session["session_id"]),
 165:             hash_token(rotated_token),
 166:             now,
 167:             _expires_at(_session_ttl_hours()),
 168:         )
 169:     else:
 170:         pipeline.storage.touch_session(int(session["session_id"]), now)
 171:     user = AuthUser(
 172:         user_id=int(session["user_id"]),
 173:         email=str(session["email"]),
 174:         name=session.get("name"),
 175:         is_admin=bool(session.get("is_admin")),
 176:         tenant_id=tenant_id,
 177:     )
 178:     return AuthResult(user=user, session_id=int(session["session_id"]), rotated_token=rotated_token)
 179: 
 180: 
 181: def _auth_required_response(request: Request) -> JSONResponse | RedirectResponse:
 182:     if request.url.path.startswith("/api/"):
 183:         return JSONResponse(status_code=401, content={"error": "unauthorized"})
 184:     params = f"next={request.url.path}"
 185:     return RedirectResponse(url=f"/login?{params}", status_code=303)
 186: 
 187: 
 188: def _require_admin(request: Request) -> AuthUser:
 189:     user = getattr(request.state, "user", None)
 190:     if not user:
 191:         raise HTTPException(status_code=401, detail="unauthorized")
 192:     if user.is_admin:
 193:         return user
 194:     membership = pipeline.storage.fetch_membership(int(user.user_id))
 195:     if membership and membership.get("role") == "admin":
 196:         return user
 197:     raise HTTPException(status_code=403, detail="admin required")
 198: 
 199: 
 200: @app.middleware("http")
 201: async def auth_middleware(request: Request, call_next):
 202:     if not AUTH_ENABLED:
 203:         return await call_next(request)
 204:     path = request.url.path
 205:     if path.startswith("/static"):
 206:         return await call_next(request)
 207:     if path in {"/login", "/logout", "/bootstrap"}:
 208:         return await call_next(request)
 209:     if pipeline.storage.count_users() == 0 and path != "/bootstrap":
 210:         return RedirectResponse(url="/bootstrap", status_code=303)
 211:     auth = _authenticate_request(request)
 212:     if not auth:
 213:         return _auth_required_response(request)
 214:     request.state.user = auth.user
 215:     request.state.session_token = auth.rotated_token
 216:     response = await call_next(request)
 217:     if auth.rotated_token:
 218:         response.set_cookie(
 219:             SESSION_COOKIE_NAME,
 220:             auth.rotated_token,
 221:             httponly=True,
 222:             samesite="lax",
 223:         )
 224:     return response
 225: ROLE_KEYS = [
 226:     "queue_time",
 227:     "start_time",
 228:     "end_time",
 229:     "process_id",
 230:     "process_name",
 231:     "module_code",
 232:     "user_id",
 233:     "dependency_id",
 234:     "master_id",
 235:     "host_id",
 236:     "status",
 237: ]
 238: 
 239: 
 240: def _parse_filter_params(
 241:     project_ids: str = "",
 242:     dataset_ids: str = "",
 243:     dataset_version_ids: str = "",
 244:     raw_format_ids: str = "",
 245:     created_after: str = "",
 246:     created_before: str = "",
 247: ) -> dict[str, object]:
 248:     def parse_csv(value: str) -> list[str]:
 249:         return [item.strip() for item in value.split(",") if item.strip()]
 250: 
 251:     filters: dict[str, object] = {}
 252:     proj_list = sorted(parse_csv(project_ids))
 253:     if proj_list:
 254:         filters["project_ids"] = proj_list
 255:     ds_list = sorted(parse_csv(dataset_ids))
 256:     if ds_list:
 257:         filters["dataset_ids"] = ds_list
 258:     dv_list = sorted(parse_csv(dataset_version_ids))
 259:     if dv_list:
 260:         filters["dataset_version_ids"] = dv_list
 261:     rf_list = sorted({int(v) for v in parse_csv(raw_format_ids) if v.isdigit()})
 262:     if rf_list:
 263:         filters["raw_format_ids"] = rf_list
 264:     if created_after:
 265:         filters["created_after"] = created_after
 266:     if created_before:
 267:         filters["created_before"] = created_before
 268:     return filters
 269: 
 270: 
 271: def _ground_truth_template(report: dict) -> str:
 272:     features = [
 273:         f.get("feature")
 274:         for f in report.get("plugins", {})
 275:         .get("analysis_gaussian_knockoffs", {})
 276:         .get("findings", [])
 277:     ]
 278:     template = {
 279:         "strict": False,
 280:         "features": [f for f in features if f],
 281:         "changepoints": [],
 282:         "dependence_shift_pairs": [],
 283:         "anomalies": [],
 284:         "min_anomaly_hits": 0,
 285:         "changepoint_tolerance": 3,
 286:     }
 287:     return yaml.safe_dump(template)
 288: 
 289: 
 290: def _known_issues_paths(sha256: str) -> tuple[Path, Path]:
 291:     KNOWN_ISSUES_DIR.mkdir(parents=True, exist_ok=True)
 292:     json_path = KNOWN_ISSUES_DIR / f"{sha256}.json"
 293:     yaml_path = KNOWN_ISSUES_DIR / f"{sha256}.yaml"
 294:     return json_path, yaml_path
 295: 
 296: 
 297: def _vector_store() -> tuple[VectorStore | None, str | None]:
 298:     if not vector_store_enabled():
 299:         return None, "Vector store is disabled (set STAT_HARNESS_ENABLE_VECTOR_STORE=1)."
 300:     try:
 301:         store = VectorStore(TENANT_CTX.db_path, tenant_id=TENANT_CTX.tenant_id)
 302:         return store, None
 303:     except RuntimeError as exc:
 304:         return None, str(exc)
 305: 
 306: 
 307: def _cursor_secret() -> bytes:
 308:     raw = os.environ.get("STAT_HARNESS_VECTOR_CURSOR_SECRET", "").strip()
 309:     if raw:
 310:         return raw.encode("utf-8")
 311:     fallback = f"{TENANT_CTX.db_path}:{TENANT_CTX.tenant_id}".encode("utf-8")
 312:     return hashlib.sha256(fallback).digest()
 313: 
 314: 
 315: def _b64encode(data: bytes) -> str:
 316:     return base64.urlsafe_b64encode(data).decode("ascii").rstrip("=")
 317: 
 318: 
 319: def _b64decode(token: str) -> bytes:
 320:     padding = "=" * (-len(token) % 4)
 321:     return base64.urlsafe_b64decode(token + padding)
 322: 
 323: 
 324: def _encode_vector(values: list[float]) -> str:
 325:     packed = struct.pack(f"{len(values)}f", *values)
 326:     return _b64encode(packed)
 327: 
 328: 
 329: def _decode_vector(token: str, dimensions: int | None = None) -> list[float]:
 330:     raw = _b64decode(token)
 331:     if len(raw) % 4 != 0:
 332:         raise ValueError("Invalid vector encoding")
 333:     count = len(raw) // 4
 334:     if dimensions is not None and count != int(dimensions):
 335:         raise ValueError("Vector dimensions mismatch")
 336:     return list(struct.unpack(f"{count}f", raw))
 337: 
 338: 
 339: def _encode_cursor(payload: dict[str, Any]) -> str:
 340:     data = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
 341:     sig = hmac.new(_cursor_secret(), data, hashlib.sha256).digest()
 342:     return f"{_b64encode(data)}.{_b64encode(sig)}"
 343: 
 344: 
 345: def _decode_cursor(token: str) -> dict[str, Any]:
 346:     try:
 347:         data_b64, sig_b64 = token.split(".", 1)
 348:     except ValueError as exc:
 349:         raise ValueError("Invalid cursor token") from exc
 350:     data = _b64decode(data_b64)
 351:     sig = _b64decode(sig_b64)
 352:     expected = hmac.new(_cursor_secret(), data, hashlib.sha256).digest()
 353:     if not hmac.compare_digest(sig, expected):
 354:         raise ValueError("Invalid cursor signature")
 355:     payload = json.loads(data.decode("utf-8"))
 356:     if payload.get("v") != 1:
 357:         raise ValueError("Unsupported cursor version")
 358:     if payload.get("tenant_id") != TENANT_CTX.tenant_id:
 359:         raise ValueError("Cursor tenant mismatch")
 360:     return payload
 361: 
 362: 
 363: def _vector_default_collection(
 364:     collection: str, collections: list[dict[str, Any]]
 365: ) -> str:
 366:     if collection:
 367:         return collection
 368:     if collections:
 369:         return str(collections[0].get("name") or "")
 370:     return ""
 371: 
 372: 
 373: def _parse_vector(text: str) -> list[float]:
 374:     parts = [part for part in text.replace("\n", ",").split(",") if part.strip()]
 375:     if not parts:
 376:         raise ValueError("Vector is empty")
 377:     values: list[float] = []
 378:     for part in parts:
 379:         values.append(float(part.strip()))
 380:     return values
 381: 
 382: 
 383: def _resolve_vector_dimensions(store: VectorStore, collection: str, dimensions: int | None) -> int:
 384:     if dimensions is not None:
 385:         return int(dimensions)
 386:     dims = store.collection_dimensions(collection)
 387:     if not dims:
 388:         raise ValueError("Unknown collection or dimensions required")
 389:     if len(dims) > 1:
 390:         raise ValueError("Multiple dimensions found; provide dimensions")
 391:     return int(dims[0])
 392: 
 393: 
 394: def _vector_k_max() -> int:
 395:     raw = os.environ.get("STAT_HARNESS_VECTOR_K_MAX", "").strip()
 396:     if not raw:
 397:         return 1000
 398:     try:
 399:         value = int(raw)
 400:     except ValueError:
 401:         return 1000
 402:     return max(1, value)
 403: 
 404: 
 405: def _normalize_k(value: int | str | None) -> int:
 406:     try:
 407:         k = int(value or 10)
 408:     except (TypeError, ValueError):
 409:         k = 10
 410:     if k < 1:
 411:         k = 1
 412:     max_k = _vector_k_max()
 413:     if k > max_k:
 414:         k = max_k
 415:     return k
 416: 
 417: 
 418: def _normalize_offset(value: int | str | None) -> int:
 419:     try:
 420:         offset = int(value or 0)
 421:     except (TypeError, ValueError):
 422:         offset = 0
 423:     if offset < 0:
 424:         offset = 0
 425:     return offset
 426: 
 427: 
 428: @app.get("/login", response_class=HTMLResponse)
 429: async def login_view(request: Request, next: str = "/") -> HTMLResponse:
 430:     if not AUTH_ENABLED:
 431:         return RedirectResponse(url=next or "/", status_code=303)
 432:     bootstrap = pipeline.storage.count_users() == 0
 433:     return TEMPLATES.TemplateResponse(
 434:         "login.html",
 435:         {"request": request, "next": next, "error": None, "bootstrap": bootstrap},
 436:     )
 437: 
 438: 
 439: @app.post("/login")
 440: async def login_action(
 441:     request: Request,
 442:     email: str = Form(...),
 443:     password: str = Form(...),
 444:     next: str = Form("/"),
 445: ) -> HTMLResponse:
 446:     if not AUTH_ENABLED:
 447:         return RedirectResponse(url=next or "/", status_code=303)
 448:     user = pipeline.storage.fetch_user_by_email(normalize_email(email))
 449:     if not user or user.get("disabled_at"):
 450:         return TEMPLATES.TemplateResponse(
 451:             "login.html",
 452:             {
 453:                 "request": request,
 454:                 "next": next,
 455:                 "error": "Invalid credentials",
 456:                 "bootstrap": pipeline.storage.count_users() == 0,
 457:             },
 458:             status_code=401,
 459:         )
 460:     membership = pipeline.storage.fetch_membership(int(user["user_id"]))
 461:     if not membership:
 462:         return TEMPLATES.TemplateResponse(
 463:             "login.html",
 464:             {
 465:                 "request": request,
 466:                 "next": next,
 467:                 "error": "Access denied",
 468:                 "bootstrap": pipeline.storage.count_users() == 0,
 469:             },
 470:             status_code=403,
 471:         )
 472:     if not verify_password(password, user.get("password_hash") or ""):
 473:         return TEMPLATES.TemplateResponse(
 474:             "login.html",
 475:             {
 476:                 "request": request,
 477:                 "next": next,
 478:                 "error": "Invalid credentials",
 479:                 "bootstrap": pipeline.storage.count_users() == 0,
 480:             },
 481:             status_code=401,
 482:         )
 483:     token = generate_session_token()
 484:     pipeline.storage.create_session(
 485:         int(user["user_id"]),
 486:         hash_token(token),
 487:         now_iso(),
 488:         _expires_at(_session_ttl_hours()),
 489:     )
 490:     response = RedirectResponse(url=next or "/", status_code=303)
 491:     response.set_cookie(
 492:         SESSION_COOKIE_NAME,
 493:         token,
 494:         httponly=True,
 495:         samesite="lax",
 496:     )
 497:     return response
 498: 
 499: 
 500: @app.get("/logout")
 501: async def logout_view(request: Request) -> RedirectResponse:
 502:     if AUTH_ENABLED:
 503:         token = request.cookies.get(SESSION_COOKIE_NAME, "").strip()
 504:         if token:
 505:             pipeline.storage.revoke_session(hash_token(token))
 506:     response = RedirectResponse(url="/login", status_code=303)
 507:     response.delete_cookie(SESSION_COOKIE_NAME)
 508:     return response
 509: 
 510: 
 511: @app.get("/bootstrap", response_class=HTMLResponse)
 512: async def bootstrap_view(request: Request) -> HTMLResponse:
 513:     if not AUTH_ENABLED:
 514:         return RedirectResponse(url="/", status_code=303)
 515:     if pipeline.storage.count_users() > 0:
 516:         return RedirectResponse(url="/login", status_code=303)
 517:     return TEMPLATES.TemplateResponse(
 518:         "bootstrap.html",
 519:         {"request": request, "error": None},
 520:     )
 521: 
 522: 
 523: @app.post("/bootstrap")
 524: async def bootstrap_action(
 525:     request: Request,
 526:     email: str = Form(...),
 527:     password: str = Form(...),
 528:     name: str = Form(""),
 529: ) -> HTMLResponse:
 530:     if not AUTH_ENABLED:
 531:         return RedirectResponse(url="/", status_code=303)
 532:     if pipeline.storage.count_users() > 0:
 533:         return RedirectResponse(url="/login", status_code=303)
 534:     normalized = normalize_email(email)
 535:     user_id = pipeline.storage.create_user(
 536:         normalized, hash_password(password), name or None, True, now_iso()
 537:     )
 538:     pipeline.storage.ensure_membership(user_id, "admin", now_iso())
 539:     token = generate_session_token()
 540:     pipeline.storage.create_session(
 541:         user_id,
 542:         hash_token(token),
 543:         now_iso(),
 544:         _expires_at(_session_ttl_hours()),
 545:     )
 546:     response = RedirectResponse(url="/", status_code=303)
 547:     response.set_cookie(
 548:         SESSION_COOKIE_NAME,
 549:         token,
 550:         httponly=True,
 551:         samesite="lax",
 552:     )
 553:     return response
 554: 
 555: 
 556: @app.get("/vectors", response_class=HTMLResponse)
 557: async def vectors_view(
 558:     request: Request,
 559:     collection: str = "",
 560:     text: str = "",
 561:     k: int = 10,
 562:     dimensions: int | None = None,
 563: ) -> HTMLResponse:
 564:     store, error = _vector_store()
 565:     collections = store.list_collections() if store else []
 566:     default_collection = _vector_default_collection(collection, collections)
 567:     k = _normalize_k(k)
 568:     return TEMPLATES.TemplateResponse(
 569:         "vectors.html",
 570:         {
 571:             "request": request,
 572:             "error": error,
 573:             "collections": collections,
 574:             "results": None,
 575:             "query": {
 576:                 "collection": collection,
 577:                 "text": text,
 578:                 "k": k,
 579:                 "offset": 0,
 580:                 "dimensions": dimensions,
 581:             },
 582:             "default_collection": default_collection,
 583:             "pagination": None,
 584:         },
 585:     )
 586: 
 587: 
 588: @app.post("/vectors/query", response_class=HTMLResponse)
 589: async def vectors_query(
 590:     request: Request,
 591:     collection: str = Form(...),
 592:     text: str = Form(""),
 593:     vector: str = Form(""),
 594:     k: int = Form(10),
 595:     cursor: str = Form(""),
 596:     dimensions: int | None = Form(None),
 597: ) -> HTMLResponse:
 598:     store, error = _vector_store()
 599:     collections = store.list_collections() if store else []
 600:     page_size = _normalize_k(k)
 601:     offset = 0
 602:     as_of = None
 603:     mode = ""
 604:     query_vector: list[float] | None = None
 605:     cursor_payload: dict[str, Any] | None = None
 606:     query_payload = {
 607:         "collection": collection,
 608:         "text": text,
 609:         "vector": vector,
 610:         "k": page_size,
 611:         "dimensions": dimensions,
 612:     }
 613:     default_collection = _vector_default_collection(collection, collections)
 614:     if error or not store:
 615:         return TEMPLATES.TemplateResponse(
 616:             "vectors.html",
 617:             {
 618:                 "request": request,
 619:                 "error": error,
 620:                 "collections": collections,
 621:                 "results": None,
 622:                 "query": query_payload,
 623:                 "default_collection": default_collection,
 624:                 "pagination": None,
 625:             },
 626:             status_code=400,
 627:         )
 628:     try:
 629:         if cursor:
 630:             cursor_payload = _decode_cursor(cursor)
 631:             collection = str(cursor_payload.get("collection") or collection)
 632:             page_size = _normalize_k(cursor_payload.get("page_size"))
 633:             offset = _normalize_offset(cursor_payload.get("offset"))
 634:             as_of = cursor_payload.get("as_of")
 635:             mode = str(cursor_payload.get("mode") or "")
 636:             dimensions = int(cursor_payload.get("dimensions") or 0) or dimensions
 637:             if mode == "text":
 638:                 text = str(cursor_payload.get("text") or "")
 639:                 query_vector = hash_embedding(text, int(dimensions or 0))
 640:             elif mode == "vector":
 641:                 vector_token = str(cursor_payload.get("vector") or "")
 642:                 query_vector = _decode_vector(vector_token, int(dimensions or 0))
 643:             else:
 644:                 raise ValueError("Invalid cursor mode")
 645:         else:
 646:             if text and vector:
 647:                 raise ValueError("Provide either text or vector, not both")
 648:             if not text and not vector:
 649:                 raise ValueError("Provide text or vector")
 650:             if vector:
 651:                 query_vector = _parse_vector(vector)
 652:                 dimensions = len(query_vector)
 653:                 mode = "vector"
 654:             else:
 655:                 dimensions = _resolve_vector_dimensions(store, collection, dimensions)
 656:                 query_vector = hash_embedding(text, int(dimensions))
 657:                 mode = "text"
 658:             as_of = now_iso()
 659:             cursor_payload = {
 660:                 "v": 1,
 661:                 "tenant_id": TENANT_CTX.tenant_id,
 662:                 "collection": collection,
 663:                 "mode": mode,
 664:                 "text": text if mode == "text" else None,
 665:                 "vector": _encode_vector(query_vector) if mode == "vector" else None,
 666:                 "dimensions": int(dimensions),
 667:                 "page_size": page_size,
 668:                 "offset": offset,
 669:                 "as_of": as_of,
 670:                 "embed": "hash_embedding:v1" if mode == "text" else "raw",
 671:             }
 672: 
 673:         fetch_k = offset + page_size
 674:         if fetch_k > _vector_k_max():
 675:             raise ValueError(
 676:                 f"Requested page exceeds max_k={_vector_k_max()}; set STAT_HARNESS_VECTOR_K_MAX"
 677:             )
 678:         results = store.query(
 679:             collection, query_vector, k=fetch_k, as_of=as_of
 680:         )
 681:         page = results[offset : offset + page_size]
 682:         has_next = len(results) > offset + page_size
 683:         has_prev = offset > 0
 684:         next_cursor = None
 685:         prev_cursor = None
 686:         if cursor_payload:
 687:             base_payload = dict(cursor_payload)
 688:             if has_next:
 689:                 base_payload["offset"] = offset + page_size
 690:                 next_cursor = _encode_cursor(base_payload)
 691:             if has_prev:
 692:                 base_payload["offset"] = max(0, offset - page_size)
 693:                 prev_cursor = _encode_cursor(base_payload)
 694:         return TEMPLATES.TemplateResponse(
 695:             "vectors.html",
 696:             {
 697:                 "request": request,
 698:                 "error": None,
 699:                 "collections": collections,
 700:                 "results": page,
 701:                 "query": {
 702:                     "collection": collection,
 703:                     "text": text,
 704:                     "vector": vector,
 705:                     "k": page_size,
 706:                     "dimensions": dimensions,
 707:                 },
 708:                 "default_collection": default_collection,
 709:                 "pagination": {
 710:                     "offset": offset,
 711:                     "page_size": page_size,
 712:                     "has_next": has_next,
 713:                     "has_prev": has_prev,
 714:                     "next_offset": offset + page_size,
 715:                     "prev_offset": max(0, offset - page_size),
 716:                     "cursor": _encode_cursor(cursor_payload) if cursor_payload else None,
 717:                     "next_cursor": next_cursor,
 718:                     "prev_cursor": prev_cursor,
 719:                     "as_of": as_of,
 720:                 },
 721:             },
 722:         )
 723:     except Exception as exc:
 724:         return TEMPLATES.TemplateResponse(
 725:             "vectors.html",
 726:             {
 727:                 "request": request,
 728:                 "error": str(exc),
 729:                 "collections": collections,
 730:                 "results": None,
 731:                 "query": query_payload,
 732:                 "default_collection": default_collection,
 733:                 "pagination": None,
 734:             },
 735:             status_code=400,
 736:         )
 737: 
 738: 
 739: @app.get("/api/vectors/collections")
 740: async def vector_collections_api(request: Request) -> JSONResponse:
 741:     if AUTH_ENABLED and not getattr(request.state, "user", None):
 742:         raise HTTPException(status_code=401, detail="unauthorized")
 743:     store, error = _vector_store()
 744:     if error or not store:
 745:         raise HTTPException(status_code=400, detail=error or "Vector store unavailable")
 746:     return JSONResponse({"collections": store.list_collections()})
 747: 
 748: 
 749: @app.post("/api/vectors/query")
 750: async def vector_query_api(request: Request, payload: dict[str, Any]) -> JSONResponse:
 751:     if AUTH_ENABLED and not getattr(request.state, "user", None):
 752:         raise HTTPException(status_code=401, detail="unauthorized")
 753:     store, error = _vector_store()
 754:     if error or not store:
 755:         raise HTTPException(status_code=400, detail=error or "Vector store unavailable")
 756:     cursor = payload.get("cursor")
 757:     collection = str(payload.get("collection") or "")
 758:     text = payload.get("text")
 759:     vector = payload.get("vector")
 760:     page_size = _normalize_k(payload.get("k"))
 761:     offset = _normalize_offset(payload.get("offset"))
 762:     dimensions = payload.get("dimensions")
 763:     as_of = None
 764:     mode = ""
 765:     query_vector: list[float] | None = None
 766:     cursor_payload: dict[str, Any] | None = None
 767:     if cursor:
 768:         cursor_payload = _decode_cursor(str(cursor))
 769:         collection = str(cursor_payload.get("collection") or collection)
 770:         page_size = _normalize_k(cursor_payload.get("page_size"))
 771:         offset = _normalize_offset(cursor_payload.get("offset"))
 772:         as_of = cursor_payload.get("as_of")
 773:         mode = str(cursor_payload.get("mode") or "")
 774:         dimensions = int(cursor_payload.get("dimensions") or 0) or dimensions
 775:         if mode == "text":
 776:             text = str(cursor_payload.get("text") or "")
 777:             query_vector = hash_embedding(text, int(dimensions or 0))
 778:         elif mode == "vector":
 779:             query_vector = _decode_vector(str(cursor_payload.get("vector") or ""), int(dimensions or 0))
 780:         else:
 781:             raise HTTPException(status_code=400, detail="Invalid cursor mode")
 782:     else:
 783:         if not collection:
 784:             raise HTTPException(status_code=400, detail="collection required")
 785:         if text and vector:
 786:             raise HTTPException(status_code=400, detail="Provide either text or vector")
 787:         if not text and not vector:
 788:             raise HTTPException(status_code=400, detail="Provide text or vector")
 789:         if vector is not None:
 790:             if not isinstance(vector, list):
 791:                 raise HTTPException(status_code=400, detail="vector must be a list")
 792:             query_vector = [float(val) for val in vector]
 793:             dimensions = len(query_vector)
 794:             mode = "vector"
 795:         else:
 796:             dims = _resolve_vector_dimensions(store, collection, dimensions)
 797:             query_vector = hash_embedding(str(text or ""), dims)
 798:             dimensions = dims
 799:             mode = "text"
 800:         as_of = now_iso()
 801:         cursor_payload = {
 802:             "v": 1,
 803:             "tenant_id": TENANT_CTX.tenant_id,
 804:             "collection": collection,
 805:             "mode": mode,
 806:             "text": text if mode == "text" else None,
 807:             "vector": _encode_vector(query_vector) if mode == "vector" else None,
 808:             "dimensions": int(dimensions),
 809:             "page_size": page_size,
 810:             "offset": offset,
 811:             "as_of": as_of,
 812:             "embed": "hash_embedding:v1" if mode == "text" else "raw",
 813:         }
 814:     try:
 815:         fetch_k = offset + page_size
 816:         if fetch_k > _vector_k_max():
 817:             raise ValueError(
 818:                 f"Requested page exceeds max_k={_vector_k_max()}; set STAT_HARNESS_VECTOR_K_MAX"
 819:             )
 820:         results = store.query(collection, query_vector, k=fetch_k, as_of=as_of)
 821:         page = results[offset : offset + page_size]
 822:         next_offset = offset + page_size if len(results) > offset + page_size else None
 823:         next_cursor = None
 824:         prev_cursor = None
 825:         if cursor_payload:
 826:             base_payload = dict(cursor_payload)
 827:             if next_offset is not None:
 828:                 base_payload["offset"] = next_offset
 829:                 next_cursor = _encode_cursor(base_payload)
 830:             if offset > 0:
 831:                 base_payload["offset"] = max(0, offset - page_size)
 832:                 prev_cursor = _encode_cursor(base_payload)
 833:         return JSONResponse(
 834:             {
 835:                 "results": page,
 836:                 "offset": offset,
 837:                 "page_size": page_size,
 838:                 "next_offset": next_offset,
 839:                 "cursor": _encode_cursor(cursor_payload) if cursor_payload else None,
 840:                 "next_cursor": next_cursor,
 841:                 "prev_cursor": prev_cursor,
 842:                 "as_of": as_of,
 843:             }
 844:         )
 845:     except Exception as exc:
 846:         raise HTTPException(status_code=400, detail=str(exc))
 847: 
 848: 
 849: def _render_admin(
 850:     request: Request,
 851:     message: str | None = None,
 852:     error: str | None = None,
 853:     new_api_key: str | None = None,
 854: ) -> HTMLResponse:
 855:     tenant_id = _current_tenant_id()
 856:     tenants = pipeline.storage.list_tenants()
 857:     users = pipeline.storage.list_users_for_tenant(tenant_id)
 858:     api_keys = pipeline.storage.list_api_keys(tenant_id)
 859:     return TEMPLATES.TemplateResponse(
 860:         "admin.html",
 861:         {
 862:             "request": request,
 863:             "tenant_id": tenant_id,
 864:             "tenants": tenants,
 865:             "users": users,
 866:             "api_keys": api_keys,
 867:             "message": message,
 868:             "error": error,
 869:             "new_api_key": new_api_key,
 870:         },
 871:     )
 872: 
 873: 
 874: @app.get("/admin", response_class=HTMLResponse)
 875: async def admin_view(request: Request) -> HTMLResponse:
 876:     _require_admin(request)
 877:     return _render_admin(request)
 878: 
 879: 
 880: @app.post("/admin/users")
 881: async def admin_create_user(
 882:     request: Request,
 883:     email: str = Form(...),
 884:     password: str = Form(...),
 885:     name: str = Form(""),
 886:     role: str = Form("member"),
 887:     is_admin: bool = Form(False),
 888: ) -> HTMLResponse:
 889:     _require_admin(request)
 890:     normalized = normalize_email(email)
 891:     existing = pipeline.storage.fetch_user_by_email(normalized)
 892:     if existing:
 893:         return _render_admin(request, error="User already exists")
 894:     user_id = pipeline.storage.create_user(
 895:         normalized, hash_password(password), name or None, bool(is_admin), now_iso()
 896:     )
 897:     pipeline.storage.ensure_membership(user_id, role or "member", now_iso())
 898:     return _render_admin(request, message="User created")
 899: 
 900: 
 901: @app.post("/admin/users/{user_id}/disable")
 902: async def admin_disable_user(request: Request, user_id: int) -> HTMLResponse:
 903:     _require_admin(request)
 904:     pipeline.storage.disable_user(int(user_id))
 905:     pipeline.storage.revoke_user_sessions(int(user_id))
 906:     pipeline.storage.revoke_user_api_keys(int(user_id))
 907:     return _render_admin(request, message="User disabled")
 908: 
 909: 
 910: @app.post("/admin/tenants")
 911: async def admin_create_tenant(
 912:     request: Request,
 913:     tenant_id: str = Form(...),
 914:     name: str = Form(""),
 915: ) -> HTMLResponse:
 916:     _require_admin(request)
 917:     tenant_id = tenant_id.strip()
 918:     if not tenant_id:
 919:         return _render_admin(request, error="tenant_id required")
 920:     pipeline.storage.create_tenant(tenant_id, name or None, now_iso())
 921:     current_user = request.state.user
 922:     pipeline.storage.ensure_membership(int(current_user.user_id), "admin", now_iso(), tenant_id)
 923:     return _render_admin(request, message="Tenant created")
 924: 
 925: 
 926: @app.post("/admin/api-keys")
 927: async def admin_create_api_key(
 928:     request: Request,
 929:     user_id: int = Form(...),
 930:     name: str = Form(""),
 931: ) -> HTMLResponse:
 932:     _require_admin(request)
 933:     token = generate_api_key()
 934:     pipeline.storage.create_api_key(
 935:         int(user_id), hash_token(token), name or None, now_iso()
 936:     )
 937:     return _render_admin(request, message="API key created", new_api_key=token)
 938: 
 939: 
 940: @app.post("/admin/api-keys/{key_id}/revoke")
 941: async def admin_revoke_api_key(request: Request, key_id: int) -> HTMLResponse:
 942:     _require_admin(request)
 943:     pipeline.storage.revoke_api_key(int(key_id))
 944:     return _render_admin(request, message="API key revoked")
 945: 
 946: 
 947: def _scope_key(scope_type: str, scope_value: str) -> str | None:
 948:     if scope_type == "sha256":
 949:         if pipeline.storage.tenant_id != DEFAULT_TENANT_ID:
 950:             return _safe_sha256(f"{pipeline.storage.tenant_id}:{scope_value}")
 951:         return _safe_sha256(scope_value)
 952:     if not scope_value:
 953:         return None
 954:     if pipeline.storage.tenant_id != DEFAULT_TENANT_ID:
 955:         return scope_key(f"{pipeline.storage.tenant_id}:{scope_type}", scope_value)
 956:     return scope_key(scope_type, scope_value)
 957: 
 958: 
 959: def _load_known_issues(scope_type: str, scope_value: str) -> dict[str, object] | None:
 960:     key = _scope_key(scope_type, scope_value)
 961:     if not key:
 962:         return None
 963:     try:
 964:         stored = pipeline.storage.fetch_known_issues(scope_value, scope_type)
 965:     except sqlite3.OperationalError:
 966:         stored = None
 967:     if stored:
 968:         return {
 969:             "strict": bool(stored.get("strict", False)),
 970:             "notes": stored.get("notes") or "",
 971:             "expected_findings": stored.get("expected_findings") or [],
 972:             "natural_language": stored.get("natural_language") or [],
 973:         }
 974:     json_path, _ = _known_issues_paths(key)
 975:     if not json_path.exists():
 976:         return None
 977:     try:
 978:         legacy = json.loads(json_path.read_text(encoding="utf-8"))
 979:     except json.JSONDecodeError:
 980:         return None
 981:     if isinstance(legacy, dict):
 982:         normalized = _normalize_known_issues(legacy)
 983:         set_id = pipeline.storage.upsert_known_issue_set(
 984:             scope_value=scope_value,
 985:             scope_type=scope_type,
 986:             upload_id=None,
 987:             strict=bool(normalized.get("strict", False)),
 988:             notes=str(normalized.get("notes") or ""),
 989:             natural_language=normalized.get("natural_language") or [],
 990:         )
 991:         pipeline.storage.replace_known_issues(
 992:             set_id, normalized.get("expected_findings") or []
 993:         )
 994:         return normalized
 995:     return None
 996: 
 997: 
 998: def _normalize_known_issues(payload: dict[str, object]) -> dict[str, object]:
 999:     strict = bool(payload.get("strict", False))
1000:     notes = str(payload.get("notes") or "").strip()
1001:     expected = payload.get("expected_findings") or []
1002:     natural = payload.get("natural_language") or payload.get("nl_issues") or []
1003:     cleaned: list[dict[str, object]] = []
1004:     cleaned_nl: list[dict[str, object]] = []
1005: 
1006:     def clean_kv(data: object) -> dict[str, object]:
1007:         if not isinstance(data, dict):
1008:             return {}
1009:         out: dict[str, object] = {}
1010:         for key, value in data.items():
1011:             if key is None:
1012:                 continue
1013:             k = str(key).strip()
1014:             if not k:
1015:                 continue
1016:             out[k] = value
1017:         return out
1018: 
1019:     if isinstance(expected, list):
1020:         for entry in expected:
1021:             if not isinstance(entry, dict):
1022:                 continue
1023:             kind = str(entry.get("kind") or "").strip()
1024:             if not kind:
1025:                 continue
1026:             item: dict[str, object] = {"kind": kind}
1027:             plugin_id = str(entry.get("plugin_id") or "").strip()
1028:             if plugin_id:
1029:                 item["plugin_id"] = plugin_id
1030:             where = clean_kv(entry.get("where"))
1031:             if where:
1032:                 item["where"] = where
1033:             contains = clean_kv(entry.get("contains"))
1034:             if contains:
1035:                 item["contains"] = contains
1036:             title = str(entry.get("title") or "").strip()
1037:             if title:
1038:                 item["title"] = title
1039:             description = str(entry.get("description") or "").strip()
1040:             if description:
1041:                 item["description"] = description
1042:             if "min_count" in entry and entry.get("min_count") not in (None, ""):
1043:                 try:
1044:                     item["min_count"] = int(entry.get("min_count"))
1045:                 except (TypeError, ValueError):
1046:                     pass
1047:             if "max_count" in entry and entry.get("max_count") not in (None, ""):
1048:                 try:
1049:                     item["max_count"] = int(entry.get("max_count"))
1050:                 except (TypeError, ValueError):
1051:                     pass
1052:             cleaned.append(item)
1053: 
1054:     if isinstance(natural, list):
1055:         for entry in natural:
1056:             if isinstance(entry, str):
1057:                 text = entry.strip()
1058:                 if text:
1059:                     cleaned_nl.append({"text": text})
1060:                 continue
1061:             if not isinstance(entry, dict):
1062:                 continue
1063:             text = str(entry.get("text") or "").strip()
1064:             if not text:
1065:                 continue
1066:             item = {"text": text}
1067:             title = str(entry.get("title") or "").strip()
1068:             if title:
1069:                 item["title"] = title
1070:             process_hint = str(entry.get("process_hint") or "").strip()
1071:             if process_hint:
1072:                 item["process_hint"] = process_hint
1073:             cleaned_nl.append(item)
1074: 
1075:     return {
1076:         "strict": strict,
1077:         "notes": notes,
1078:         "expected_findings": cleaned,
1079:         "natural_language": cleaned_nl,
1080:     }
1081: 
1082: 
1083: def _save_known_issues(
1084:     scope_type: str,
1085:     scope_value: str,
1086:     upload_id: str | None,
1087:     payload: dict[str, object],
1088: ) -> dict[str, object]:
1089:     key = _scope_key(scope_type, scope_value)
1090:     if not key:
1091:         raise ValueError("Invalid known-issues scope")
1092:     normalized = _normalize_known_issues(payload)
1093:     compiled, warnings = compile_known_issues(
1094:         normalized.get("natural_language") or []
1095:     )
1096:     combined = list(normalized.get("expected_findings") or [])
1097:     combined.extend(compiled)
1098: 
1099:     def _dedupe(items: list[dict[str, object]]) -> list[dict[str, object]]:
1100:         seen = set()
1101:         out = []
1102:         for item in items:
1103:             key = (
1104:                 item.get("plugin_id"),
1105:                 item.get("kind"),
1106:                 json_dumps(item.get("where")),
1107:                 json_dumps(item.get("contains")),
1108:                 item.get("title"),
1109:                 item.get("description"),
1110:             )
1111:             if key in seen:
1112:                 continue
1113:             seen.add(key)
1114:             out.append(item)
1115:         return out
1116: 
1117:     combined = _dedupe(combined)
1118:     set_id = pipeline.storage.upsert_known_issue_set(
1119:         scope_value=scope_value,
1120:         scope_type=scope_type,
1121:         upload_id=upload_id,
1122:         strict=bool(normalized.get("strict", False)),
1123:         notes=str(normalized.get("notes") or ""),
1124:         natural_language=normalized.get("natural_language") or [],
1125:     )
1126:     pipeline.storage.replace_known_issues(
1127:         set_id, combined
1128:     )
1129:     json_path, yaml_path = _known_issues_paths(key)
1130:     payload_out = dict(normalized)
1131:     payload_out["expected_findings"] = combined
1132:     payload_out["compiled_count"] = len(compiled)
1133:     payload_out["compile_warnings"] = warnings
1134:     json_path.write_text(json_dumps(payload_out), encoding="utf-8")
1135:     yaml_payload = {
1136:         "strict": payload_out.get("strict", False),
1137:         "notes": payload_out.get("notes", ""),
1138:         "expected_findings": payload_out.get("expected_findings") or [],
1139:     }
1140:     yaml_path.write_text(yaml.safe_dump(yaml_payload, sort_keys=False), encoding="utf-8")
1141:     return payload_out
1142: 
1143: 
1144: def _known_issues_yaml(scope_type: str, scope_value: str) -> str | None:
1145:     key = _scope_key(scope_type, scope_value)
1146:     if not key:
1147:         return None
1148:     try:
1149:         stored = pipeline.storage.fetch_known_issues(scope_value, scope_type)
1150:     except sqlite3.OperationalError:
1151:         stored = None
1152:     if stored:
1153:         payload = {
1154:             "strict": bool(stored.get("strict", False)),
1155:             "notes": stored.get("notes") or "",
1156:             "expected_findings": stored.get("expected_findings") or [],
1157:         }
1158:         return yaml.safe_dump(payload, sort_keys=False)
1159:     _, yaml_path = _known_issues_paths(key)
1160:     if yaml_path.exists():
1161:         return yaml_path.read_text(encoding="utf-8")
1162:     return None
1163: 
1164: 
1165: def _safe_sha256(value: str) -> str | None:
1166:     if re.fullmatch(r"[a-f0-9]{64}", value or ""):
1167:         return value
1168:     return None
1169: 
1170: 
1171: def _slugify(value: str) -> str:
1172:     slug = re.sub(r"[^a-z0-9]+", "-", value.lower()).strip("-")
1173:     return slug or uuid.uuid4().hex[:8]
1174: 
1175: 
1176: def _unique_project_id(name: str) -> str:
1177:     base = _slugify(name)
1178:     candidate = base
1179:     for idx in range(1, 100):
1180:         if not pipeline.storage.fetch_project(candidate):
1181:             return candidate
1182:         candidate = f"{base}-{idx}"
1183:     return f"{base}-{uuid.uuid4().hex[:4]}"
1184: 
1185: 
1186: def _bool_from_form(value: str | None, default: bool = False) -> bool:
1187:     if value is None:
1188:         return default
1189:     lowered = str(value).strip().lower()
1190:     if lowered in {"1", "true", "yes", "on"}:
1191:         return True
1192:     if lowered in {"0", "false", "no", "off", ""}:
1193:         return False
1194:     return default
1195: 
1196: 
1197: def _parse_int(value: str | None, default: int) -> int:
1198:     if value is None or value == "":
1199:         return default
1200:     return int(value)
1201: 
1202: 
1203: def _parse_float(value: str | None, default: float) -> float:
1204:     if value is None or value == "":
1205:         return default
1206:     return float(value)
1207: 
1208: 
1209: def _format_hours(value: float | int | None) -> str:
1210:     if value is None:
1211:         return "n/a"
1212:     return f"{float(value):.2f}h"
1213: 
1214: 
1215: def _format_ratio(value: float | int | None) -> str:
1216:     if value is None:
1217:         return "n/a"
1218:     return f"{float(value):.2f}x"
1219: 
1220: 
1221: def _merge_settings(base: dict[str, object], override: dict[str, object]) -> dict[str, object]:
1222:     merged = dict(base)
1223:     for key, value in override.items():
1224:         if isinstance(value, dict) and isinstance(merged.get(key), dict):
1225:             merged[key] = {**merged[key], **value}
1226:         else:
1227:             merged[key] = value
1228:     return merged
1229: 
1230: 
1231: def _run_meta_from_plugins(plugin_ids: list[str]) -> object:
1232:     if not plugin_ids:
1233:         return "auto"
1234:     if len(plugin_ids) == 1 and plugin_ids[0] == "all":
1235:         return "all"
1236:     return plugin_ids
1237: 
1238: 
1239: def _expected_plugins_for_run(run_row: dict[str, object]) -> list[str]:
1240:     specs = pipeline.manager.discover()
1241:     spec_map = {spec.plugin_id: spec for spec in specs}
1242:     analysis_plugins = [spec.plugin_id for spec in specs if spec.type == "analysis"]
1243:     profile_plugins = [spec.plugin_id for spec in specs if spec.type == "profile"]
1244:     planner_plugins = [spec.plugin_id for spec in specs if spec.type == "planner"]
1245:     transform_plugins = [spec.plugin_id for spec in specs if spec.type == "transform"]
1246:     report_plugins = [spec.plugin_id for spec in specs if spec.type == "report"]
1247:     ingest_plugins = [spec.plugin_id for spec in specs if spec.type == "ingest"]
1248: 
1249:     settings = {}
1250:     raw_settings = run_row.get("settings_json")
1251:     if raw_settings:
1252:         try:
1253:             settings = json.loads(raw_settings)
1254:         except json.JSONDecodeError:
1255:             settings = {}
1256:     meta = {}
1257:     if isinstance(settings, dict):
1258:         meta = settings.get("__run_meta", {}) if isinstance(settings.get("__run_meta"), dict) else {}
1259:     meta_plugins = meta.get("plugins")
1260: 
1261:     selected: list[str] = []
1262:     if isinstance(meta_plugins, list):
1263:         selected = [str(pid) for pid in meta_plugins if pid]
1264:     elif isinstance(meta_plugins, str):
1265:         if meta_plugins == "all":
1266:             selected = list(analysis_plugins)
1267:         elif meta_plugins == "auto":
1268:             selected = list(profile_plugins) + list(planner_plugins) + list(analysis_plugins)
1269:         else:
1270:             selected = [meta_plugins]
1271:     else:
1272:         selected = list(analysis_plugins)
1273: 
1274:     expected: list[str] = []
1275:     input_filename = str(run_row.get("input_filename") or "")
1276:     if not input_filename.startswith("db://"):
1277:         if ingest_plugins:
1278:             expected.append(ingest_plugins[0])
1279:         else:
1280:             expected.append("ingest_tabular")
1281: 
1282:     for pid in selected:
1283:         if pid in spec_map and pid not in expected:
1284:             expected.append(pid)
1285:         elif pid in {"all", "auto"}:
1286:             continue
1287:         elif pid not in expected:
1288:             expected.append(pid)
1289: 
1290:     for pid in transform_plugins:
1291:         if pid in selected and pid not in expected:
1292:             expected.append(pid)
1293: 
1294:     for pid in report_plugins or ["report_bundle"]:
1295:         if pid not in expected:
1296:             expected.append(pid)
1297: 
1298:     return expected
1299: 
1300: 
1301: def _run_label(project: dict | None, run_index: int) -> str:
1302:     project_name = (project or {}).get("name") or (project or {}).get("project_id") or "Project"
1303:     return f"{project_name} Run {run_index}"
1304: 
1305: 
1306: def _annotate_runs(
1307:     runs: list[dict[str, object]], project: dict | None
1308: ) -> list[dict[str, object]]:
1309:     if not runs:
1310:         return runs
1311:     sorted_runs = sorted(
1312:         runs,
1313:         key=lambda row: row.get("created_at") or "",
1314:     )
1315:     index_by_id = {
1316:         run.get("run_id"): idx + 1
1317:         for idx, run in enumerate(sorted_runs)
1318:     }
1319:     annotated = []
1320:     for run in runs:
1321:         run_id = run.get("run_id")
1322:         label = _run_label(project, index_by_id.get(run_id, 0) or 0)
1323:         copy = dict(run)
1324:         copy["label"] = label
1325:         copy["evaluation"] = _load_evaluation_summary(run_id)
1326:         annotated.append(copy)
1327:     return annotated
1328: 
1329: 
1330: def _load_evaluation_summary(run_id: str | None) -> dict[str, object] | None:
1331:     if not run_id:
1332:         return None
1333:     run_dir = APPDATA_DIR / "runs" / str(run_id)
1334:     eval_path = run_dir / "evaluation.json"
1335:     if not eval_path.exists():
1336:         return None
1337:     try:
1338:         payload = json.loads(eval_path.read_text(encoding="utf-8"))
1339:     except json.JSONDecodeError:
1340:         return None
1341:     if not isinstance(payload, dict):
1342:         return None
1343:     return {
1344:         "result": payload.get("result"),
1345:         "ok": payload.get("ok"),
1346:         "evaluated_at": payload.get("evaluated_at"),
1347:         "messages": payload.get("messages") if isinstance(payload.get("messages"), list) else [],
1348:     }
1349: 
1350: 
1351: def _build_known_issue_results(
1352:     report: dict, known_payload: dict[str, object] | None
1353: ) -> dict[str, object] | None:
1354:     if not known_payload or not isinstance(known_payload, dict):
1355:         return None
1356:     expected = known_payload.get("expected_findings") or []
1357:     if not isinstance(expected, list):
1358:         expected = []
1359:     strict = bool(known_payload.get("strict", True))
1360:     results: list[dict[str, object]] = []
1361:     expected_matchers: list[dict[str, object]] = []
1362:     for entry in expected:
1363:         if not isinstance(entry, dict):
1364:             continue
1365:         kind = entry.get("kind")
1366:         if not kind:
1367:             continue
1368:         plugin_id = entry.get("plugin_id") or None
1369:         where = entry.get("where") or {}
1370:         contains = entry.get("contains") or {}
1371:         try:
1372:             min_count = int(entry.get("min_count", 1) or 1)
1373:         except (TypeError, ValueError):
1374:             min_count = 1
1375:         max_count = entry.get("max_count")
1376:         try:
1377:             max_count_val = int(max_count) if max_count is not None else None
1378:         except (TypeError, ValueError):
1379:             max_count_val = None
1380:         candidates = eval_core._collect_findings_for_plugin(
1381:             report, plugin_id, str(kind)
1382:         )
1383:         matches = [
1384:             item
1385:             for item in candidates
1386:             if eval_core._matches_expected(item, where, contains)
1387:         ]
1388:         fallbacks = sorted(
1389:             {
1390:                 str(item.get("baseline_match_fallback"))
1391:                 for item in matches
1392:                 if item.get("baseline_match_fallback")
1393:             }
1394:         )
1395:         baseline_modes = sorted(
1396:             {
1397:                 str(item.get("baseline_match_mode"))
1398:                 for item in matches
1399:                 if item.get("baseline_match_mode")
1400:             }
1401:         )
1402:         baseline_sources = sorted(
1403:             {
1404:                 str(item.get("baseline_host_source"))
1405:                 for item in matches
1406:                 if item.get("baseline_host_source")
1407:             }
1408:         )
1409:         baseline_hosts = sorted(
1410:             {
1411:                 str(item.get("baseline_host_count"))
1412:                 for item in matches
1413:                 if item.get("baseline_host_count") is not None
1414:             }
1415:         )
1416:         status = "pass"
1417:         if len(matches) < min_count:
1418:             status = "fail"
1419:         if max_count_val is not None and len(matches) > max_count_val:
1420:             status = "fail"
1421:         results.append(
1422:             {
1423:                 "title": entry.get("title") or str(kind),
1424:                 "description": entry.get("description") or "",
1425:                 "plugin_id": plugin_id,
1426:                 "kind": kind,
1427:                 "min_count": min_count,
1428:                 "max_count": max_count_val,
1429:                 "matched": len(matches),
1430:                 "status": status,
1431:                 "baseline_match_fallbacks": fallbacks,
1432:                 "baseline_match_modes": baseline_modes,
1433:                 "baseline_host_sources": baseline_sources,
1434:                 "baseline_host_counts": baseline_hosts,
1435:             }
1436:         )
1437:         expected_matchers.append(
1438:             {
1439:                 "plugin_id": plugin_id,
1440:                 "kind": str(kind),
1441:                 "where": where,
1442:                 "contains": contains,
1443:             }
1444:         )
1445: 
1446:     unexpected: list[dict[str, object]] = []
1447:     if strict and expected_matchers:
1448:         seen: set[tuple[str, str]] = set()
1449:         for pid, plugin in report.get("plugins", {}).items():
1450:             for item in plugin.get("findings", []):
1451:                 kind = item.get("kind")
1452:                 if not kind:
1453:                     continue
1454:                 if not any(matcher["kind"] == kind for matcher in expected_matchers):
1455:                     continue
1456:                 matched = False
1457:                 for matcher in expected_matchers:
1458:                     if matcher["kind"] != kind:
1459:                         continue
1460:                     if matcher["plugin_id"] and matcher["plugin_id"] != pid:
1461:                         continue
1462:                     if eval_core._matches_expected(
1463:                         item, matcher["where"], matcher["contains"]
1464:                     ):
1465:                         matched = True
1466:                         break
1467:                 if not matched:
1468:                     key = (str(pid), str(kind))
1469:                     if key not in seen:
1470:                         seen.add(key)
1471:                         unexpected.append({"plugin_id": pid, "kind": kind})
1472: 
1473:     ok = all(item.get("status") == "pass" for item in results) and (
1474:         not strict or not unexpected
1475:     )
1476:     return {
1477:         "strict": strict,
1478:         "results": results,
1479:         "unexpected": unexpected,
1480:         "ok": ok,
1481:     }
1482: 
1483: 
1484: def _build_insights(report: dict) -> dict[str, list[dict[str, object]]]:
1485:     insights: dict[str, list[dict[str, object]]] = {}
1486:     for plugin_id, plugin in report.get("plugins", {}).items():
1487:         entries: list[dict[str, object]] = []
1488:         for finding in plugin.get("findings", []):
1489:             kind = finding.get("kind") or "finding"
1490:             measurement = finding.get("measurement_type", "measured")
1491:             title = f"{kind}"
1492:             bullets: list[str] = []
1493:             if kind == "close_cycle_contention":
1494:                 process = finding.get("process") or "process"
1495:                 title = f"Close-cycle contention: {process}"
1496:                 bullets = [
1497:                     f"Close vs open median duration: {finding.get('median_duration_close')}s vs {finding.get('median_duration_open')}s ({_format_ratio(finding.get('slowdown_ratio'))})",
1498:                     f"Close-cycle runs: {finding.get('close_count')} | Open-cycle runs: {finding.get('open_count')}",
1499:                     f"Correlation (close volume vs median duration): {finding.get('correlation')}",
1500:                     f"Estimated improvement if removed: {float(finding.get('estimated_improvement_pct') or 0) * 100:.1f}%",
1501:                 ]
1502:             elif kind == "eligible_wait_process_stats":
1503:                 process = finding.get("process") or "process"
1504:                 title = f"Eligible wait (standalone): {process}"
1505:                 bullets = [
1506:                     f"Total runs: {finding.get('runs_total')} (close: {finding.get('runs_close')}, open: {finding.get('runs_open')})",
1507:                     f"Eligible-wait hours: {_format_hours(finding.get('eligible_wait_hours_total'))} (close: {_format_hours(finding.get('eligible_wait_hours_close'))})",
1508:                     f">threshold eligible-wait: {_format_hours(finding.get('eligible_wait_gt_hours_total'))} (close: {_format_hours(finding.get('eligible_wait_gt_hours_close'))})",
1509:                     f"Pre-eligible wait: {_format_hours(finding.get('wait_pre_hours_total'))}",
1510:                 ]
1511:             elif kind == "eligible_wait_impact":
1512:                 process = finding.get("process") or "process"
1513:                 title = f"Eligible-wait impact without {process}"
1514:                 bullets = [
1515:                     f"Total eligible-wait: {_format_hours(finding.get('eligible_wait_hours_total'))} → {_format_hours(finding.get('eligible_wait_hours_without_target'))}",
1516:                     f"Close-window eligible-wait: {_format_hours(finding.get('eligible_wait_hours_close_total'))} → {_format_hours(finding.get('eligible_wait_hours_close_without_target'))}",
1517:                     f"Open-window eligible-wait: {_format_hours(finding.get('eligible_wait_hours_open_total'))} → {_format_hours(finding.get('eligible_wait_hours_open_without_target'))}",
1518:                     f">threshold eligible-wait: {_format_hours(finding.get('eligible_wait_gt_hours_total'))} → {_format_hours(finding.get('eligible_wait_gt_hours_without_target'))}",
1519:                     f"Close-window >threshold: {_format_hours(finding.get('eligible_wait_gt_hours_close_total'))} → {_format_hours(finding.get('eligible_wait_gt_hours_close_without_target'))}",
1520:                     f"Open-window >threshold: {_format_hours(finding.get('eligible_wait_gt_hours_open_total'))} → {_format_hours(finding.get('eligible_wait_gt_hours_open_without_target'))}",
1521:                 ]
1522:             elif kind == "capacity_scale_model":
1523:                 process = finding.get("process") or "process"
1524:                 title = f"Modeled capacity scaling: {process}"
1525:                 bullets = [
1526:                     f">threshold eligible-wait (post-removal): {_format_hours(finding.get('eligible_wait_gt_hours_without_target'))}",
1527:                     f"Scale factor: {finding.get('scale_factor')} → modeled {_format_hours(finding.get('eligible_wait_gt_hours_modeled'))}",
1528:                     f"Close-window modeled: {_format_hours(finding.get('eligible_wait_gt_hours_close_without_target'))} → {_format_hours(finding.get('eligible_wait_gt_hours_close_modeled'))}",
1529:                     f"Open-window modeled: {_format_hours(finding.get('eligible_wait_gt_hours_open_without_target'))} → {_format_hours(finding.get('eligible_wait_gt_hours_open_modeled'))}",
1530:                 ]
1531:             elif kind == "sequence_classification":
1532:                 process = finding.get("process") or "process"
1533:                 title = f"Sequence-linked share: {process}"
1534:                 bullets = [
1535:                     f"Sequence runs: {finding.get('sequence_runs')} ({float(finding.get('sequence_ratio') or 0) * 100:.1f}%)",
1536:                     f"Standalone runs: {finding.get('standalone_runs')}",
1537:                 ]
1538:             elif kind == "process_variant":
1539:                 title = "Process sequence variant"
1540:                 bullets = [
1541:                     f"Variant length: {len(finding.get('variant') or [])}",
1542:                     f"Count: {finding.get('count')} (fraction: {float(finding.get('fraction') or 0) * 100:.1f}%)",
1543:                 ]
1544:             elif kind == "rare_variant":
1545:                 title = "Rare process variant"
1546:                 bullets = [
1547:                     f"Count: {finding.get('count')}",
1548:                 ]
1549: 
1550:             entries.append(
1551:                 {
1552:                     "title": title,
1553:                     "kind": kind,
1554:                     "measurement_type": measurement,
1555:                     "bullets": bullets,
1556:                     "evidence": finding.get("evidence") or {},
1557:                 }
1558:             )
1559:         insights[plugin_id] = entries
1560:     return insights
1561: 
1562: 
1563: def _normalize_settings_from_form(
1564:     template_name: str,
1565:     lowercase: str | None,
1566:     strip: str | None,
1567:     collapse_whitespace: str | None,
1568:     numeric_coercion: str | None,
1569:     numeric_threshold: str | None,
1570:     exclude_name_patterns: str,
1571:     chunk_size: str | None,
1572:     sample_rows: str | None,
1573: ) -> dict[str, object]:
1574:     patterns = [
1575:         token.strip()
1576:         for token in (exclude_name_patterns or "").split(",")
1577:         if token.strip()
1578:     ]
1579:     return {
1580:         "template_name": template_name.strip(),
1581:         "lowercase": _bool_from_form(lowercase, True),
1582:         "strip": _bool_from_form(strip, True),
1583:         "collapse_whitespace": _bool_from_form(collapse_whitespace, True),
1584:         "numeric_coercion": _bool_from_form(numeric_coercion, True),
1585:         "numeric_threshold": _parse_float(numeric_threshold, 0.98),
1586:         "exclude_name_patterns": patterns,
1587:         "chunk_size": _parse_int(chunk_size, 1000),
1588:         "sample_rows": _parse_int(sample_rows, 500),
1589:     }
1590: 
1591: 
1592: def _auto_seed(value: str | None, fallback: int = 0) -> int:
1593:     if not value:
1594:         return fallback
1595:     return int(stable_hash(value))
1596: 
1597: 
1598: def _load_evaluation_result(run_dir: Path) -> dict[str, object] | None:
1599:     eval_path = run_dir / "evaluation.json"
1600:     if not eval_path.exists():
1601:         return None
1602:     try:
1603:         data = json.loads(eval_path.read_text(encoding="utf-8"))
1604:     except json.JSONDecodeError:
1605:         return None
1606:     if not isinstance(data, dict):
1607:         return None
1608:     return data
1609: 
1610: 
1611: @app.get("/", response_class=HTMLResponse)
1612: async def index(request: Request) -> HTMLResponse:
1613:     projects = pipeline.storage.list_projects()
1614:     return TEMPLATES.TemplateResponse(
1615:         "index.html", {"request": request, "projects": projects}
1616:     )
1617: 
1618: 
1619: @app.get("/known-issues", response_class=HTMLResponse)
1620: async def known_issues(
1621:     request: Request, upload_id: str = "", erp_type: str = "unknown"
1622: ) -> HTMLResponse:
1623:     uploads = pipeline.storage.list_uploads()
1624:     selected_id = upload_id or (uploads[0]["upload_id"] if uploads else "")
1625:     upload_row = pipeline.storage.fetch_upload(selected_id) if selected_id else None
1626:     known_payload = None
1627:     known_yaml = ""
1628:     sha256 = None
1629:     scope_type = "erp_type"
1630:     scope_value = (erp_type or "unknown").strip() or "unknown"
1631:     if upload_row and upload_row.get("sha256"):
1632:         sha256 = upload_row["sha256"]
1633:         known_payload = _load_known_issues(scope_type, scope_value)
1634:         known_yaml = _known_issues_yaml(scope_type, scope_value)
1635:     if known_payload is None:
1636:         known_payload = {
1637:             "strict": True,
1638:             "notes": "",
1639:             "expected_findings": [],
1640:             "natural_language": [],
1641:         }
1642:         known_yaml = ""
1643:     specs = pipeline.manager.discover()
1644:     plugin_ids = [spec.plugin_id for spec in specs if spec.type == "analysis"]
1645:     return TEMPLATES.TemplateResponse(
1646:         "known_issues.html",
1647:         {
1648:             "request": request,
1649:             "uploads": uploads,
1650:             "selected_upload": upload_row,
1651:             "selected_upload_id": selected_id,
1652:             "known_issues": known_payload,
1653:             "known_issues_json": json_dumps(known_payload),
1654:             "plugin_ids": plugin_ids,
1655:             "sha256": sha256 or "",
1656:             "known_issues_yaml": known_yaml,
1657:             "erp_type": scope_value,
1658:         },
1659:     )
1660: 
1661: 
1662: @app.get("/wizard", response_class=HTMLResponse)
1663: async def wizard(request: Request) -> HTMLResponse:
1664:     uploads = pipeline.storage.list_uploads()
1665:     specs = pipeline.manager.discover()
1666:     plugin_ids = [spec.plugin_id for spec in specs if spec.type == "analysis"]
1667:     return TEMPLATES.TemplateResponse(
1668:         "wizard.html",
1669:         {
1670:             "request": request,
1671:             "uploads": uploads,
1672:             "plugin_ids": plugin_ids,
1673:             "erp_type": "unknown",
1674:         },
1675:     )
1676: 
1677: 
1678: @app.get("/plugins", response_class=HTMLResponse)
1679: async def plugins(request: Request) -> HTMLResponse:
1680:     specs = pipeline.manager.discover()
1681:     return TEMPLATES.TemplateResponse(
1682:         "plugins.html", {"request": request, "plugins": specs}
1683:     )
1684: 
1685: 
1686: @app.get("/projects", response_class=HTMLResponse)
1687: async def projects_view(request: Request) -> HTMLResponse:
1688:     projects = pipeline.storage.list_projects()
1689:     return TEMPLATES.TemplateResponse(
1690:         "projects.html", {"request": request, "projects": projects}
1691:     )
1692: 
1693: 
1694: @app.post("/projects", response_class=HTMLResponse)
1695: async def create_project(
1696:     request: Request, name: str = Form(...), erp_type: str = Form("unknown")
1697: ) -> HTMLResponse:
1698:     clean_name = (name or "").strip()
1699:     if not clean_name:
1700:         raise HTTPException(status_code=400, detail="Project name required")
1701:     project_id = _unique_project_id(clean_name)
1702:     pipeline.storage.ensure_project(project_id, project_id, now_iso())
1703:     pipeline.storage.update_project_name(project_id, clean_name)
1704:     if erp_type:
1705:         pipeline.storage.update_project_erp_type(project_id, erp_type.strip() or "unknown")
1706:     return TEMPLATES.TemplateResponse(
1707:         "project.html",
1708:         {
1709:             "request": request,
1710:             "project_id": project_id,
1711:             "datasets": pipeline.storage.list_dataset_versions_by_project(project_id),
1712:             "project": pipeline.storage.fetch_project(project_id),
1713:             "runs": _annotate_runs(
1714:                 pipeline.storage.list_runs_by_project(project_id),
1715:                 pipeline.storage.fetch_project(project_id),
1716:             ),
1717:             "known_issues": _load_known_issues(
1718:                 "erp_type", (erp_type or "unknown").strip() or "unknown"
1719:             )
1720:             or {
1721:                 "strict": True,
1722:                 "notes": "",
1723:                 "expected_findings": [],
1724:                 "natural_language": [],
1725:             },
1726:             "message": "Project created.",
1727:         },
1728:     )
1729: 
1730: 
1731: @app.get("/projects/{project_id}", response_class=HTMLResponse)
1732: async def project_detail(request: Request, project_id: str) -> HTMLResponse:
1733:     datasets = pipeline.storage.list_dataset_versions_by_project(project_id)
1734:     project = pipeline.storage.fetch_project(project_id)
1735:     runs = _annotate_runs(pipeline.storage.list_runs_by_project(project_id), project)
1736:     known = {
1737:         "strict": True,
1738:         "notes": "",
1739:         "expected_findings": [],
1740:         "natural_language": [],
1741:     }
1742:     if project and project.get("erp_type"):
1743:         known = (
1744:             _load_known_issues("erp_type", project.get("erp_type") or "unknown")
1745:             or known
1746:         )
1747:     plugin_settings = pipeline.storage.fetch_project_plugin_settings(project_id)
1748:     return TEMPLATES.TemplateResponse(
1749:         "project.html",
1750:         {
1751:             "request": request,
1752:             "project_id": project_id,
1753:             "datasets": datasets,
1754:             "project": project,
1755:             "runs": runs,
1756:             "known_issues": known,
1757:             "plugin_settings": plugin_settings,
1758:         },
1759:     )
1760: 
1761: 
1762: @app.post("/projects/{project_id}/erp", response_class=HTMLResponse)
1763: async def update_project_erp(
1764:     request: Request, project_id: str, erp_type: str = Form("")
1765: ) -> HTMLResponse:
1766:     value = (erp_type or "unknown").strip() or "unknown"
1767:     pipeline.storage.update_project_erp_type(project_id, value)
1768:     datasets = pipeline.storage.list_dataset_versions_by_project(project_id)
1769:     project = pipeline.storage.fetch_project(project_id)
1770:     runs = _annotate_runs(pipeline.storage.list_runs_by_project(project_id), project)
1771:     known = _load_known_issues("erp_type", value) or {
1772:         "strict": True,
1773:         "notes": "",
1774:         "expected_findings": [],
1775:         "natural_language": [],
1776:     }
1777:     return TEMPLATES.TemplateResponse(
1778:         "project.html",
1779:         {
1780:             "request": request,
1781:             "project_id": project_id,
1782:             "datasets": datasets,
1783:             "project": project,
1784:             "runs": runs,
1785:             "known_issues": known,
1786:             "message": "ERP type updated.",
1787:         },
1788:     )
1789: 
1790: 
1791: @app.get("/projects/{project_id}/roles", response_class=HTMLResponse)
1792: async def project_roles(
1793:     request: Request, project_id: str, dataset_version_id: str = ""
1794: ) -> HTMLResponse:
1795:     datasets = pipeline.storage.list_dataset_versions_by_project(project_id)
1796:     selected = dataset_version_id or (datasets[0]["dataset_version_id"] if datasets else "")
1797:     columns = (
1798:         pipeline.storage.fetch_dataset_columns(selected) if selected else []
1799:     )
1800:     overrides = pipeline.storage.fetch_project_role_overrides(project_id)
1801:     return TEMPLATES.TemplateResponse(
1802:         "project_roles.html",
1803:         {
1804:             "request": request,
1805:             "project_id": project_id,
1806:             "datasets": datasets,
1807:             "dataset_version_id": selected,
1808:             "columns": columns,
1809:             "role_keys": ROLE_KEYS,
1810:             "overrides": overrides,
1811:         },
1812:     )
1813: 
1814: 
1815: @app.get("/projects/{project_id}/settings", response_class=HTMLResponse)
1816: async def project_settings_view(request: Request, project_id: str) -> HTMLResponse:
1817:     project = pipeline.storage.fetch_project(project_id)
1818:     if not project:
1819:         raise HTTPException(status_code=404, detail="Project not found")
1820:     specs = pipeline.manager.discover()
1821:     plugin_specs = [spec for spec in specs if spec.type == "analysis"]
1822:     settings = pipeline.storage.fetch_project_plugin_settings(project_id)
1823:     return TEMPLATES.TemplateResponse(
1824:         "project_settings.html",
1825:         {
1826:             "request": request,
1827:             "project_id": project_id,
1828:             "project": project,
1829:             "plugin_specs": plugin_specs,
1830:             "plugin_settings": settings,
1831:         },
1832:     )
1833: 
1834: 
1835: @app.post("/projects/{project_id}/settings", response_class=HTMLResponse)
1836: async def project_settings_save(request: Request, project_id: str) -> HTMLResponse:
1837:     project = pipeline.storage.fetch_project(project_id)
1838:     if not project:
1839:         raise HTTPException(status_code=404, detail="Project not found")
1840:     form = await request.form()
1841:     specs = pipeline.manager.discover()
1842:     plugin_specs = [spec for spec in specs if spec.type == "analysis"]
1843:     updated: dict[str, dict[str, object]] = {}
1844:     for spec in plugin_specs:
1845:         field_name = f"settings__{spec.plugin_id}"
1846:         raw = str(form.get(field_name) or "").strip()
1847:         if not raw:
1848:             continue
1849:         try:
1850:             payload = json.loads(raw)
1851:         except json.JSONDecodeError as exc:
1852:             raise HTTPException(
1853:                 status_code=400,
1854:                 detail=f"Invalid JSON for {spec.plugin_id}: {exc}",
1855:             ) from exc
1856:         if not isinstance(payload, dict):
1857:             raise HTTPException(
1858:                 status_code=400,
1859:                 detail=f"Settings for {spec.plugin_id} must be a JSON object.",
1860:             )
1861:         updated[spec.plugin_id] = payload
1862: 
1863:     pipeline.storage.replace_project_plugin_settings(project_id, updated)
1864:     return TEMPLATES.TemplateResponse(
1865:         "project_settings.html",
1866:         {
1867:             "request": request,
1868:             "project_id": project_id,
1869:             "project": project,
1870:             "plugin_specs": plugin_specs,
1871:             "plugin_settings": updated,
1872:             "message": "Settings saved.",
1873:         },
1874:     )
1875: 
1876: 
1877: @app.post("/projects/{project_id}/roles", response_class=HTMLResponse)
1878: async def save_project_roles(request: Request, project_id: str) -> HTMLResponse:
1879:     form = await request.form()
1880:     overrides: dict[str, str] = {}
1881:     for role in ROLE_KEYS:
1882:         value = str(form.get(role) or "").strip()
1883:         if value:
1884:             overrides[role] = value
1885:     pipeline.storage.replace_project_role_overrides(project_id, overrides)
1886:     dataset_version_id = str(form.get("dataset_version_id") or "")
1887:     return HTMLResponse(
1888:         "Role overrides saved. "
1889:         f"<a href='/projects/{project_id}/roles?dataset_version_id={dataset_version_id}'>Back</a>"
1890:     )
1891: 
1892: 
1893: @app.get("/projects/{project_id}/known-issues", response_class=HTMLResponse)
1894: async def project_known_issues(request: Request, project_id: str) -> HTMLResponse:
1895:     project = pipeline.storage.fetch_project(project_id)
1896:     if not project:
1897:         raise HTTPException(status_code=404, detail="Project not found")
1898:     erp_type = (project.get("erp_type") or "unknown").strip() or "unknown"
1899:     known_payload = _load_known_issues("erp_type", erp_type)
1900:     if known_payload is None:
1901:         known_payload = {
1902:             "strict": True,
1903:             "notes": "",
1904:             "expected_findings": [],
1905:             "natural_language": [],
1906:         }
1907:     specs = pipeline.manager.discover()
1908:     plugin_ids = [spec.plugin_id for spec in specs if spec.type == "analysis"]
1909:     return TEMPLATES.TemplateResponse(
1910:         "project_known_issues.html",
1911:         {
1912:             "request": request,
1913:             "project_id": project_id,
1914:             "project": project,
1915:             "known_issues": known_payload,
1916:             "plugin_ids": plugin_ids,
1917:         },
1918:     )
1919: 
1920: 
1921: @app.get("/templates", response_class=HTMLResponse)
1922: async def templates_view(request: Request) -> HTMLResponse:
1923:     templates = pipeline.storage.list_templates()
1924:     return TEMPLATES.TemplateResponse(
1925:         "templates.html", {"request": request, "templates": templates}
1926:     )
1927: 
1928: 
1929: @app.get("/templates/{template_id}", response_class=HTMLResponse)
1930: async def template_detail(request: Request, template_id: int) -> HTMLResponse:
1931:     template = pipeline.storage.fetch_template(template_id)
1932:     if not template:
1933:         raise HTTPException(status_code=404, detail="Template not found")
1934:     fields = pipeline.storage.fetch_template_fields(template_id)
1935:     return TEMPLATES.TemplateResponse(
1936:         "template.html",
1937:         {"request": request, "template": template, "fields": fields},
1938:     )
1939: 
1940: 
1941: @app.get("/raw-formats", response_class=HTMLResponse)
1942: async def raw_formats_view(request: Request) -> HTMLResponse:
1943:     formats = pipeline.storage.list_raw_formats()
1944:     return TEMPLATES.TemplateResponse(
1945:         "raw_formats.html", {"request": request, "formats": formats}
1946:     )
1947: 
1948: 
1949: @app.get("/raw-formats/{format_id}", response_class=HTMLResponse)
1950: async def raw_format_detail(request: Request, format_id: int) -> HTMLResponse:
1951:     notes = pipeline.storage.list_raw_format_notes(format_id)
1952:     mappings = pipeline.storage.list_raw_format_mappings(format_id)
1953:     templates = pipeline.storage.list_templates()
1954:     return TEMPLATES.TemplateResponse(
1955:         "raw_format.html",
1956:         {
1957:             "request": request,
1958:             "format_id": format_id,
1959:             "notes": notes,
1960:             "mappings": mappings,
1961:             "templates": templates,
1962:         },
1963:     )
1964: 
1965: 
1966: @app.post("/api/upload")
1967: async def upload(file: UploadFile = File(...)) -> JSONResponse:
1968:     upload_id = uuid.uuid4().hex
1969:     upload_dir = APPDATA_DIR / "uploads" / upload_id
1970:     upload_dir.mkdir(parents=True, exist_ok=True)
1971:     filename = Path(file.filename).name
1972:     if not filename:
1973:         raise HTTPException(status_code=400, detail="Invalid filename")
1974:     allowed = {".csv", ".tsv", ".txt", ".json", ".xlsx"}
1975:     if Path(filename).suffix.lower() not in allowed:
1976:         raise HTTPException(status_code=400, detail="Unsupported file type")
1977:     target = upload_dir / filename
1978:     hasher = hashlib.sha256()
1979:     total = 0
1980:     max_bytes = max_upload_bytes()
1981:     chunk_size = 8 * 1024 * 1024
1982:     with target.open("wb") as handle:
1983:         while True:
1984:             chunk = await file.read(chunk_size)
1985:             if not chunk:
1986:                 break
1987:             total += len(chunk)
1988:             if max_bytes is not None and total > max_bytes:
1989:                 handle.flush()
1990:                 try:
1991:                     target.unlink()
1992:                 except FileNotFoundError:
1993:                     pass
1994:                 shutil.rmtree(upload_dir, ignore_errors=True)
1995:                 raise HTTPException(status_code=413, detail="File too large")
1996:             hasher.update(chunk)
1997:             handle.write(chunk)
1998:     sha256 = hasher.hexdigest()
1999:     existing = pipeline.storage.fetch_upload_by_sha256(sha256)
2000:     if existing:
2001:         try:
2002:             target.unlink()
2003:         except FileNotFoundError:
2004:             pass
2005:         shutil.rmtree(upload_dir, ignore_errors=True)
2006:         return JSONResponse(
2007:             {
2008:                 "upload_id": existing.get("upload_id"),
2009:                 "filename": existing.get("filename") or filename,
2010:                 "uploaded_filename": filename,
2011:                 "sha256": existing.get("sha256") or sha256,
2012:                 "deduplicated": True,
2013:             }
2014:         )
2015:     pipeline.storage.create_upload(upload_id, filename, total, sha256, now_iso())
2016:     return JSONResponse(
2017:         {"upload_id": upload_id, "filename": filename, "sha256": sha256, "deduplicated": False}
2018:     )
2019: 
2020: 
2021: @app.post("/api/upload/raw")
2022: async def upload_raw(request: Request, filename: str = Query(...)) -> JSONResponse:
2023:     upload_id = uuid.uuid4().hex
2024:     upload_dir = APPDATA_DIR / "uploads" / upload_id
2025:     upload_dir.mkdir(parents=True, exist_ok=True)
2026:     filename = Path(filename).name
2027:     if not filename:
2028:         raise HTTPException(status_code=400, detail="Invalid filename")
2029:     allowed = {".csv", ".tsv", ".txt", ".json", ".xlsx"}
2030:     if Path(filename).suffix.lower() not in allowed:
2031:         raise HTTPException(status_code=400, detail="Unsupported file type")
2032:     target = upload_dir / filename
2033:     hasher = hashlib.sha256()
2034:     total = 0
2035:     max_bytes = max_upload_bytes()
2036:     chunk_size = 8 * 1024 * 1024
2037:     with target.open("wb") as handle:
2038:         async for chunk in request.stream():
2039:             if not chunk:
2040:                 continue
2041:             total += len(chunk)
2042:             if max_bytes is not None and total > max_bytes:
2043:                 handle.flush()
2044:                 try:
2045:                     target.unlink()
2046:                 except FileNotFoundError:
2047:                     pass
2048:                 shutil.rmtree(upload_dir, ignore_errors=True)
2049:                 raise HTTPException(status_code=413, detail="File too large")
2050:             hasher.update(chunk)
2051:             handle.write(chunk)
2052:             if total % (chunk_size * 16) == 0:
2053:                 handle.flush()
2054:     sha256 = hasher.hexdigest()
2055:     existing = pipeline.storage.fetch_upload_by_sha256(sha256)
2056:     if existing:
2057:         try:
2058:             target.unlink()
2059:         except FileNotFoundError:
2060:             pass
2061:         shutil.rmtree(upload_dir, ignore_errors=True)
2062:         return JSONResponse(
2063:             {
2064:                 "upload_id": existing.get("upload_id"),
2065:                 "filename": existing.get("filename") or filename,
2066:                 "uploaded_filename": filename,
2067:                 "sha256": existing.get("sha256") or sha256,
2068:                 "deduplicated": True,
2069:             }
2070:         )
2071:     pipeline.storage.create_upload(upload_id, filename, total, sha256, now_iso())
2072:     return JSONResponse(
2073:         {"upload_id": upload_id, "filename": filename, "sha256": sha256, "deduplicated": False}
2074:     )
2075: 
2076: 
2077: @app.post("/api/runs")
2078: async def create_run(
2079:     background: BackgroundTasks,
2080:     upload_id: str = Form(...),
2081:     project_id: str = Form(""),
2082:     plugins: str = Form(""),
2083:     settings_json: str = Form(""),
2084:     run_seed: int | None = Form(None),
2085:     normalize_template_name: str = Form(""),
2086:     normalize_lowercase: str | None = Form(None),
2087:     normalize_strip: str | None = Form(None),
2088:     normalize_collapse_whitespace: str | None = Form(None),
2089:     normalize_numeric_coercion: str | None = Form(None),
2090:     normalize_numeric_threshold: str = Form("0.98"),
2091:     normalize_exclude_name_patterns: str = Form("id,uuid,guid,key"),
2092:     normalize_chunk_size: str = Form("1000"),
2093:     normalize_sample_rows: str = Form("500"),
2094: ) -> JSONResponse:
2095:     upload_dir = APPDATA_DIR / "uploads" / upload_id
2096:     upload_row = pipeline.storage.fetch_upload(upload_id)
2097:     if not upload_row:
2098:         raise HTTPException(status_code=404, detail="Upload not found")
2099:     if not upload_dir.exists():
2100:         raise HTTPException(status_code=404, detail="Upload not found")
2101:     files = list(upload_dir.iterdir())
2102:     if not files:
2103:         raise HTTPException(status_code=400, detail="No uploaded file")
2104:     input_file = files[0]
2105:     plugin_ids = [p for p in plugins.split(",") if p]
2106:     settings: dict[str, object] = {}
2107:     if settings_json:
2108:         try:
2109:             settings = json.loads(settings_json)
2110:         except json.JSONDecodeError as exc:
2111:             raise HTTPException(status_code=400, detail=str(exc)) from exc
2112:     if not isinstance(settings, dict):
2113:         raise HTTPException(status_code=400, detail="settings_json must be an object")
2114: 
2115:     project_settings = {}
2116:     if project_id:
2117:         project_settings = pipeline.storage.fetch_project_plugin_settings(project_id)
2118: 
2119:     normalize_settings = _normalize_settings_from_form(
2120:         normalize_template_name,
2121:         normalize_lowercase,
2122:         normalize_strip,
2123:         normalize_collapse_whitespace,
2124:         normalize_numeric_coercion,
2125:         normalize_numeric_threshold,
2126:         normalize_exclude_name_patterns,
2127:         normalize_chunk_size,
2128:         normalize_sample_rows,
2129:     )
2130:     plugin_settings = dict(settings.get("transform_normalize_mixed", {}))
2131:     plugin_settings.update(normalize_settings)
2132:     settings["transform_normalize_mixed"] = plugin_settings
2133:     if project_settings:
2134:         settings = _merge_settings(project_settings, settings)
2135:     settings["__run_meta"] = {"plugins": _run_meta_from_plugins(plugin_ids)}
2136: 
2137:     if run_seed in (None, 0):
2138:         run_seed = _auto_seed(upload_row.get("sha256") or upload_id, 0)
2139: 
2140:     run_id = uuid.uuid4().hex
2141: 
2142:     def run_pipeline() -> None:
2143:         pipeline.run(
2144:             input_file,
2145:             plugin_ids,
2146:             settings,
2147:             int(run_seed or 0),
2148:             run_id=run_id,
2149:             upload_id=upload_id,
2150:             project_id=project_id.strip() or None,
2151:         )
2152:         run_dir = APPDATA_DIR / "runs" / run_id
2153:         report = build_report(
2154:             pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
2155:         )
2156:         write_report(report, run_dir)
2157:         project_row = (
2158:             pipeline.storage.fetch_project(project_id.strip())
2159:             if project_id.strip()
2160:             else None
2161:         )
2162:         ground_truth, _source = _resolve_ground_truth(
2163:             "known", report, upload_row, project_row, None
2164:         )
2165:         if ground_truth:
2166:             gt_path = run_dir / "ground_truth.yaml"
2167:             gt_path.write_text(ground_truth, encoding="utf-8")
2168:             ok, messages = evaluate_report(run_dir / "report.json", gt_path)
2169:             _write_evaluation(run_dir, ok, messages)
2170: 
2171:     background.add_task(run_pipeline)
2172:     return JSONResponse({"status": "queued", "run_id": run_id})
2173: 
2174: 
2175: @app.post("/runs/auto-evaluate", response_class=HTMLResponse)
2176: async def run_auto_evaluate(
2177:     request: Request,
2178:     background: BackgroundTasks,
2179:     upload_id: str = Form(...),
2180:     project_id: str = Form(""),
2181:     run_seed: int | None = Form(None),
2182:     normalize_template_name: str = Form(""),
2183:     normalize_lowercase: str | None = Form(None),
2184:     normalize_strip: str | None = Form(None),
2185:     normalize_collapse_whitespace: str | None = Form(None),
2186:     normalize_numeric_coercion: str | None = Form(None),
2187:     normalize_numeric_threshold: str = Form("0.98"),
2188:     normalize_exclude_name_patterns: str = Form("id,uuid,guid,key"),
2189:     normalize_chunk_size: str = Form("1000"),
2190:     normalize_sample_rows: str = Form("500"),
2191: ) -> HTMLResponse:
2192:     upload_dir = APPDATA_DIR / "uploads" / upload_id
2193:     upload_row = pipeline.storage.fetch_upload(upload_id)
2194:     if not upload_row:
2195:         raise HTTPException(status_code=404, detail="Upload not found")
2196:     if not upload_dir.exists():
2197:         raise HTTPException(status_code=404, detail="Upload not found")
2198:     files = list(upload_dir.iterdir())
2199:     if not files:
2200:         raise HTTPException(status_code=400, detail="No uploaded file")
2201:     input_file = files[0]
2202: 
2203:     if run_seed in (None, 0):
2204:         run_seed = _auto_seed(upload_row.get("sha256") or upload_id, 0)
2205: 
2206:     settings: dict[str, object] = {}
2207:     project_settings = {}
2208:     if project_id:
2209:         project_settings = pipeline.storage.fetch_project_plugin_settings(project_id)
2210:     normalize_settings = _normalize_settings_from_form(
2211:         normalize_template_name,
2212:         normalize_lowercase,
2213:         normalize_strip,
2214:         normalize_collapse_whitespace,
2215:         normalize_numeric_coercion,
2216:         normalize_numeric_threshold,
2217:         normalize_exclude_name_patterns,
2218:         normalize_chunk_size,
2219:         normalize_sample_rows,
2220:     )
2221:     settings["transform_normalize_mixed"] = normalize_settings
2222:     if project_settings:
2223:         settings = _merge_settings(project_settings, settings)
2224:     settings["__run_meta"] = {"plugins": _run_meta_from_plugins([])}
2225: 
2226:     run_id = uuid.uuid4().hex
2227: 
2228:     def run_pipeline() -> None:
2229:         pipeline.run(
2230:             input_file,
2231:             [],
2232:             settings,
2233:             int(run_seed or 0),
2234:             run_id=run_id,
2235:             upload_id=upload_id,
2236:             project_id=project_id.strip() or None,
2237:         )
2238:         run_dir = APPDATA_DIR / "runs" / run_id
2239:         report = build_report(
2240:             pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
2241:         )
2242:         write_report(report, run_dir)
2243: 
2244:     background.add_task(run_pipeline)
2245:     return TEMPLATES.TemplateResponse(
2246:         "auto_evaluate.html",
2247:         {
2248:             "request": request,
2249:             "run_id": run_id,
2250:         },
2251:     )
2252: 
2253: 
2254: @app.post("/api/runs/auto-evaluate")
2255: async def api_auto_evaluate(
2256:     background: BackgroundTasks,
2257:     upload_id: str = Form(...),
2258:     project_id: str = Form(""),
2259:     run_seed: int | None = Form(None),
2260:     normalize_template_name: str = Form(""),
2261:     normalize_lowercase: str | None = Form(None),
2262:     normalize_strip: str | None = Form(None),
2263:     normalize_collapse_whitespace: str | None = Form(None),
2264:     normalize_numeric_coercion: str | None = Form(None),
2265:     normalize_numeric_threshold: str = Form("0.98"),
2266:     normalize_exclude_name_patterns: str = Form("id,uuid,guid,key"),
2267:     normalize_chunk_size: str = Form("1000"),
2268:     normalize_sample_rows: str = Form("500"),
2269: ) -> JSONResponse:
2270:     upload_dir = APPDATA_DIR / "uploads" / upload_id
2271:     upload_row = pipeline.storage.fetch_upload(upload_id)
2272:     if not upload_row:
2273:         raise HTTPException(status_code=404, detail="Upload not found")
2274:     if not upload_dir.exists():
2275:         raise HTTPException(status_code=404, detail="Upload not found")
2276:     files = list(upload_dir.iterdir())
2277:     if not files:
2278:         raise HTTPException(status_code=400, detail="No uploaded file")
2279:     input_file = files[0]
2280: 
2281:     if run_seed in (None, 0):
2282:         run_seed = _auto_seed(upload_row.get("sha256") or upload_id, 0)
2283: 
2284:     settings: dict[str, object] = {}
2285:     project_settings = {}
2286:     if project_id:
2287:         project_settings = pipeline.storage.fetch_project_plugin_settings(project_id)
2288:     normalize_settings = _normalize_settings_from_form(
2289:         normalize_template_name,
2290:         normalize_lowercase,
2291:         normalize_strip,
2292:         normalize_collapse_whitespace,
2293:         normalize_numeric_coercion,
2294:         normalize_numeric_threshold,
2295:         normalize_exclude_name_patterns,
2296:         normalize_chunk_size,
2297:         normalize_sample_rows,
2298:     )
2299:     settings["transform_normalize_mixed"] = normalize_settings
2300:     if project_settings:
2301:         settings = _merge_settings(project_settings, settings)
2302:     settings["__run_meta"] = {"plugins": _run_meta_from_plugins([])}
2303: 
2304:     run_id = uuid.uuid4().hex
2305: 
2306:     def run_pipeline() -> None:
2307:         pipeline.run(
2308:             input_file,
2309:             [],
2310:             settings,
2311:             int(run_seed or 0),
2312:             run_id=run_id,
2313:             upload_id=upload_id,
2314:             project_id=project_id.strip() or None,
2315:         )
2316:         run_dir = APPDATA_DIR / "runs" / run_id
2317:         report = build_report(
2318:             pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
2319:         )
2320:         write_report(report, run_dir)
2321: 
2322:     background.add_task(run_pipeline)
2323:     return JSONResponse({"status": "queued", "run_id": run_id})
2324: 
2325: 
2326: @app.get("/api/projects")
2327: async def list_projects() -> JSONResponse:
2328:     return JSONResponse({"projects": pipeline.storage.list_projects()})
2329: 
2330: 
2331: @app.get("/api/uploads")
2332: async def list_uploads(limit: int = 50) -> JSONResponse:
2333:     return JSONResponse({"uploads": pipeline.storage.list_uploads(limit=limit)})
2334: 
2335: 
2336: @app.get("/api/known-issues")
2337: async def get_known_issues(
2338:     upload_id: str | None = None,
2339:     sha256: str | None = None,
2340:     erp_type: str | None = None,
2341: ) -> JSONResponse:
2342:     scope_type = "sha256"
2343:     scope_value = ""
2344:     if erp_type:
2345:         scope_type = "erp_type"
2346:         scope_value = erp_type.strip() or "unknown"
2347:     elif sha256:
2348:         scope_value = sha256
2349:     elif upload_id:
2350:         upload_row = pipeline.storage.fetch_upload(str(upload_id))
2351:         if not upload_row:
2352:             raise HTTPException(status_code=404, detail="Upload not found")
2353:         scope_value = upload_row.get("sha256") or ""
2354: 
2355:     key = _scope_key(scope_type, scope_value)
2356:     if not key:
2357:         raise HTTPException(status_code=400, detail="Invalid known-issues scope")
2358:     known_payload = _load_known_issues(scope_type, scope_value)
2359:     if known_payload is None:
2360:         known_payload = {
2361:             "strict": True,
2362:             "notes": "",
2363:             "expected_findings": [],
2364:             "natural_language": [],
2365:         }
2366:     return JSONResponse(
2367:         {
2368:             "sha256": key,
2369:             "scope_type": scope_type,
2370:             "scope_value": scope_value,
2371:             "known_issues": known_payload,
2372:             "ground_truth_yaml": _known_issues_yaml(scope_type, scope_value) or "",
2373:         }
2374:     )
2375: 
2376: 
2377: @app.post("/api/known-issues")
2378: async def save_known_issues(payload: dict) -> JSONResponse:
2379:     upload_id = payload.get("upload_id")
2380:     if not upload_id:
2381:         raise HTTPException(status_code=400, detail="upload_id required")
2382:     upload_row = pipeline.storage.fetch_upload(str(upload_id))
2383:     if not upload_row:
2384:         raise HTTPException(status_code=404, detail="Upload not found")
2385:     erp_type = str(payload.get("erp_type") or "").strip()
2386:     scope_type = "erp_type" if erp_type else "sha256"
2387:     scope_value = erp_type or (upload_row.get("sha256") or "")
2388:     known = payload.get("known_issues") or payload.get("payload") or {}
2389:     if not isinstance(known, dict):
2390:         raise HTTPException(status_code=400, detail="known_issues must be an object")
2391:     try:
2392:         normalized = _save_known_issues(scope_type, scope_value, str(upload_id), known)
2393:     except ValueError as exc:
2394:         raise HTTPException(status_code=400, detail=str(exc)) from exc
2395:     key = _scope_key(scope_type, scope_value) or ""
2396:     return JSONResponse(
2397:         {
2398:             "status": "ok",
2399:             "upload_id": upload_id,
2400:             "sha256": key,
2401:             "scope_type": scope_type,
2402:             "scope_value": scope_value,
2403:             "known_issues": normalized,
2404:             "ground_truth_yaml": _known_issues_yaml(scope_type, scope_value) or "",
2405:         }
2406:     )
2407: 
2408: 
2409: @app.get("/api/projects/{project_id}")
2410: async def get_project(project_id: str) -> JSONResponse:
2411:     datasets = pipeline.storage.list_dataset_versions_by_project(project_id)
2412:     project = pipeline.storage.fetch_project(project_id)
2413:     return JSONResponse({"project": project, "datasets": datasets})
2414: 
2415: 
2416: @app.get("/api/projects/{project_id}/known-issues")
2417: async def get_project_known_issues(project_id: str) -> JSONResponse:
2418:     project = pipeline.storage.fetch_project(project_id)
2419:     if not project:
2420:         raise HTTPException(status_code=404, detail="Project not found")
2421:     erp_type = (project.get("erp_type") or "unknown").strip() or "unknown"
2422:     known_payload = _load_known_issues("erp_type", erp_type)
2423:     if known_payload is None:
2424:         known_payload = {
2425:             "strict": True,
2426:             "notes": "",
2427:             "expected_findings": [],
2428:             "natural_language": [],
2429:         }
2430:     return JSONResponse(
2431:         {
2432:             "project_id": project_id,
2433:             "erp_type": erp_type,
2434:             "known_issues": known_payload,
2435:         }
2436:     )
2437: 
2438: 
2439: @app.post("/api/projects/{project_id}/known-issues")
2440: async def save_project_known_issues(project_id: str, payload: dict) -> JSONResponse:
2441:     project = pipeline.storage.fetch_project(project_id)
2442:     if not project:
2443:         raise HTTPException(status_code=404, detail="Project not found")
2444:     erp_type = (project.get("erp_type") or "unknown").strip() or "unknown"
2445:     known = payload.get("known_issues") or payload.get("payload") or {}
2446:     if not isinstance(known, dict):
2447:         raise HTTPException(status_code=400, detail="known_issues must be an object")
2448:     normalized = _save_known_issues("erp_type", erp_type, None, known)
2449:     return JSONResponse(
2450:         {
2451:             "status": "ok",
2452:             "project_id": project_id,
2453:             "erp_type": erp_type,
2454:             "known_issues": normalized,
2455:         }
2456:     )
2457: 
2458: 
2459: @app.get("/api/templates")
2460: async def list_templates() -> JSONResponse:
2461:     return JSONResponse({"templates": pipeline.storage.list_templates()})
2462: 
2463: 
2464: @app.post("/api/templates")
2465: async def create_template(payload: dict) -> JSONResponse:
2466:     name = payload.get("name")
2467:     if not name:
2468:         raise HTTPException(status_code=400, detail="Template name required")
2469:     fields = payload.get("fields") or []
2470:     if not isinstance(fields, list) or not fields:
2471:         raise HTTPException(status_code=400, detail="Template fields required")
2472:     template_id = pipeline.storage.create_template(
2473:         name=name,
2474:         fields=fields,
2475:         description=payload.get("description"),
2476:         version=payload.get("version"),
2477:         created_at=now_iso(),
2478:     )
2479:     return JSONResponse({"template_id": template_id})
2480: 
2481: 
2482: @app.post("/templates/{template_id}/map", response_class=HTMLResponse)
2483: async def map_template(
2484:     request: Request,
2485:     template_id: int,
2486:     dataset_version_id: str = Form(...),
2487:     mapping_json: str = Form(...),
2488: ) -> HTMLResponse:
2489:     try:
2490:         mapping = json.loads(mapping_json)
2491:     except json.JSONDecodeError as exc:
2492:         raise HTTPException(status_code=400, detail=str(exc)) from exc
2493:     settings = {"transform_template": {"template_id": int(template_id), "mapping": mapping}}
2494:     run_id = pipeline.run(
2495:         None,
2496:         ["transform_template"],
2497:         settings,
2498:         0,
2499:         dataset_version_id=dataset_version_id,
2500:     )
2501:     return HTMLResponse(
2502:         f"Template mapping queued: {run_id}. <a href='/runs/{run_id}'>View</a>"
2503:     )
2504: 
2505: 
2506: @app.get("/api/templates/{template_id}")
2507: async def get_template(template_id: int) -> JSONResponse:
2508:     template = pipeline.storage.fetch_template(template_id)
2509:     if not template:
2510:         raise HTTPException(status_code=404, detail="Template not found")
2511:     fields = pipeline.storage.fetch_template_fields(template_id)
2512:     return JSONResponse({"template": template, "fields": fields})
2513: 
2514: 
2515: @app.post("/templates/{template_id}/run", response_class=HTMLResponse)
2516: async def run_template_combined(
2517:     request: Request,
2518:     template_id: int,
2519:     plugins: str = Form(""),
2520:     settings_json: str = Form(""),
2521:     run_seed: int | None = Form(None),
2522:     project_ids: str = Form(""),
2523:     dataset_ids: str = Form(""),
2524:     dataset_version_ids: str = Form(""),
2525:     raw_format_ids: str = Form(""),
2526:     created_after: str = Form(""),
2527:     created_before: str = Form(""),
2528: ) -> HTMLResponse:
2529:     plugin_ids = [p for p in plugins.split(",") if p]
2530:     settings = {}
2531:     if settings_json:
2532:         try:
2533:             settings = json.loads(settings_json)
2534:         except json.JSONDecodeError as exc:
2535:             raise HTTPException(status_code=400, detail=str(exc)) from exc
2536: 
2537:     filters = _parse_filter_params(
2538:         project_ids,
2539:         dataset_ids,
2540:         dataset_version_ids,
2541:         raw_format_ids,
2542:         created_after,
2543:         created_before,
2544:     )
2545: 
2546:     dataset_version_id = pipeline.storage.ensure_template_aggregate_dataset(
2547:         int(template_id), now_iso(), filters=filters or None
2548:     )
2549:     if run_seed in (None, 0):
2550:         run_seed = _auto_seed(dataset_version_id, 0)
2551:     run_id = pipeline.run(
2552:         None,
2553:         plugin_ids,
2554:         settings,
2555:         int(run_seed or 0),
2556:         dataset_version_id=dataset_version_id,
2557:     )
2558:     return HTMLResponse(
2559:         f"Run started: {run_id}. <a href='/runs/{run_id}'>View</a>"
2560:     )
2561: 
2562: 
2563: @app.get("/api/trace/row")
2564: async def trace_row_api(
2565:     dataset_version_id: str,
2566:     row_index: int,
2567:     source_dataset_version_id: str | None = None,
2568:     max_rows: int = 50,
2569: ) -> JSONResponse:
2570:     payload = pipeline.storage.fetch_row_trace(
2571:         dataset_version_id,
2572:         int(row_index),
2573:         source_dataset_version_id=source_dataset_version_id,
2574:         max_rows=max_rows,
2575:     )
2576:     return JSONResponse(payload)
2577: 
2578: 
2579: @app.get("/trace/row", response_class=HTMLResponse)
2580: async def trace_row_view(
2581:     request: Request,
2582:     dataset_version_id: str,
2583:     row_index: int,
2584:     source_dataset_version_id: str | None = None,
2585:     max_rows: int = 50,
2586: ) -> HTMLResponse:
2587:     payload = pipeline.storage.fetch_row_trace(
2588:         dataset_version_id,
2589:         int(row_index),
2590:         source_dataset_version_id=source_dataset_version_id,
2591:         max_rows=max_rows,
2592:     )
2593:     return TEMPLATES.TemplateResponse(
2594:         "row_trace.html",
2595:         {
2596:             "request": request,
2597:             "payload": payload,
2598:             "dataset_version_id": dataset_version_id,
2599:             "row_index": row_index,
2600:             "source_dataset_version_id": source_dataset_version_id or "",
2601:         },
2602:     )
2603: 
2604: 
2605: @app.get("/templates/{template_id}/results", response_class=HTMLResponse)
2606: async def template_results(request: Request, template_id: int) -> HTMLResponse:
2607:     filters = _parse_filter_params(
2608:         request.query_params.get("project_ids", ""),
2609:         request.query_params.get("dataset_ids", ""),
2610:         request.query_params.get("dataset_version_ids", ""),
2611:         request.query_params.get("raw_format_ids", ""),
2612:         request.query_params.get("created_after", ""),
2613:         request.query_params.get("created_before", ""),
2614:     )
2615:     dataset_version_id = pipeline.storage.ensure_template_aggregate_dataset(
2616:         int(template_id), now_iso(), filters=filters or None
2617:     )
2618:     results = pipeline.storage.fetch_latest_plugin_results_for_dataset(
2619:         dataset_version_id
2620:     )
2621:     return TEMPLATES.TemplateResponse(
2622:         "template_results.html",
2623:         {
2624:             "request": request,
2625:             "template_id": template_id,
2626:             "dataset_version_id": dataset_version_id,
2627:             "results": results,
2628:             "filters": filters,
2629:             "filter_params": {
2630:                 "project_ids": request.query_params.get("project_ids", ""),
2631:                 "dataset_ids": request.query_params.get("dataset_ids", ""),
2632:                 "dataset_version_ids": request.query_params.get("dataset_version_ids", ""),
2633:                 "raw_format_ids": request.query_params.get("raw_format_ids", ""),
2634:                 "created_after": request.query_params.get("created_after", ""),
2635:                 "created_before": request.query_params.get("created_before", ""),
2636:             },
2637:         },
2638:     )
2639: 
2640: 
2641: @app.post("/api/datasets/{dataset_version_id}/template")
2642: async def set_dataset_template(dataset_version_id: str, payload: dict) -> JSONResponse:
2643:     template_id = payload.get("template_id")
2644:     mapping = payload.get("mapping") or {}
2645:     if not template_id:
2646:         raise HTTPException(status_code=400, detail="template_id required")
2647:     if not isinstance(mapping, dict):
2648:         raise HTTPException(status_code=400, detail="mapping must be object")
2649:     settings = {"transform_template": {"template_id": int(template_id), "mapping": mapping}}
2650:     run_id = pipeline.run(
2651:         None,
2652:         ["transform_template"],
2653:         settings,
2654:         0,
2655:         dataset_version_id=dataset_version_id,
2656:     )
2657:     return JSONResponse(
2658:         {"status": "queued", "template_id": template_id, "run_id": run_id}
2659:     )
2660: 
2661: 
2662: @app.get("/api/raw-formats")
2663: async def list_raw_formats() -> JSONResponse:
2664:     return JSONResponse({"formats": pipeline.storage.list_raw_formats()})
2665: 
2666: 
2667: @app.post("/api/raw-formats/{format_id}/notes")
2668: async def add_raw_format_note(format_id: int, payload: dict) -> JSONResponse:
2669:     note = payload.get("note")
2670:     if not note:
2671:         raise HTTPException(status_code=400, detail="note required")
2672:     pipeline.storage.add_raw_format_note(format_id, note, now_iso())
2673:     return JSONResponse({"status": "ok"})
2674: 
2675: 
2676: @app.post("/raw-formats/{format_id}/notes", response_class=HTMLResponse)
2677: async def add_raw_format_note_form(
2678:     request: Request, format_id: int, note: str = Form(...)
2679: ) -> HTMLResponse:
2680:     pipeline.storage.add_raw_format_note(format_id, note, now_iso())
2681:     return HTMLResponse("Note added. <a href='/raw-formats'>Back</a>")
2682: 
2683: 
2684: @app.get("/api/raw-formats/{format_id}/notes")
2685: async def list_raw_format_notes(format_id: int) -> JSONResponse:
2686:     return JSONResponse({"notes": pipeline.storage.list_raw_format_notes(format_id)})
2687: 
2688: 
2689: @app.get("/api/raw-formats/{format_id}/mappings")
2690: async def list_raw_format_mappings(format_id: int) -> JSONResponse:
2691:     return JSONResponse(
2692:         {"mappings": pipeline.storage.list_raw_format_mappings(format_id)}
2693:     )
2694: 
2695: 
2696: @app.post("/raw-formats/{format_id}/mappings", response_class=HTMLResponse)
2697: async def add_raw_format_mapping_form(
2698:     request: Request,
2699:     format_id: int,
2700:     template_id: int = Form(...),
2701:     mapping_json: str = Form(...),
2702:     notes: str = Form(""),
2703: ) -> HTMLResponse:
2704:     try:
2705:         mapping = json.loads(mapping_json)
2706:     except json.JSONDecodeError as exc:
2707:         raise HTTPException(status_code=400, detail=str(exc)) from exc
2708:     mapping_hash = hashlib.sha256(json_dumps(mapping).encode("utf-8")).hexdigest()
2709:     pipeline.storage.add_raw_format_mapping(
2710:         format_id,
2711:         int(template_id),
2712:         json_dumps(mapping),
2713:         mapping_hash,
2714:         notes or None,
2715:         now_iso(),
2716:     )
2717:     return HTMLResponse("Mapping saved. <a href='/raw-formats'>Back</a>")
2718: 
2719: 
2720: 
2721: def _write_evaluation(run_dir: Path, ok: bool, messages: list[str]) -> dict[str, object]:
2722:     payload = {
2723:         "evaluated_at": now_iso(),
2724:         "result": "passed" if ok else "failed",
2725:         "ok": bool(ok),
2726:         "messages": messages,
2727:     }
2728:     eval_path = run_dir / "evaluation.json"
2729:     eval_path.write_text(json_dumps(payload), encoding="utf-8")
2730:     log_path = run_dir / "logs" / "evaluate.log"
2731:     log_path.parent.mkdir(parents=True, exist_ok=True)
2732:     with log_path.open("a", encoding="utf-8") as handle:
2733:         handle.write(f"[{payload['evaluated_at']}] {payload['result']}\n")
2734:         for message in messages:
2735:             handle.write(f"- {message}\n")
2736:     run_log = run_dir / "logs" / "run.log"
2737:     with run_log.open("a", encoding="utf-8") as handle:
2738:         handle.write(f"[{payload['evaluated_at']}] evaluation {payload['result']}\n")
2739:         for message in messages:
2740:             handle.write(f"- {message}\n")
2741:     run_id = run_dir.name
2742:     run_row = pipeline.storage.fetch_run(run_id)
2743:     if run_row:
2744:         if not ok:
2745:             pipeline.storage.update_run_status(
2746:                 run_id,
2747:                 "failed",
2748:                 error={"type": "KnownIssuesFailed", "messages": messages},
2749:             )
2750:         else:
2751:             if run_row.get("status") != "running":
2752:                 pipeline.storage.update_run_status(run_id, "completed", error=None)
2753:     return payload
2754: 
2755: def _resolve_ground_truth(
2756:     mode: str,
2757:     report: dict,
2758:     upload_row: dict | None,
2759:     project_row: dict | None,
2760:     provided: str | None,
2761: ) -> tuple[str, str]:
2762:     if provided:
2763:         return provided, "manual"
2764:     mode = (mode or "auto").lower()
2765:     if mode in {"known", "auto"}:
2766:         if project_row and project_row.get("erp_type"):
2767:             erp_type = str(project_row.get("erp_type") or "unknown").strip() or "unknown"
2768:             known = _known_issues_yaml("erp_type", erp_type)
2769:             if known:
2770:                 return known, "known"
2771:         if upload_row and upload_row.get("sha256"):
2772:             sha256 = _safe_sha256(upload_row.get("sha256") or "")
2773:             if sha256:
2774:                 known = _known_issues_yaml("sha256", sha256)
2775:                 if known:
2776:                     return known, "known"
2777:     if mode in {"template", "auto"}:
2778:         return _ground_truth_template(report), "template"
2779:     return "", ""
2780: 
2781: 
2782: def _plugin_code_hash(spec) -> str | None:
2783:     module_path, _ = spec.entrypoint.split(":", 1)
2784:     if module_path.endswith(".py"):
2785:         module_file = spec.path / module_path
2786:     else:
2787:         module_file = spec.path / f"{module_path}.py"
2788:     return file_sha256(module_file) if module_file.exists() else None
2789: 
2790: 
2791: @app.get("/api/datasets/{dataset_version_id}/status")
2792: async def dataset_status(dataset_version_id: str) -> JSONResponse:
2793:     specs = [s for s in pipeline.manager.discover() if s.type == "analysis"]
2794:     dataset = pipeline.storage.get_dataset_version(dataset_version_id)
2795:     if not dataset:
2796:         raise HTTPException(status_code=404, detail="Dataset not found")
2797:     dataset_hash = dataset.get("data_hash")
2798:     results = pipeline.storage.fetch_latest_plugin_results_for_dataset(
2799:         dataset_version_id
2800:     )
2801:     deliveries = pipeline.storage.fetch_deliveries_for_dataset(dataset_version_id)
2802:     results_map = {row["plugin_id"]: row for row in results}
2803:     delivery_map = {}
2804:     for row in deliveries:
2805:         if row["plugin_id"] not in delivery_map:
2806:             delivery_map[row["plugin_id"]] = row
2807: 
2808:     status_rows = []
2809:     for spec in specs:
2810:         current_hash = _plugin_code_hash(spec)
2811:         result = results_map.get(spec.plugin_id)
2812:         delivery = delivery_map.get(spec.plugin_id)
2813:         status = "pending"
2814:         if result:
2815:             status = "ran"
2816:         if delivery:
2817:             if delivery.get("code_hash") == current_hash and delivery.get(
2818:                 "dataset_hash"
2819:             ) == dataset_hash:
2820:                 status = "delivered"
2821:             else:
2822:                 status = "stale"
2823:         if not result:
2824:             status = "pending"
2825:         if result and not delivery:
2826:             status = "needs_delivery"
2827:         status_rows.append(
2828:             {
2829:                 "plugin_id": spec.plugin_id,
2830:                 "plugin_version": spec.version,
2831:                 "executed_at": result.get("executed_at") if result else None,
2832:                 "delivered_at": delivery.get("delivered_at") if delivery else None,
2833:                 "status": status,
2834:             }
2835:         )
2836:     return JSONResponse({"dataset_version_id": dataset_version_id, "plugins": status_rows})
2837: 
2838: 
2839: @app.post("/api/datasets/{dataset_version_id}/rerun")
2840: async def rerun_dataset(
2841:     background: BackgroundTasks, dataset_version_id: str
2842: ) -> JSONResponse:
2843:     ctx = pipeline.storage.get_dataset_version_context(dataset_version_id)
2844:     if not ctx:
2845:         raise HTTPException(status_code=404, detail="Dataset not found")
2846:     project_id = ctx.get("project_id")
2847:     project_settings = (
2848:         pipeline.storage.fetch_project_plugin_settings(project_id)
2849:         if project_id
2850:         else {}
2851:     )
2852:     settings: dict[str, object] = {}
2853:     if project_settings:
2854:         settings = _merge_settings(project_settings, settings)
2855:     settings["__run_meta"] = {"plugins": _run_meta_from_plugins(["all"])}
2856:     run_seed = _auto_seed(dataset_version_id, 0)
2857:     run_id = uuid.uuid4().hex
2858: 
2859:     def run_pipeline() -> None:
2860:         pipeline.run(
2861:             None,
2862:             ["all"],
2863:             settings,
2864:             int(run_seed or 0),
2865:             run_id=run_id,
2866:             dataset_version_id=dataset_version_id,
2867:             project_id=project_id,
2868:         )
2869:         run_dir = APPDATA_DIR / "runs" / run_id
2870:         report = build_report(
2871:             pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
2872:         )
2873:         write_report(report, run_dir)
2874:         project_row = (
2875:             pipeline.storage.fetch_project(project_id)
2876:             if project_id
2877:             else None
2878:         )
2879:         ground_truth, _source = _resolve_ground_truth(
2880:             "known", report, None, project_row, None
2881:         )
2882:         if ground_truth:
2883:             gt_path = run_dir / "ground_truth.yaml"
2884:             gt_path.write_text(ground_truth, encoding="utf-8")
2885:             ok, messages = evaluate_report(run_dir / "report.json", gt_path)
2886:             _write_evaluation(run_dir, ok, messages)
2887: 
2888:     background.add_task(run_pipeline)
2889:     return JSONResponse({"status": "queued", "run_id": run_id})
2890: 
2891: 
2892: @app.get("/api/runs/{run_id}/progress")
2893: async def run_progress(run_id: str) -> JSONResponse:
2894:     run_row = pipeline.storage.fetch_run(run_id)
2895:     if not run_row:
2896:         raise HTTPException(status_code=404, detail="Run not found")
2897:     expected = _expected_plugins_for_run(run_row)
2898:     executions = pipeline.storage.fetch_plugin_executions(run_id)
2899:     latest: dict[str, dict[str, object]] = {}
2900:     for row in executions:
2901:         latest[str(row.get("plugin_id"))] = row
2902: 
2903:     plugin_statuses = []
2904:     completed = 0
2905:     for plugin_id in expected:
2906:         entry = latest.get(plugin_id)
2907:         status = "pending"
2908:         started_at = None
2909:         completed_at = None
2910:         if entry:
2911:             started_at = entry.get("started_at")
2912:             completed_at = entry.get("completed_at")
2913:             status = str(entry.get("status") or "running")
2914:             if completed_at is None:
2915:                 status = "running"
2916:         else:
2917:             if run_row.get("status") == "completed":
2918:                 status = "not_run"
2919:         if status not in {"pending", "running"}:
2920:             completed += 1
2921:         plugin_statuses.append(
2922:             {
2923:                 "plugin_id": plugin_id,
2924:                 "status": status,
2925:                 "started_at": started_at,
2926:                 "completed_at": completed_at,
2927:             }
2928:         )
2929: 
2930:     total = len(expected) if expected else len(executions)
2931:     percent = int((completed / total) * 100) if total else 0
2932:     errors = []
2933:     for row in pipeline.storage.fetch_plugin_results(run_id):
2934:         if row.get("status") != "error" and not row.get("error_json"):
2935:             continue
2936:         message = row.get("summary") or "Plugin error"
2937:         err_payload = {}
2938:         if row.get("error_json"):
2939:             try:
2940:                 err_payload = json.loads(row["error_json"])
2941:             except json.JSONDecodeError:
2942:                 err_payload = {}
2943:         if err_payload.get("message"):
2944:             message = err_payload["message"]
2945:         errors.append(
2946:             {
2947:                 "plugin_id": row.get("plugin_id"),
2948:                 "message": message,
2949:                 "type": err_payload.get("type"),
2950:             }
2951:         )
2952:     return JSONResponse(
2953:         {
2954:             "run_id": run_id,
2955:             "run_status": run_row.get("status"),
2956:             "percent_complete": percent,
2957:             "completed_plugins": completed,
2958:             "total_plugins": total,
2959:             "plugins": plugin_statuses,
2960:             "errors": errors,
2961:         }
2962:     )
2963: 
2964: 
2965: @app.get("/api/jobs")
2966: async def list_jobs(status: str = "queued") -> JSONResponse:
2967:     return JSONResponse({"jobs": pipeline.storage.list_analysis_jobs(status=status)})
2968: 
2969: 
2970: @app.post("/datasets/{dataset_version_id}/run", response_class=HTMLResponse)
2971: async def run_dataset_plugin(
2972:     request: Request,
2973:     dataset_version_id: str,
2974:     plugins: str = Form(""),
2975:     settings_json: str = Form(""),
2976:     run_seed: int | None = Form(None),
2977: ) -> HTMLResponse:
2978:     plugin_ids = [p for p in plugins.split(",") if p]
2979:     settings = {}
2980:     if settings_json:
2981:         try:
2982:             settings = json.loads(settings_json)
2983:         except json.JSONDecodeError as exc:
2984:             raise HTTPException(status_code=400, detail=str(exc)) from exc
2985:     if run_seed in (None, 0):
2986:         run_seed = _auto_seed(dataset_version_id, 0)
2987:     run_id = pipeline.run(
2988:         None,
2989:         plugin_ids,
2990:         settings,
2991:         int(run_seed or 0),
2992:         dataset_version_id=dataset_version_id,
2993:     )
2994:     return HTMLResponse(
2995:         f"Run started: {run_id}. <a href='/runs/{run_id}'>View</a>"
2996:     )
2997: 
2998: 
2999: @app.post("/datasets/{dataset_version_id}/backfill", response_class=HTMLResponse)
3000: async def backfill_dataset(
3001:     request: Request,
3002:     dataset_version_id: str,
3003:     plugin_id: str = Form(...),
3004: ) -> HTMLResponse:
3005:     spec_map = {s.plugin_id: s for s in pipeline.manager.discover()}
3006:     if plugin_id not in spec_map:
3007:         raise HTTPException(status_code=404, detail="Plugin not found")
3008:     spec = spec_map[plugin_id]
3009:     module_path, _ = spec.entrypoint.split(":", 1)
3010:     if module_path.endswith(".py"):
3011:         module_file = spec.path / module_path
3012:     else:
3013:         module_file = spec.path / f"{module_path}.py"
3014:     code_hash = file_sha256(module_file) if module_file.exists() else None
3015:     settings_hash = hashlib.sha256(json_dumps({}).encode("utf-8")).hexdigest()
3016:     pipeline.storage.enqueue_analysis_job(
3017:         dataset_version_id,
3018:         plugin_id,
3019:         spec.version,
3020:         code_hash,
3021:         settings_hash,
3022:         0,
3023:         now_iso(),
3024:     )
3025:     _run_job_queue()
3026:     return HTMLResponse("Backfill queued. <a href='/projects'>Back</a>")
3027: 
3028: 
3029: def _run_job_queue() -> None:
3030:     jobs = pipeline.storage.list_analysis_jobs(status="queued")
3031:     for job in jobs:
3032:         job_id = int(job["job_id"])
3033:         pipeline.storage.update_analysis_job_status(
3034:             job_id, "running", started_at=now_iso()
3035:         )
3036:         try:
3037:             pipeline.run(
3038:                 None,
3039:                 [job["plugin_id"]],
3040:                 {},
3041:                 int(job.get("run_seed") or 0),
3042:                 dataset_version_id=job["dataset_version_id"],
3043:             )
3044:             pipeline.storage.update_analysis_job_status(
3045:                 job_id, "completed", completed_at=now_iso()
3046:             )
3047:         except Exception as exc:  # pragma: no cover - failure path
3048:             pipeline.storage.update_analysis_job_status(
3049:                 job_id, "error", completed_at=now_iso(), error={"message": str(exc)}
3050:             )
3051: 
3052: 
3053: @app.post("/api/jobs/run")
3054: async def run_jobs(background: BackgroundTasks) -> JSONResponse:
3055:     background.add_task(_run_job_queue)
3056:     return JSONResponse({"status": "queued"})
3057: 
3058: 
3059: @app.post("/api/backfill")
3060: async def backfill(payload: dict, background: BackgroundTasks) -> JSONResponse:
3061:     plugin_id = payload.get("plugin_id")
3062:     dataset_version_id = payload.get("dataset_version_id")
3063:     if not plugin_id:
3064:         raise HTTPException(status_code=400, detail="plugin_id required")
3065:     spec_map = {s.plugin_id: s for s in pipeline.manager.discover()}
3066:     if plugin_id not in spec_map:
3067:         raise HTTPException(status_code=404, detail="Plugin not found")
3068:     spec = spec_map[plugin_id]
3069:     module_path, _ = spec.entrypoint.split(":", 1)
3070:     if module_path.endswith(".py"):
3071:         module_file = spec.path / module_path
3072:     else:
3073:         module_file = spec.path / f"{module_path}.py"
3074:     code_hash = file_sha256(module_file) if module_file.exists() else None
3075:     settings_hash = hashlib.sha256(json_dumps({}).encode("utf-8")).hexdigest()
3076: 
3077:     if dataset_version_id:
3078:         pipeline.storage.enqueue_analysis_job(
3079:             dataset_version_id,
3080:             plugin_id,
3081:             spec.version,
3082:             code_hash,
3083:             settings_hash,
3084:             0,
3085:             now_iso(),
3086:         )
3087:     else:
3088:         for dataset in pipeline.storage.list_dataset_versions():
3089:             pipeline.storage.enqueue_analysis_job(
3090:                 dataset["dataset_version_id"],
3091:                 plugin_id,
3092:                 spec.version,
3093:                 code_hash,
3094:                 settings_hash,
3095:                 0,
3096:                 now_iso(),
3097:             )
3098:     background.add_task(_run_job_queue)
3099:     return JSONResponse({"status": "queued"})
3100: 
3101: 
3102: @app.get("/api/trace")
3103: async def trace(entity_type: str, key: str, max_depth: int = 5) -> JSONResponse:
3104:     return JSONResponse(
3105:         pipeline.storage.trace_from_entity(entity_type, key, max_depth)
3106:     )
3107: 
3108: 
3109: @app.get("/trace", response_class=HTMLResponse)
3110: async def trace_view(
3111:     request: Request,
3112:     entity_type: str | None = None,
3113:     key: str | None = None,
3114:     max_depth: int = 5,
3115: ) -> HTMLResponse:
3116:     result = ""
3117:     if entity_type and key:
3118:         payload = pipeline.storage.trace_from_entity(entity_type, key, max_depth)
3119:         result = json_dumps(payload)
3120:     return TEMPLATES.TemplateResponse(
3121:         "trace.html",
3122:         {
3123:             "request": request,
3124:             "entity_type": entity_type or "",
3125:             "key": key or "",
3126:             "max_depth": max_depth,
3127:             "result": result,
3128:         },
3129:     )
3130: 
3131: 
3132: @app.post("/datasets/{dataset_version_id}/deliver", response_class=HTMLResponse)
3133: async def deliver_dataset(
3134:     request: Request,
3135:     dataset_version_id: str,
3136:     plugin_id: str = Form(""),
3137:     notes: str = Form(""),
3138: ) -> HTMLResponse:
3139:     dataset = pipeline.storage.get_dataset_version_context(dataset_version_id)
3140:     if not dataset:
3141:         raise HTTPException(status_code=404, detail="Dataset not found")
3142:     dataset_hash = dataset.get("data_hash")
3143:     specs = {s.plugin_id: s for s in pipeline.manager.discover() if s.type == "analysis"}
3144:     results = pipeline.storage.fetch_latest_plugin_results_for_dataset(
3145:         dataset_version_id
3146:     )
3147:     results_map = {row["plugin_id"]: row for row in results}
3148: 
3149:     target_plugins = []
3150:     if plugin_id:
3151:         if plugin_id not in specs:
3152:             raise HTTPException(status_code=404, detail="Plugin not found")
3153:         target_plugins = [plugin_id]
3154:     else:
3155:         target_plugins = list(results_map.keys())
3156: 
3157:     for pid in target_plugins:
3158:         result = results_map.get(pid)
3159:         if not result:
3160:             continue
3161:         spec = specs.get(pid)
3162:         module_path, _ = spec.entrypoint.split(":", 1)
3163:         if module_path.endswith(".py"):
3164:             module_file = spec.path / module_path
3165:         else:
3166:             module_file = spec.path / f"{module_path}.py"
3167:         code_hash = file_sha256(module_file) if module_file.exists() else None
3168:         pipeline.storage.record_delivery(
3169:             dataset["project_id"],
3170:             dataset_version_id,
3171:             pid,
3172:             result.get("plugin_version") or spec.version,
3173:             code_hash,
3174:             dataset_hash,
3175:             now_iso(),
3176:             notes or None,
3177:         )
3178:     return HTMLResponse("Delivery recorded. <a href='/projects'>Back</a>")
3179: 
3180: 
3181: @app.post("/api/datasets/{dataset_version_id}/deliveries")
3182: async def mark_delivery(
3183:     dataset_version_id: str,
3184:     plugin_id: str = Form(""),
3185:     notes: str = Form(""),
3186: ) -> JSONResponse:
3187:     dataset = pipeline.storage.get_dataset_version_context(dataset_version_id)
3188:     if not dataset:
3189:         raise HTTPException(status_code=404, detail="Dataset not found")
3190:     dataset_hash = dataset.get("data_hash")
3191:     specs = {s.plugin_id: s for s in pipeline.manager.discover() if s.type == "analysis"}
3192:     results = pipeline.storage.fetch_latest_plugin_results_for_dataset(
3193:         dataset_version_id
3194:     )
3195:     results_map = {row["plugin_id"]: row for row in results}
3196: 
3197:     target_plugins = []
3198:     if plugin_id:
3199:         if plugin_id not in specs:
3200:             raise HTTPException(status_code=404, detail="Plugin not found")
3201:         target_plugins = [plugin_id]
3202:     else:
3203:         target_plugins = list(results_map.keys())
3204: 
3205:     if not target_plugins:
3206:         raise HTTPException(status_code=400, detail="No plugins to deliver")
3207: 
3208:     for pid in target_plugins:
3209:         result = results_map.get(pid)
3210:         if not result:
3211:             continue
3212:         spec = specs.get(pid)
3213:         code_hash = _plugin_code_hash(spec) if spec else None
3214:         pipeline.storage.record_delivery(
3215:             dataset["project_id"],
3216:             dataset_version_id,
3217:             pid,
3218:             result.get("plugin_version") or (spec.version if spec else None),
3219:             code_hash,
3220:             dataset_hash,
3221:             now_iso(),
3222:             notes or None,
3223:         )
3224:     return JSONResponse({"status": "ok", "delivered": target_plugins})
3225: 
3226: 
3227: @app.get("/runs/{run_id}", response_class=HTMLResponse)
3228: async def run_status(request: Request, run_id: str) -> HTMLResponse:
3229:     run_row = pipeline.storage.fetch_run(run_id)
3230:     project = None
3231:     run_label = ""
3232:     evaluation = _load_evaluation_summary(run_id)
3233:     if run_row and run_row.get("project_id"):
3234:         project = pipeline.storage.fetch_project(run_row["project_id"])
3235:         runs = _annotate_runs(
3236:             pipeline.storage.list_runs_by_project(run_row["project_id"]), project
3237:         )
3238:         for entry in runs:
3239:             if entry.get("run_id") == run_id:
3240:                 run_label = str(entry.get("label") or "")
3241:                 break
3242:     return TEMPLATES.TemplateResponse(
3243:         "run.html",
3244:         {
3245:             "request": request,
3246:             "run_id": run_id,
3247:             "run_label": run_label,
3248:             "project": project,
3249:             "evaluation": evaluation,
3250:         },
3251:     )
3252: 
3253: 
3254: @app.get("/runs/{run_id}/report", response_class=HTMLResponse)
3255: async def run_report_view(request: Request, run_id: str) -> HTMLResponse:
3256:     run_row = pipeline.storage.fetch_run(run_id)
3257:     if not run_row:
3258:         raise HTTPException(status_code=404, detail="Run not found")
3259:     run_dir = pipeline.base_dir / "runs" / run_id
3260:     report_path = run_dir / "report.json"
3261:     if report_path.exists():
3262:         report = json.loads(report_path.read_text(encoding="utf-8"))
3263:     else:
3264:         report = build_report(
3265:             pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
3266:         )
3267:         write_report(report, run_dir)
3268:     project = None
3269:     if run_row.get("project_id"):
3270:         project = pipeline.storage.fetch_project(run_row["project_id"])
3271:     runs = []
3272:     if project:
3273:         runs = _annotate_runs(pipeline.storage.list_runs_by_project(project["project_id"]), project)
3274:     run_label = ""
3275:     if runs:
3276:         for entry in runs:
3277:             if entry.get("run_id") == run_id:
3278:                 run_label = str(entry.get("label") or "")
3279:                 break
3280:     evaluation = _load_evaluation_summary(run_id)
3281:     insights = _build_insights(report)
3282:     specs = {spec.plugin_id: spec for spec in pipeline.manager.discover()}
3283:     plugin_labels = {pid: spec.name for pid, spec in specs.items()}
3284:     plugin_descriptions = {
3285:         pid: str(spec.settings.get("description") or "") for pid, spec in specs.items()
3286:     }
3287:     upload_row = None
3288:     if run_row.get("upload_id"):
3289:         upload_row = pipeline.storage.fetch_upload(run_row["upload_id"])
3290:     known_payload = None
3291:     if project and project.get("erp_type"):
3292:         erp_type = str(project.get("erp_type") or "unknown").strip() or "unknown"
3293:         known_payload = _load_known_issues("erp_type", erp_type)
3294:     if known_payload is None and upload_row and upload_row.get("sha256"):
3295:         known_payload = _load_known_issues("sha256", str(upload_row.get("sha256")))
3296:     known_results = _build_known_issue_results(report, known_payload)
3297:     report_errors = []
3298:     for plugin_id, plugin in report.get("plugins", {}).items():
3299:         if plugin.get("error"):
3300:             report_errors.append(
3301:                 {
3302:                     "plugin_id": plugin_id,
3303:                     "message": plugin["error"].get("message"),
3304:                     "type": plugin["error"].get("type"),
3305:                 }
3306:             )
3307:     return TEMPLATES.TemplateResponse(
3308:         "report.html",
3309:         {
3310:             "request": request,
3311:             "run_id": run_id,
3312:             "report": report,
3313:             "project": project,
3314:             "insights": insights,
3315:             "run_label": run_label,
3316:             "evaluation": evaluation,
3317:             "known_results": known_results,
3318:             "plugin_labels": plugin_labels,
3319:             "plugin_descriptions": plugin_descriptions,
3320:             "report_errors": report_errors,
3321:         },
3322:     )
3323: 
3324: 
3325: @app.get("/runs/{run_id}/evaluate", response_class=HTMLResponse)
3326: async def run_evaluate_view(
3327:     request: Request, run_id: str, template: int = 0, known: int = 0
3328: ) -> HTMLResponse:
3329:     run_row = pipeline.storage.fetch_run(run_id)
3330:     if not run_row:
3331:         raise HTTPException(status_code=404, detail="Run not found")
3332:     run_dir = pipeline.base_dir / "runs" / run_id
3333:     report_path = run_dir / "report.json"
3334:     if report_path.exists():
3335:         report = json.loads(report_path.read_text(encoding="utf-8"))
3336:     else:
3337:         report = build_report(
3338:             pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
3339:         )
3340:         write_report(report, run_dir)
3341:     ground_truth = ""
3342:     ground_truth_source = ""
3343:     upload_row = (
3344:         pipeline.storage.fetch_upload(run_row["upload_id"])
3345:         if run_row.get("upload_id")
3346:         else None
3347:     )
3348:     project_row = None
3349:     if run_row.get("dataset_version_id"):
3350:         ctx = pipeline.storage.get_dataset_version_context(run_row["dataset_version_id"])
3351:         if ctx and ctx.get("project_id"):
3352:             project_row = pipeline.storage.fetch_project(ctx["project_id"])
3353:     mode = "template" if template else "known" if known else "auto"
3354:     ground_truth, ground_truth_source = _resolve_ground_truth(
3355:         mode, report, upload_row, project_row, None
3356:     )
3357:     if not ground_truth and (known or template):
3358:         ground_truth = _ground_truth_template(report)
3359:         ground_truth_source = "template"
3360:     last_eval = _load_evaluation_result(run_dir) or {}
3361:     result = last_eval.get("result")
3362:     messages = last_eval.get("messages") if isinstance(last_eval.get("messages"), list) else []
3363:     evaluated_at = last_eval.get("evaluated_at") or ""
3364:     return TEMPLATES.TemplateResponse(
3365:         "evaluate.html",
3366:         {
3367:             "request": request,
3368:             "run_id": run_id,
3369:             "run_status": run_row.get("status") or "",
3370:             "ground_truth": ground_truth,
3371:             "ground_truth_source": ground_truth_source,
3372:             "upload_id": run_row.get("upload_id") or "",
3373:             "result": result,
3374:             "messages": messages,
3375:             "evaluated_at": evaluated_at,
3376:         },
3377:     )
3378: 
3379: 
3380: @app.post("/runs/{run_id}/evaluate", response_class=HTMLResponse)
3381: async def run_evaluate(
3382:     request: Request, run_id: str, ground_truth: str = Form(...)
3383: ) -> HTMLResponse:
3384:     run_row = pipeline.storage.fetch_run(run_id)
3385:     if not run_row:
3386:         raise HTTPException(status_code=404, detail="Run not found")
3387:     run_dir = pipeline.base_dir / "runs" / run_id
3388:     report_path = run_dir / "report.json"
3389:     if report_path.exists():
3390:         report = json.loads(report_path.read_text(encoding="utf-8"))
3391:     else:
3392:         report = build_report(
3393:             pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
3394:         )
3395:         write_report(report, run_dir)
3396:     gt_path = run_dir / "ground_truth.yaml"
3397:     gt_path.write_text(ground_truth, encoding="utf-8")
3398:     ok, messages = evaluate_report(report_path, gt_path)
3399:     eval_payload = _write_evaluation(run_dir, ok, messages)
3400:     result = eval_payload["result"]
3401:     return TEMPLATES.TemplateResponse(
3402:         "evaluate.html",
3403:         {
3404:             "request": request,
3405:             "run_id": run_id,
3406:             "run_status": run_row.get("status") or "",
3407:             "ground_truth": ground_truth,
3408:             "ground_truth_source": "manual",
3409:             "upload_id": run_row.get("upload_id") or "",
3410:             "result": result,
3411:             "messages": messages,
3412:             "evaluated_at": eval_payload["evaluated_at"],
3413:         },
3414:     )
3415: 
3416: 
3417: @app.post("/api/runs/{run_id}/evaluate")
3418: async def api_run_evaluate(run_id: str, payload: dict) -> JSONResponse:
3419:     run_row = pipeline.storage.fetch_run(run_id)
3420:     if not run_row:
3421:         raise HTTPException(status_code=404, detail="Run not found")
3422:     run_dir = APPDATA_DIR / "runs" / run_id
3423:     report_path = run_dir / "report.json"
3424:     if report_path.exists():
3425:         report = json.loads(report_path.read_text(encoding="utf-8"))
3426:     else:
3427:         report = build_report(
3428:             pipeline.storage, run_id, run_dir, Path("docs/report.schema.json")
3429:         )
3430:         write_report(report, run_dir)
3431: 
3432:     upload_row = (
3433:         pipeline.storage.fetch_upload(run_row["upload_id"])
3434:         if run_row.get("upload_id")
3435:         else None
3436:     )
3437:     project_row = None
3438:     if run_row.get("dataset_version_id"):
3439:         ctx = pipeline.storage.get_dataset_version_context(run_row["dataset_version_id"])
3440:         if ctx and ctx.get("project_id"):
3441:             project_row = pipeline.storage.fetch_project(ctx["project_id"])
3442:     mode = str(payload.get("mode") or "auto")
3443:     provided = payload.get("ground_truth")
3444:     if provided is not None and not isinstance(provided, str):
3445:         provided = None
3446:     ground_truth, source = _resolve_ground_truth(
3447:         mode, report, upload_row, project_row, provided
3448:     )
3449:     gt_path = run_dir / "ground_truth.yaml"
3450:     gt_path.write_text(ground_truth or "", encoding="utf-8")
3451:     ok, messages = evaluate_report(report_path, gt_path)
3452:     eval_payload = _write_evaluation(run_dir, ok, messages)
3453:     eval_payload["ground_truth_source"] = source
3454:     return JSONResponse(eval_payload)
3455: 
3456: 
3457: @app.get("/api/runs/{run_id}/report.json")
3458: async def get_report_json(run_id: str) -> FileResponse:
3459:     report_path = APPDATA_DIR / "runs" / run_id / "report.json"
3460:     if not report_path.exists():
3461:         raise HTTPException(status_code=404, detail="Report not found")
3462:     return FileResponse(report_path)
3463: 
3464: 
3465: @app.get("/api/runs/{run_id}/evaluation")
3466: async def get_evaluation(run_id: str) -> JSONResponse:
3467:     eval_path = APPDATA_DIR / "runs" / run_id / "evaluation.json"
3468:     if not eval_path.exists():
3469:         raise HTTPException(status_code=404, detail="Evaluation not found")
3470:     try:
3471:         payload = json.loads(eval_path.read_text(encoding="utf-8"))
3472:     except json.JSONDecodeError as exc:
3473:         raise HTTPException(status_code=400, detail=str(exc)) from exc
3474:     if not isinstance(payload, dict):
3475:         raise HTTPException(status_code=400, detail="Invalid evaluation payload")
3476:     return JSONResponse(payload)
3477: 
3478: 
3479: @app.get("/api/runs/{run_id}/report.md")
3480: async def get_report_md(run_id: str) -> FileResponse:
3481:     report_path = APPDATA_DIR / "runs" / run_id / "report.md"
3482:     if not report_path.exists():
3483:         raise HTTPException(status_code=404, detail="Report not found")
3484:     return FileResponse(report_path)
3485: 
3486: 
3487: @app.get("/api/runs/{run_id}")
3488: async def get_run_status(run_id: str) -> JSONResponse:
3489:     run_row = pipeline.storage.fetch_run(run_id)
3490:     if not run_row:
3491:         raise HTTPException(status_code=404, detail="Run not found")
3492:     return JSONResponse({"run": run_row})
3493: 
3494: 
3495: @app.get("/api/runs/{run_id}/artifacts/{plugin_id}/{artifact_path:path}")
3496: async def get_artifact(run_id: str, plugin_id: str, artifact_path: str) -> FileResponse:
3497:     base = APPDATA_DIR / "runs" / run_id / "artifacts" / plugin_id
3498:     try:
3499:         resolved = safe_join(base, artifact_path)
3500:     except ValueError as exc:
3501:         raise HTTPException(status_code=400, detail=str(exc)) from exc
3502:     if not resolved.exists():
3503:         raise HTTPException(status_code=404, detail="Artifact not found")
3504:     return FileResponse(resolved)
````

## File: .gitignore
````
 1: .venv/
 2: .venv_wsl/
 3: __pycache__/
 4: *.pyc
 5: appdata/
 6: .dist/
 7: .pytest_cache/
 8: .mypy_cache/
 9: .ruff_cache/
10: .pip-tmp/
11: temp/
12: *.egg-info/
````
